# THIS IS AN AUTOGENERATED FILE. DO NOT EDIT THIS FILE DIRECTLY.

./.cdk/cdk.out/asset.0abb85d9ad4ef3caf6ea4175306e904938c09de879460bc97b8f074b3e60e4a1/index.js.map:4: TODO: remove\n\n\n  get _type() {\n    return this.type;\n  }\n\n  _typeCheck(_value) {\n    return true;\n  }\n\n  clone(spec) {\n    if (this._mutate) {\n      if (spec) Object.assign(this.spec, spec);\n      return this;\n    } // if the nested value is a schema we can skip cloning, since\n    // they are already immutable\n\n\n    const next = Object.create(Object.getPrototypeOf(this)); // @ts-expect-error this is readonly\n\n    next.type = this.type;\n    next._typeError = this._typeError;\n    next._whitelistError = this._whitelistError;\n    next._blacklistError = this._blacklistError;\n    next._whitelist = this._whitelist.clone();\n    next._blacklist = this._blacklist.clone();\n    next.exclusiveTests = _extends({}, this.exclusiveTests); // @ts-expect-error this is readonly\n\n    next.deps = [...this.deps];\n    next.conditions = [...this.conditions];\n    next.tests = [...this.tests];\n    next.transforms = [...this.transforms];\n    next.spec = (0, _nanoclone.default)(_extends({}, this.spec, spec));\n    return next;\n  }\n\n  label(label) {\n    let next = this.clone();\n    next.spec.label = label;\n    return next;\n  }\n\n  meta(...args) {\n    if (args.length === 0) return this.spec.meta;\n    let next = this.clone();\n    next.spec.meta = Object.assign(next.spec.meta || {}, args[0]);\n    return next;\n  } // withContext<TContext extends AnyObject>(): BaseSchema<\n  //   TCast,\n  //   TContext,\n  //   TOutput\n  // > {\n  //   return this as any;\n  // }\n\n\n  withMutation(fn) {\n    let before = this._mutate;\n    this._mutate = true;\n    let result = fn(this);\n    this._mutate = before;\n    return result;\n  }\n\n  concat(schema) {\n    if (!schema || schema === this) return this;\n    if (schema.type !== this.type && this.type !== 'mixed') throw new TypeError(`You cannot \\`concat()\\` schema's of different types: ${this.type} and ${schema.type}`);\n    let base = this;\n    let combined = schema.clone();\n\n    const mergedSpec = _extends({}, base.spec, combined.spec); // if (combined.spec.nullable === UNSET)\n    //   mergedSpec.nullable = base.spec.nullable;\n    // if (combined.spec.presence === UNSET)\n    //   mergedSpec.presence = base.spec.presence;\n\n\n    combined.spec = mergedSpec;\n    combined._typeError || (combined._typeError = base._typeError);\n    combined._whitelistError || (combined._whitelistError = base._whitelistError);\n    combined._blacklistError || (combined._blacklistError = base._blacklistError); // manually merge the blacklist/whitelist (the other `schema` takes\n    // precedence in case of conflicts)\n\n    combined._whitelist = base._whitelist.merge(schema._whitelist, schema._blacklist);\n    combined._blacklist = base._blacklist.merge(schema._blacklist, schema._whitelist); // start with the current tests\n\n    combined.tests = base.tests;\n    combined.exclusiveTests = base.exclusiveTests; // manually add the new tests to ensure\n    // the deduping logic is consistent\n\n    combined.withMutation(next => {\n      schema.tests.forEach(fn => {\n        next.test(fn.OPTIONS);\n      });\n    });\n    combined.transforms = [...base.transforms, ...combined.transforms];\n    return combined;\n  }\n\n  isType(v) {\n    if (this.spec.nullable && v === null) return true;\n    return this._typeCheck(v);\n  }\n\n  resolve(options) {\n    let schema = this;\n\n    if (schema.conditions.length) {\n      let conditions = schema.conditions;\n      schema = schema.clone();\n      schema.conditions = [];\n      schema = conditions.reduce((schema, condition) => condition.resolve(schema, options), schema);\n      schema = schema.resolve(options);\n    }\n\n    return schema;\n  }\n  /**\n   *\n   * @param {*} value\n   * @param {Object} options\n   * @param {*=} options.parent\n   * @param {*=} options.context\n   */\n\n\n  cast(value, options = {}) {\n    let resolvedSchema = this.resolve(_extends({\n      value\n    }, options));\n\n    let result = resolvedSchema._cast(value, options);\n\n    if (value !== undefined && options.assert !== false && resolvedSchema.isType(result) !== true) {\n      let formattedValue = (0, _printValue.default)(value);\n      let formattedResult = (0, _printValue.default)(result);\n      throw new TypeError(`The value of ${options.path || 'field'} could not be cast to a value ` + `that satisfies the schema type: \"${resolvedSchema._type}\". \\n\\n` + `attempted value: ${formattedValue} \\n` + (formattedResult !== formattedValue ? `result of cast: ${formattedResult}` : ''));\n    }\n\n    return result;\n  }\n\n  _cast(rawValue, _options) {\n    let value = rawValue === undefined ? rawValue : this.transforms.reduce((value, fn) => fn.call(this, value, rawValue, this), rawValue);\n\n    if (value === undefined) {\n      value = this.getDefault();\n    }\n\n    return value;\n  }\n\n  _validate(_value, options = {}, cb) {\n    let {\n      sync,\n      path,\n      from = [],\n      originalValue = _value,\n      strict = this.spec.strict,\n      abortEarly = this.spec.abortEarly\n    } = options;\n    let value = _value;\n\n    if (!strict) {\n      // this._validating = true;\n      value = this._cast(value, _extends({\n        assert: false\n      }, options)); // this._validating = false;\n    } // value is cast, we can check if it meets type requirements\n\n\n    let args = {\n      value,\n      path,\n      options,\n      originalValue,\n      schema: this,\n      label: this.spec.label,\n      sync,\n      from\n    };\n    let initialTests = [];\n    if (this._typeError) initialTests.push(this._typeError);\n    let finalTests = [];\n    if (this._whitelistError) finalTests.push(this._whitelistError);\n    if (this._blacklistError) finalTests.push(this._blacklistError);\n    (0, _runTests.default)({\n      args,\n      value,\n      path,\n      sync,\n      tests: initialTests,\n      endEarly: abortEarly\n    }, err => {\n      if (err) return void cb(err, value);\n      (0, _runTests.default)({\n        tests: this.tests.concat(finalTests),\n        args,\n        path,\n        sync,\n        value,\n        endEarly: abortEarly\n      }, cb);\n    });\n  }\n\n  validate(value, options, maybeCb) {\n    let schema = this.resolve(_extends({}, options, {\n      value\n    })); // callback case is for nested validations\n\n    return typeof maybeCb === 'function' ? schema._validate(value, options, maybeCb) : new Promise((resolve, reject) => schema._validate(value, options, (err, value) => {\n      if (err) reject(err);else resolve(value);\n    }));\n  }\n\n  validateSync(value, options) {\n    let schema = this.resolve(_extends({}, options, {\n      value\n    }));\n    let result;\n\n    schema._validate(value, _extends({}, options, {\n      sync: true\n    }), (err, value) => {\n      if (err) throw err;\n      result = value;\n    });\n\n    return result;\n  }\n\n  isValid(value, options) {\n    return this.validate(value, options).then(() => true, err => {\n      if (_ValidationError.default.isError(err)) return false;\n      throw err;\n    });\n  }\n\n  isValidSync(value, options) {\n    try {\n      this.validateSync(value, options);\n      return true;\n    } catch (err) {\n      if (_ValidationError.default.isError(err)) return false;\n      throw err;\n    }\n  }\n\n  _getDefault() {\n    let defaultValue = this.spec.default;\n\n    if (defaultValue == null) {\n      return defaultValue;\n    }\n\n    return typeof defaultValue === 'function' ? defaultValue.call(this) : (0, _nanoclone.default)(defaultValue);\n  }\n\n  getDefault(options) {\n    let schema = this.resolve(options || {});\n    return schema._getDefault();\n  }\n\n  default(def) {\n    if (arguments.length === 0) {\n      return this._getDefault();\n    }\n\n    let next = this.clone({\n      default: def\n    });\n    return next;\n  }\n\n  strict(isStrict = true) {\n    let next = this.clone();\n    next.spec.strict = isStrict;\n    return next;\n  }\n\n  _isPresent(value) {\n    return value != null;\n  }\n\n  defined(message = _locale.mixed.defined) {\n    return this.test({\n      message,\n      name: 'defined',\n      exclusive: true,\n\n      test(value) {\n        return value !== undefined;\n      }\n\n    });\n  }\n\n  required(message = _locale.mixed.required) {\n    return this.clone({\n      presence: 'required'\n    }).withMutation(s => s.test({\n      message,\n      name: 'required',\n      exclusive: true,\n\n      test(value) {\n        return this.schema._isPresent(value);\n      }\n\n    }));\n  }\n\n  notRequired() {\n    let next = this.clone({\n      presence: 'optional'\n    });\n    next.tests = next.tests.filter(test => test.OPTIONS.name !== 'required');\n    return next;\n  }\n\n  nullable(isNullable = true) {\n    let next = this.clone({\n      nullable: isNullable !== false\n    });\n    return next;\n  }\n\n  transform(fn) {\n    let next = this.clone();\n    next.transforms.push(fn);\n    return next;\n  }\n  /**\n   * Adds a test function to the schema's queue of tests.\n   * tests can be exclusive or non-exclusive.\n   *\n   * - exclusive tests, will replace any existing tests of the same name.\n   * - non-exclusive: can be stacked\n   *\n   * If a non-exclusive test is added to a schema with an exclusive test of the same name\n   * the exclusive test is removed and further tests of the same name will be stacked.\n   *\n   * If an exclusive test is added to a schema with non-exclusive tests of the same name\n   * the previous tests are removed and further tests of the same name will replace each other.\n   */\n\n\n  test(...args) {\n    let opts;\n\n    if (args.length === 1) {\n      if (typeof args[0] === 'function') {\n        opts = {\n          test: args[0]\n        };\n      } else {\n        opts = args[0];\n      }\n    } else if (args.length === 2) {\n      opts = {\n        name: args[0],\n        test: args[1]\n      };\n    } else {\n      opts = {\n        name: args[0],\n        message: args[1],\n        test: args[2]\n      };\n    }\n\n    if (opts.message === undefined) opts.message = _locale.mixed.default;\n    if (typeof opts.test !== 'function') throw new TypeError('`test` is a required parameters');\n    let next = this.clone();\n    let validate = (0, _createValidation.default)(opts);\n    let isExclusive = opts.exclusive || opts.name && next.exclusiveTests[opts.name] === true;\n\n    if (opts.exclusive) {\n      if (!opts.name) throw new TypeError('Exclusive tests must provide a unique `name` identifying the test');\n    }\n\n    if (opts.name) next.exclusiveTests[opts.name] = !!opts.exclusive;\n    next.tests = next.tests.filter(fn => {\n      if (fn.OPTIONS.name === opts.name) {\n        if (isExclusive) return false;\n        if (fn.OPTIONS.test === validate.OPTIONS.test) return false;\n      }\n\n      return true;\n    });\n    next.tests.push(validate);\n    return next;\n  }\n\n  when(keys, options) {\n    if (!Array.isArray(keys) && typeof keys !== 'string') {\n      options = keys;\n      keys = '.';\n    }\n\n    let next = this.clone();\n    let deps = (0, _toArray.default)(keys).map(key => new _Reference.default(key));\n    deps.forEach(dep => {\n      // @ts-ignore\n      if (dep.isSibling) next.deps.push(dep.key);\n    });\n    next.conditions.push(new _Condition.default(deps, options));\n    return next;\n  }\n\n  typeError(message) {\n    let next = this.clone();\n    next._typeError = (0, _createValidation.default)({\n      message,\n      name: 'typeError',\n\n      test(value) {\n        if (value !== undefined && !this.schema.isType(value)) return this.createError({\n          params: {\n            type: this.schema._type\n          }\n        });\n        return true;\n      }\n\n    });\n    return next;\n  }\n\n  oneOf(enums, message = _locale.mixed.oneOf) {\n    let next = this.clone();\n    enums.forEach(val => {\n      next._whitelist.add(val);\n\n      next._blacklist.delete(val);\n    });\n    next._whitelistError = (0, _createValidation.default)({\n      message,\n      name: 'oneOf',\n\n      test(value) {\n        if (value === undefined) return true;\n        let valids = this.schema._whitelist;\n        let resolved = valids.resolveAll(this.resolve);\n        return resolved.includes(value) ? true : this.createError({\n          params: {\n            values: valids.toArray().join(', '),\n            resolved\n          }\n        });\n      }\n\n    });\n    return next;\n  }\n\n  notOneOf(enums, message = _locale.mixed.notOneOf) {\n    let next = this.clone();\n    enums.forEach(val => {\n      next._blacklist.add(val);\n\n      next._whitelist.delete(val);\n    });\n    next._blacklistError = (0, _createValidation.default)({\n      message,\n      name: 'notOneOf',\n\n      test(value) {\n        let invalids = this.schema._blacklist;\n        let resolved = invalids.resolveAll(this.resolve);\n        if (resolved.includes(value)) return this.createError({\n          params: {\n            values: invalids.toArray().join(', '),\n            resolved\n          }\n        });\n        return true;\n      }\n\n    });\n    return next;\n  }\n\n  strip(strip = true) {\n    let next = this.clone();\n    next.spec.strip = strip;\n    return next;\n  }\n\n  describe() {\n    const next = this.clone();\n    const {\n      label,\n      meta\n    } = next.spec;\n    const description = {\n      meta,\n      label,\n      type: next.type,\n      oneOf: next._whitelist.describe(),\n      notOneOf: next._blacklist.describe(),\n      tests: next.tests.map(fn => ({\n        name: fn.OPTIONS.name,\n        params: fn.OPTIONS.params\n      })).filter((n, idx, list) => list.findIndex(c => c.name === n.name) === idx)\n    };\n    return description;\n  }\n\n} // eslint-disable-next-line @typescript-eslint/no-unused-vars\n\n\nexports.default = BaseSchema;\n// @ts-expect-error\nBaseSchema.prototype.__isYupSchema__ = true;\n\nfor (const method of ['validate', 'validateSync']) BaseSchema.prototype[`${method}At`] = function (path, value, options = {}) {\n  const {\n    parent,\n    parentPath,\n    schema\n  } = (0, _reach.getIn)(this, path, value, options.context);\n  return schema[method](parent && parent[parentPath], _extends({}, options, {\n    parent,\n    path\n  }));\n};\n\nfor (const alias of ['equals', 'is']) BaseSchema.prototype[alias] = BaseSchema.prototype.oneOf;\n\nfor (const alias of ['not', 'nope']) BaseSchema.prototype[alias] = BaseSchema.prototype.notOneOf;\n\nBaseSchema.prototype.optional = BaseSchema.prototype.notRequired;", "\"use strict\";\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\nexports.create = create;\nexports.default = void 0;\n\nvar _schema = _interopRequireDefault(require(\"./schema\"));\n\nfunction _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { default: obj }; }\n\nconst Mixed = _schema.default;\nvar _default = Mixed;\nexports.default = _default;\n\nfunction create() {\n  return new Mixed();\n} // XXX: this is using the Base schema so that `addMethod(mixed)` works as a base class\n\n\ncreate.prototype = Mixed.prototype;", "\"use strict\";\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\nexports.default = void 0;\n\nconst isAbsent = value => value == null;\n\nvar _default = isAbsent;\nexports.default = _default;", "\"use strict\";\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\nexports.create = create;\nexports.default = void 0;\n\nvar _schema = _interopRequireDefault(require(\"./schema\"));\n\nvar _locale = require(\"./locale\");\n\nvar _isAbsent = _interopRequireDefault(require(\"./util/isAbsent\"));\n\nfunction _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { default: obj }; }\n\nfunction create() {\n  return new BooleanSchema();\n}\n\nclass BooleanSchema extends _schema.default {\n  constructor() {\n    super({\n      type: 'boolean'\n    });\n    this.withMutation(() => {\n      this.transform(function (value) {\n        if (!this.isType(value)) {\n          if (/^(true|1)$/i.test(String(value))) return true;\n          if (/^(false|0)$/i.test(String(value))) return false;\n        }\n\n        return value;\n      });\n    });\n  }\n\n  _typeCheck(v) {\n    if (v instanceof Boolean) v = v.valueOf();\n    return typeof v === 'boolean';\n  }\n\n  isTrue(message = _locale.boolean.isValue) {\n    return this.test({\n      message,\n      name: 'is-value',\n      exclusive: true,\n      params: {\n        value: 'true'\n      },\n\n      test(value) {\n        return (0, _isAbsent.default)(value) || value === true;\n      }\n\n    });\n  }\n\n  isFalse(message = _locale.boolean.isValue) {\n    return this.test({\n      message,\n      name: 'is-value',\n      exclusive: true,\n      params: {\n        value: 'false'\n      },\n\n      test(value) {\n        return (0, _isAbsent.default)(value) || value === false;\n      }\n\n    });\n  }\n\n}\n\nexports.default = BooleanSchema;\ncreate.prototype = BooleanSchema.prototype;", "\"use strict\";\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\nexports.create = create;\nexports.default = void 0;\n\nvar _locale = require(\"./locale\");\n\nvar _isAbsent = _interopRequireDefault(require(\"./util/isAbsent\"));\n\nvar _schema = _interopRequireDefault(require(\"./schema\"));\n\nfunction _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { default: obj }; }\n\n// eslint-disable-next-line\nlet rEmail = /^((([a-z]|\\d|[!#\\$%&'\\*\\+\\-\\/=\\?\\^_`{\\|}~]|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])+(\\.([a-z]|\\d|[!#\\$%&'\\*\\+\\-\\/=\\?\\^_`{\\|}~]|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])+)*)|((\\x22)((((\\x20|\\x09)*(\\x0d\\x0a))?(\\x20|\\x09)+)?(([\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x7f]|\\x21|[\\x23-\\x5b]|[\\x5d-\\x7e]|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])|(\\\\([\\x01-\\x09\\x0b\\x0c\\x0d-\\x7f]|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF]))))*(((\\x20|\\x09)*(\\x0d\\x0a))?(\\x20|\\x09)+)?(\\x22)))@((([a-z]|\\d|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])|(([a-z]|\\d|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])([a-z]|\\d|-|\\.|_|~|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])*([a-z]|\\d|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])))\\.)+(([a-z]|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])|(([a-z]|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])([a-z]|\\d|-|\\.|_|~|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])*([a-z]|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])))$/i; // eslint-disable-next-line\n\nlet rUrl = /^((https?|ftp):)?\\/\\/(((([a-z]|\\d|-|\\.|_|~|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])|(%[\\da-f]{2})|[!\\$&'\\(\\)\\*\\+,;=]|:)*@)?(((\\d|[1-9]\\d|1\\d\\d|2[0-4]\\d|25[0-5])\\.(\\d|[1-9]\\d|1\\d\\d|2[0-4]\\d|25[0-5])\\.(\\d|[1-9]\\d|1\\d\\d|2[0-4]\\d|25[0-5])\\.(\\d|[1-9]\\d|1\\d\\d|2[0-4]\\d|25[0-5]))|((([a-z]|\\d|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])|(([a-z]|\\d|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])([a-z]|\\d|-|\\.|_|~|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])*([a-z]|\\d|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])))\\.)+(([a-z]|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])|(([a-z]|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])([a-z]|\\d|-|\\.|_|~|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])*([a-z]|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])))\\.?)(:\\d*)?)(\\/((([a-z]|\\d|-|\\.|_|~|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])|(%[\\da-f]{2})|[!\\$&'\\(\\)\\*\\+,;=]|:|@)+(\\/(([a-z]|\\d|-|\\.|_|~|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])|(%[\\da-f]{2})|[!\\$&'\\(\\)\\*\\+,;=]|:|@)*)*)?)?(\\?((([a-z]|\\d|-|\\.|_|~|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])|(%[\\da-f]{2})|[!\\$&'\\(\\)\\*\\+,;=]|:|@)|[\\uE000-\\uF8FF]|\\/|\\?)*)?(\\#((([a-z]|\\d|-|\\.|_|~|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])|(%[\\da-f]{2})|[!\\$&'\\(\\)\\*\\+,;=]|:|@)|\\/|\\?)*)?$/i; // eslint-disable-next-line\n\nlet rUUID = /^(?:[0-9a-f]{8}-[0-9a-f]{4}-[1-5][0-9a-f]{3}-[89ab][0-9a-f]{3}-[0-9a-f]{12}|00000000-0000-0000-0000-000000000000)$/i;\n\nlet isTrimmed = value => (0, _isAbsent.default)(value) || value === value.trim();\n\nlet objStringTag = {}.toString();\n\nfunction create() {\n  return new StringSchema();\n}\n\nclass StringSchema extends _schema.default {\n  constructor() {\n    super({\n      type: 'string'\n    });\n    this.withMutation(() => {\n      this.transform(function (value) {\n        if (this.isType(value)) return value;\n        if (Array.isArray(value)) return value;\n        const strValue = value != null && value.toString ? value.toString() : value;\n        if (strValue === objStringTag) return value;\n        return strValue;\n      });\n    });\n  }\n\n  _typeCheck(value) {\n    if (value instanceof String) value = value.valueOf();\n    return typeof value === 'string';\n  }\n\n  _isPresent(value) {\n    return super._isPresent(value) && !!value.length;\n  }\n\n  length(length, message = _locale.string.length) {\n    return this.test({\n      message,\n      name: 'length',\n      exclusive: true,\n      params: {\n        length\n      },\n\n      test(value) {\n        return (0, _isAbsent.default)(value) || value.length === this.resolve(length);\n      }\n\n    });\n  }\n\n  min(min, message = _locale.string.min) {\n    return this.test({\n      message,\n      name: 'min',\n      exclusive: true,\n      params: {\n        min\n      },\n\n      test(value) {\n        return (0, _isAbsent.default)(value) || value.length >= this.resolve(min);\n      }\n\n    });\n  }\n\n  max(max, message = _locale.string.max) {\n    return this.test({\n      name: 'max',\n      exclusive: true,\n      message,\n      params: {\n        max\n      },\n\n      test(value) {\n        return (0, _isAbsent.default)(value) || value.length <= this.resolve(max);\n      }\n\n    });\n  }\n\n  matches(regex, options) {\n    let excludeEmptyString = false;\n    let message;\n    let name;\n\n    if (options) {\n      if (typeof options === 'object') {\n        ({\n          excludeEmptyString = false,\n          message,\n          name\n        } = options);\n      } else {\n        message = options;\n      }\n    }\n\n    return this.test({\n      name: name || 'matches',\n      message: message || _locale.string.matches,\n      params: {\n        regex\n      },\n      test: value => (0, _isAbsent.default)(value) || value === '' && excludeEmptyString || value.search(regex) !== -1\n    });\n  }\n\n  email(message = _locale.string.email) {\n    return this.matches(rEmail, {\n      name: 'email',\n      message,\n      excludeEmptyString: true\n    });\n  }\n\n  url(message = _locale.string.url) {\n    return this.matches(rUrl, {\n      name: 'url',\n      message,\n      excludeEmptyString: true\n    });\n  }\n\n  uuid(message = _locale.string.uuid) {\n    return this.matches(rUUID, {\n      name: 'uuid',\n      message,\n      excludeEmptyString: false\n    });\n  } //-- transforms --\n\n\n  ensure() {\n    return this.default('').transform(val => val === null ? '' : val);\n  }\n\n  trim(message = _locale.string.trim) {\n    return this.transform(val => val != null ? val.trim() : val).test({\n      message,\n      name: 'trim',\n      test: isTrimmed\n    });\n  }\n\n  lowercase(message = _locale.string.lowercase) {\n    return this.transform(value => !(0, _isAbsent.default)(value) ? value.toLowerCase() : value).test({\n      message,\n      name: 'string_case',\n      exclusive: true,\n      test: value => (0, _isAbsent.default)(value) || value === value.toLowerCase()\n    });\n  }\n\n  uppercase(message = _locale.string.uppercase) {\n    return this.transform(value => !(0, _isAbsent.default)(value) ? value.toUpperCase() : value).test({\n      message,\n      name: 'string_case',\n      exclusive: true,\n      test: value => (0, _isAbsent.default)(value) || value === value.toUpperCase()\n    });\n  }\n\n}\n\nexports.default = StringSchema;\ncreate.prototype = StringSchema.prototype; //\n// String Interfaces\n//", "\"use strict\";\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\nexports.create = create;\nexports.default = void 0;\n\nvar _locale = require(\"./locale\");\n\nvar _isAbsent = _interopRequireDefault(require(\"./util/isAbsent\"));\n\nvar _schema = _interopRequireDefault(require(\"./schema\"));\n\nfunction _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { default: obj }; }\n\nlet isNaN = value => value != +value;\n\nfunction create() {\n  return new NumberSchema();\n}\n\nclass NumberSchema extends _schema.default {\n  constructor() {\n    super({\n      type: 'number'\n    });\n    this.withMutation(() => {\n      this.transform(function (value) {\n        let parsed = value;\n\n        if (typeof parsed === 'string') {\n          parsed = parsed.replace(/\\s/g, '');\n          if (parsed === '') return NaN; // don't use parseFloat to avoid positives on alpha-numeric strings\n\n          parsed = +parsed;\n        }\n\n        if (this.isType(parsed)) return parsed;\n        return parseFloat(parsed);\n      });\n    });\n  }\n\n  _typeCheck(value) {\n    if (value instanceof Number) value = value.valueOf();\n    return typeof value === 'number' && !isNaN(value);\n  }\n\n  min(min, message = _locale.number.min) {\n    return this.test({\n      message,\n      name: 'min',\n      exclusive: true,\n      params: {\n        min\n      },\n\n      test(value) {\n        return (0, _isAbsent.default)(value) || value >= this.resolve(min);\n      }\n\n    });\n  }\n\n  max(max, message = _locale.number.max) {\n    return this.test({\n      message,\n      name: 'max',\n      exclusive: true,\n      params: {\n        max\n      },\n\n      test(value) {\n        return (0, _isAbsent.default)(value) || value <= this.resolve(max);\n      }\n\n    });\n  }\n\n  lessThan(less, message = _locale.number.lessThan) {\n    return this.test({\n      message,\n      name: 'max',\n      exclusive: true,\n      params: {\n        less\n      },\n\n      test(value) {\n        return (0, _isAbsent.default)(value) || value < this.resolve(less);\n      }\n\n    });\n  }\n\n  moreThan(more, message = _locale.number.moreThan) {\n    return this.test({\n      message,\n      name: 'min',\n      exclusive: true,\n      params: {\n        more\n      },\n\n      test(value) {\n        return (0, _isAbsent.default)(value) || value > this.resolve(more);\n      }\n\n    });\n  }\n\n  positive(msg = _locale.number.positive) {\n    return this.moreThan(0, msg);\n  }\n\n  negative(msg = _locale.number.negative) {\n    return this.lessThan(0, msg);\n  }\n\n  integer(message = _locale.number.integer) {\n    return this.test({\n      name: 'integer',\n      message,\n      test: val => (0, _isAbsent.default)(val) || Number.isInteger(val)\n    });\n  }\n\n  truncate() {\n    return this.transform(value => !(0, _isAbsent.default)(value) ? value | 0 : value);\n  }\n\n  round(method) {\n    var _method;\n\n    let avail = ['ceil', 'floor', 'round', 'trunc'];\n    method = ((_method = method) == null ? void 0 : _method.toLowerCase()) || 'round'; // this exists for symemtry with the new Math.trunc\n\n    if (method === 'trunc') return this.truncate();\n    if (avail.indexOf(method.toLowerCase()) === -1) throw new TypeError('Only valid options for round() are: ' + avail.join(', '));\n    return this.transform(value => !(0, _isAbsent.default)(value) ? Math[method](value) : value);\n  }\n\n}\n\nexports.default = NumberSchema;\ncreate.prototype = NumberSchema.prototype; //\n// Number Interfaces\n//", "\"use strict\";\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\nexports.default = parseIsoDate;\n\n/* eslint-disable */\n\n/**\n *\n * Date.parse with progressive enhancement for ISO 8601 <https://github.com/csnover/js-iso8601>\n * NON-CONFORMANT EDITION.\n * \u00A9 2011 Colin Snover <http://zetafleet.com>\n * Released under MIT license.\n */\n//              1 YYYY                 2 MM        3 DD              4 HH     5 mm        6 ss            7 msec         8 Z 9 \u00B1    10 tzHH    11 tzmm\nvar isoReg = /^(\\d{4}|[+\\-]\\d{6})(?:-?(\\d{2})(?:-?(\\d{2}))?)?(?:[ T]?(\\d{2}):?(\\d{2})(?::?(\\d{2})(?:[,\\.](\\d{1,}))?)?(?:(Z)|([+\\-])(\\d{2})(?::?(\\d{2}))?)?)?$/;\n\nfunction parseIsoDate(date) {\n  var numericKeys = [1, 4, 5, 6, 7, 10, 11],\n      minutesOffset = 0,\n      timestamp,\n      struct;\n\n  if (struct = isoReg.exec(date)) {\n    // avoid NaN timestamps caused by \u201Cundefined\u201D values being passed to Date.UTC\n    for (var i = 0, k; k = numericKeys[i]; ++i) struct[k] = +struct[k] || 0; // allow undefined days and months\n\n\n    struct[2] = (+struct[2] || 1) - 1;\n    struct[3] = +struct[3] || 1; // allow arbitrary sub-second precision beyond milliseconds\n\n    struct[7] = struct[7] ? String(struct[7]).substr(0, 3) : 0; // timestamps without timezone identifiers should be considered local time\n\n    if ((struct[8] === undefined || struct[8] === '') && (struct[9] === undefined || struct[9] === '')) timestamp = +new Date(struct[1], struct[2], struct[3], struct[4], struct[5], struct[6], struct[7]);else {\n      if (struct[8] !== 'Z' && struct[9] !== undefined) {\n        minutesOffset = struct[10] * 60 + struct[11];\n        if (struct[9] === '+') minutesOffset = 0 - minutesOffset;\n      }\n\n      timestamp = Date.UTC(struct[1], struct[2], struct[3], struct[4], struct[5] + minutesOffset, struct[6], struct[7]);\n    }\n  } else timestamp = Date.parse ? Date.parse(date) : NaN;\n\n  return timestamp;\n}", "\"use strict\";\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\nexports.create = create;\nexports.default = void 0;\n\nvar _isodate = _interopRequireDefault(require(\"./util/isodate\"));\n\nvar _locale = require(\"./locale\");\n\nvar _isAbsent = _interopRequireDefault(require(\"./util/isAbsent\"));\n\nvar _Reference = _interopRequireDefault(require(\"./Reference\"));\n\nvar _schema = _interopRequireDefault(require(\"./schema\"));\n\nfunction _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { default: obj }; }\n\n// @ts-ignore\nlet invalidDate = new Date('');\n\nlet isDate = obj => Object.prototype.toString.call(obj) === '[object Date]';\n\nfunction create() {\n  return new DateSchema();\n}\n\nclass DateSchema extends _schema.default {\n  constructor() {\n    super({\n      type: 'date'\n    });\n    this.withMutation(() => {\n      this.transform(function (value) {\n        if (this.isType(value)) return value;\n        value = (0, _isodate.default)(value); // 0 is a valid timestamp equivalent to 1970-01-01T00:00:00Z(unix epoch) or before.\n\n        return !isNaN(value) ? new Date(value) : invalidDate;\n      });\n    });\n  }\n\n  _typeCheck(v) {\n    return isDate(v) && !isNaN(v.getTime());\n  }\n\n  prepareParam(ref, name) {\n    let param;\n\n    if (!_Reference.default.isRef(ref)) {\n      let cast = this.cast(ref);\n      if (!this._typeCheck(cast)) throw new TypeError(`\\`${name}\\` must be a Date or a value that can be \\`cast()\\` to a Date`);\n      param = cast;\n    } else {\n      param = ref;\n    }\n\n    return param;\n  }\n\n  min(min, message = _locale.date.min) {\n    let limit = this.prepareParam(min, 'min');\n    return this.test({\n      message,\n      name: 'min',\n      exclusive: true,\n      params: {\n        min\n      },\n\n      test(value) {\n        return (0, _isAbsent.default)(value) || value >= this.resolve(limit);\n      }\n\n    });\n  }\n\n  max(max, message = _locale.date.max) {\n    let limit = this.prepareParam(max, 'max');\n    return this.test({\n      message,\n      name: 'max',\n      exclusive: true,\n      params: {\n        max\n      },\n\n      test(value) {\n        return (0, _isAbsent.default)(value) || value <= this.resolve(limit);\n      }\n\n    });\n  }\n\n}\n\nexports.default = DateSchema;\nDateSchema.INVALID_DATE = invalidDate;\ncreate.prototype = DateSchema.prototype;\ncreate.INVALID_DATE = invalidDate;", "/**\n * A specialized version of `_.reduce` for arrays without support for\n * iteratee shorthands.\n *\n * @private\n * @param {Array} [array] The array to iterate over.\n * @param {Function} iteratee The function invoked per iteration.\n * @param {*} [accumulator] The initial value.\n * @param {boolean} [initAccum] Specify using the first element of `array` as\n *  the initial value.\n * @returns {*} Returns the accumulated value.\n */\nfunction arrayReduce(array, iteratee, accumulator, initAccum) {\n  var index = -1,\n      length = array == null ? 0 : array.length;\n\n  if (initAccum && length) {\n    accumulator = array[++index];\n  }\n  while (++index < length) {\n    accumulator = iteratee(accumulator, array[index], index, array);\n  }\n  return accumulator;\n}\n\nmodule.exports = arrayReduce;\n", "/**\n * The base implementation of `_.propertyOf` without support for deep paths.\n *\n * @private\n * @param {Object} object The object to query.\n * @returns {Function} Returns the new accessor function.\n */\nfunction basePropertyOf(object) {\n  return function(key) {\n    return object == null ? undefined : object[key];\n  };\n}\n\nmodule.exports = basePropertyOf;\n", "var basePropertyOf = require('./_basePropertyOf');\n\n/** Used to map Latin Unicode letters to basic Latin letters. */\nvar deburredLetters = {\n  // Latin-1 Supplement block.\n  '\\xc0': 'A',  '\\xc1': 'A', '\\xc2': 'A', '\\xc3': 'A', '\\xc4': 'A', '\\xc5': 'A',\n  '\\xe0': 'a',  '\\xe1': 'a', '\\xe2': 'a', '\\xe3': 'a', '\\xe4': 'a', '\\xe5': 'a',\n  '\\xc7': 'C',  '\\xe7': 'c',\n  '\\xd0': 'D',  '\\xf0': 'd',\n  '\\xc8': 'E',  '\\xc9': 'E', '\\xca': 'E', '\\xcb': 'E',\n  '\\xe8': 'e',  '\\xe9': 'e', '\\xea': 'e', '\\xeb': 'e',\n  '\\xcc': 'I',  '\\xcd': 'I', '\\xce': 'I', '\\xcf': 'I',\n  '\\xec': 'i',  '\\xed': 'i', '\\xee': 'i', '\\xef': 'i',\n  '\\xd1': 'N',  '\\xf1': 'n',\n  '\\xd2': 'O',  '\\xd3': 'O', '\\xd4': 'O', '\\xd5': 'O', '\\xd6': 'O', '\\xd8': 'O',\n  '\\xf2': 'o',  '\\xf3': 'o', '\\xf4': 'o', '\\xf5': 'o', '\\xf6': 'o', '\\xf8': 'o',\n  '\\xd9': 'U',  '\\xda': 'U', '\\xdb': 'U', '\\xdc': 'U',\n  '\\xf9': 'u',  '\\xfa': 'u', '\\xfb': 'u', '\\xfc': 'u',\n  '\\xdd': 'Y',  '\\xfd': 'y', '\\xff': 'y',\n  '\\xc6': 'Ae', '\\xe6': 'ae',\n  '\\xde': 'Th', '\\xfe': 'th',\n  '\\xdf': 'ss',\n  // Latin Extended-A block.\n  '\\u0100': 'A',  '\\u0102': 'A', '\\u0104': 'A',\n  '\\u0101': 'a',  '\\u0103': 'a', '\\u0105': 'a',\n  '\\u0106': 'C',  '\\u0108': 'C', '\\u010a': 'C', '\\u010c': 'C',\n  '\\u0107': 'c',  '\\u0109': 'c', '\\u010b': 'c', '\\u010d': 'c',\n  '\\u010e': 'D',  '\\u0110': 'D', '\\u010f': 'd', '\\u0111': 'd',\n  '\\u0112': 'E',  '\\u0114': 'E', '\\u0116': 'E', '\\u0118': 'E', '\\u011a': 'E',\n  '\\u0113': 'e',  '\\u0115': 'e', '\\u0117': 'e', '\\u0119': 'e', '\\u011b': 'e',\n  '\\u011c': 'G',  '\\u011e': 'G', '\\u0120': 'G', '\\u0122': 'G',\n  '\\u011d': 'g',  '\\u011f': 'g', '\\u0121': 'g', '\\u0123': 'g',\n  '\\u0124': 'H',  '\\u0126': 'H', '\\u0125': 'h', '\\u0127': 'h',\n  '\\u0128': 'I',  '\\u012a': 'I', '\\u012c': 'I', '\\u012e': 'I', '\\u0130': 'I',\n  '\\u0129': 'i',  '\\u012b': 'i', '\\u012d': 'i', '\\u012f': 'i', '\\u0131': 'i',\n  '\\u0134': 'J',  '\\u0135': 'j',\n  '\\u0136': 'K',  '\\u0137': 'k', '\\u0138': 'k',\n  '\\u0139': 'L',  '\\u013b': 'L', '\\u013d': 'L', '\\u013f': 'L', '\\u0141': 'L',\n  '\\u013a': 'l',  '\\u013c': 'l', '\\u013e': 'l', '\\u0140': 'l', '\\u0142': 'l',\n  '\\u0143': 'N',  '\\u0145': 'N', '\\u0147': 'N', '\\u014a': 'N',\n  '\\u0144': 'n',  '\\u0146': 'n', '\\u0148': 'n', '\\u014b': 'n',\n  '\\u014c': 'O',  '\\u014e': 'O', '\\u0150': 'O',\n  '\\u014d': 'o',  '\\u014f': 'o', '\\u0151': 'o',\n  '\\u0154': 'R',  '\\u0156': 'R', '\\u0158': 'R',\n  '\\u0155': 'r',  '\\u0157': 'r', '\\u0159': 'r',\n  '\\u015a': 'S',  '\\u015c': 'S', '\\u015e': 'S', '\\u0160': 'S',\n  '\\u015b': 's',  '\\u015d': 's', '\\u015f': 's', '\\u0161': 's',\n  '\\u0162': 'T',  '\\u0164': 'T', '\\u0166': 'T',\n  '\\u0163': 't',  '\\u0165': 't', '\\u0167': 't',\n  '\\u0168': 'U',  '\\u016a': 'U', '\\u016c': 'U', '\\u016e': 'U', '\\u0170': 'U', '\\u0172': 'U',\n  '\\u0169': 'u',  '\\u016b': 'u', '\\u016d': 'u', '\\u016f': 'u', '\\u0171': 'u', '\\u0173': 'u',\n  '\\u0174': 'W',  '\\u0175': 'w',\n  '\\u0176': 'Y',  '\\u0177': 'y', '\\u0178': 'Y',\n  '\\u0179': 'Z',  '\\u017b': 'Z', '\\u017d': 'Z',\n  '\\u017a': 'z',  '\\u017c': 'z', '\\u017e': 'z',\n  '\\u0132': 'IJ', '\\u0133': 'ij',\n  '\\u0152': 'Oe', '\\u0153': 'oe',\n  '\\u0149': \"'n\", '\\u017f': 's'\n};\n\n/**\n * Used by `_.deburr` to convert Latin-1 Supplement and Latin Extended-A\n * letters to basic Latin letters.\n *\n * @private\n * @param {string} letter The matched letter to deburr.\n * @returns {string} Returns the deburred letter.\n */\nvar deburrLetter = basePropertyOf(deburredLetters);\n\nmodule.exports = deburrLetter;\n", "var deburrLetter = require('./_deburrLetter'),\n    toString = require('./toString');\n\n/** Used to match Latin Unicode letters (excluding mathematical operators). */\nvar reLatin = /[\\xc0-\\xd6\\xd8-\\xf6\\xf8-\\xff\\u0100-\\u017f]/g;\n\n/** Used to compose unicode character classes. */\nvar rsComboMarksRange = '\\\\u0300-\\\\u036f',\n    reComboHalfMarksRange = '\\\\ufe20-\\\\ufe2f',\n    rsComboSymbolsRange = '\\\\u20d0-\\\\u20ff',\n    rsComboRange = rsComboMarksRange + reComboHalfMarksRange + rsComboSymbolsRange;\n\n/** Used to compose unicode capture groups. */\nvar rsCombo = '[' + rsComboRange + ']';\n\n/**\n * Used to match [combining diacritical marks](https://en.wikipedia.org/wiki/Combining_Diacritical_Marks) and\n * [combining diacritical marks for symbols](https://en.wikipedia.org/wiki/Combining_Diacritical_Marks_for_Symbols).\n */\nvar reComboMark = RegExp(rsCombo, 'g');\n\n/**\n * Deburrs `string` by converting\n * [Latin-1 Supplement](https://en.wikipedia.org/wiki/Latin-1_Supplement_(Unicode_block)#Character_table)\n * and [Latin Extended-A](https://en.wikipedia.org/wiki/Latin_Extended-A)\n * letters to basic Latin letters and removing\n * [combining diacritical marks](https://en.wikipedia.org/wiki/Combining_Diacritical_Marks).\n *\n * @static\n * @memberOf _\n * @since 3.0.0\n * @category String\n * @param {string} [string=''] The string to deburr.\n * @returns {string} Returns the deburred string.\n * @example\n *\n * _.deburr('d\u00E9j\u00E0 vu');\n * // => 'deja vu'\n */\nfunction deburr(string) {\n  string = toString(string);\n  return string && string.replace(reLatin, deburrLetter).replace(reComboMark, '');\n}\n\nmodule.exports = deburr;\n", "/** Used to match words composed of alphanumeric characters. */\nvar reAsciiWord = /[^\\x00-\\x2f\\x3a-\\x40\\x5b-\\x60\\x7b-\\x7f]+/g;\n\n/**\n * Splits an ASCII `string` into an array of its words.\n *\n * @private\n * @param {string} The string to inspect.\n * @returns {Array} Returns the words of `string`.\n */\nfunction asciiWords(string) {\n  return string.match(reAsciiWord) || [];\n}\n\nmodule.exports = asciiWords;\n", "/** Used to detect strings that need a more robust regexp to match words. */\nvar reHasUnicodeWord = /[a-z][A-Z]|[A-Z]{2}[a-z]|[0-9][a-zA-Z]|[a-zA-Z][0-9]|[^a-zA-Z0-9 ]/;\n\n/**\n * Checks if `string` contains a word composed of Unicode symbols.\n *\n * @private\n * @param {string} string The string to inspect.\n * @returns {boolean} Returns `true` if a word is found, else `false`.\n */\nfunction hasUnicodeWord(string) {\n  return reHasUnicodeWord.test(string);\n}\n\nmodule.exports = hasUnicodeWord;\n", "/** Used to compose unicode character classes. */\nvar rsAstralRange = '\\\\ud800-\\\\udfff',\n    rsComboMarksRange = '\\\\u0300-\\\\u036f',\n    reComboHalfMarksRange = '\\\\ufe20-\\\\ufe2f',\n    rsComboSymbolsRange = '\\\\u20d0-\\\\u20ff',\n    rsComboRange = rsComboMarksRange + reComboHalfMarksRange + rsComboSymbolsRange,\n    rsDingbatRange = '\\\\u2700-\\\\u27bf',\n    rsLowerRange = 'a-z\\\\xdf-\\\\xf6\\\\xf8-\\\\xff',\n    rsMathOpRange = '\\\\xac\\\\xb1\\\\xd7\\\\xf7',\n    rsNonCharRange = '\\\\x00-\\\\x2f\\\\x3a-\\\\x40\\\\x5b-\\\\x60\\\\x7b-\\\\xbf',\n    rsPunctuationRange = '\\\\u2000-\\\\u206f',\n    rsSpaceRange = ' \\\\t\\\\x0b\\\\f\\\\xa0\\\\ufeff\\\\n\\\\r\\\\u2028\\\\u2029\\\\u1680\\\\u180e\\\\u2000\\\\u2001\\\\u2002\\\\u2003\\\\u2004\\\\u2005\\\\u2006\\\\u2007\\\\u2008\\\\u2009\\\\u200a\\\\u202f\\\\u205f\\\\u3000',\n    rsUpperRange = 'A-Z\\\\xc0-\\\\xd6\\\\xd8-\\\\xde',\n    rsVarRange = '\\\\ufe0e\\\\ufe0f',\n    rsBreakRange = rsMathOpRange + rsNonCharRange + rsPunctuationRange + rsSpaceRange;\n\n/** Used to compose unicode capture groups. */\nvar rsApos = \"['\\u2019]\",\n    rsBreak = '[' + rsBreakRange + ']',\n    rsCombo = '[' + rsComboRange + ']',\n    rsDigits = '\\\\d+',\n    rsDingbat = '[' + rsDingbatRange + ']',\n    rsLower = '[' + rsLowerRange + ']',\n    rsMisc = '[^' + rsAstralRange + rsBreakRange + rsDigits + rsDingbatRange + rsLowerRange + rsUpperRange + ']',\n    rsFitz = '\\\\ud83c[\\\\udffb-\\\\udfff]',\n    rsModifier = '(?:' + rsCombo + '|' + rsFitz + ')',\n    rsNonAstral = '[^' + rsAstralRange + ']',\n    rsRegional = '(?:\\\\ud83c[\\\\udde6-\\\\uddff]){2}',\n    rsSurrPair = '[\\\\ud800-\\\\udbff][\\\\udc00-\\\\udfff]',\n    rsUpper = '[' + rsUpperRange + ']',\n    rsZWJ = '\\\\u200d';\n\n/** Used to compose unicode regexes. */\nvar rsMiscLower = '(?:' + rsLower + '|' + rsMisc + ')',\n    rsMiscUpper = '(?:' + rsUpper + '|' + rsMisc + ')',\n    rsOptContrLower = '(?:' + rsApos + '(?:d|ll|m|re|s|t|ve))?',\n    rsOptContrUpper = '(?:' + rsApos + '(?:D|LL|M|RE|S|T|VE))?',\n    reOptMod = rsModifier + '?',\n    rsOptVar = '[' + rsVarRange + ']?',\n    rsOptJoin = '(?:' + rsZWJ + '(?:' + [rsNonAstral, rsRegional, rsSurrPair].join('|') + ')' + rsOptVar + reOptMod + ')*',\n    rsOrdLower = '\\\\d*(?:1st|2nd|3rd|(?![123])\\\\dth)(?=\\\\b|[A-Z_])',\n    rsOrdUpper = '\\\\d*(?:1ST|2ND|3RD|(?![123])\\\\dTH)(?=\\\\b|[a-z_])',\n    rsSeq = rsOptVar + reOptMod + rsOptJoin,\n    rsEmoji = '(?:' + [rsDingbat, rsRegional, rsSurrPair].join('|') + ')' + rsSeq;\n\n/** Used to match complex or compound words. */\nvar reUnicodeWord = RegExp([\n  rsUpper + '?' + rsLower + '+' + rsOptContrLower + '(?=' + [rsBreak, rsUpper, '$'].join('|') + ')',\n  rsMiscUpper + '+' + rsOptContrUpper + '(?=' + [rsBreak, rsUpper + rsMiscLower, '$'].join('|') + ')',\n  rsUpper + '?' + rsMiscLower + '+' + rsOptContrLower,\n  rsUpper + '+' + rsOptContrUpper,\n  rsOrdUpper,\n  rsOrdLower,\n  rsDigits,\n  rsEmoji\n].join('|'), 'g');\n\n/**\n * Splits a Unicode `string` into an array of its words.\n *\n * @private\n * @param {string} The string to inspect.\n * @returns {Array} Returns the words of `string`.\n */\nfunction unicodeWords(string) {\n  return string.match(reUnicodeWord) || [];\n}\n\nmodule.exports = unicodeWords;\n", "var asciiWords = require('./_asciiWords'),\n    hasUnicodeWord = require('./_hasUnicodeWord'),\n    toString = require('./toString'),\n    unicodeWords = require('./_unicodeWords');\n\n/**\n * Splits `string` into an array of its words.\n *\n * @static\n * @memberOf _\n * @since 3.0.0\n * @category String\n * @param {string} [string=''] The string to inspect.\n * @param {RegExp|string} [pattern] The pattern to match words.\n * @param- {Object} [guard] Enables use as an iteratee for methods like `_.map`.\n * @returns {Array} Returns the words of `string`.\n * @example\n *\n * _.words('fred, barney, & pebbles');\n * // => ['fred', 'barney', 'pebbles']\n *\n * _.words('fred, barney, & pebbles', /[^, ]+/g);\n * // => ['fred', 'barney', '&', 'pebbles']\n */\nfunction words(string, pattern, guard) {\n  string = toString(string);\n  pattern = guard ? undefined : pattern;\n\n  if (pattern === undefined) {\n    return hasUnicodeWord(string) ? unicodeWords(string) : asciiWords(string);\n  }\n  return string.match(pattern) || [];\n}\n\nmodule.exports = words;\n", "var arrayReduce = require('./_arrayReduce'),\n    deburr = require('./deburr'),\n    words = require('./words');\n\n/** Used to compose unicode capture groups. */\nvar rsApos = \"['\\u2019]\";\n\n/** Used to match apostrophes. */\nvar reApos = RegExp(rsApos, 'g');\n\n/**\n * Creates a function like `_.camelCase`.\n *\n * @private\n * @param {Function} callback The function to combine each word.\n * @returns {Function} Returns the new compounder function.\n */\nfunction createCompounder(callback) {\n  return function(string) {\n    return arrayReduce(words(deburr(string).replace(reApos, '')), callback, '');\n  };\n}\n\nmodule.exports = createCompounder;\n", "var createCompounder = require('./_createCompounder');\n\n/**\n * Converts `string` to\n * [snake case](https://en.wikipedia.org/wiki/Snake_case).\n *\n * @static\n * @memberOf _\n * @since 3.0.0\n * @category String\n * @param {string} [string=''] The string to convert.\n * @returns {string} Returns the snake cased string.\n * @example\n *\n * _.snakeCase('Foo Bar');\n * // => 'foo_bar'\n *\n * _.snakeCase('fooBar');\n * // => 'foo_bar'\n *\n * _.snakeCase('--FOO-BAR--');\n * // => 'foo_bar'\n */\nvar snakeCase = createCompounder(function(result, word, index) {\n  return result + (index ? '_' : '') + word.toLowerCase();\n});\n\nmodule.exports = snakeCase;\n", "/**\n * The base implementation of `_.slice` without an iteratee call guard.\n *\n * @private\n * @param {Array} array The array to slice.\n * @param {number} [start=0] The start position.\n * @param {number} [end=array.length] The end position.\n * @returns {Array} Returns the slice of `array`.\n */\nfunction baseSlice(array, start, end) {\n  var index = -1,\n      length = array.length;\n\n  if (start < 0) {\n    start = -start > length ? 0 : (length + start);\n  }\n  end = end > length ? length : end;\n  if (end < 0) {\n    end += length;\n  }\n  length = start > end ? 0 : ((end - start) >>> 0);\n  start >>>= 0;\n\n  var result = Array(length);\n  while (++index < length) {\n    result[index] = array[index + start];\n  }\n  return result;\n}\n\nmodule.exports = baseSlice;\n", "var baseSlice = require('./_baseSlice');\n\n/**\n * Casts `array` to a slice if it's needed.\n *\n * @private\n * @param {Array} array The array to inspect.\n * @param {number} start The start position.\n * @param {number} [end=array.length] The end position.\n * @returns {Array} Returns the cast slice.\n */\nfunction castSlice(array, start, end) {\n  var length = array.length;\n  end = end === undefined ? length : end;\n  return (!start && end >= length) ? array : baseSlice(array, start, end);\n}\n\nmodule.exports = castSlice;\n", "/** Used to compose unicode character classes. */\nvar rsAstralRange = '\\\\ud800-\\\\udfff',\n    rsComboMarksRange = '\\\\u0300-\\\\u036f',\n    reComboHalfMarksRange = '\\\\ufe20-\\\\ufe2f',\n    rsComboSymbolsRange = '\\\\u20d0-\\\\u20ff',\n    rsComboRange = rsComboMarksRange + reComboHalfMarksRange + rsComboSymbolsRange,\n    rsVarRange = '\\\\ufe0e\\\\ufe0f';\n\n/** Used to compose unicode capture groups. */\nvar rsZWJ = '\\\\u200d';\n\n/** Used to detect strings with [zero-width joiners or code points from the astral planes](http://eev.ee/blog/2015/09/12/dark-corners-of-unicode/). */\nvar reHasUnicode = RegExp('[' + rsZWJ + rsAstralRange  + rsComboRange + rsVarRange + ']');\n\n/**\n * Checks if `string` contains Unicode symbols.\n *\n * @private\n * @param {string} string The string to inspect.\n * @returns {boolean} Returns `true` if a symbol is found, else `false`.\n */\nfunction hasUnicode(string) {\n  return reHasUnicode.test(string);\n}\n\nmodule.exports = hasUnicode;\n", "/**\n * Converts an ASCII `string` to an array.\n *\n * @private\n * @param {string} string The string to convert.\n * @returns {Array} Returns the converted array.\n */\nfunction asciiToArray(string) {\n  return string.split('');\n}\n\nmodule.exports = asciiToArray;\n", "/** Used to compose unicode character classes. */\nvar rsAstralRange = '\\\\ud800-\\\\udfff',\n    rsComboMarksRange = '\\\\u0300-\\\\u036f',\n    reComboHalfMarksRange = '\\\\ufe20-\\\\ufe2f',\n    rsComboSymbolsRange = '\\\\u20d0-\\\\u20ff',\n    rsComboRange = rsComboMarksRange + reComboHalfMarksRange + rsComboSymbolsRange,\n    rsVarRange = '\\\\ufe0e\\\\ufe0f';\n\n/** Used to compose unicode capture groups. */\nvar rsAstral = '[' + rsAstralRange + ']',\n    rsCombo = '[' + rsComboRange + ']',\n    rsFitz = '\\\\ud83c[\\\\udffb-\\\\udfff]',\n    rsModifier = '(?:' + rsCombo + '|' + rsFitz + ')',\n    rsNonAstral = '[^' + rsAstralRange + ']',\n    rsRegional = '(?:\\\\ud83c[\\\\udde6-\\\\uddff]){2}',\n    rsSurrPair = '[\\\\ud800-\\\\udbff][\\\\udc00-\\\\udfff]',\n    rsZWJ = '\\\\u200d';\n\n/** Used to compose unicode regexes. */\nvar reOptMod = rsModifier + '?',\n    rsOptVar = '[' + rsVarRange + ']?',\n    rsOptJoin = '(?:' + rsZWJ + '(?:' + [rsNonAstral, rsRegional, rsSurrPair].join('|') + ')' + rsOptVar + reOptMod + ')*',\n    rsSeq = rsOptVar + reOptMod + rsOptJoin,\n    rsSymbol = '(?:' + [rsNonAstral + rsCombo + '?', rsCombo, rsRegional, rsSurrPair, rsAstral].join('|') + ')';\n\n/** Used to match [string symbols](https://mathiasbynens.be/notes/javascript-unicode). */\nvar reUnicode = RegExp(rsFitz + '(?=' + rsFitz + ')|' + rsSymbol + rsSeq, 'g');\n\n/**\n * Converts a Unicode `string` to an array.\n *\n * @private\n * @param {string} string The string to convert.\n * @returns {Array} Returns the converted array.\n */\nfunction unicodeToArray(string) {\n  return string.match(reUnicode) || [];\n}\n\nmodule.exports = unicodeToArray;\n", "var asciiToArray = require('./_asciiToArray'),\n    hasUnicode = require('./_hasUnicode'),\n    unicodeToArray = require('./_unicodeToArray');\n\n/**\n * Converts `string` to an array.\n *\n * @private\n * @param {string} string The string to convert.\n * @returns {Array} Returns the converted array.\n */\nfunction stringToArray(string) {\n  return hasUnicode(string)\n    ? unicodeToArray(string)\n    : asciiToArray(string);\n}\n\nmodule.exports = stringToArray;\n", "var castSlice = require('./_castSlice'),\n    hasUnicode = require('./_hasUnicode'),\n    stringToArray = require('./_stringToArray'),\n    toString = require('./toString');\n\n/**\n * Creates a function like `_.lowerFirst`.\n *\n * @private\n * @param {string} methodName The name of the `String` case method to use.\n * @returns {Function} Returns the new case function.\n */\nfunction createCaseFirst(methodName) {\n  return function(string) {\n    string = toString(string);\n\n    var strSymbols = hasUnicode(string)\n      ? stringToArray(string)\n      : undefined;\n\n    var chr = strSymbols\n      ? strSymbols[0]\n      : string.charAt(0);\n\n    var trailing = strSymbols\n      ? castSlice(strSymbols, 1).join('')\n      : string.slice(1);\n\n    return chr[methodName]() + trailing;\n  };\n}\n\nmodule.exports = createCaseFirst;\n", "var createCaseFirst = require('./_createCaseFirst');\n\n/**\n * Converts the first character of `string` to upper case.\n *\n * @static\n * @memberOf _\n * @since 4.0.0\n * @category String\n * @param {string} [string=''] The string to convert.\n * @returns {string} Returns the converted string.\n * @example\n *\n * _.upperFirst('fred');\n * // => 'Fred'\n *\n * _.upperFirst('FRED');\n * // => 'FRED'\n */\nvar upperFirst = createCaseFirst('toUpperCase');\n\nmodule.exports = upperFirst;\n", "var toString = require('./toString'),\n    upperFirst = require('./upperFirst');\n\n/**\n * Converts the first character of `string` to upper case and the remaining\n * to lower case.\n *\n * @static\n * @memberOf _\n * @since 3.0.0\n * @category String\n * @param {string} [string=''] The string to capitalize.\n * @returns {string} Returns the capitalized string.\n * @example\n *\n * _.capitalize('FRED');\n * // => 'Fred'\n */\nfunction capitalize(string) {\n  return upperFirst(toString(string).toLowerCase());\n}\n\nmodule.exports = capitalize;\n", "var capitalize = require('./capitalize'),\n    createCompounder = require('./_createCompounder');\n\n/**\n * Converts `string` to [camel case](https://en.wikipedia.org/wiki/CamelCase).\n *\n * @static\n * @memberOf _\n * @since 3.0.0\n * @category String\n * @param {string} [string=''] The string to convert.\n * @returns {string} Returns the camel cased string.\n * @example\n *\n * _.camelCase('Foo Bar');\n * // => 'fooBar'\n *\n * _.camelCase('--foo-bar--');\n * // => 'fooBar'\n *\n * _.camelCase('__FOO_BAR__');\n * // => 'fooBar'\n */\nvar camelCase = createCompounder(function(result, word, index) {\n  word = word.toLowerCase();\n  return result + (index ? capitalize(word) : word);\n});\n\nmodule.exports = camelCase;\n", "var baseAssignValue = require('./_baseAssignValue'),\n    baseForOwn = require('./_baseForOwn'),\n    baseIteratee = require('./_baseIteratee');\n\n/**\n * The opposite of `_.mapValues`; this method creates an object with the\n * same values as `object` and keys generated by running each own enumerable\n * string keyed property of `object` thru `iteratee`. The iteratee is invoked\n * with three arguments: (value, key, object).\n *\n * @static\n * @memberOf _\n * @since 3.8.0\n * @category Object\n * @param {Object} object The object to iterate over.\n * @param {Function} [iteratee=_.identity] The function invoked per iteration.\n * @returns {Object} Returns the new mapped object.\n * @see _.mapValues\n * @example\n *\n * _.mapKeys({ 'a': 1, 'b': 2 }, function(value, key) {\n *   return key + value;\n * });\n * // => { 'a1': 1, 'b2': 2 }\n */\nfunction mapKeys(object, iteratee) {\n  var result = {};\n  iteratee = baseIteratee(iteratee, 3);\n\n  baseForOwn(object, function(value, key, object) {\n    baseAssignValue(result, iteratee(value, key, object), value);\n  });\n  return result;\n}\n\nmodule.exports = mapKeys;\n", "\n/**\n * Topological sorting function\n *\n * @param {Array} edges\n * @returns {Array}\n */\n\nmodule.exports = function(edges) {\n  return toposort(uniqueNodes(edges), edges)\n}\n\nmodule.exports.array = toposort\n\nfunction toposort(nodes, edges) {\n  var cursor = nodes.length\n    , sorted = new Array(cursor)\n    , visited = {}\n    , i = cursor\n    // Better data structures make algorithm much faster.\n    , outgoingEdges = makeOutgoingEdges(edges)\n    , nodesHash = makeNodesHash(nodes)\n\n  // check for unknown nodes\n  edges.forEach(function(edge) {\n    if (!nodesHash.has(edge[0]) || !nodesHash.has(edge[1])) {\n      throw new Error('Unknown node. There is an unknown node in the supplied edges.')\n    }\n  })\n\n  while (i--) {\n    if (!visited[i]) visit(nodes[i], i, new Set())\n  }\n\n  return sorted\n\n  function visit(node, i, predecessors) {\n    if(predecessors.has(node)) {\n      var nodeRep\n      try {\n        nodeRep = \", node was:\" + JSON.stringify(node)\n      } catch(e) {\n        nodeRep = \"\"\n      }\n      throw new Error('Cyclic dependency' + nodeRep)\n    }\n\n    if (!nodesHash.has(node)) {\n      throw new Error('Found unknown node. Make sure to provided all involved nodes. Unknown node: '+JSON.stringify(node))\n    }\n\n    if (visited[i]) return;\n    visited[i] = true\n\n    var outgoing = outgoingEdges.get(node) || new Set()\n    outgoing = Array.from(outgoing)\n\n    if (i = outgoing.length) {\n      predecessors.add(node)\n      do {\n        var child = outgoing[--i]\n        visit(child, nodesHash.get(child), predecessors)\n      } while (i)\n      predecessors.delete(node)\n    }\n\n    sorted[--cursor] = node\n  }\n}\n\nfunction uniqueNodes(arr){\n  var res = new Set()\n  for (var i = 0, len = arr.length; i < len; i++) {\n    var edge = arr[i]\n    res.add(edge[0])\n    res.add(edge[1])\n  }\n  return Array.from(res)\n}\n\nfunction makeOutgoingEdges(arr){\n  var edges = new Map()\n  for (var i = 0, len = arr.length; i < len; i++) {\n    var edge = arr[i]\n    if (!edges.has(edge[0])) edges.set(edge[0], new Set())\n    if (!edges.has(edge[1])) edges.set(edge[1], new Set())\n    edges.get(edge[0]).add(edge[1])\n  }\n  return edges\n}\n\nfunction makeNodesHash(arr){\n  var res = new Map()\n  for (var i = 0, len = arr.length; i < len; i++) {\n    res.set(arr[i], i)\n  }\n  return res\n}\n", "\"use strict\";\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\nexports.default = sortFields;\n\nvar _has = _interopRequireDefault(require(\"lodash/has\"));\n\nvar _toposort = _interopRequireDefault(require(\"toposort\"));\n\nvar _propertyExpr = require(\"property-expr\");\n\nvar _Reference = _interopRequireDefault(require(\"../Reference\"));\n\nvar _isSchema = _interopRequireDefault(require(\"./isSchema\"));\n\nfunction _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { default: obj }; }\n\n// @ts-expect-error\nfunction sortFields(fields, excludedEdges = []) {\n  let edges = [];\n  let nodes = new Set();\n  let excludes = new Set(excludedEdges.map(([a, b]) => `${a}-${b}`));\n\n  function addNode(depPath, key) {\n    let node = (0, _propertyExpr.split)(depPath)[0];\n    nodes.add(node);\n    if (!excludes.has(`${key}-${node}`)) edges.push([key, node]);\n  }\n\n  for (const key in fields) if ((0, _has.default)(fields, key)) {\n    let value = fields[key];\n    nodes.add(key);\n    if (_Reference.default.isRef(value) && value.isSibling) addNode(value.path, key);else if ((0, _isSchema.default)(value) && 'deps' in value) value.deps.forEach(path => addNode(path, key));\n  }\n\n  return _toposort.default.array(Array.from(nodes), edges).reverse();\n}", "\"use strict\";\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\nexports.default = sortByKeyOrder;\n\nfunction findIndex(arr, err) {\n  let idx = Infinity;\n  arr.some((key, ii) => {\n    var _err$path;\n\n    if (((_err$path = err.path) == null ? void 0 : _err$path.indexOf(key)) !== -1) {\n      idx = ii;\n      return true;\n    }\n  });\n  return idx;\n}\n\nfunction sortByKeyOrder(keys) {\n  return (a, b) => {\n    return findIndex(keys, a) - findIndex(keys, b);\n  };\n}", "\"use strict\";\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\nexports.create = create;\nexports.default = void 0;\n\nvar _has = _interopRequireDefault(require(\"lodash/has\"));\n\nvar _snakeCase = _interopRequireDefault(require(\"lodash/snakeCase\"));\n\nvar _camelCase = _interopRequireDefault(require(\"lodash/camelCase\"));\n\nvar _mapKeys = _interopRequireDefault(require(\"lodash/mapKeys\"));\n\nvar _mapValues = _interopRequireDefault(require(\"lodash/mapValues\"));\n\nvar _propertyExpr = require(\"property-expr\");\n\nvar _locale = require(\"./locale\");\n\nvar _sortFields = _interopRequireDefault(require(\"./util/sortFields\"));\n\nvar _sortByKeyOrder = _interopRequireDefault(require(\"./util/sortByKeyOrder\"));\n\nvar _runTests = _interopRequireDefault(require(\"./util/runTests\"));\n\nvar _ValidationError = _interopRequireDefault(require(\"./ValidationError\"));\n\nvar _schema = _interopRequireDefault(require(\"./schema\"));\n\nfunction _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { default: obj }; }\n\nfunction _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nlet isObject = obj => Object.prototype.toString.call(obj) === '[object Object]';\n\nfunction unknown(ctx, value) {\n  let known = Object.keys(ctx.fields);\n  return Object.keys(value).filter(key => known.indexOf(key) === -1);\n}\n\nconst defaultSort = (0, _sortByKeyOrder.default)([]);\n\nclass ObjectSchema extends _schema.default {\n  constructor(spec) {\n    super({\n      type: 'object'\n    });\n    this.fields = Object.create(null);\n    this._sortErrors = defaultSort;\n    this._nodes = [];\n    this._excludedEdges = [];\n    this.withMutation(() => {\n      this.transform(function coerce(value) {\n        if (typeof value === 'string') {\n          try {\n            value = JSON.parse(value);\n          } catch (err) {\n            value = null;\n          }\n        }\n\n        if (this.isType(value)) return value;\n        return null;\n      });\n\n      if (spec) {\n        this.shape(spec);\n      }\n    });\n  }\n\n  _typeCheck(value) {\n    return isObject(value) || typeof value === 'function';\n  }\n\n  _cast(_value, options = {}) {\n    var _options$stripUnknown;\n\n    let value = super._cast(_value, options); //should ignore nulls here\n\n\n    if (value === undefined) return this.getDefault();\n    if (!this._typeCheck(value)) return value;\n    let fields = this.fields;\n    let strip = (_options$stripUnknown = options.stripUnknown) != null ? _options$stripUnknown : this.spec.noUnknown;\n\n    let props = this._nodes.concat(Object.keys(value).filter(v => this._nodes.indexOf(v) === -1));\n\n    let intermediateValue = {}; // is filled during the transform below\n\n    let innerOptions = _extends({}, options, {\n      parent: intermediateValue,\n      __validating: options.__validating || false\n    });\n\n    let isChanged = false;\n\n    for (const prop of props) {\n      let field = fields[prop];\n      let exists = (0, _has.default)(value, prop);\n\n      if (field) {\n        let fieldValue;\n        let inputValue = value[prop]; // safe to mutate since this is fired in sequence\n\n        innerOptions.path = (options.path ? `${options.path}.` : '') + prop; // innerOptions.value = value[prop];\n\n        field = field.resolve({\n          value: inputValue,\n          context: options.context,\n          parent: intermediateValue\n        });\n        let fieldSpec = 'spec' in field ? field.spec : undefined;\n        let strict = fieldSpec == null ? void 0 : fieldSpec.strict;\n\n        if (fieldSpec == null ? void 0 : fieldSpec.strip) {\n          isChanged = isChanged || prop in value;\n          continue;\n        }\n\n        fieldValue = !options.__validating || !strict ? // TODO: use _cast, this is double resolving\n        field.cast(value[prop], innerOptions) : value[prop];\n\n        if (fieldValue !== undefined) {\n          intermediateValue[prop] = fieldValue;\n        }\n      } else if (exists && !strip) {\n        intermediateValue[prop] = value[prop];\n      }\n\n      if (intermediateValue[prop] !== value[prop]) {\n        isChanged = true;\n      }\n    }\n\n    return isChanged ? intermediateValue : value;\n  }\n\n  _validate(_value, opts = {}, callback) {\n    let errors = [];\n    let {\n      sync,\n      from = [],\n      originalValue = _value,\n      abortEarly = this.spec.abortEarly,\n      recursive = this.spec.recursive\n    } = opts;\n    from = [{\n      schema: this,\n      value: originalValue\n    }, ...from]; // this flag is needed for handling `strict` correctly in the context of\n    // validation vs just casting. e.g strict() on a field is only used when validating\n\n    opts.__validating = true;\n    opts.originalValue = originalValue;\n    opts.from = from;\n\n    super._validate(_value, opts, (err, value) => {\n      if (err) {\n        if (!_ValidationError.default.isError(err) || abortEarly) {\n          return void callback(err, value);\n        }\n\n        errors.push(err);\n      }\n\n      if (!recursive || !isObject(value)) {\n        callback(errors[0] || null, value);\n        return;\n      }\n\n      originalValue = originalValue || value;\n\n      let tests = this._nodes.map(key => (_, cb) => {\n        let path = key.indexOf('.') === -1 ? (opts.path ? `${opts.path}.` : '') + key : `${opts.path || ''}[\"${key}\"]`;\n        let field = this.fields[key];\n\n        if (field && 'validate' in field) {\n          field.validate(value[key], _extends({}, opts, {\n            // @ts-ignore\n            path,\n            from,\n            // inner fields are always strict:\n            // 1. this isn't strict so the casting will also have cast inner values\n            // 2. this is strict in which case the nested values weren't cast either\n            strict: true,\n            parent: value,\n            originalValue: originalValue[key]\n          }), cb);\n          return;\n        }\n\n        cb(null);\n      });\n\n      (0, _runTests.default)({\n        sync,\n        tests,\n        value,\n        errors,\n        endEarly: abortEarly,\n        sort: this._sortErrors,\n        path: opts.path\n      }, callback);\n    });\n  }\n\n  clone(spec) {\n    const next = super.clone(spec);\n    next.fields = _extends({}, this.fields);\n    next._nodes = this._nodes;\n    next._excludedEdges = this._excludedEdges;\n    next._sortErrors = this._sortErrors;\n    return next;\n  }\n\n  concat(schema) {\n    let next = super.concat(schema);\n    let nextFields = next.fields;\n\n    for (let [field, schemaOrRef] of Object.entries(this.fields)) {\n      const target = nextFields[field];\n\n      if (target === undefined) {\n        nextFields[field] = schemaOrRef;\n      } else if (target instanceof _schema.default && schemaOrRef instanceof _schema.default) {\n        nextFields[field] = schemaOrRef.concat(target);\n      }\n    }\n\n    return next.withMutation(() => next.shape(nextFields, this._excludedEdges));\n  }\n\n  getDefaultFromShape() {\n    let dft = {};\n\n    this._nodes.forEach(key => {\n      const field = this.fields[key];\n      dft[key] = 'default' in field ? field.getDefault() : undefined;\n    });\n\n    return dft;\n  }\n\n  _getDefault() {\n    if ('default' in this.spec) {\n      return super._getDefault();\n    } // if there is no default set invent one\n\n\n    if (!this._nodes.length) {\n      return undefined;\n    }\n\n    return this.getDefaultFromShape();\n  }\n\n  shape(additions, excludes = []) {\n    let next = this.clone();\n    let fields = Object.assign(next.fields, additions);\n    next.fields = fields;\n    next._sortErrors = (0, _sortByKeyOrder.default)(Object.keys(fields));\n\n    if (excludes.length) {\n      // this is a convenience for when users only supply a single pair\n      if (!Array.isArray(excludes[0])) excludes = [excludes];\n      next._excludedEdges = [...next._excludedEdges, ...excludes];\n    }\n\n    next._nodes = (0, _sortFields.default)(fields, next._excludedEdges);\n    return next;\n  }\n\n  pick(keys) {\n    const picked = {};\n\n    for (const key of keys) {\n      if (this.fields[key]) picked[key] = this.fields[key];\n    }\n\n    return this.clone().withMutation(next => {\n      next.fields = {};\n      return next.shape(picked);\n    });\n  }\n\n  omit(keys) {\n    const next = this.clone();\n    const fields = next.fields;\n    next.fields = {};\n\n    for (const key of keys) {\n      delete fields[key];\n    }\n\n    return next.withMutation(() => next.shape(fields));\n  }\n\n  from(from, to, alias) {\n    let fromGetter = (0, _propertyExpr.getter)(from, true);\n    return this.transform(obj => {\n      if (obj == null) return obj;\n      let newObj = obj;\n\n      if ((0, _has.default)(obj, from)) {\n        newObj = _extends({}, obj);\n        if (!alias) delete newObj[from];\n        newObj[to] = fromGetter(obj);\n      }\n\n      return newObj;\n    });\n  }\n\n  noUnknown(noAllow = true, message = _locale.object.noUnknown) {\n    if (typeof noAllow === 'string') {\n      message = noAllow;\n      noAllow = true;\n    }\n\n    let next = this.test({\n      name: 'noUnknown',\n      exclusive: true,\n      message: message,\n\n      test(value) {\n        if (value == null) return true;\n        const unknownKeys = unknown(this.schema, value);\n        return !noAllow || unknownKeys.length === 0 || this.createError({\n          params: {\n            unknown: unknownKeys.join(', ')\n          }\n        });\n      }\n\n    });\n    next.spec.noUnknown = noAllow;\n    return next;\n  }\n\n  unknown(allow = true, message = _locale.object.noUnknown) {\n    return this.noUnknown(!allow, message);\n  }\n\n  transformKeys(fn) {\n    return this.transform(obj => obj && (0, _mapKeys.default)(obj, (_, key) => fn(key)));\n  }\n\n  camelCase() {\n    return this.transformKeys(_camelCase.default);\n  }\n\n  snakeCase() {\n    return this.transformKeys(_snakeCase.default);\n  }\n\n  constantCase() {\n    return this.transformKeys(key => (0, _snakeCase.default)(key).toUpperCase());\n  }\n\n  describe() {\n    let base = super.describe();\n    base.fields = (0, _mapValues.default)(this.fields, value => value.describe());\n    return base;\n  }\n\n}\n\nexports.default = ObjectSchema;\n\nfunction create(spec) {\n  return new ObjectSchema(spec);\n}\n\ncreate.prototype = ObjectSchema.prototype;", "\"use strict\";\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\nexports.create = create;\nexports.default = void 0;\n\nvar _isAbsent = _interopRequireDefault(require(\"./util/isAbsent\"));\n\nvar _isSchema = _interopRequireDefault(require(\"./util/isSchema\"));\n\nvar _printValue = _interopRequireDefault(require(\"./util/printValue\"));\n\nvar _locale = require(\"./locale\");\n\nvar _runTests = _interopRequireDefault(require(\"./util/runTests\"));\n\nvar _ValidationError = _interopRequireDefault(require(\"./ValidationError\"));\n\nvar _schema = _interopRequireDefault(require(\"./schema\"));\n\nfunction _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { default: obj }; }\n\nfunction _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction create(type) {\n  return new ArraySchema(type);\n}\n\nclass ArraySchema extends _schema.default {\n  constructor(type) {\n    super({\n      type: 'array'\n    }); // `undefined` specifically means uninitialized, as opposed to\n    // \"no subtype\"\n\n    this.innerType = void 0;\n    this.innerType = type;\n    this.withMutation(() => {\n      this.transform(function (values) {\n        if (typeof values === 'string') try {\n          values = JSON.parse(values);\n        } catch (err) {\n          values = null;\n        }\n        return this.isType(values) ? values : null;\n      });\n    });\n  }\n\n  _typeCheck(v) {\n    return Array.isArray(v);\n  }\n\n  get _subType() {\n    return this.innerType;\n  }\n\n  _cast(_value, _opts) {\n    const value = super._cast(_value, _opts); //should ignore nulls here\n\n\n    if (!this._typeCheck(value) || !this.innerType) return value;\n    let isChanged = false;\n    const castArray = value.map((v, idx) => {\n      const castElement = this.innerType.cast(v, _extends({}, _opts, {\n        path: `${_opts.path || ''}[${idx}]`\n      }));\n\n      if (castElement !== v) {\n        isChanged = true;\n      }\n\n      return castElement;\n    });\n    return isChanged ? castArray : value;\n  }\n\n  _validate(_value, options = {}, callback) {\n    var _options$abortEarly, _options$recursive;\n\n    let errors = [];\n    let sync = options.sync;\n    let path = options.path;\n    let innerType = this.innerType;\n    let endEarly = (_options$abortEarly = options.abortEarly) != null ? _options$abortEarly : this.spec.abortEarly;\n    let recursive = (_options$recursive = options.recursive) != null ? _options$recursive : this.spec.recursive;\n    let originalValue = options.originalValue != null ? options.originalValue : _value;\n\n    super._validate(_value, options, (err, value) => {\n      if (err) {\n        if (!_ValidationError.default.isError(err) || endEarly) {\n          return void callback(err, value);\n        }\n\n        errors.push(err);\n      }\n\n      if (!recursive || !innerType || !this._typeCheck(value)) {\n        callback(errors[0] || null, value);\n        return;\n      }\n\n      originalValue = originalValue || value; // #950 Ensure that sparse array empty slots are validated\n\n      let tests = new Array(value.length);\n\n      for (let idx = 0; idx < value.length; idx++) {\n        let item = value[idx];\n        let path = `${options.path || ''}[${idx}]`; // object._validate note for isStrict explanation\n\n        let innerOptions = _extends({}, options, {\n          path,\n          strict: true,\n          parent: value,\n          index: idx,\n          originalValue: originalValue[idx]\n        });\n\n        tests[idx] = (_, cb) => innerType.validate(item, innerOptions, cb);\n      }\n\n      (0, _runTests.default)({\n        sync,\n        path,\n        value,\n        errors,\n        endEarly,\n        tests\n      }, callback);\n    });\n  }\n\n  clone(spec) {\n    const next = super.clone(spec);\n    next.innerType = this.innerType;\n    return next;\n  }\n\n  concat(schema) {\n    let next = super.concat(schema);\n    next.innerType = this.innerType;\n    if (schema.innerType) next.innerType = next.innerType ? // @ts-expect-error Lazy doesn't have concat()\n    next.innerType.concat(schema.innerType) : schema.innerType;\n    return next;\n  }\n\n  of(schema) {\n    // FIXME: this should return a new instance of array without the default to be\n    let next = this.clone();\n    if (!(0, _isSchema.default)(schema)) throw new TypeError('`array.of()` sub-schema must be a valid yup schema not: ' + (0, _printValue.default)(schema)); // FIXME(ts):\n\n    next.innerType = schema;\n    return next;\n  }\n\n  length(length, message = _locale.array.length) {\n    return this.test({\n      message,\n      name: 'length',\n      exclusive: true,\n      params: {\n        length\n      },\n\n      test(value) {\n        return (0, _isAbsent.default)(value) || value.length === this.resolve(length);\n      }\n\n    });\n  }\n\n  min(min, message) {\n    message = message || _locale.array.min;\n    return this.test({\n      message,\n      name: 'min',\n      exclusive: true,\n      params: {\n        min\n      },\n\n      // FIXME(ts): Array<typeof T>\n      test(value) {\n        return (0, _isAbsent.default)(value) || value.length >= this.resolve(min);\n      }\n\n    });\n  }\n\n  max(max, message) {\n    message = message || _locale.array.max;\n    return this.test({\n      message,\n      name: 'max',\n      exclusive: true,\n      params: {\n        max\n      },\n\n      test(value) {\n        return (0, _isAbsent.default)(value) || value.length <= this.resolve(max);\n      }\n\n    });\n  }\n\n  ensure() {\n    return this.default(() => []).transform((val, original) => {\n      // We don't want to return `null` for nullable schema\n      if (this._typeCheck(val)) return val;\n      return original == null ? [] : [].concat(original);\n    });\n  }\n\n  compact(rejector) {\n    let reject = !rejector ? v => !!v : (v, i, a) => !rejector(v, i, a);\n    return this.transform(values => values != null ? values.filter(reject) : values);\n  }\n\n  describe() {\n    let base = super.describe();\n    if (this.innerType) base.innerType = this.innerType.describe();\n    return base;\n  }\n\n  nullable(isNullable = true) {\n    return super.nullable(isNullable);\n  }\n\n  defined() {\n    return super.defined();\n  }\n\n  required(msg) {\n    return super.required(msg);\n  }\n\n}\n\nexports.default = ArraySchema;\ncreate.prototype = ArraySchema.prototype; //\n// Interfaces\n//", "\"use strict\";\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\nexports.create = create;\nexports.default = void 0;\n\nvar _isSchema = _interopRequireDefault(require(\"./util/isSchema\"));\n\nfunction _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { default: obj }; }\n\nfunction create(builder) {\n  return new Lazy(builder);\n}\n\nclass Lazy {\n  constructor(builder) {\n    this.type = 'lazy';\n    this.__isYupSchema__ = true;\n    this.__inputType = void 0;\n    this.__outputType = void 0;\n\n    this._resolve = (value, options = {}) => {\n      let schema = this.builder(value, options);\n      if (!(0, _isSchema.default)(schema)) throw new TypeError('lazy() functions must return a valid schema');\n      return schema.resolve(options);\n    };\n\n    this.builder = builder;\n  }\n\n  resolve(options) {\n    return this._resolve(options.value, options);\n  }\n\n  cast(value, options) {\n    return this._resolve(value, options).cast(value, options);\n  }\n\n  validate(value, options, maybeCb) {\n    // @ts-expect-error missing public callback on type\n    return this._resolve(value, options).validate(value, options, maybeCb);\n  }\n\n  validateSync(value, options) {\n    return this._resolve(value, options).validateSync(value, options);\n  }\n\n  validateAt(path, value, options) {\n    return this._resolve(value, options).validateAt(path, value, options);\n  }\n\n  validateSyncAt(path, value, options) {\n    return this._resolve(value, options).validateSyncAt(path, value, options);\n  }\n\n  describe() {\n    return null;\n  }\n\n  isValid(value, options) {\n    return this._resolve(value, options).isValid(value, options);\n  }\n\n  isValidSync(value, options) {\n    return this._resolve(value, options).isValidSync(value, options);\n  }\n\n}\n\nvar _default = Lazy;\nexports.default = _default;", "\"use strict\";\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\nexports.default = setLocale;\n\nvar _locale = _interopRequireDefault(require(\"./locale\"));\n\nfunction _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { default: obj }; }\n\nfunction setLocale(custom) {\n  Object.keys(custom).forEach(type => {\n    // @ts-ignore\n    Object.keys(custom[type]).forEach(method => {\n      // @ts-ignore\n      _locale.default[type][method] = custom[type][method];\n    });\n  });\n}", "\"use strict\";\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\nObject.defineProperty(exports, \"ArraySchema\", {\n  enumerable: true,\n  get: function () {\n    return _array.default;\n  }\n});\nObject.defineProperty(exports, \"BaseSchema\", {\n  enumerable: true,\n  get: function () {\n    return _schema.default;\n  }\n});\nObject.defineProperty(exports, \"BooleanSchema\", {\n  enumerable: true,\n  get: function () {\n    return _boolean.default;\n  }\n});\nObject.defineProperty(exports, \"DateSchema\", {\n  enumerable: true,\n  get: function () {\n    return _date.default;\n  }\n});\nObject.defineProperty(exports, \"MixedSchema\", {\n  enumerable: true,\n  get: function () {\n    return _mixed.default;\n  }\n});\nObject.defineProperty(exports, \"NumberSchema\", {\n  enumerable: true,\n  get: function () {\n    return _number.default;\n  }\n});\nObject.defineProperty(exports, \"ObjectSchema\", {\n  enumerable: true,\n  get: function () {\n    return _object.default;\n  }\n});\nObject.defineProperty(exports, \"StringSchema\", {\n  enumerable: true,\n  get: function () {\n    return _string.default;\n  }\n});\nObject.defineProperty(exports, \"ValidationError\", {\n  enumerable: true,\n  get: function () {\n    return _ValidationError.default;\n  }\n});\nexports.addMethod = addMethod;\nObject.defineProperty(exports, \"array\", {\n  enumerable: true,\n  get: function () {\n    return _array.create;\n  }\n});\nObject.defineProperty(exports, \"bool\", {\n  enumerable: true,\n  get: function () {\n    return _boolean.create;\n  }\n});\nObject.defineProperty(exports, \"boolean\", {\n  enumerable: true,\n  get: function () {\n    return _boolean.create;\n  }\n});\nObject.defineProperty(exports, \"date\", {\n  enumerable: true,\n  get: function () {\n    return _date.create;\n  }\n});\nObject.defineProperty(exports, \"isSchema\", {\n  enumerable: true,\n  get: function () {\n    return _isSchema.default;\n  }\n});\nObject.defineProperty(exports, \"lazy\", {\n  enumerable: true,\n  get: function () {\n    return _Lazy.create;\n  }\n});\nObject.defineProperty(exports, \"mixed\", {\n  enumerable: true,\n  get: function () {\n    return _mixed.create;\n  }\n});\nObject.defineProperty(exports, \"number\", {\n  enumerable: true,\n  get: function () {\n    return _number.create;\n  }\n});\nObject.defineProperty(exports, \"object\", {\n  enumerable: true,\n  get: function () {\n    return _object.create;\n  }\n});\nObject.defineProperty(exports, \"reach\", {\n  enumerable: true,\n  get: function () {\n    return _reach.default;\n  }\n});\nObject.defineProperty(exports, \"ref\", {\n  enumerable: true,\n  get: function () {\n    return _Reference.create;\n  }\n});\nObject.defineProperty(exports, \"setLocale\", {\n  enumerable: true,\n  get: function () {\n    return _setLocale.default;\n  }\n});\nObject.defineProperty(exports, \"string\", {\n  enumerable: true,\n  get: function () {\n    return _string.create;\n  }\n});\n\nvar _mixed = _interopRequireWildcard(require(\"./mixed\"));\n\nvar _boolean = _interopRequireWildcard(require(\"./boolean\"));\n\nvar _string = _interopRequireWildcard(require(\"./string\"));\n\nvar _number = _interopRequireWildcard(require(\"./number\"));\n\nvar _date = _interopRequireWildcard(require(\"./date\"));\n\nvar _object = _interopRequireWildcard(require(\"./object\"));\n\nvar _array = _interopRequireWildcard(require(\"./array\"));\n\nvar _Reference = require(\"./Reference\");\n\nvar _Lazy = require(\"./Lazy\");\n\nvar _ValidationError = _interopRequireDefault(require(\"./ValidationError\"));\n\nvar _reach = _interopRequireDefault(require(\"./util/reach\"));\n\nvar _isSchema = _interopRequireDefault(require(\"./util/isSchema\"));\n\nvar _setLocale = _interopRequireDefault(require(\"./setLocale\"));\n\nvar _schema = _interopRequireDefault(require(\"./schema\"));\n\nfunction _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { default: obj }; }\n\nfunction _getRequireWildcardCache(nodeInterop) { if (typeof WeakMap !== \"function\") return null; var cacheBabelInterop = new WeakMap(); var cacheNodeInterop = new WeakMap(); return (_getRequireWildcardCache = function (nodeInterop) { return nodeInterop ? cacheNodeInterop : cacheBabelInterop; })(nodeInterop); }\n\nfunction _interopRequireWildcard(obj, nodeInterop) { if (!nodeInterop && obj && obj.__esModule) { return obj; } if (obj === null || typeof obj !== \"object\" && typeof obj !== \"function\") { return { default: obj }; } var cache = _getRequireWildcardCache(nodeInterop); if (cache && cache.has(obj)) { return cache.get(obj); } var newObj = {}; var hasPropertyDescriptor = Object.defineProperty && Object.getOwnPropertyDescriptor; for (var key in obj) { if (key !== \"default\" && Object.prototype.hasOwnProperty.call(obj, key)) { var desc = hasPropertyDescriptor ? Object.getOwnPropertyDescriptor(obj, key) : null; if (desc && (desc.get || desc.set)) { Object.defineProperty(newObj, key, desc); } else { newObj[key] = obj[key]; } } } newObj.default = obj; if (cache) { cache.set(obj, newObj); } return newObj; }\n\nfunction addMethod(schemaType, name, fn) {\n  if (!schemaType || !(0, _isSchema.default)(schemaType.prototype)) throw new TypeError('You must provide a yup schema constructor function');\n  if (typeof name !== 'string') throw new TypeError('A Method name must be provided');\n  if (typeof fn !== 'function') throw new TypeError('Method function must be provided');\n  schemaType.prototype[name] = fn;\n}", "import handler from \"../handler-lib\";\nimport { randomUUID } from \"crypto\";\n// types\nimport { UserRoles } from \"../../utils/types\";\nimport { number, object, string } from \"yup\";\n// utils\nimport { putBanner } from \"../../storage/banners\";\nimport { hasPermissions } from \"../../utils/auth/authorization\";\nimport { error } from \"../../utils/constants/constants\";\nimport { validateData } from \"../../utils/validation/validation\";\nimport {\n  badRequest,\n  created,\n  forbidden,\n  internalServerError,\n} from \"../../utils/responses/response-lib\";\n\nconst validationSchema = object().shape({\n  title: string().required(),\n  description: string().required(),\n  link: string().url().notRequired(),\n  startDate: number().required(),\n  endDate: number().required(),\n});\n\nexport const createBanner = handler(async (event, _context) => {\n  if (!hasPermissions(event, [UserRoles.ADMIN])) {\n    return forbidden(error.UNAUTHORIZED);\n  }\n  const unvalidatedPayload = JSON.parse(event.body!);\n\n  let validatedPayload;\n  try {\n    validatedPayload = await validateData(validationSchema, unvalidatedPayload);\n  } catch {\n    return badRequest(error.INVALID_DATA);\n  }\n\n  const { title, description, link, startDate, endDate } = validatedPayload;\n  const currentTime = Date.now();\n\n  const newBanner = {\n    key: randomUUID(),\n    createdAt: currentTime,\n    lastAltered: currentTime,\n    lastAlteredBy: event?.headers[\"cognito-identity-id\"],\n    title,\n    description,\n    link,\n    startDate,\n    endDate,\n  };\n  try {\n    await putBanner(newBanner);\n  } catch {\n    return internalServerError(error.DYNAMO_CREATION_ERROR);\n  }\n  return created(newBanner);\n});\n", "import util from \"util\";\nimport { Logger } from \"@smithy/types\";\n\ntype LogLevel = \"trace\" | \"debug\" | \"info\" | \"warn\" | \"error\";\ntype LogEntry = {\n  date: Date;\n  level: LogLevel;\n  string: string;\n};\n\nconst logs: LogEntry[] = [];\n\nconst buildLoggerForLevel = (level: LogLevel) => {\n  return function (...content: any[]) {\n    logs.push({\n      date: new Date(),\n      level: level,\n      string: util.format.apply(null, content),\n    });\n\n    /*\n     * If we have a function logging thousands of messages,\n     * better to take the console performance hit mid-operation\n     * than to let memory usage run away as well.\n     */\n    if (logs.length > 99) {\n      flush();\n    }\n  };\n};\n\n/*\n * Individual functions are exported to support handler-lib;\n * This integrates SDK client logging with lambda logging.\n */\nexport const trace = buildLoggerForLevel(\"trace\");\nexport const debug = buildLoggerForLevel(\"debug\");\nexport const info = buildLoggerForLevel(\"info\");\nexport const warn = buildLoggerForLevel(\"warn\");\nexport const error = buildLoggerForLevel(\"error\");\n\nexport function flush() {\n  while (logs.length > 0) {\n    const { date, level, string } = logs.shift()!;\n    // eslint-disable-next-line no-console\n    console[level](date, string);\n  }\n}\n\n/*\n * This is only called at the beginning of a lambda handler,\n * so the log buffer should be empty anyway. But it doesn't\n * hurt to make sure!\n */\nexport const init = flush;\n\n/**\n * A logger suitable for passing to any AWS client constructor.\n * Note that the `trace` log level is excluded.\n *\n * This logger accumulates log messages in an internal buffer,\n * eventually flushing them to the console.\n */\nexport const logger: Logger = { debug, info, warn, error };\n", "import { SSMClient, GetParameterCommand } from \"@aws-sdk/client-ssm\";\nimport jwt_decode from \"jwt-decode\";\nimport { CognitoJwtVerifier } from \"aws-jwt-verify\";\n// types\nimport { APIGatewayProxyEvent, UserRoles } from \"../types\";\nimport { logger } from \"../debugging/debug-lib\";\n\ninterface DecodedToken {\n  \"custom:cms_roles\": UserRoles;\n  \"custom:cms_state\": string | undefined;\n}\n\nconst loadCognitoValues = async () => {\n  if (\n    process.env.COGNITO_USER_POOL_ID &&\n    process.env.COGNITO_USER_POOL_CLIENT_ID\n  ) {\n    return {\n      userPoolId: process.env.COGNITO_USER_POOL_ID,\n      userPoolClientId: process.env.COGNITO_USER_POOL_CLIENT_ID,\n    };\n  } else {\n    const ssmClient = new SSMClient({ logger });\n    const stage = process.env.stage!;\n    const getParam = async (identifier: string) => {\n      const command = new GetParameterCommand({\n        Name: `/${stage}/ui-auth/${identifier}`,\n      });\n      const result = await ssmClient.send(command);\n      return result.Parameter?.Value;\n    };\n    const userPoolId = await getParam(\"cognito_user_pool_id\");\n    const userPoolClientId = await getParam(\"cognito_user_pool_client_id\");\n    if (userPoolId && userPoolClientId) {\n      process.env[\"COGNITO_USER_POOL_ID\"] = userPoolId;\n      process.env[\"COGNITO_USER_POOL_CLIENT_ID\"] = userPoolClientId;\n      return { userPoolId, userPoolClientId };\n    } else {\n      throw new Error(\"cannot load cognito values\");\n    }\n  }\n};\n\nexport const isAuthenticated = async (event: APIGatewayProxyEvent) => {\n  const isLocalStack = event.requestContext.accountId === \"000000000000\";\n  if (isLocalStack) {\n    return true;\n  }\n\n  const cognitoValues = await loadCognitoValues();\n\n  // Verifier that expects valid access tokens:\n  const verifier = CognitoJwtVerifier.create({\n    userPoolId: cognitoValues.userPoolId,\n    tokenUse: \"id\",\n    clientId: cognitoValues.userPoolClientId,\n  });\n\n  try {\n    await verifier.verify(event?.headers?.[\"x-api-key\"]!);\n    return true;\n  } catch {\n    return false;\n  }\n};\n\nexport const hasPermissions = (\n  event: APIGatewayProxyEvent,\n  allowedRoles: UserRoles[],\n  state?: string\n) => {\n  let isAllowed = false;\n  // decode the idToken\n  if (event?.headers?.[\"x-api-key\"]) {\n    const decoded = jwt_decode(event.headers[\"x-api-key\"]) as DecodedToken;\n    const idmUserRoles = decoded[\"custom:cms_roles\"];\n    const idmUserState = decoded[\"custom:cms_state\"];\n    const mfpUserRole = idmUserRoles\n      ?.split(\",\")\n      .find((role) => role.includes(\"mdctmfp\")) as UserRoles;\n\n    isAllowed =\n      allowedRoles.includes(mfpUserRole) &&\n      (!state || idmUserState?.includes(state))!;\n  }\n\n  return isAllowed;\n};\n\nexport const isAuthorizedToFetchState = (\n  event: APIGatewayProxyEvent,\n  state: string\n) => {\n  // If this is a state user for the matching state, authorize them.\n  if (hasPermissions(event, [UserRoles.STATE_USER], state)) {\n    return true;\n  }\n\n  const nonStateUserRoles = Object.values(UserRoles).filter(\n    (role) => role !== UserRoles.STATE_USER\n  );\n\n  // If they are any other user type, they don't need to belong to this state.\n  return hasPermissions(event, nonStateUserRoles);\n};\n", "// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n// SPDX-License-Identifier: Apache-2.0\n/**\n * Base Error for all other errors in this file\n */\nexport class JwtBaseError extends Error {\n}\n/**\n * An error that is raised because an actual value does not match with the expected value\n */\nexport class FailedAssertionError extends JwtBaseError {\n    constructor(msg, actual, expected) {\n        super(msg);\n        this.failedAssertion = {\n            actual,\n            expected,\n        };\n    }\n}\n/**\n * JWT errors\n */\nexport class JwtParseError extends JwtBaseError {\n    constructor(msg, error) {\n        const message = error != null ? `${msg}: ${error}` : msg;\n        super(message);\n    }\n}\nexport class ParameterValidationError extends JwtBaseError {\n}\nexport class JwtInvalidSignatureError extends JwtBaseError {\n}\nexport class JwtInvalidSignatureAlgorithmError extends FailedAssertionError {\n}\nexport class JwtInvalidClaimError extends FailedAssertionError {\n    withRawJwt({ header, payload }) {\n        this.rawJwt = {\n            header,\n            payload,\n        };\n        return this;\n    }\n}\nexport class JwtInvalidIssuerError extends JwtInvalidClaimError {\n}\nexport class JwtInvalidAudienceError extends JwtInvalidClaimError {\n}\nexport class JwtInvalidScopeError extends JwtInvalidClaimError {\n}\nexport class JwtExpiredError extends JwtInvalidClaimError {\n}\nexport class JwtNotBeforeError extends JwtInvalidClaimError {\n}\n/**\n * Amazon Cognito specific erros\n */\nexport class CognitoJwtInvalidGroupError extends JwtInvalidClaimError {\n}\nexport class CognitoJwtInvalidTokenUseError extends JwtInvalidClaimError {\n}\nexport class CognitoJwtInvalidClientIdError extends JwtInvalidClaimError {\n}\n/**\n * ASN.1 errors\n */\nexport class Asn1DecodingError extends JwtBaseError {\n}\n/**\n * JWK errors\n */\nexport class JwksValidationError extends JwtBaseError {\n}\nexport class JwkValidationError extends JwtBaseError {\n}\nexport class JwtWithoutValidKidError extends JwtBaseError {\n}\nexport class KidNotFoundInJwksError extends JwtBaseError {\n}\nexport class WaitPeriodNotYetEndedJwkError extends JwtBaseError {\n}\nexport class JwksNotAvailableInCacheError extends JwtBaseError {\n}\nexport class JwkInvalidUseError extends FailedAssertionError {\n}\nexport class JwkInvalidKtyError extends FailedAssertionError {\n}\n/**\n * HTTPS fetch errors\n */\nexport class FetchError extends JwtBaseError {\n    constructor(uri, msg) {\n        super(`Failed to fetch ${uri}: ${msg}`);\n    }\n}\nexport class NonRetryableFetchError extends FetchError {\n}\n/**\n * Web compatibility errors\n */\nexport class NotSupportedError extends JwtBaseError {\n}\n", "// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n// SPDX-License-Identifier: Apache-2.0\n//\n// Node.js implementations for the node-web-compatibility layer\nimport { createPublicKey, createVerify } from \"crypto\";\nimport { constructPublicKeyInDerFormat } from \"./asn1.js\";\nimport { fetchJson } from \"./https-node.js\";\n/**\n * Enum to map supported JWT signature algorithms with OpenSSL message digest algorithm names\n */\nvar JwtSignatureAlgorithms;\n(function (JwtSignatureAlgorithms) {\n    JwtSignatureAlgorithms[\"RS256\"] = \"RSA-SHA256\";\n    JwtSignatureAlgorithms[\"RS384\"] = \"RSA-SHA384\";\n    JwtSignatureAlgorithms[\"RS512\"] = \"RSA-SHA512\";\n})(JwtSignatureAlgorithms || (JwtSignatureAlgorithms = {}));\nexport const nodeWebCompat = {\n    fetchJson,\n    transformJwkToKeyObjectSync: (jwk) => createPublicKey({\n        key: constructPublicKeyInDerFormat(Buffer.from(jwk.n, \"base64\"), Buffer.from(jwk.e, \"base64\")),\n        format: \"der\",\n        type: \"spki\",\n    }),\n    transformJwkToKeyObjectAsync: async (jwk) => createPublicKey({\n        key: constructPublicKeyInDerFormat(Buffer.from(jwk.n, \"base64\"), Buffer.from(jwk.e, \"base64\")),\n        format: \"der\",\n        type: \"spki\",\n    }),\n    parseB64UrlString: (b64) => Buffer.from(b64, \"base64\").toString(\"utf8\"),\n    verifySignatureSync: ({ alg, keyObject, jwsSigningInput, signature }) => \n    // eslint-disable-next-line security/detect-object-injection\n    createVerify(JwtSignatureAlgorithms[alg])\n        .update(jwsSigningInput)\n        .verify(keyObject, signature, \"base64\"),\n    verifySignatureAsync: async ({ alg, keyObject, jwsSigningInput, signature, }) => \n    // eslint-disable-next-line security/detect-object-injection\n    createVerify(JwtSignatureAlgorithms[alg])\n        .update(jwsSigningInput)\n        .verify(keyObject, signature, \"base64\"),\n    defaultFetchTimeouts: {\n        socketIdle: 500,\n        response: 1500,\n    },\n    setTimeoutUnref: (...args) => setTimeout(...args).unref(),\n};\n", "// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n// SPDX-License-Identifier: Apache-2.0\n//\n// Utility to encode RSA public keys (a pair of modulus (n) and exponent (e)) into DER-encoding, per ASN.1 specification.\nimport { Asn1DecodingError } from \"./error.js\";\n/** Enum with possible values for supported ASN.1 classes */\nvar Asn1Class;\n(function (Asn1Class) {\n    Asn1Class[Asn1Class[\"Universal\"] = 0] = \"Universal\";\n})(Asn1Class || (Asn1Class = {}));\n/** Enum with possible values for supported ASN.1 encodings */\nvar Asn1Encoding;\n(function (Asn1Encoding) {\n    Asn1Encoding[Asn1Encoding[\"Primitive\"] = 0] = \"Primitive\";\n    Asn1Encoding[Asn1Encoding[\"Constructed\"] = 1] = \"Constructed\";\n})(Asn1Encoding || (Asn1Encoding = {}));\n/** Enum with possible values for supported ASN.1 tags */\nvar Asn1Tag;\n(function (Asn1Tag) {\n    Asn1Tag[Asn1Tag[\"BitString\"] = 3] = \"BitString\";\n    Asn1Tag[Asn1Tag[\"ObjectIdentifier\"] = 6] = \"ObjectIdentifier\";\n    Asn1Tag[Asn1Tag[\"Sequence\"] = 16] = \"Sequence\";\n    Asn1Tag[Asn1Tag[\"Null\"] = 5] = \"Null\";\n    Asn1Tag[Asn1Tag[\"Integer\"] = 2] = \"Integer\";\n})(Asn1Tag || (Asn1Tag = {}));\n/**\n * Encode an ASN.1 identifier per ASN.1 spec (DER-encoding)\n * See https://www.itu.int/ITU-T/studygroups/com17/languages/X.690-0207.pdf chapter 8.1.2\n *\n * @param identifier - The ASN.1 identifier\n * @returns The buffer\n */\nfunction encodeIdentifier(identifier) {\n    const identifierAsNumber = (identifier.class << 7) |\n        (identifier.primitiveOrConstructed << 5) |\n        identifier.tag;\n    return Buffer.from([identifierAsNumber]);\n}\n/**\n * Encode the length of an ASN.1 type per ASN.1 spec (DER-encoding)\n * See https://www.itu.int/ITU-T/studygroups/com17/languages/X.690-0207.pdf chapter 8.1.3\n *\n * @param length - The length of the ASN.1 type\n * @returns The buffer\n */\nfunction encodeLength(length) {\n    if (length < 128) {\n        return Buffer.from([length]);\n    }\n    const integers = [];\n    while (length > 0) {\n        integers.push(length % 256);\n        length = length >> 8;\n    }\n    integers.reverse();\n    return Buffer.from([128 | integers.length, ...integers]);\n}\n/**\n * Encode a buffer (that represent an integer) as integer per ASN.1 spec (DER-encoding)\n * See https://www.itu.int/ITU-T/studygroups/com17/languages/X.690-0207.pdf chapter 8.3\n *\n * @param buffer - The buffer that represent an integer to encode\n * @returns The buffer\n */\nfunction encodeBufferAsInteger(buffer) {\n    return Buffer.concat([\n        encodeIdentifier({\n            class: Asn1Class.Universal,\n            primitiveOrConstructed: Asn1Encoding.Primitive,\n            tag: Asn1Tag.Integer,\n        }),\n        encodeLength(buffer.length),\n        buffer,\n    ]);\n}\n/**\n * Encode an object identifier (a string such as \"1.2.840.113549.1.1.1\") per ASN.1 spec (DER-encoding)\n * See https://www.itu.int/ITU-T/studygroups/com17/languages/X.690-0207.pdf chapter 8.19\n *\n * @param oid - The object identifier to encode\n * @returns The buffer\n */\nfunction encodeObjectIdentifier(oid) {\n    const oidComponents = oid.split(\".\").map((i) => parseInt(i));\n    const firstSubidentifier = oidComponents[0] * 40 + oidComponents[1];\n    const subsequentSubidentifiers = oidComponents\n        .slice(2)\n        .reduce((expanded, component) => {\n        const bytes = [];\n        do {\n            bytes.push(component % 128);\n            component = component >> 7;\n        } while (component);\n        return expanded.concat(bytes.map((b, index) => (index ? b + 128 : b)).reverse());\n    }, []);\n    const oidBuffer = Buffer.from([\n        firstSubidentifier,\n        ...subsequentSubidentifiers,\n    ]);\n    return Buffer.concat([\n        encodeIdentifier({\n            class: Asn1Class.Universal,\n            primitiveOrConstructed: Asn1Encoding.Primitive,\n            tag: Asn1Tag.ObjectIdentifier,\n        }),\n        encodeLength(oidBuffer.length),\n        oidBuffer,\n    ]);\n}\n/**\n * Encode a buffer as bit string per ASN.1 spec (DER-encoding)\n * See https://www.itu.int/ITU-T/studygroups/com17/languages/X.690-0207.pdf chapter 8.6\n *\n * @param buffer - The buffer to encode\n * @returns The buffer\n */\nfunction encodeBufferAsBitString(buffer) {\n    const bitString = Buffer.concat([Buffer.from([0]), buffer]);\n    return Buffer.concat([\n        encodeIdentifier({\n            class: Asn1Class.Universal,\n            primitiveOrConstructed: Asn1Encoding.Primitive,\n            tag: Asn1Tag.BitString,\n        }),\n        encodeLength(bitString.length),\n        bitString,\n    ]);\n}\n/**\n * Encode a sequence of DER-encoded items per ASN.1 spec (DER-encoding)\n * See https://www.itu.int/ITU-T/studygroups/com17/languages/X.690-0207.pdf chapter 8.9\n *\n * @param sequenceItems - The sequence of DER-encoded items\n * @returns The buffer\n */\nfunction encodeSequence(sequenceItems) {\n    const concatenated = Buffer.concat(sequenceItems);\n    return Buffer.concat([\n        encodeIdentifier({\n            class: Asn1Class.Universal,\n            primitiveOrConstructed: Asn1Encoding.Constructed,\n            tag: Asn1Tag.Sequence,\n        }),\n        encodeLength(concatenated.length),\n        concatenated,\n    ]);\n}\n/**\n * Encode null per ASN.1 spec (DER-encoding)\n * See https://www.itu.int/ITU-T/studygroups/com17/languages/X.690-0207.pdf chapter 8.8\n *\n * @returns The buffer\n */\nfunction encodeNull() {\n    return Buffer.concat([\n        encodeIdentifier({\n            class: Asn1Class.Universal,\n            primitiveOrConstructed: Asn1Encoding.Primitive,\n            tag: Asn1Tag.Null,\n        }),\n        encodeLength(0),\n    ]);\n}\n/**\n * RSA encryption object identifier constant\n *\n * From: https://tools.ietf.org/html/rfc8017\n *\n * pkcs-1    OBJECT IDENTIFIER ::= {\n *     iso(1) member-body(2) us(840) rsadsi(113549) pkcs(1) 1\n * }\n *\n * -- When rsaEncryption is used in an AlgorithmIdentifier,\n * -- the parameters MUST be present and MUST be NULL.\n * --\n * rsaEncryption    OBJECT IDENTIFIER ::= { pkcs-1 1 }\n *\n * See also: http://www.oid-info.com/get/1.2.840.113549.1.1.1\n */\nconst ALGORITHM_RSA_ENCRYPTION = encodeSequence([\n    encodeObjectIdentifier(\"1.2.840.113549.1.1.1\"),\n    encodeNull(), // parameters\n]);\n/**\n * Transform an RSA public key, which is a pair of modulus (n) and exponent (e),\n *  into a buffer per ASN.1 spec (DER-encoding)\n *\n * @param n - The modulus of the public key as buffer\n * @param e - The exponent of the public key as buffer\n * @returns The buffer, which is the public key encoded per ASN.1 spec (DER-encoding)\n */\nexport function constructPublicKeyInDerFormat(n, e) {\n    return encodeSequence([\n        ALGORITHM_RSA_ENCRYPTION,\n        encodeBufferAsBitString(encodeSequence([encodeBufferAsInteger(n), encodeBufferAsInteger(e)])),\n    ]);\n}\n/**\n * Decode an ASN.1 identifier (a number) into its parts: class, primitiveOrConstructed, tag\n * See https://www.itu.int/ITU-T/studygroups/com17/languages/X.690-0207.pdf chapter 8.1.2\n *\n * @param identifier - The identifier\n * @returns An object with properties class, primitiveOrConstructed, tag\n */\nfunction decodeIdentifier(identifier) {\n    if (identifier >> 3 === 0b11111) {\n        throw new Asn1DecodingError(\"Decoding of identifier with tag > 30 not implemented\");\n    }\n    return {\n        class: identifier >> 6,\n        primitiveOrConstructed: (identifier >> 5) & 0b001,\n        tag: identifier & 0b11111, // bit 1-5\n    };\n}\n/**\n * Decode an ASN.1 block of length value combinations,\n * and return the length and byte range of the first length value combination.\n * See https://www.itu.int/ITU-T/studygroups/com17/languages/X.690-0207.pdf chapter 8.1.3 - 8.1.5\n *\n * @param blockOfLengthValues - The ASN.1 length value\n * @returns The length and byte range of the first included length value\n */\nfunction decodeLengthValue(blockOfLengthValues) {\n    if (!(blockOfLengthValues[0] & 0b10000000)) {\n        return {\n            length: blockOfLengthValues[0],\n            firstByteOffset: 1,\n            lastByteOffset: 1 + blockOfLengthValues[0],\n        };\n    }\n    const nrLengthOctets = blockOfLengthValues[0] & 0b01111111;\n    const length = Buffer.from(blockOfLengthValues.subarray(1, 1 + 1 + nrLengthOctets)).readUIntBE(0, nrLengthOctets);\n    return {\n        length,\n        firstByteOffset: 1 + nrLengthOctets,\n        lastByteOffset: 1 + nrLengthOctets + length,\n    };\n}\n/**\n * Decode an ASN.1 sequence into its constituent parts, each part being an identifier-length-value triplet\n * See https://www.itu.int/ITU-T/studygroups/com17/languages/X.690-0207.pdf chapter 8.9\n *\n * @param sequenceValue - The ASN.1 sequence value\n * @returns Array of identifier-length-value triplets\n */\nfunction decodeSequence(sequence) {\n    const { tag } = decodeIdentifier(sequence[0]);\n    if (tag !== Asn1Tag.Sequence) {\n        throw new Asn1DecodingError(`Expected a sequence to decode, but got tag ${tag}`);\n    }\n    const { firstByteOffset, lastByteOffset } = decodeLengthValue(sequence.subarray(1));\n    const sequenceValue = sequence.subarray(1 + firstByteOffset, 1 + 1 + lastByteOffset);\n    const parts = [];\n    let offset = 0;\n    while (offset < sequenceValue.length) {\n        // Silence false postive: accessing an octet in a Buffer at a particular index\n        // is to be done with index operator: [index]\n        // eslint-disable-next-line security/detect-object-injection\n        const identifier = decodeIdentifier(sequenceValue[offset]);\n        const next = decodeLengthValue(sequenceValue.subarray(offset + 1));\n        const value = sequenceValue.subarray(offset + 1 + next.firstByteOffset, offset + 1 + next.lastByteOffset);\n        parts.push({ identifier, length: next.length, value });\n        offset += 1 + next.lastByteOffset;\n    }\n    return parts;\n}\n/**\n * Decode an ASN.1 sequence that is wrapped in a bit string\n * (Which is the way RSA public keys are encoded in ASN.1 DER-encoding)\n * See https://www.itu.int/ITU-T/studygroups/com17/languages/X.690-0207.pdf chapter 8.6 and 8.9\n *\n * @param bitStringValue - The ASN.1 bit string value\n * @returns Array of identifier-length-value triplets\n */\nfunction decodeBitStringWrappedSequenceValue(bitStringValue) {\n    const wrappedSequence = bitStringValue.subarray(1);\n    return decodeSequence(wrappedSequence);\n}\n/**\n * Decode an ASN.1 DER-encoded public key, into its modulus (n) and exponent (e)\n *\n * @param publicKey - The ASN.1 DER-encoded public key\n * @returns Object with modulus (n) and exponent (e)\n */\nexport function deconstructPublicKeyInDerFormat(publicKey) {\n    const [, pubkeyinfo] = decodeSequence(publicKey);\n    const [n, e] = decodeBitStringWrappedSequenceValue(pubkeyinfo.value);\n    return { n: n.value, e: e.value };\n}\n", "// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n// SPDX-License-Identifier: Apache-2.0\n//\n// NodeJS implementation for fetching JSON documents over HTTPS\nimport { request } from \"https\";\nimport { validateHttpsJsonResponse } from \"./https-common.js\";\nimport { pipeline } from \"stream\";\nimport { TextDecoder } from \"util\";\nimport { safeJsonParse } from \"./safe-json-parse.js\";\nimport { FetchError, NonRetryableFetchError } from \"./error.js\";\n/**\n * Execute a HTTPS request\n * @param uri - The URI\n * @param requestOptions - The RequestOptions to use\n * @param data - Data to send to the URI (e.g. POST data)\n * @returns - The response as parsed JSON\n */\nexport async function fetchJson(uri, requestOptions, data) {\n    let responseTimeout;\n    return new Promise((resolve, reject) => {\n        const req = request(uri, {\n            method: \"GET\",\n            ...requestOptions,\n        }, (response) => {\n            // Capture response data\n            // @types/node is incomplete so cast to any\n            // eslint-disable-next-line @typescript-eslint/no-explicit-any\n            pipeline([\n                response,\n                getJsonDestination(uri, response.statusCode, response.headers),\n            ], done);\n        });\n        if (requestOptions?.responseTimeout) {\n            responseTimeout = setTimeout(() => done(new FetchError(uri, `Response time-out (after ${requestOptions.responseTimeout} ms.)`)), requestOptions.responseTimeout);\n            responseTimeout.unref(); // Don't block Node from exiting\n        }\n        function done(...args) {\n            if (responseTimeout)\n                clearTimeout(responseTimeout);\n            if (args[0] == null) {\n                resolve(args[1]);\n                return;\n            }\n            // In case of errors, let the Agent (if any) know to abandon the socket\n            // This is probably best, because the socket may have become stale\n            /* istanbul ignore next */\n            req.socket?.emit(\"agentRemove\");\n            // Turn error into FetchError so the URI is nicely captured in the message\n            let error = args[0];\n            if (!(error instanceof FetchError)) {\n                error = new FetchError(uri, error.message);\n            }\n            req.destroy();\n            reject(error);\n        }\n        // Handle errors while sending request\n        req.on(\"error\", done);\n        // Signal end of request (include optional data)\n        req.end(data);\n    });\n}\n/**\n * Ensures the HTTPS response contains valid JSON\n *\n * @param uri - The URI you were requesting\n * @param statusCode - The response status code to your HTTPS request\n * @param headers - The response headers to your HTTPS request\n *\n * @returns - Async function that can be used as destination in a stream.pipeline, it will return the JSON, if valid, or throw an error otherwise\n */\nfunction getJsonDestination(uri, statusCode, headers) {\n    return async (responseIterable) => {\n        validateHttpsJsonResponse(uri, statusCode, headers[\"content-type\"]);\n        const collected = [];\n        for await (const chunk of responseIterable) {\n            collected.push(chunk);\n        }\n        try {\n            return safeJsonParse(new TextDecoder(\"utf8\", { fatal: true, ignoreBOM: true }).decode(Buffer.concat(collected)));\n        }\n        catch (err) {\n            throw new NonRetryableFetchError(uri, err);\n        }\n    };\n}\n", "// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n// SPDX-License-Identifier: Apache-2.0\n//\n// Lower-level HTTPS functionalities, common for Node.js and Web\nimport { FetchError, NonRetryableFetchError } from \"./error.js\";\n/**\n * Sanity check a HTTPS response where we expect to get JSON data back\n *\n * @param uri the uri that was being requested\n * @param statusCode the HTTP status code, should be 200\n * @param contentType the value of the \"Content-Type\" header in the response, should start with \"application/json\"\n * @returns void - throws an error if the status code or content type aren't as expected\n */\nexport function validateHttpsJsonResponse(uri, statusCode, contentType) {\n    if (statusCode === 429) {\n        throw new FetchError(uri, \"Too many requests\");\n    }\n    else if (statusCode !== 200) {\n        throw new NonRetryableFetchError(uri, `Status code is ${statusCode}, expected 200`);\n    }\n    if (!contentType ||\n        !contentType.toLowerCase().startsWith(\"application/json\")) {\n        throw new NonRetryableFetchError(uri, `Content-type is \"${contentType}\", expected \"application/json\"`);\n    }\n}\n", "// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n// SPDX-License-Identifier: Apache-2.0\n//\n// Utility to parse JSON safely\n/**\n * Check if a piece of JSON is a JSON object, and not e.g. a mere string or null\n *\n * @param j - the JSON\n */\nexport function isJsonObject(j) {\n    // It is not enough to check that `typeof j === \"object\"`\n    // because in JS `typeof null` is also \"object\", and so is `typeof []`.\n    // So we need to check that j is an object, and not null, and not an array\n    return typeof j === \"object\" && !Array.isArray(j) && j !== null;\n}\n/**\n * Parse a string as JSON, while removing __proto__ and constructor, so JS prototype pollution is prevented\n *\n * @param s - the string to JSON parse\n */\nexport function safeJsonParse(s) {\n    return JSON.parse(s, (_, value) => {\n        if (typeof value === \"object\" && !Array.isArray(value) && value !== null) {\n            delete value.__proto__;\n            delete value.constructor;\n        }\n        return value;\n    });\n}\n", "// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n// SPDX-License-Identifier: Apache-2.0\n//\n// Utilities for fetching the JWKS URI, to get the public keys with which to verify JWTs\nimport { NonRetryableFetchError } from \"./error.js\";\nimport { nodeWebCompat } from \"#node-web-compat\";\n/**\n * Execute a HTTPS request\n * @param uri - The URI\n * @param requestOptions - The RequestOptions to use (depending on the runtime context, either Node.js RequestOptions or Web Fetch init)\n * @param data - Data to send to the URI (e.g. POST data)\n * @returns - The response as parsed JSON\n */\nexport const fetchJson = nodeWebCompat.fetchJson;\n/**\n * HTTPS Fetcher for URIs with JSON body\n *\n * @param defaultRequestOptions - The default RequestOptions to use on individual HTTPS requests\n */\nexport class SimpleJsonFetcher {\n    constructor(props) {\n        this.defaultRequestOptions = {\n            timeout: nodeWebCompat.defaultFetchTimeouts.socketIdle,\n            responseTimeout: nodeWebCompat.defaultFetchTimeouts.response,\n            ...props?.defaultRequestOptions,\n        };\n    }\n    /**\n     * Execute a HTTPS request (with 1 immediate retry in case of errors)\n     * @param uri - The URI\n     * @param requestOptions - The RequestOptions to use\n     * @param data - Data to send to the URI (e.g. POST data)\n     * @returns - The response as parsed JSON\n     */\n    async fetch(uri, requestOptions, data) {\n        requestOptions = { ...this.defaultRequestOptions, ...requestOptions };\n        try {\n            return await fetchJson(uri, requestOptions, data);\n        }\n        catch (err) {\n            if (err instanceof NonRetryableFetchError) {\n                throw err;\n            }\n            // Retry once, immediately\n            return fetchJson(uri, requestOptions, data);\n        }\n    }\n}\n", "// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n// SPDX-License-Identifier: Apache-2.0\n//\n// Utilities to assert that supplied values match with expected values\nimport { FailedAssertionError } from \"./error.js\";\n/**\n * Assert value is a non-empty string and equal to the expected value,\n * or throw an error otherwise\n *\n * @param name - Name for the value being checked\n * @param actual - The value to check\n * @param expected - The expected value\n * @param errorConstructor - Constructor for the concrete error to be thrown\n */\nexport function assertStringEquals(name, actual, expected, errorConstructor = FailedAssertionError) {\n    if (!actual) {\n        throw new errorConstructor(`Missing ${name}. Expected: ${expected}`, actual, expected);\n    }\n    if (typeof actual !== \"string\") {\n        throw new errorConstructor(`${name} is not of type string`, actual, expected);\n    }\n    if (expected !== actual) {\n        throw new errorConstructor(`${name} not allowed: ${actual}. Expected: ${expected}`, actual, expected);\n    }\n}\n/**\n * Assert value is a non-empty string and is indeed one of the expected values,\n * or throw an error otherwise\n *\n * @param name - Name for the value being checked\n * @param actual - The value to check\n * @param expected - The array of expected values. For your convenience you can provide\n * @param errorConstructor - Constructor for the concrete error to be thrown\n * a string here as well, which will mean an array with just that string\n */\nexport function assertStringArrayContainsString(name, actual, expected, errorConstructor = FailedAssertionError) {\n    if (!actual) {\n        throw new errorConstructor(`Missing ${name}. ${expectationMessage(expected)}`, actual, expected);\n    }\n    if (typeof actual !== \"string\") {\n        throw new errorConstructor(`${name} is not of type string`, actual, expected);\n    }\n    return assertStringArraysOverlap(name, actual, expected, errorConstructor);\n}\n/**\n * Assert value is an array of strings, where at least one of the strings is indeed one of the expected values,\n * or throw an error otherwise\n *\n * @param name - Name for the value being checked\n * @param actual - The value to check, must be an array of strings, or a single string (which will be treated\n * as an array with just that string)\n * @param expected - The array of expected values. For your convenience you can provide\n * a string here as well, which will mean an array with just that string\n * @param errorConstructor - Constructor for the concrete error to be thrown\n */\nexport function assertStringArraysOverlap(name, actual, expected, errorConstructor = FailedAssertionError) {\n    if (!actual) {\n        throw new errorConstructor(`Missing ${name}. ${expectationMessage(expected)}`, actual, expected);\n    }\n    const expectedAsSet = new Set(Array.isArray(expected) ? expected : [expected]);\n    if (typeof actual === \"string\") {\n        actual = [actual];\n    }\n    if (!Array.isArray(actual)) {\n        throw new errorConstructor(`${name} is not an array`, actual, expected);\n    }\n    const overlaps = actual.some((actualItem) => {\n        if (typeof actualItem !== \"string\") {\n            throw new errorConstructor(`${name} includes elements that are not of type string`, actual, expected);\n        }\n        return expectedAsSet.has(actualItem);\n    });\n    if (!overlaps) {\n        throw new errorConstructor(`${name} not allowed: ${actual.join(\", \")}. ${expectationMessage(expected)}`, actual, expected);\n    }\n}\n/**\n * Get a nicely readable message regarding an expectation\n *\n * @param expected - The expected value.\n */\nfunction expectationMessage(expected) {\n    if (Array.isArray(expected)) {\n        if (expected.length > 1) {\n            return `Expected one of: ${expected.join(\", \")}`;\n        }\n        return `Expected: ${expected[0]}`;\n    }\n    return `Expected: ${expected}`;\n}\n/**\n * Assert value is not a promise, or throw an error otherwise\n *\n * @param actual - The value to check\n * @param errorFactory - Function that returns the error to be thrown\n */\nexport function assertIsNotPromise(actual, errorFactory) {\n    if (actual && typeof actual.then === \"function\") {\n        throw errorFactory();\n    }\n}\n", "// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n// SPDX-License-Identifier: Apache-2.0\nimport { SimpleJsonFetcher, fetchJson } from \"./https.js\";\nimport { isJsonObject } from \"./safe-json-parse.js\";\nimport { JwkValidationError, JwksNotAvailableInCacheError, JwksValidationError, KidNotFoundInJwksError, WaitPeriodNotYetEndedJwkError, JwtWithoutValidKidError, JwkInvalidUseError, JwkInvalidKtyError, } from \"./error.js\";\nimport { nodeWebCompat } from \"#node-web-compat\";\nimport { assertStringEquals } from \"./assert.js\";\nconst optionalJwkFieldNames = [\n    \"use\",\n    \"alg\",\n    \"kid\",\n    \"n\",\n    \"e\", // https://datatracker.ietf.org/doc/html/rfc7518#section-6.3.1.2\n];\nconst mandatoryJwkFieldNames = [\n    \"kty\", // https://datatracker.ietf.org/doc/html/rfc7517#section-4.1\n];\nexport function findJwkInJwks(jwks, kid) {\n    return jwks.keys.find((jwk) => jwk.kid != null && jwk.kid === kid);\n}\nexport async function fetchJwks(jwksUri) {\n    const jwks = await fetchJson(jwksUri);\n    assertIsJwks(jwks);\n    return jwks;\n}\nexport async function fetchJwk(jwksUri, decomposedJwt) {\n    if (!decomposedJwt.header.kid) {\n        throw new JwtWithoutValidKidError(\"JWT header does not have valid kid claim\");\n    }\n    const jwks = await fetchJwks(jwksUri);\n    const jwk = findJwkInJwks(jwks, decomposedJwt.header.kid);\n    if (!jwk) {\n        throw new KidNotFoundInJwksError(`JWK for kid \"${decomposedJwt.header.kid}\" not found in the JWKS`);\n    }\n    return jwk;\n}\nexport function assertIsJwks(jwks) {\n    if (!jwks) {\n        throw new JwksValidationError(\"JWKS empty\");\n    }\n    if (!isJsonObject(jwks)) {\n        throw new JwksValidationError(\"JWKS should be an object\");\n    }\n    if (!Object.keys(jwks).includes(\"keys\")) {\n        throw new JwksValidationError(\"JWKS does not include keys\");\n    }\n    if (!Array.isArray(jwks.keys)) {\n        throw new JwksValidationError(\"JWKS keys should be an array\");\n    }\n    for (const jwk of jwks.keys) {\n        assertIsJwk(jwk);\n    }\n}\nexport function assertIsRsaSignatureJwk(jwk) {\n    // Check JWK use\n    assertStringEquals(\"JWK use\", jwk.use, \"sig\", JwkInvalidUseError);\n    // Check JWK kty\n    assertStringEquals(\"JWK kty\", jwk.kty, \"RSA\", JwkInvalidKtyError);\n    // Check modulus (n) has a value\n    if (!jwk.n)\n        throw new JwkValidationError(\"Missing modulus (n)\");\n    // Check exponent (e) has a value\n    if (!jwk.e)\n        throw new JwkValidationError(\"Missing exponent (e)\");\n}\nexport function assertIsJwk(jwk) {\n    if (!jwk) {\n        throw new JwkValidationError(\"JWK empty\");\n    }\n    if (!isJsonObject(jwk)) {\n        throw new JwkValidationError(\"JWK should be an object\");\n    }\n    for (const field of mandatoryJwkFieldNames) {\n        // disable eslint rule because `field` is trusted\n        // eslint-disable-next-line security/detect-object-injection\n        if (typeof jwk[field] !== \"string\") {\n            throw new JwkValidationError(`JWK ${field} should be a string`);\n        }\n    }\n    for (const field of optionalJwkFieldNames) {\n        // disable eslint rule because `field` is trusted\n        // eslint-disable-next-line security/detect-object-injection\n        if (field in jwk && typeof jwk[field] !== \"string\") {\n            throw new JwkValidationError(`JWK ${field} should be a string`);\n        }\n    }\n}\nexport function isJwks(jwks) {\n    try {\n        assertIsJwks(jwks);\n        return true;\n    }\n    catch {\n        return false;\n    }\n}\nexport function isJwk(jwk) {\n    try {\n        assertIsJwk(jwk);\n        return true;\n    }\n    catch {\n        return false;\n    }\n}\nexport class SimplePenaltyBox {\n    constructor(props) {\n        this.waitingUris = new Map();\n        this.waitSeconds = props?.waitSeconds ?? 10;\n    }\n    async wait(jwksUri) {\n        // SimplePenaltyBox does not actually wait but bluntly throws an error\n        // Any waiting and retries are expected to be done upstream (e.g. in the browser / app)\n        if (this.waitingUris.has(jwksUri)) {\n            throw new WaitPeriodNotYetEndedJwkError(\"Not allowed to fetch JWKS yet, still waiting for back off period to end\");\n        }\n    }\n    release(jwksUri) {\n        const i = this.waitingUris.get(jwksUri);\n        if (i) {\n            clearTimeout(i);\n            this.waitingUris.delete(jwksUri);\n        }\n    }\n    registerFailedAttempt(jwksUri) {\n        const i = nodeWebCompat.setTimeoutUnref(() => {\n            this.waitingUris.delete(jwksUri);\n        }, this.waitSeconds * 1000);\n        this.waitingUris.set(jwksUri, i);\n    }\n    registerSuccessfulAttempt(jwksUri) {\n        this.release(jwksUri);\n    }\n}\nexport class SimpleJwksCache {\n    constructor(props) {\n        this.jwksCache = new Map();\n        this.fetchingJwks = new Map();\n        this.penaltyBox = props?.penaltyBox ?? new SimplePenaltyBox();\n        this.fetcher = props?.fetcher ?? new SimpleJsonFetcher();\n    }\n    addJwks(jwksUri, jwks) {\n        this.jwksCache.set(jwksUri, jwks);\n    }\n    async getJwks(jwksUri) {\n        const existingFetch = this.fetchingJwks.get(jwksUri);\n        if (existingFetch) {\n            return existingFetch;\n        }\n        const jwksPromise = this.fetcher.fetch(jwksUri).then((res) => {\n            assertIsJwks(res);\n            return res;\n        });\n        this.fetchingJwks.set(jwksUri, jwksPromise);\n        let jwks;\n        try {\n            jwks = await jwksPromise;\n        }\n        finally {\n            this.fetchingJwks.delete(jwksUri);\n        }\n        this.jwksCache.set(jwksUri, jwks);\n        return jwks;\n    }\n    getCachedJwk(jwksUri, decomposedJwt) {\n        if (typeof decomposedJwt.header.kid !== \"string\") {\n            throw new JwtWithoutValidKidError(\"JWT header does not have valid kid claim\");\n        }\n        if (!this.jwksCache.has(jwksUri)) {\n            throw new JwksNotAvailableInCacheError(`JWKS for uri ${jwksUri} not yet available in cache`);\n        }\n        const jwk = findJwkInJwks(this.jwksCache.get(jwksUri), decomposedJwt.header.kid);\n        if (!jwk) {\n            throw new KidNotFoundInJwksError(`JWK for kid ${decomposedJwt.header.kid} not found in the JWKS`);\n        }\n        return jwk;\n    }\n    async getJwk(jwksUri, decomposedJwt) {\n        if (typeof decomposedJwt.header.kid !== \"string\") {\n            throw new JwtWithoutValidKidError(\"JWT header does not have valid kid claim\");\n        }\n        // Try to get JWK from cache:\n        const cachedJwks = this.jwksCache.get(jwksUri);\n        if (cachedJwks) {\n            const cachedJwk = findJwkInJwks(cachedJwks, decomposedJwt.header.kid);\n            if (cachedJwk) {\n                return cachedJwk;\n            }\n        }\n        // Await any wait period that is currently in effect\n        // This prevents us from flooding the JWKS URI with requests\n        await this.penaltyBox.wait(jwksUri, decomposedJwt.header.kid);\n        // Fetch the JWKS and (try to) locate the JWK\n        const jwks = await this.getJwks(jwksUri);\n        const jwk = findJwkInJwks(jwks, decomposedJwt.header.kid);\n        // If the JWK could not be located, someone might be messing around with us\n        // Register the failed attempt with the penaltyBox, so it can enforce a wait period\n        // before trying again next time (instead of flooding the JWKS URI with requests)\n        if (!jwk) {\n            this.penaltyBox.registerFailedAttempt(jwksUri, decomposedJwt.header.kid);\n            throw new KidNotFoundInJwksError(`JWK for kid \"${decomposedJwt.header.kid}\" not found in the JWKS`);\n        }\n        else {\n            this.penaltyBox.registerSuccessfulAttempt(jwksUri, decomposedJwt.header.kid);\n        }\n        return jwk;\n    }\n}\n", "// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n// SPDX-License-Identifier: Apache-2.0\nexport const supportedSignatureAlgorithms = [\n    \"RS256\",\n    \"RS384\",\n    \"RS512\",\n];\n", "// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n// SPDX-License-Identifier: Apache-2.0\nimport { assertStringArrayContainsString, assertStringArraysOverlap, } from \"./assert.js\";\nimport { safeJsonParse, isJsonObject } from \"./safe-json-parse.js\";\nimport { JwtExpiredError, JwtNotBeforeError, JwtInvalidIssuerError, JwtInvalidAudienceError, JwtInvalidScopeError, JwtParseError, ParameterValidationError, } from \"./error.js\";\nimport { nodeWebCompat } from \"#node-web-compat\";\n/**\n * Assert that the argument is a valid JWT header object.\n * Throws an error in case it is not.\n *\n * @param header\n * @returns void\n */\nfunction assertJwtHeader(header) {\n    if (!isJsonObject(header)) {\n        throw new JwtParseError(\"JWT header is not an object\");\n    }\n    if (header.alg !== undefined && typeof header.alg !== \"string\") {\n        throw new JwtParseError(\"JWT header alg claim is not a string\");\n    }\n    if (header.kid !== undefined && typeof header.kid !== \"string\") {\n        throw new JwtParseError(\"JWT header kid claim is not a string\");\n    }\n}\n/**\n * Assert that the argument is a valid JWT payload object.\n * Throws an error in case it is not.\n *\n * @param payload\n * @returns void\n */\nfunction assertJwtPayload(payload) {\n    if (!isJsonObject(payload)) {\n        throw new JwtParseError(\"JWT payload is not an object\");\n    }\n    if (payload.exp !== undefined && !Number.isFinite(payload.exp)) {\n        throw new JwtParseError(\"JWT payload exp claim is not a number\");\n    }\n    if (payload.iss !== undefined && typeof payload.iss !== \"string\") {\n        throw new JwtParseError(\"JWT payload iss claim is not a string\");\n    }\n    if (payload.aud !== undefined &&\n        typeof payload.aud !== \"string\" &&\n        (!Array.isArray(payload.aud) ||\n            payload.aud.some((aud) => typeof aud !== \"string\"))) {\n        throw new JwtParseError(\"JWT payload aud claim is not a string or array of strings\");\n    }\n    if (payload.nbf !== undefined && !Number.isFinite(payload.nbf)) {\n        throw new JwtParseError(\"JWT payload nbf claim is not a number\");\n    }\n    if (payload.iat !== undefined && !Number.isFinite(payload.iat)) {\n        throw new JwtParseError(\"JWT payload iat claim is not a number\");\n    }\n    if (payload.scope !== undefined && typeof payload.scope !== \"string\") {\n        throw new JwtParseError(\"JWT payload scope claim is not a string\");\n    }\n    if (payload.jti !== undefined && typeof payload.jti !== \"string\") {\n        throw new JwtParseError(\"JWT payload jti claim is not a string\");\n    }\n}\n/**\n * Sanity check, decompose and JSON parse a JWT string into its constituent parts:\n * - header object\n * - payload object\n * - signature string\n *\n * @param jwt The JWT (as string)\n * @returns the decomposed JWT\n */\nexport function decomposeJwt(jwt) {\n    // Sanity checks on JWT\n    if (!jwt) {\n        throw new JwtParseError(\"Empty JWT\");\n    }\n    if (typeof jwt !== \"string\") {\n        throw new JwtParseError(\"JWT is not a string\");\n    }\n    if (!jwt.match(/^[A-Za-z0-9_-]+\\.[A-Za-z0-9_-]+\\.[A-Za-z0-9_-]+$/)) {\n        throw new JwtParseError(\"JWT string does not consist of exactly 3 parts (header, payload, signature)\");\n    }\n    const [headerB64, payloadB64, signatureB64] = jwt.split(\".\");\n    // B64 decode header and payload\n    const [headerString, payloadString] = [headerB64, payloadB64].map(nodeWebCompat.parseB64UrlString);\n    // Parse header\n    let header;\n    try {\n        header = safeJsonParse(headerString);\n    }\n    catch (err) {\n        throw new JwtParseError(\"Invalid JWT. Header is not a valid JSON object\", err);\n    }\n    assertJwtHeader(header);\n    // parse payload\n    let payload;\n    try {\n        payload = safeJsonParse(payloadString);\n    }\n    catch (err) {\n        throw new JwtParseError(\"Invalid JWT. Payload is not a valid JSON object\", err);\n    }\n    assertJwtPayload(payload);\n    return {\n        header,\n        headerB64,\n        payload,\n        payloadB64,\n        signatureB64,\n    };\n}\n/**\n * Validate JWT payload fields. Throws an error in case there's any validation issue.\n *\n * @param payload The (JSON parsed) JWT payload\n * @param options The options to use during validation\n * @returns void\n */\nexport function validateJwtFields(payload, options) {\n    // Check expiry\n    if (payload.exp !== undefined) {\n        if (payload.exp + (options.graceSeconds ?? 0) < Date.now() / 1000) {\n            throw new JwtExpiredError(`Token expired at ${new Date(payload.exp * 1000).toISOString()}`, payload.exp);\n        }\n    }\n    // Check not before\n    if (payload.nbf !== undefined) {\n        if (payload.nbf - (options.graceSeconds ?? 0) > Date.now() / 1000) {\n            throw new JwtNotBeforeError(`Token can't be used before ${new Date(payload.nbf * 1000).toISOString()}`, payload.nbf);\n        }\n    }\n    // Check JWT issuer\n    if (options.issuer !== null) {\n        if (options.issuer === undefined) {\n            throw new ParameterValidationError(\"issuer must be provided or set to null explicitly\");\n        }\n        assertStringArrayContainsString(\"Issuer\", payload.iss, options.issuer, JwtInvalidIssuerError);\n    }\n    // Check audience\n    if (options.audience !== null) {\n        if (options.audience === undefined) {\n            throw new ParameterValidationError(\"audience must be provided or set to null explicitly\");\n        }\n        assertStringArraysOverlap(\"Audience\", payload.aud, options.audience, JwtInvalidAudienceError);\n    }\n    // Check scope\n    if (options.scope != null) {\n        assertStringArraysOverlap(\"Scope\", payload.scope?.split(\" \"), options.scope, JwtInvalidScopeError);\n    }\n}\n", "// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n// SPDX-License-Identifier: Apache-2.0\nimport { SimpleJwksCache, isJwk, isJwks, fetchJwk, assertIsRsaSignatureJwk, findJwkInJwks, } from \"./jwk.js\";\nimport { assertIsNotPromise, assertStringArrayContainsString, assertStringEquals, } from \"./assert.js\";\nimport { supportedSignatureAlgorithms, } from \"./jwt-model.js\";\nimport { decomposeJwt, validateJwtFields } from \"./jwt.js\";\nimport { JwtInvalidClaimError, JwtInvalidIssuerError, JwtInvalidSignatureAlgorithmError, JwtInvalidSignatureError, KidNotFoundInJwksError, ParameterValidationError, } from \"./error.js\";\nimport { nodeWebCompat } from \"#node-web-compat\";\n/**\n * Sanity check the JWT header and the selected JWK\n *\n * @param header: the JWT header (decoded and JSON parsed)\n * @param jwk: the JWK\n */\nfunction validateJwtHeaderAndJwk(header, jwk) {\n    // Check that the JWK is in fact a JWK for RSA signatures\n    assertIsRsaSignatureJwk(jwk);\n    // Check that JWT signature algorithm matches JWK\n    if (jwk.alg) {\n        assertStringEquals(\"JWT signature algorithm\", header.alg, jwk.alg, JwtInvalidSignatureAlgorithmError);\n    }\n    // Check JWT signature algorithm is one of the supported signature algorithms\n    assertStringArrayContainsString(\"JWT signature algorithm\", header.alg, supportedSignatureAlgorithms, JwtInvalidSignatureAlgorithmError);\n}\n/**\n * Verify a JWT asynchronously (thus allowing for the JWKS to be fetched from the JWKS URI)\n *\n * @param jwt The JWT\n * @param jwksUri The JWKS URI, where the JWKS can be fetched from\n * @param options Verification options\n * @returns Promise that resolves to the payload of the JWT\u2013\u2013if the JWT is valid, otherwise the promise rejects\n */\nexport async function verifyJwt(jwt, jwksUri, options) {\n    return verifyDecomposedJwt(decomposeJwt(jwt), jwksUri, options);\n}\n/**\n * Verify (asynchronously) a JWT that is already decomposed (by function `decomposeJwt`)\n *\n * @param decomposedJwt The decomposed JWT\n * @param jwksUri The JWKS URI, where the JWKS can be fetched from\n * @param options Verification options\n * @param jwkFetcher A function that can execute the fetch of the JWKS from the JWKS URI\n * @param transformJwkToKeyObjectFn A function that can transform a JWK into a crypto native key object\n * @returns Promise that resolves to the payload of the JWT\u2013\u2013if the JWT is valid, otherwise the promise rejects\n */\nasync function verifyDecomposedJwt(decomposedJwt, jwksUri, options, jwkFetcher = fetchJwk, transformJwkToKeyObjectFn = nodeWebCompat.transformJwkToKeyObjectAsync) {\n    const { header, headerB64, payload, payloadB64, signatureB64 } = decomposedJwt;\n    const jwk = await jwkFetcher(jwksUri, decomposedJwt);\n    validateJwtHeaderAndJwk(decomposedJwt.header, jwk);\n    // Transform the JWK to native key format, that can be used with verifySignature\n    const keyObject = await transformJwkToKeyObjectFn(jwk, header.alg, payload.iss);\n    // Verify the JWT signature\n    const valid = await nodeWebCompat.verifySignatureAsync({\n        jwsSigningInput: `${headerB64}.${payloadB64}`,\n        signature: signatureB64,\n        alg: header.alg,\n        keyObject,\n    });\n    if (!valid) {\n        throw new JwtInvalidSignatureError(\"Invalid signature\");\n    }\n    try {\n        validateJwtFields(payload, options);\n        if (options.customJwtCheck) {\n            await options.customJwtCheck({ header, payload, jwk });\n        }\n    }\n    catch (err) {\n        if (options.includeRawJwtInErrors && err instanceof JwtInvalidClaimError) {\n            throw err.withRawJwt(decomposedJwt);\n        }\n        throw err;\n    }\n    return payload;\n}\n/**\n * Verify a JWT synchronously, using a JWKS or JWK that has already been fetched\n *\n * @param jwt The JWT\n * @param jwkOrJwks The JWKS that includes the right JWK (indexed by kid). Alternatively, provide the right JWK directly\n * @param options Verification options\n * @param transformJwkToKeyObjectFn A function that can transform a JWK into a crypto native key object\n * @returns The (JSON parsed) payload of the JWT\u2013\u2013if the JWT is valid, otherwise an error is thrown\n */\nexport function verifyJwtSync(jwt, jwkOrJwks, options, transformJwkToKeyObjectFn = nodeWebCompat.transformJwkToKeyObjectSync) {\n    return verifyDecomposedJwtSync(decomposeJwt(jwt), jwkOrJwks, options, transformJwkToKeyObjectFn);\n}\n/**\n * Verify (synchronously) a JWT that is already decomposed (by function `decomposeJwt`)\n *\n * @param decomposedJwt The decomposed JWT\n * @param jwkOrJwks The JWKS that includes the right JWK (indexed by kid). Alternatively, provide the right JWK directly\n * @param options Verification options\n * @param transformJwkToKeyObjectFn A function that can transform a JWK into a crypto native key object\n * @returns The (JSON parsed) payload of the JWT\u2013\u2013if the JWT is valid, otherwise an error is thrown\n */\nfunction verifyDecomposedJwtSync(decomposedJwt, jwkOrJwks, options, transformJwkToKeyObjectFn) {\n    const { header, headerB64, payload, payloadB64, signatureB64 } = decomposedJwt;\n    let jwk;\n    if (isJwk(jwkOrJwks)) {\n        jwk = jwkOrJwks;\n    }\n    else if (isJwks(jwkOrJwks)) {\n        const locatedJwk = header.kid\n            ? findJwkInJwks(jwkOrJwks, header.kid)\n            : undefined;\n        if (!locatedJwk) {\n            throw new KidNotFoundInJwksError(`JWK for kid ${header.kid} not found in the JWKS`);\n        }\n        jwk = locatedJwk;\n    }\n    else {\n        throw new ParameterValidationError([\n            `Expected a valid JWK or JWKS (parsed as JavaScript object), but received: ${jwkOrJwks}.`,\n            \"If you're passing a JWKS URI, use the async verify() method instead, it will download and parse the JWKS for you\",\n        ].join());\n    }\n    validateJwtHeaderAndJwk(decomposedJwt.header, jwk);\n    // Transform the JWK to native key format, that can be used with verifySignature\n    const keyObject = transformJwkToKeyObjectFn(jwk, header.alg, payload.iss);\n    // Verify the JWT signature (JWS)\n    const valid = nodeWebCompat.verifySignatureSync({\n        jwsSigningInput: `${headerB64}.${payloadB64}`,\n        signature: signatureB64,\n        alg: header.alg,\n        keyObject,\n    });\n    if (!valid) {\n        throw new JwtInvalidSignatureError(\"Invalid signature\");\n    }\n    try {\n        validateJwtFields(payload, options);\n        if (options.customJwtCheck) {\n            const res = options.customJwtCheck({ header, payload, jwk });\n            assertIsNotPromise(res, () => new ParameterValidationError(\"Custom JWT checks must be synchronous but a promise was returned\"));\n        }\n    }\n    catch (err) {\n        if (options.includeRawJwtInErrors && err instanceof JwtInvalidClaimError) {\n            throw err.withRawJwt(decomposedJwt);\n        }\n        throw err;\n    }\n    return payload;\n}\n/**\n * Abstract class representing a verifier for JWTs signed with RSA (e.g. RS256, RS384, RS512)\n *\n * A class is used, because there is state:\n * - The JWKS is fetched (downloaded) from the JWKS URI and cached in memory\n * - Verification properties at verifier level, are used as default options for individual verify calls\n *\n * When instantiating this class, relevant type parameters should be provided, for your concrete case:\n * @param StillToProvide The verification options that you want callers of verify to provide on individual verify calls\n * @param SpecificVerifyProperties The verification options that you'll use\n * @param IssuerConfig The issuer config that you'll use (config options are used as default verification options)\n * @param MultiIssuer Verify multiple issuers (true) or just a single one (false)\n */\nexport class JwtRsaVerifierBase {\n    constructor(verifyProperties, jwksCache = new SimpleJwksCache()) {\n        this.jwksCache = jwksCache;\n        this.issuersConfig = new Map();\n        this.publicKeyCache = new KeyObjectCache();\n        if (Array.isArray(verifyProperties)) {\n            if (!verifyProperties.length) {\n                throw new ParameterValidationError(\"Provide at least one issuer configuration\");\n            }\n            for (const prop of verifyProperties) {\n                if (this.issuersConfig.has(prop.issuer)) {\n                    throw new ParameterValidationError(`issuer ${prop.issuer} supplied multiple times`);\n                }\n                this.issuersConfig.set(prop.issuer, this.withJwksUri(prop));\n            }\n        }\n        else {\n            this.issuersConfig.set(verifyProperties.issuer, this.withJwksUri(verifyProperties));\n        }\n    }\n    get expectedIssuers() {\n        return Array.from(this.issuersConfig.keys());\n    }\n    getIssuerConfig(issuer) {\n        if (!issuer) {\n            if (this.issuersConfig.size !== 1) {\n                throw new ParameterValidationError(\"issuer must be provided\");\n            }\n            issuer = this.issuersConfig.keys().next().value;\n        }\n        const config = this.issuersConfig.get(issuer);\n        if (!config) {\n            throw new ParameterValidationError(`issuer not configured: ${issuer}`);\n        }\n        return config;\n    }\n    /**\n     * This method loads a JWKS that you provide, into the JWKS cache, so that it is\n     * available for JWT verification. Use this method to speed up the first JWT verification\n     * (when the JWKS would otherwise have to be downloaded from the JWKS uri), or to provide the JWKS\n     * in case the JwtVerifier does not have internet access to download the JWKS\n     *\n     * @param jwksThe JWKS\n     * @param issuer The issuer for which you want to cache the JWKS\n     *  Supply this field, if you instantiated the JwtVerifier with multiple issuers\n     * @returns void\n     */\n    cacheJwks(...[jwks, issuer]) {\n        const issuerConfig = this.getIssuerConfig(issuer);\n        this.jwksCache.addJwks(issuerConfig.jwksUri, jwks);\n        this.publicKeyCache.clearCache(issuerConfig.issuer);\n    }\n    /**\n     * Hydrate the JWKS cache for (all of) the configured issuer(s).\n     * This will fetch and cache the latest and greatest JWKS for concerned issuer(s).\n     *\n     * @param issuer The issuer to fetch the JWKS for\n     * @returns void\n     */\n    async hydrate() {\n        const jwksFetches = this.expectedIssuers\n            .map((issuer) => this.getIssuerConfig(issuer).jwksUri)\n            .map((jwksUri) => this.jwksCache.getJwks(jwksUri));\n        await Promise.all(jwksFetches);\n    }\n    /**\n     * Verify (synchronously) a JWT that is signed using RS256 / RS384 / RS512.\n     *\n     * @param jwt The JWT, as string\n     * @param props Verification properties\n     * @returns The payload of the JWT\u2013\u2013if the JWT is valid, otherwise an error is thrown\n     */\n    verifySync(...[jwt, properties]) {\n        const { decomposedJwt, jwksUri, verifyProperties } = this.getVerifyParameters(jwt, properties);\n        return this.verifyDecomposedJwtSync(decomposedJwt, jwksUri, verifyProperties);\n    }\n    /**\n     * Verify (synchronously) an already decomposed JWT, that is signed using RS256 / RS384 / RS512.\n     *\n     * @param decomposedJwt The decomposed Jwt\n     * @param jwk The JWK to verify the JWTs signature with\n     * @param verifyProperties The properties to use for verification\n     * @returns The payload of the JWT\u2013\u2013if the JWT is valid, otherwise an error is thrown\n     */\n    verifyDecomposedJwtSync(decomposedJwt, jwksUri, verifyProperties) {\n        const jwk = this.jwksCache.getCachedJwk(jwksUri, decomposedJwt);\n        return verifyDecomposedJwtSync(decomposedJwt, jwk, verifyProperties, this.publicKeyCache.transformJwkToKeyObjectSync.bind(this.publicKeyCache));\n    }\n    /**\n     * Verify (asynchronously) a JWT that is signed using RS256 / RS384 / RS512.\n     * This call is asynchronous, and the JWKS will be fetched from the JWKS uri,\n     * in case it is not yet available in the cache.\n     *\n     * @param jwt The JWT, as string\n     * @param props Verification properties\n     * @returns Promise that resolves to the payload of the JWT\u2013\u2013if the JWT is valid, otherwise the promise rejects\n     */\n    async verify(...[jwt, properties]) {\n        const { decomposedJwt, jwksUri, verifyProperties } = this.getVerifyParameters(jwt, properties);\n        return this.verifyDecomposedJwt(decomposedJwt, jwksUri, verifyProperties);\n    }\n    /**\n     * Verify (asynchronously) an already decomposed JWT, that is signed using RS256 / RS384 / RS512.\n     *\n     * @param decomposedJwt The decomposed Jwt\n     * @param jwk The JWK to verify the JWTs signature with\n     * @param verifyProperties The properties to use for verification\n     * @returns The payload of the JWT\u2013\u2013if the JWT is valid, otherwise an error is thrown\n     */\n    verifyDecomposedJwt(decomposedJwt, jwksUri, verifyProperties) {\n        return verifyDecomposedJwt(decomposedJwt, jwksUri, verifyProperties, this.jwksCache.getJwk.bind(this.jwksCache), this.publicKeyCache.transformJwkToKeyObjectAsync.bind(this.publicKeyCache));\n    }\n    /**\n     * Get the verification parameters to use, by merging the issuer configuration,\n     * with the overriding properties that are now provided\n     *\n     * @param jwt: the JWT that is going to be verified\n     * @param verifyProperties: the overriding properties, that override the issuer configuration\n     * @returns The merged verification parameters\n     */\n    getVerifyParameters(jwt, verifyProperties) {\n        const decomposedJwt = decomposeJwt(jwt);\n        assertStringArrayContainsString(\"Issuer\", decomposedJwt.payload.iss, this.expectedIssuers, JwtInvalidIssuerError);\n        const issuerConfig = this.getIssuerConfig(decomposedJwt.payload.iss);\n        return {\n            decomposedJwt,\n            jwksUri: issuerConfig.jwksUri,\n            verifyProperties: {\n                ...issuerConfig,\n                ...verifyProperties,\n            },\n        };\n    }\n    /**\n     * Get issuer config with JWKS URI, by adding a default JWKS URI if needed\n     *\n     * @param config: the issuer config.\n     * @returns The config with JWKS URI\n     */\n    withJwksUri(config) {\n        if (config.jwksUri) {\n            return config;\n        }\n        const issuerUri = new URL(config.issuer).pathname.replace(/\\/$/, \"\");\n        return {\n            jwksUri: new URL(`${issuerUri}/.well-known/jwks.json`, config.issuer)\n                .href,\n            ...config,\n        };\n    }\n}\n/**\n * Class representing a verifier for JWTs signed with RSA (e.g. RS256 / RS384 / RS512)\n */\nexport class JwtRsaVerifier extends JwtRsaVerifierBase {\n    // eslint-disable-next-line @typescript-eslint/explicit-module-boundary-types\n    static create(verifyProperties, additionalProperties) {\n        return new this(verifyProperties, additionalProperties?.jwksCache);\n    }\n}\n/**\n * Class representing a cache of RSA public keys in native key object format\n *\n * Because it takes a bit of compute time to turn a JWK into native key object format,\n * we want to cache this computation.\n */\nexport class KeyObjectCache {\n    constructor(transformJwkToKeyObjectSyncFn = nodeWebCompat.transformJwkToKeyObjectSync, transformJwkToKeyObjectAsyncFn = nodeWebCompat.transformJwkToKeyObjectAsync) {\n        this.transformJwkToKeyObjectSyncFn = transformJwkToKeyObjectSyncFn;\n        this.transformJwkToKeyObjectAsyncFn = transformJwkToKeyObjectAsyncFn;\n        this.publicKeys = new Map();\n    }\n    /**\n     * Transform the JWK into an RSA public key in native key object format.\n     * If the transformed JWK is already in the cache, it is returned from the cache instead.\n     *\n     * @param jwk: the JWK\n     * @param jwtHeaderAlg: the alg from the JWT header (used if absent on JWK)\n     * @param issuer: the issuer that uses the JWK for signing JWTs (used for caching the transformation)\n     * @returns the RSA public key in native key object format\n     */\n    transformJwkToKeyObjectSync(jwk, jwtHeaderAlg, issuer) {\n        const alg = jwk.alg ?? jwtHeaderAlg;\n        if (!issuer || !jwk.kid || !alg) {\n            return this.transformJwkToKeyObjectSyncFn(jwk, alg, issuer);\n        }\n        const fromCache = this.publicKeys.get(issuer)?.get(jwk.kid)?.get(alg);\n        if (fromCache)\n            return fromCache;\n        const publicKey = this.transformJwkToKeyObjectSyncFn(jwk, alg, issuer);\n        this.putKeyObjectInCache(issuer, jwk.kid, alg, publicKey);\n        return publicKey;\n    }\n    /**\n     * Transform the JWK into an RSA public key in native key object format (async).\n     * If the transformed JWK is already in the cache, it is returned from the cache instead.\n     *\n     * @param jwk: the JWK\n     * @param jwtHeaderAlg: the alg from the JWT header (used if absent on JWK)\n     * @param issuer: the issuer that uses the JWK for signing JWTs (used for caching the transformation)\n     * @returns the RSA public key in native key object format\n     */\n    async transformJwkToKeyObjectAsync(jwk, jwtHeaderAlg, issuer) {\n        const alg = jwk.alg ?? jwtHeaderAlg;\n        if (!issuer || !jwk.kid || !alg) {\n            return this.transformJwkToKeyObjectAsyncFn(jwk, alg, issuer);\n        }\n        const fromCache = this.publicKeys.get(issuer)?.get(jwk.kid)?.get(alg);\n        if (fromCache)\n            return fromCache;\n        const publicKey = await this.transformJwkToKeyObjectAsyncFn(jwk, alg, issuer);\n        this.putKeyObjectInCache(issuer, jwk.kid, alg, publicKey);\n        return publicKey;\n    }\n    putKeyObjectInCache(issuer, kid, alg, publicKey) {\n        const cachedIssuer = this.publicKeys.get(issuer);\n        const cachedIssuerKid = cachedIssuer?.get(kid);\n        if (cachedIssuerKid) {\n            cachedIssuerKid.set(alg, publicKey);\n        }\n        else if (cachedIssuer) {\n            cachedIssuer.set(kid, new Map([[alg, publicKey]]));\n        }\n        else {\n            this.publicKeys.set(issuer, new Map([[kid, new Map([[alg, publicKey]])]]));\n        }\n    }\n    clearCache(issuer) {\n        this.publicKeys.delete(issuer);\n    }\n}\n", "// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n// SPDX-License-Identifier: Apache-2.0\nimport { CognitoJwtInvalidClientIdError, CognitoJwtInvalidGroupError, CognitoJwtInvalidTokenUseError, JwtInvalidClaimError, ParameterValidationError, } from \"./error.js\";\nimport { JwtRsaVerifierBase } from \"./jwt-rsa.js\";\nimport { assertStringArrayContainsString, assertStringEquals, assertStringArraysOverlap, } from \"./assert.js\";\n/**\n * Validate claims of a decoded Cognito JWT.\n * This function throws an error in case there's any validation issue.\n *\n * @param payload - The JSON parsed payload of the Cognito JWT\n * @param options - Validation options\n * @param options.groups - The cognito groups, of which at least one must be present in the JWT's cognito:groups claim\n * @param options.tokenUse - The required token use of the JWT: \"id\" or \"access\"\n * @param options.clientId - The required clientId of the JWT. May be an array of string, of which at least one must match\n * @returns void\n */\nexport function validateCognitoJwtFields(payload, options) {\n    // Check groups\n    if (options.groups != null) {\n        assertStringArraysOverlap(\"Cognito group\", payload[\"cognito:groups\"], options.groups, CognitoJwtInvalidGroupError);\n    }\n    // Check token use\n    assertStringArrayContainsString(\"Token use\", payload.token_use, [\"id\", \"access\"], CognitoJwtInvalidTokenUseError);\n    if (options.tokenUse !== null) {\n        if (options.tokenUse === undefined) {\n            throw new ParameterValidationError(\"tokenUse must be provided or set to null explicitly\");\n        }\n        assertStringEquals(\"Token use\", payload.token_use, options.tokenUse, CognitoJwtInvalidTokenUseError);\n    }\n    // Check clientId aka audience\n    if (options.clientId !== null) {\n        if (options.clientId === undefined) {\n            throw new ParameterValidationError(\"clientId must be provided or set to null explicitly\");\n        }\n        if (payload.token_use === \"id\") {\n            assertStringArrayContainsString('Client ID (\"audience\")', payload.aud, options.clientId, CognitoJwtInvalidClientIdError);\n        }\n        else {\n            assertStringArrayContainsString(\"Client ID\", payload.client_id, options.clientId, CognitoJwtInvalidClientIdError);\n        }\n    }\n}\n/**\n * Class representing a verifier for JWTs signed by Amazon Cognito\n */\nexport class CognitoJwtVerifier extends JwtRsaVerifierBase {\n    constructor(props, jwksCache) {\n        const issuerConfig = Array.isArray(props)\n            ? props.map((p) => ({\n                ...p,\n                ...CognitoJwtVerifier.parseUserPoolId(p.userPoolId),\n                audience: null, // checked instead by validateCognitoJwtFields\n            }))\n            : {\n                ...props,\n                ...CognitoJwtVerifier.parseUserPoolId(props.userPoolId),\n                audience: null, // checked instead by validateCognitoJwtFields\n            };\n        super(issuerConfig, jwksCache);\n    }\n    /**\n     * Parse a User Pool ID, to extract the issuer and JWKS URI\n     *\n     * @param userPoolId The User Pool ID\n     * @returns The issuer and JWKS URI for the User Pool\n     */\n    static parseUserPoolId(userPoolId) {\n        // Disable safe regexp check as userPoolId is provided by developer, i.e. is not user input\n        // eslint-disable-next-line security/detect-unsafe-regex\n        const match = userPoolId.match(/^(?<region>(\\w+-)?\\w+-\\w+-\\d)+_\\w+$/);\n        if (!match) {\n            throw new ParameterValidationError(`Invalid Cognito User Pool ID: ${userPoolId}`);\n        }\n        const region = match.groups.region;\n        const issuer = `https://cognito-idp.${region}.amazonaws.com/${userPoolId}`;\n        return {\n            issuer,\n            jwksUri: `${issuer}/.well-known/jwks.json`,\n        };\n    }\n    // eslint-disable-next-line @typescript-eslint/explicit-module-boundary-types\n    static create(verifyProperties, additionalProperties) {\n        return new this(verifyProperties, additionalProperties?.jwksCache);\n    }\n    /**\n     * Verify (synchronously) a JWT that is signed by Amazon Cognito.\n     *\n     * @param jwt The JWT, as string\n     * @param props Verification properties\n     * @returns The payload of the JWT\u2013\u2013if the JWT is valid, otherwise an error is thrown\n     */\n    verifySync(...[jwt, properties]) {\n        const { decomposedJwt, jwksUri, verifyProperties } = this.getVerifyParameters(jwt, properties);\n        this.verifyDecomposedJwtSync(decomposedJwt, jwksUri, verifyProperties);\n        try {\n            validateCognitoJwtFields(decomposedJwt.payload, verifyProperties);\n        }\n        catch (err) {\n            if (verifyProperties.includeRawJwtInErrors &&\n                err instanceof JwtInvalidClaimError) {\n                throw err.withRawJwt(decomposedJwt);\n            }\n            throw err;\n        }\n        return decomposedJwt.payload;\n    }\n    /**\n     * Verify (asynchronously) a JWT that is signed by Amazon Cognito.\n     * This call is asynchronous, and the JWKS will be fetched from the JWKS uri,\n     * in case it is not yet available in the cache.\n     *\n     * @param jwt The JWT, as string\n     * @param props Verification properties\n     * @returns Promise that resolves to the payload of the JWT\u2013\u2013if the JWT is valid, otherwise the promise rejects\n     */\n    async verify(...[jwt, properties]) {\n        const { decomposedJwt, jwksUri, verifyProperties } = this.getVerifyParameters(jwt, properties);\n        await this.verifyDecomposedJwt(decomposedJwt, jwksUri, verifyProperties);\n        try {\n            validateCognitoJwtFields(decomposedJwt.payload, verifyProperties);\n        }\n        catch (err) {\n            if (verifyProperties.includeRawJwtInErrors &&\n                err instanceof JwtInvalidClaimError) {\n                throw err.withRawJwt(decomposedJwt);\n            }\n            throw err;\n        }\n        return decomposedJwt.payload;\n    }\n    /**\n     * This method loads a JWKS that you provide, into the JWKS cache, so that it is\n     * available for JWT verification. Use this method to speed up the first JWT verification\n     * (when the JWKS would otherwise have to be downloaded from the JWKS uri), or to provide the JWKS\n     * in case the JwtVerifier does not have internet access to download the JWKS\n     *\n     * @param jwks The JWKS\n     * @param userPoolId The userPoolId for which you want to cache the JWKS\n     *  Supply this field, if you instantiated the CognitoJwtVerifier with multiple userPoolIds\n     * @returns void\n     */\n    cacheJwks(...[jwks, userPoolId]) {\n        let issuer;\n        if (userPoolId !== undefined) {\n            issuer = CognitoJwtVerifier.parseUserPoolId(userPoolId).issuer;\n        }\n        else if (this.expectedIssuers.length > 1) {\n            throw new ParameterValidationError(\"userPoolId must be provided\");\n        }\n        const issuerConfig = this.getIssuerConfig(issuer);\n        super.cacheJwks(jwks, issuerConfig.issuer);\n    }\n}\n", "/**\n * The response for a successful request.\n * Should include a body for GET, PUT, or POST.\n * Need not include a body for DELETE\n */\nexport const ok = (body?: Object) => new HttpResponse(StatusCodes.Ok, body);\n\n/**\n * The response for a successful POST or PUT request,\n * which resulted in the creation of a new resource.\n */\nexport const created = (body: Object) =>\n  new HttpResponse(StatusCodes.Created, body);\n\n/**\n * The response for a successful request which returns no content.\n */\nexport const noContent = (body?: Object) =>\n  new HttpResponse(StatusCodes.NoContent, body);\n\n/**\n * The response for a failed request, due to client-side issues.\n * Typically indicates a missing parameter or malformed body.\n */\nexport const badRequest = (body?: Object) =>\n  new HttpResponse(StatusCodes.BadRequest, body);\n\n/**\n * The response for a client without any authorization.\n * Typically indicates an issue with the request's headers or token.\n *\n * Note: The usual name for HTTP 401 is \"Unauthorized\", but that's misleading.\n * Authentication is for identity; authorization is for permissions.\n */\nexport const unauthenticated = (body?: Object) =>\n  new HttpResponse(StatusCodes.Unauthenticated, body);\n\n/**\n * The response for a client without sufficient permissions.\n * This is specific to the requested operation.\n * For example, a regular user requesting an admin-only endpoint.\n */\nexport const forbidden = (body?: Object) =>\n  new HttpResponse(StatusCodes.Forbidden, body);\n\n/**\n * The response for a request that assumes the existence of a missing resource.\n * For example, attempting to submit a report that isn't in the database.\n */\nexport const notFound = (body?: Object) =>\n  new HttpResponse(StatusCodes.NotFound, body);\n\n/**\n * The response for a request that assumes the server is in a different state.\n * For example, attempting to submit a report that's already submitted.\n */\nexport const conflict = (body?: Object) =>\n  new HttpResponse(StatusCodes.Conflict, body);\n\n/**\n * The response for a request that errored out on the server side.\n * Typically indicates there is nothing the client can do to resolve the issue.\n */\nexport const internalServerError = (body?: Object) =>\n  new HttpResponse(StatusCodes.InternalServerError, body);\n\n/**\n * Note: Production code shouldn't need to reference this directly.\n * Use a helper method instead.\n *\n * This enum is listed mainly for the purpose of unit testing.\n */\nexport enum StatusCodes {\n  Ok = 200,\n  Created = 201,\n  NoContent = 204,\n  BadRequest = 400,\n  Unauthenticated = 401,\n  Forbidden = 403,\n  NotFound = 404,\n  Conflict = 409,\n  InternalServerError = 500,\n}\n\n/**\n * Note: Production code shouldn't need to reference this directly.\n * Use a helper method instead.\n */\nexport class HttpResponse {\n  readonly statusCode: number;\n  readonly body: string | undefined;\n  readonly headers = {\n    \"Access-Control-Allow-Origin\": \"*\",\n    \"Access-Control-Allow-Credentials\": true,\n  };\n  constructor(statusCode: number, body?: Object | undefined) {\n    this.statusCode = statusCode;\n    if (body !== undefined) {\n      this.body = JSON.stringify(body);\n    }\n  }\n}\n", "import { ReportType } from \"../types/reports\";\n\nexport const error = {\n  // generic errors\n  UNAUTHORIZED: \"User is not authorized to access this resource.\",\n  NO_KEY: \"Must provide key for table.\",\n  MISSING_DATA: \"Missing required data.\",\n  INVALID_DATA: \"Provided data is not valid.\",\n  NO_MATCHING_RECORD: \"No matching record found.\",\n  SERVER_ERROR: \"An unspecified server error occured.\",\n  // bucket errors\n  S3_OBJECT_CREATION_ERROR: \"Report could not be created due to an S3 error.\",\n  S3_OBJECT_UPDATE_ERROR: \"Report could not be updated due to an S3 error.\",\n  S3_OBJECT_GET_ERROR: \"Error while fetching report.\",\n  // dynamo errors\n  DYNAMO_CREATION_ERROR: \"Report could not be created due to a database error.\",\n  DYNAMO_UPDATE_ERROR: \"Report could not be updated due to a database error.\",\n  // template errors\n  NO_TEMPLATE_NAME: \"Must request template for download.\",\n  INVALID_TEMPLATE_NAME: \"Requested template does not exist or does not match.\",\n  NOT_IN_DATABASE: \"Record not found in database.\",\n  UNABLE_TO_COPY:\n    \"Unable to copy over report if todays date is in the same period and year as a previous report.\",\n  MISSING_FORM_TEMPLATE: \"Form Template not found in S3.\",\n  MISSING_FIELD_DATA: \"Field Data not found in S3.\",\n  NO_WORKPLANS_FOUND: \"No record of Work Plans found in database\",\n  // admin action errors\n  ALREADY_ARCHIVED: \"Cannot update archived report.\",\n  ALREADY_LOCKED: \"Cannot update locked report.\",\n  REPORT_INCOMPLETE: \"Cannot submit incomplete form.\",\n} as const;\n\nexport const buckets = {\n  FORM_TEMPLATE: \"formTemplates\",\n  FIELD_DATA: \"fieldData\",\n};\n\n// STATES\nexport enum States {\n  AL = \"Alabama\",\n  AK = \"Alaska\",\n  AS = \"American Samoa\",\n  AZ = \"Arizona\",\n  AR = \"Arkansas\",\n  CA = \"California\",\n  CO = \"Colorado\",\n  CT = \"Connecticut\",\n  DE = \"Delaware\",\n  DC = \"District of Columbia\",\n  FM = \"Federated States of Micronesia\",\n  FL = \"Florida\",\n  GA = \"Georgia\",\n  GU = \"Guam\",\n  HI = \"Hawaii\",\n  ID = \"Idaho\",\n  IL = \"Illinois\",\n  IN = \"Indiana\",\n  IA = \"Iowa\",\n  KS = \"Kansas\",\n  KY = \"Kentucky\",\n  LA = \"Louisiana\",\n  ME = \"Maine\",\n  MH = \"Marshall Islands\",\n  MD = \"Maryland\",\n  MA = \"Massachusetts\",\n  MI = \"Michigan\",\n  MN = \"Minnesota\",\n  MS = \"Mississippi\",\n  MO = \"Missouri\",\n  MT = \"Montana\",\n  NE = \"Nebraska\",\n  NV = \"Nevada\",\n  NH = \"New Hampshire\",\n  NJ = \"New Jersey\",\n  NM = \"New Mexico\",\n  NY = \"New York\",\n  NC = \"North Carolina\",\n  ND = \"North Dakota\",\n  MP = \"Northern Mariana Islands\",\n  OH = \"Ohio\",\n  OK = \"Oklahoma\",\n  OR = \"Oregon\",\n  PW = \"Palau\",\n  PA = \"Pennsylvania\",\n  PR = \"Puerto Rico\",\n  RI = \"Rhode Island\",\n  SC = \"South Carolina\",\n  SD = \"South Dakota\",\n  TN = \"Tennessee\",\n  TX = \"Texas\",\n  UT = \"Utah\",\n  VT = \"Vermont\",\n  VI = \"Virgin Islands\",\n  VA = \"Virginia\",\n  WA = \"Washington\",\n  WV = \"West Virginia\",\n  WI = \"Wisconsin\",\n  WY = \"Wyoming\",\n}\n\n// REPORTS\n\nexport const reportTables: { [key in ReportType]: string } = {\n  SAR: process.env.SarReportsTable!,\n  WP: process.env.WpReportsTable!,\n};\n\nexport const reportBuckets: { [key in ReportType]: string } = {\n  SAR: process.env.SAR_FORM_BUCKET!,\n  WP: process.env.WP_FORM_BUCKET!,\n};\n\nexport const reportNames: { [key in ReportType]: string } = {\n  SAR: \"SAR\",\n  WP: \"Work Plan\",\n};\n\nexport const tableTopics: { [key in ReportType]: string } = {\n  SAR: \"sar-reports\",\n  WP: \"wp-reports\",\n};\n\nexport const bucketTopics: { [key in ReportType]: string } = {\n  SAR: \"sar-form\",\n  WP: \"wp-form\",\n};\n\nexport const DEFAULT_TARGET_POPULATION_NAMES = [\n  \"Older adults\",\n  \"Individuals with physical disabilities (PD)\",\n  \"Individuals with intellectual and developmental disabilities (I/DD)\",\n  \"Individuals with mental health and substance use disorders (MH/SUD)\",\n  \"HCBS infrastructure/system-level development\",\n];\n", "/*! @license DOMPurify 3.2.4 | (c) Cure53 and other contributors | Released under the Apache license 2.0 and Mozilla Public License 2.0 | github.com/cure53/DOMPurify/blob/3.2.4/LICENSE */\n\nconst {\n  entries,\n  setPrototypeOf,\n  isFrozen,\n  getPrototypeOf,\n  getOwnPropertyDescriptor\n} = Object;\nlet {\n  freeze,\n  seal,\n  create\n} = Object; // eslint-disable-line import/no-mutable-exports\nlet {\n  apply,\n  construct\n} = typeof Reflect !== 'undefined' && Reflect;\nif (!freeze) {\n  freeze = function freeze(x) {\n    return x;\n  };\n}\nif (!seal) {\n  seal = function seal(x) {\n    return x;\n  };\n}\nif (!apply) {\n  apply = function apply(fun, thisValue, args) {\n    return fun.apply(thisValue, args);\n  };\n}\nif (!construct) {\n  construct = function construct(Func, args) {\n    return new Func(...args);\n  };\n}\nconst arrayForEach = unapply(Array.prototype.forEach);\nconst arrayLastIndexOf = unapply(Array.prototype.lastIndexOf);\nconst arrayPop = unapply(Array.prototype.pop);\nconst arrayPush = unapply(Array.prototype.push);\nconst arraySplice = unapply(Array.prototype.splice);\nconst stringToLowerCase = unapply(String.prototype.toLowerCase);\nconst stringToString = unapply(String.prototype.toString);\nconst stringMatch = unapply(String.prototype.match);\nconst stringReplace = unapply(String.prototype.replace);\nconst stringIndexOf = unapply(String.prototype.indexOf);\nconst stringTrim = unapply(String.prototype.trim);\nconst objectHasOwnProperty = unapply(Object.prototype.hasOwnProperty);\nconst regExpTest = unapply(RegExp.prototype.test);\nconst typeErrorCreate = unconstruct(TypeError);\n/**\n * Creates a new function that calls the given function with a specified thisArg and arguments.\n *\n * @param func - The function to be wrapped and called.\n * @returns A new function that calls the given function with a specified thisArg and arguments.\n */\nfunction unapply(func) {\n  return function (thisArg) {\n    for (var _len = arguments.length, args = new Array(_len > 1 ? _len - 1 : 0), _key = 1; _key < _len; _key++) {\n      args[_key - 1] = arguments[_key];\n    }\n    return apply(func, thisArg, args);\n  };\n}\n/**\n * Creates a new function that constructs an instance of the given constructor function with the provided arguments.\n *\n * @param func - The constructor function to be wrapped and called.\n * @returns A new function that constructs an instance of the given constructor function with the provided arguments.\n */\nfunction unconstruct(func) {\n  return function () {\n    for (var _len2 = arguments.length, args = new Array(_len2), _key2 = 0; _key2 < _len2; _key2++) {\n      args[_key2] = arguments[_key2];\n    }\n    return construct(func, args);\n  };\n}\n/**\n * Add properties to a lookup table\n *\n * @param set - The set to which elements will be added.\n * @param array - The array containing elements to be added to the set.\n * @param transformCaseFunc - An optional function to transform the case of each element before adding to the set.\n * @returns The modified set with added elements.\n */\nfunction addToSet(set, array) {\n  let transformCaseFunc = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : stringToLowerCase;\n  if (setPrototypeOf) {\n    // Make 'in' and truthy checks like Boolean(set.constructor)\n    // independent of any properties defined on Object.prototype.\n    // Prevent prototype setters from intercepting set as a this value.\n    setPrototypeOf(set, null);\n  }\n  let l = array.length;\n  while (l--) {\n    let element = array[l];\n    if (typeof element === 'string') {\n      const lcElement = transformCaseFunc(element);\n      if (lcElement !== element) {\n        // Config presets (e.g. tags.js, attrs.js) are immutable.\n        if (!isFrozen(array)) {\n          array[l] = lcElement;\n        }\n        element = lcElement;\n      }\n    }\n    set[element] = true;\n  }\n  return set;\n}\n/**\n * Clean up an array to harden against CSPP\n *\n * @param array - The array to be cleaned.\n * @returns The cleaned version of the array\n */\nfunction cleanArray(array) {\n  for (let index = 0; index < array.length; index++) {\n    const isPropertyExist = objectHasOwnProperty(array, index);\n    if (!isPropertyExist) {\n      array[index] = null;\n    }\n  }\n  return array;\n}\n/**\n * Shallow clone an object\n *\n * @param object - The object to be cloned.\n * @returns A new object that copies the original.\n */\nfunction clone(object) {\n  const newObject = create(null);\n  for (const [property, value] of entries(object)) {\n    const isPropertyExist = objectHasOwnProperty(object, property);\n    if (isPropertyExist) {\n      if (Array.isArray(value)) {\n        newObject[property] = cleanArray(value);\n      } else if (value && typeof value === 'object' && value.constructor === Object) {\n        newObject[property] = clone(value);\n      } else {\n        newObject[property] = value;\n      }\n    }\n  }\n  return newObject;\n}\n/**\n * This method automatically checks if the prop is function or getter and behaves accordingly.\n *\n * @param object - The object to look up the getter function in its prototype chain.\n * @param prop - The property name for which to find the getter function.\n * @returns The getter function found in the prototype chain or a fallback function.\n */\nfunction lookupGetter(object, prop) {\n  while (object !== null) {\n    const desc = getOwnPropertyDescriptor(object, prop);\n    if (desc) {\n      if (desc.get) {\n        return unapply(desc.get);\n      }\n      if (typeof desc.value === 'function') {\n        return unapply(desc.value);\n      }\n    }\n    object = getPrototypeOf(object);\n  }\n  function fallbackValue() {\n    return null;\n  }\n  return fallbackValue;\n}\n\nconst html$1 = freeze(['a', 'abbr', 'acronym', 'address', 'area', 'article', 'aside', 'audio', 'b', 'bdi', 'bdo', 'big', 'blink', 'blockquote', 'body', 'br', 'button', 'canvas', 'caption', 'center', 'cite', 'code', 'col', 'colgroup', 'content', 'data', 'datalist', 'dd', 'decorator', 'del', 'details', 'dfn', 'dialog', 'dir', 'div', 'dl', 'dt', 'element', 'em', 'fieldset', 'figcaption', 'figure', 'font', 'footer', 'form', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'head', 'header', 'hgroup', 'hr', 'html', 'i', 'img', 'input', 'ins', 'kbd', 'label', 'legend', 'li', 'main', 'map', 'mark', 'marquee', 'menu', 'menuitem', 'meter', 'nav', 'nobr', 'ol', 'optgroup', 'option', 'output', 'p', 'picture', 'pre', 'progress', 'q', 'rp', 'rt', 'ruby', 's', 'samp', 'section', 'select', 'shadow', 'small', 'source', 'spacer', 'span', 'strike', 'strong', 'style', 'sub', 'summary', 'sup', 'table', 'tbody', 'td', 'template', 'textarea', 'tfoot', 'th', 'thead', 'time', 'tr', 'track', 'tt', 'u', 'ul', 'var', 'video', 'wbr']);\nconst svg$1 = freeze(['svg', 'a', 'altglyph', 'altglyphdef', 'altglyphitem', 'animatecolor', 'animatemotion', 'animatetransform', 'circle', 'clippath', 'defs', 'desc', 'ellipse', 'filter', 'font', 'g', 'glyph', 'glyphref', 'hkern', 'image', 'line', 'lineargradient', 'marker', 'mask', 'metadata', 'mpath', 'path', 'pattern', 'polygon', 'polyline', 'radialgradient', 'rect', 'stop', 'style', 'switch', 'symbol', 'text', 'textpath', 'title', 'tref', 'tspan', 'view', 'vkern']);\nconst svgFilters = freeze(['feBlend', 'feColorMatrix', 'feComponentTransfer', 'feComposite', 'feConvolveMatrix', 'feDiffuseLighting', 'feDisplacementMap', 'feDistantLight', 'feDropShadow', 'feFlood', 'feFuncA', 'feFuncB', 'feFuncG', 'feFuncR', 'feGaussianBlur', 'feImage', 'feMerge', 'feMergeNode', 'feMorphology', 'feOffset', 'fePointLight', 'feSpecularLighting', 'feSpotLight', 'feTile', 'feTurbulence']);\n// List of SVG elements that are disallowed by default.\n// We still need to know them so that we can do namespace\n// checks properly in case one wants to add them to\n// allow-list.\nconst svgDisallowed = freeze(['animate', 'color-profile', 'cursor', 'discard', 'font-face', 'font-face-format', 'font-face-name', 'font-face-src', 'font-face-uri', 'foreignobject', 'hatch', 'hatchpath', 'mesh', 'meshgradient', 'meshpatch', 'meshrow', 'missing-glyph', 'script', 'set', 'solidcolor', 'unknown', 'use']);\nconst mathMl$1 = freeze(['math', 'menclose', 'merror', 'mfenced', 'mfrac', 'mglyph', 'mi', 'mlabeledtr', 'mmultiscripts', 'mn', 'mo', 'mover', 'mpadded', 'mphantom', 'mroot', 'mrow', 'ms', 'mspace', 'msqrt', 'mstyle', 'msub', 'msup', 'msubsup', 'mtable', 'mtd', 'mtext', 'mtr', 'munder', 'munderover', 'mprescripts']);\n// Similarly to SVG, we want to know all MathML elements,\n// even those that we disallow by default.\nconst mathMlDisallowed = freeze(['maction', 'maligngroup', 'malignmark', 'mlongdiv', 'mscarries', 'mscarry', 'msgroup', 'mstack', 'msline', 'msrow', 'semantics', 'annotation', 'annotation-xml', 'mprescripts', 'none']);\nconst text = freeze(['#text']);\n\nconst html = freeze(['accept', 'action', 'align', 'alt', 'autocapitalize', 'autocomplete', 'autopictureinpicture', 'autoplay', 'background', 'bgcolor', 'border', 'capture', 'cellpadding', 'cellspacing', 'checked', 'cite', 'class', 'clear', 'color', 'cols', 'colspan', 'controls', 'controlslist', 'coords', 'crossorigin', 'datetime', 'decoding', 'default', 'dir', 'disabled', 'disablepictureinpicture', 'disableremoteplayback', 'download', 'draggable', 'enctype', 'enterkeyhint', 'face', 'for', 'headers', 'height', 'hidden', 'high', 'href', 'hreflang', 'id', 'inputmode', 'integrity', 'ismap', 'kind', 'label', 'lang', 'list', 'loading', 'loop', 'low', 'max', 'maxlength', 'media', 'method', 'min', 'minlength', 'multiple', 'muted', 'name', 'nonce', 'noshade', 'novalidate', 'nowrap', 'open', 'optimum', 'pattern', 'placeholder', 'playsinline', 'popover', 'popovertarget', 'popovertargetaction', 'poster', 'preload', 'pubdate', 'radiogroup', 'readonly', 'rel', 'required', 'rev', 'reversed', 'role', 'rows', 'rowspan', 'spellcheck', 'scope', 'selected', 'shape', 'size', 'sizes', 'span', 'srclang', 'start', 'src', 'srcset', 'step', 'style', 'summary', 'tabindex', 'title', 'translate', 'type', 'usemap', 'valign', 'value', 'width', 'wrap', 'xmlns', 'slot']);\nconst svg = freeze(['accent-height', 'accumulate', 'additive', 'alignment-baseline', 'amplitude', 'ascent', 'attributename', 'attributetype', 'azimuth', 'basefrequency', 'baseline-shift', 'begin', 'bias', 'by', 'class', 'clip', 'clippathunits', 'clip-path', 'clip-rule', 'color', 'color-interpolation', 'color-interpolation-filters', 'color-profile', 'color-rendering', 'cx', 'cy', 'd', 'dx', 'dy', 'diffuseconstant', 'direction', 'display', 'divisor', 'dur', 'edgemode', 'elevation', 'end', 'exponent', 'fill', 'fill-opacity', 'fill-rule', 'filter', 'filterunits', 'flood-color', 'flood-opacity', 'font-family', 'font-size', 'font-size-adjust', 'font-stretch', 'font-style', 'font-variant', 'font-weight', 'fx', 'fy', 'g1', 'g2', 'glyph-name', 'glyphref', 'gradientunits', 'gradienttransform', 'height', 'href', 'id', 'image-rendering', 'in', 'in2', 'intercept', 'k', 'k1', 'k2', 'k3', 'k4', 'kerning', 'keypoints', 'keysplines', 'keytimes', 'lang', 'lengthadjust', 'letter-spacing', 'kernelmatrix', 'kernelunitlength', 'lighting-color', 'local', 'marker-end', 'marker-mid', 'marker-start', 'markerheight', 'markerunits', 'markerwidth', 'maskcontentunits', 'maskunits', 'max', 'mask', 'media', 'method', 'mode', 'min', 'name', 'numoctaves', 'offset', 'operator', 'opacity', 'order', 'orient', 'orientation', 'origin', 'overflow', 'paint-order', 'path', 'pathlength', 'patterncontentunits', 'patterntransform', 'patternunits', 'points', 'preservealpha', 'preserveaspectratio', 'primitiveunits', 'r', 'rx', 'ry', 'radius', 'refx', 'refy', 'repeatcount', 'repeatdur', 'restart', 'result', 'rotate', 'scale', 'seed', 'shape-rendering', 'slope', 'specularconstant', 'specularexponent', 'spreadmethod', 'startoffset', 'stddeviation', 'stitchtiles', 'stop-color', 'stop-opacity', 'stroke-dasharray', 'stroke-dashoffset', 'stroke-linecap', 'stroke-linejoin', 'stroke-miterlimit', 'stroke-opacity', 'stroke', 'stroke-width', 'style', 'surfacescale', 'systemlanguage', 'tabindex', 'tablevalues', 'targetx', 'targety', 'transform', 'transform-origin', 'text-anchor', 'text-decoration', 'text-rendering', 'textlength', 'type', 'u1', 'u2', 'unicode', 'values', 'viewbox', 'visibility', 'version', 'vert-adv-y', 'vert-origin-x', 'vert-origin-y', 'width', 'word-spacing', 'wrap', 'writing-mode', 'xchannelselector', 'ychannelselector', 'x', 'x1', 'x2', 'xmlns', 'y', 'y1', 'y2', 'z', 'zoomandpan']);\nconst mathMl = freeze(['accent', 'accentunder', 'align', 'bevelled', 'close', 'columnsalign', 'columnlines', 'columnspan', 'denomalign', 'depth', 'dir', 'display', 'displaystyle', 'encoding', 'fence', 'frame', 'height', 'href', 'id', 'largeop', 'length', 'linethickness', 'lspace', 'lquote', 'mathbackground', 'mathcolor', 'mathsize', 'mathvariant', 'maxsize', 'minsize', 'movablelimits', 'notation', 'numalign', 'open', 'rowalign', 'rowlines', 'rowspacing', 'rowspan', 'rspace', 'rquote', 'scriptlevel', 'scriptminsize', 'scriptsizemultiplier', 'selection', 'separator', 'separators', 'stretchy', 'subscriptshift', 'supscriptshift', 'symmetric', 'voffset', 'width', 'xmlns']);\nconst xml = freeze(['xlink:href', 'xml:id', 'xlink:title', 'xml:space', 'xmlns:xlink']);\n\n// eslint-disable-next-line unicorn/better-regex\nconst MUSTACHE_EXPR = seal(/\\{\\{[\\w\\W]*|[\\w\\W]*\\}\\}/gm); // Specify template detection regex for SAFE_FOR_TEMPLATES mode\nconst ERB_EXPR = seal(/<%[\\w\\W]*|[\\w\\W]*%>/gm);\nconst TMPLIT_EXPR = seal(/\\$\\{[\\w\\W]*/gm); // eslint-disable-line unicorn/better-regex\nconst DATA_ATTR = seal(/^data-[\\-\\w.\\u00B7-\\uFFFF]+$/); // eslint-disable-line no-useless-escape\nconst ARIA_ATTR = seal(/^aria-[\\-\\w]+$/); // eslint-disable-line no-useless-escape\nconst IS_ALLOWED_URI = seal(/^(?:(?:(?:f|ht)tps?|mailto|tel|callto|sms|cid|xmpp):|[^a-z]|[a-z+.\\-]+(?:[^a-z+.\\-:]|$))/i // eslint-disable-line no-useless-escape\n);\nconst IS_SCRIPT_OR_DATA = seal(/^(?:\\w+script|data):/i);\nconst ATTR_WHITESPACE = seal(/[\\u0000-\\u0020\\u00A0\\u1680\\u180E\\u2000-\\u2029\\u205F\\u3000]/g // eslint-disable-line no-control-regex\n);\nconst DOCTYPE_NAME = seal(/^html$/i);\nconst CUSTOM_ELEMENT = seal(/^[a-z][.\\w]*(-[.\\w]+)+$/i);\n\nvar EXPRESSIONS = /*#__PURE__*/Object.freeze({\n  __proto__: null,\n  ARIA_ATTR: ARIA_ATTR,\n  ATTR_WHITESPACE: ATTR_WHITESPACE,\n  CUSTOM_ELEMENT: CUSTOM_ELEMENT,\n  DATA_ATTR: DATA_ATTR,\n  DOCTYPE_NAME: DOCTYPE_NAME,\n  ERB_EXPR: ERB_EXPR,\n  IS_ALLOWED_URI: IS_ALLOWED_URI,\n  IS_SCRIPT_OR_DATA: IS_SCRIPT_OR_DATA,\n  MUSTACHE_EXPR: MUSTACHE_EXPR,\n  TMPLIT_EXPR: TMPLIT_EXPR\n});\n\n/* eslint-disable @typescript-eslint/indent */\n// https://developer.mozilla.org/en-US/docs/Web/API/Node/nodeType\nconst NODE_TYPE = {\n  element: 1,\n  attribute: 2,\n  text: 3,\n  cdataSection: 4,\n  entityReference: 5,\n  // Deprecated\n  entityNode: 6,\n  // Deprecated\n  progressingInstruction: 7,\n  comment: 8,\n  document: 9,\n  documentType: 10,\n  documentFragment: 11,\n  notation: 12 // Deprecated\n};\nconst getGlobal = function getGlobal() {\n  return typeof window === 'undefined' ? null : window;\n};\n/**\n * Creates a no-op policy for internal use only.\n * Don't export this function outside this module!\n * @param trustedTypes The policy factory.\n * @param purifyHostElement The Script element used to load DOMPurify (to determine policy name suffix).\n * @return The policy created (or null, if Trusted Types\n * are not supported or creating the policy failed).\n */\nconst _createTrustedTypesPolicy = function _createTrustedTypesPolicy(trustedTypes, purifyHostElement) {\n  if (typeof trustedTypes !== 'object' || typeof trustedTypes.createPolicy !== 'function') {\n    return null;\n  }\n  // Allow the callers to control the unique policy name\n  // by adding a data-tt-policy-suffix to the script element with the DOMPurify.\n  // Policy creation with duplicate names throws in Trusted Types.\n  let suffix = null;\n  const ATTR_NAME = 'data-tt-policy-suffix';\n  if (purifyHostElement && purifyHostElement.hasAttribute(ATTR_NAME)) {\n    suffix = purifyHostElement.getAttribute(ATTR_NAME);\n  }\n  const policyName = 'dompurify' + (suffix ? '#' + suffix : '');\n  try {\n    return trustedTypes.createPolicy(policyName, {\n      createHTML(html) {\n        return html;\n      },\n      createScriptURL(scriptUrl) {\n        return scriptUrl;\n      }\n    });\n  } catch (_) {\n    // Policy creation failed (most likely another DOMPurify script has\n    // already run). Skip creating the policy, as this will only cause errors\n    // if TT are enforced.\n    console.warn('TrustedTypes policy ' + policyName + ' could not be created.');\n    return null;\n  }\n};\nconst _createHooksMap = function _createHooksMap() {\n  return {\n    afterSanitizeAttributes: [],\n    afterSanitizeElements: [],\n    afterSanitizeShadowDOM: [],\n    beforeSanitizeAttributes: [],\n    beforeSanitizeElements: [],\n    beforeSanitizeShadowDOM: [],\n    uponSanitizeAttribute: [],\n    uponSanitizeElement: [],\n    uponSanitizeShadowNode: []\n  };\n};\nfunction createDOMPurify() {\n  let window = arguments.length > 0 && arguments[0] !== undefined ? arguments[0] : getGlobal();\n  const DOMPurify = root => createDOMPurify(root);\n  DOMPurify.version = '3.2.4';\n  DOMPurify.removed = [];\n  if (!window || !window.document || window.document.nodeType !== NODE_TYPE.document || !window.Element) {\n    // Not running in a browser, provide a factory function\n    // so that you can pass your own Window\n    DOMPurify.isSupported = false;\n    return DOMPurify;\n  }\n  let {\n    document\n  } = window;\n  const originalDocument = document;\n  const currentScript = originalDocument.currentScript;\n  const {\n    DocumentFragment,\n    HTMLTemplateElement,\n    Node,\n    Element,\n    NodeFilter,\n    NamedNodeMap = window.NamedNodeMap || window.MozNamedAttrMap,\n    HTMLFormElement,\n    DOMParser,\n    trustedTypes\n  } = window;\n  const ElementPrototype = Element.prototype;\n  const cloneNode = lookupGetter(ElementPrototype, 'cloneNode');\n  const remove = lookupGetter(ElementPrototype, 'remove');\n  const getNextSibling = lookupGetter(ElementPrototype, 'nextSibling');\n  const getChildNodes = lookupGetter(ElementPrototype, 'childNodes');\n  const getParentNode = lookupGetter(ElementPrototype, 'parentNode');\n  // As per issue #47, the web-components registry is inherited by a\n  // new document created via createHTMLDocument. As per the spec\n  // (http://w3c.github.io/webcomponents/spec/custom/#creating-and-passing-registries)\n  // a new empty registry is used when creating a template contents owner\n  // document, so we use that as our parent document to ensure nothing\n  // is inherited.\n  if (typeof HTMLTemplateElement === 'function') {\n    const template = document.createElement('template');\n    if (template.content && template.content.ownerDocument) {\n      document = template.content.ownerDocument;\n    }\n  }\n  let trustedTypesPolicy;\n  let emptyHTML = '';\n  const {\n    implementation,\n    createNodeIterator,\n    createDocumentFragment,\n    getElementsByTagName\n  } = document;\n  const {\n    importNode\n  } = originalDocument;\n  let hooks = _createHooksMap();\n  /**\n   * Expose whether this browser supports running the full DOMPurify.\n   */\n  DOMPurify.isSupported = typeof entries === 'function' && typeof getParentNode === 'function' && implementation && implementation.createHTMLDocument !== undefined;\n  const {\n    MUSTACHE_EXPR,\n    ERB_EXPR,\n    TMPLIT_EXPR,\n    DATA_ATTR,\n    ARIA_ATTR,\n    IS_SCRIPT_OR_DATA,\n    ATTR_WHITESPACE,\n    CUSTOM_ELEMENT\n  } = EXPRESSIONS;\n  let {\n    IS_ALLOWED_URI: IS_ALLOWED_URI$1\n  } = EXPRESSIONS;\n  /**\n   * We consider the elements and attributes below to be safe. Ideally\n   * don't add any new ones but feel free to remove unwanted ones.\n   */\n  /* allowed element names */\n  let ALLOWED_TAGS = null;\n  const DEFAULT_ALLOWED_TAGS = addToSet({}, [...html$1, ...svg$1, ...svgFilters, ...mathMl$1, ...text]);\n  /* Allowed attribute names */\n  let ALLOWED_ATTR = null;\n  const DEFAULT_ALLOWED_ATTR = addToSet({}, [...html, ...svg, ...mathMl, ...xml]);\n  /*\n   * Configure how DOMPurify should handle custom elements and their attributes as well as customized built-in elements.\n   * @property {RegExp|Function|null} tagNameCheck one of [null, regexPattern, predicate]. Default: `null` (disallow any custom elements)\n   * @property {RegExp|Function|null} attributeNameCheck one of [null, regexPattern, predicate]. Default: `null` (disallow any attributes not on the allow list)\n   * @property {boolean} allowCustomizedBuiltInElements allow custom elements derived from built-ins if they pass CUSTOM_ELEMENT_HANDLING.tagNameCheck. Default: `false`.\n   */\n  let CUSTOM_ELEMENT_HANDLING = Object.seal(create(null, {\n    tagNameCheck: {\n      writable: true,\n      configurable: false,\n      enumerable: true,\n      value: null\n    },\n    attributeNameCheck: {\n      writable: true,\n      configurable: false,\n      enumerable: true,\n      value: null\n    },\n    allowCustomizedBuiltInElements: {\n      writable: true,\n      configurable: false,\n      enumerable: true,\n      value: false\n    }\n  }));\n  /* Explicitly forbidden tags (overrides ALLOWED_TAGS/ADD_TAGS) */\n  let FORBID_TAGS = null;\n  /* Explicitly forbidden attributes (overrides ALLOWED_ATTR/ADD_ATTR) */\n  let FORBID_ATTR = null;\n  /* Decide if ARIA attributes are okay */\n  let ALLOW_ARIA_ATTR = true;\n  /* Decide if custom data attributes are okay */\n  let ALLOW_DATA_ATTR = true;\n  /* Decide if unknown protocols are okay */\n  let ALLOW_UNKNOWN_PROTOCOLS = false;\n  /* Decide if self-closing tags in attributes are allowed.\n   * Usually removed due to a mXSS issue in jQuery 3.0 */\n  let ALLOW_SELF_CLOSE_IN_ATTR = true;\n  /* Output should be safe for common template engines.\n   * This means, DOMPurify removes data attributes, mustaches and ERB\n   */\n  let SAFE_FOR_TEMPLATES = false;\n  /* Output should be safe even for XML used within HTML and alike.\n   * This means, DOMPurify removes comments when containing risky content.\n   */\n  let SAFE_FOR_XML = true;\n  /* Decide if document with <html>... should be returned */\n  let WHOLE_DOCUMENT = false;\n  /* Track whether config is already set on this instance of DOMPurify. */\n  let SET_CONFIG = false;\n  /* Decide if all elements (e.g. style, script) must be children of\n   * document.body. By default, browsers might move them to document.head */\n  let FORCE_BODY = false;\n  /* Decide if a DOM `HTMLBodyElement` should be returned, instead of a html\n   * string (or a TrustedHTML object if Trusted Types are supported).\n   * If `WHOLE_DOCUMENT` is enabled a `HTMLHtmlElement` will be returned instead\n   */\n  let RETURN_DOM = false;\n  /* Decide if a DOM `DocumentFragment` should be returned, instead of a html\n   * string  (or a TrustedHTML object if Trusted Types are supported) */\n  let RETURN_DOM_FRAGMENT = false;\n  /* Try to return a Trusted Type object instead of a string, return a string in\n   * case Trusted Types are not supported  */\n  let RETURN_TRUSTED_TYPE = false;\n  /* Output should be free from DOM clobbering attacks?\n   * This sanitizes markups named with colliding, clobberable built-in DOM APIs.\n   */\n  let SANITIZE_DOM = true;\n  /* Achieve full DOM Clobbering protection by isolating the namespace of named\n   * properties and JS variables, mitigating attacks that abuse the HTML/DOM spec rules.\n   *\n   * HTML/DOM spec rules that enable DOM Clobbering:\n   *   - Named Access on Window (\u00A77.3.3)\n   *   - DOM Tree Accessors (\u00A73.1.5)\n   *   - Form Element Parent-Child Relations (\u00A74.10.3)\n   *   - Iframe srcdoc / Nested WindowProxies (\u00A74.8.5)\n   *   - HTMLCollection (\u00A74.2.10.2)\n   *\n   * Namespace isolation is implemented by prefixing `id` and `name` attributes\n   * with a constant string, i.e., `user-content-`\n   */\n  let SANITIZE_NAMED_PROPS = false;\n  const SANITIZE_NAMED_PROPS_PREFIX = 'user-content-';\n  /* Keep element content when removing element? */\n  let KEEP_CONTENT = true;\n  /* If a `Node` is passed to sanitize(), then performs sanitization in-place instead\n   * of importing it into a new Document and returning a sanitized copy */\n  let IN_PLACE = false;\n  /* Allow usage of profiles like html, svg and mathMl */\n  let USE_PROFILES = {};\n  /* Tags to ignore content of when KEEP_CONTENT is true */\n  let FORBID_CONTENTS = null;\n  const DEFAULT_FORBID_CONTENTS = addToSet({}, ['annotation-xml', 'audio', 'colgroup', 'desc', 'foreignobject', 'head', 'iframe', 'math', 'mi', 'mn', 'mo', 'ms', 'mtext', 'noembed', 'noframes', 'noscript', 'plaintext', 'script', 'style', 'svg', 'template', 'thead', 'title', 'video', 'xmp']);\n  /* Tags that are safe for data: URIs */\n  let DATA_URI_TAGS = null;\n  const DEFAULT_DATA_URI_TAGS = addToSet({}, ['audio', 'video', 'img', 'source', 'image', 'track']);\n  /* Attributes safe for values like \"javascript:\" */\n  let URI_SAFE_ATTRIBUTES = null;\n  const DEFAULT_URI_SAFE_ATTRIBUTES = addToSet({}, ['alt', 'class', 'for', 'id', 'label', 'name', 'pattern', 'placeholder', 'role', 'summary', 'title', 'value', 'style', 'xmlns']);\n  const MATHML_NAMESPACE = 'http://www.w3.org/1998/Math/MathML';\n  const SVG_NAMESPACE = 'http://www.w3.org/2000/svg';\n  const HTML_NAMESPACE = 'http://www.w3.org/1999/xhtml';\n  /* Document namespace */\n  let NAMESPACE = HTML_NAMESPACE;\n  let IS_EMPTY_INPUT = false;\n  /* Allowed XHTML+XML namespaces */\n  let ALLOWED_NAMESPACES = null;\n  const DEFAULT_ALLOWED_NAMESPACES = addToSet({}, [MATHML_NAMESPACE, SVG_NAMESPACE, HTML_NAMESPACE], stringToString);\n  let MATHML_TEXT_INTEGRATION_POINTS = addToSet({}, ['mi', 'mo', 'mn', 'ms', 'mtext']);\n  let HTML_INTEGRATION_POINTS = addToSet({}, ['annotation-xml']);\n  // Certain elements are allowed in both SVG and HTML\n  // namespace. We need to specify them explicitly\n  // so that they don't get erroneously deleted from\n  // HTML namespace.\n  const COMMON_SVG_AND_HTML_ELEMENTS = addToSet({}, ['title', 'style', 'font', 'a', 'script']);\n  /* Parsing of strict XHTML documents */\n  let PARSER_MEDIA_TYPE = null;\n  const SUPPORTED_PARSER_MEDIA_TYPES = ['application/xhtml+xml', 'text/html'];\n  const DEFAULT_PARSER_MEDIA_TYPE = 'text/html';\n  let transformCaseFunc = null;\n  /* Keep a reference to config to pass to hooks */\n  let CONFIG = null;\n  /* Ideally, do not touch anything below this line */\n  /* ______________________________________________ */\n  const formElement = document.createElement('form');\n  const isRegexOrFunction = function isRegexOrFunction(testValue) {\n    return testValue instanceof RegExp || testValue instanceof Function;\n  };\n  /**\n   * _parseConfig\n   *\n   * @param cfg optional config literal\n   */\n  // eslint-disable-next-line complexity\n  const _parseConfig = function _parseConfig() {\n    let cfg = arguments.length > 0 && arguments[0] !== undefined ? arguments[0] : {};\n    if (CONFIG && CONFIG === cfg) {\n      return;\n    }\n    /* Shield configuration object from tampering */\n    if (!cfg || typeof cfg !== 'object') {\n      cfg = {};\n    }\n    /* Shield configuration object from prototype pollution */\n    cfg = clone(cfg);\n    PARSER_MEDIA_TYPE =\n    // eslint-disable-next-line unicorn/prefer-includes\n    SUPPORTED_PARSER_MEDIA_TYPES.indexOf(cfg.PARSER_MEDIA_TYPE) === -1 ? DEFAULT_PARSER_MEDIA_TYPE : cfg.PARSER_MEDIA_TYPE;\n    // HTML tags and attributes are not case-sensitive, converting to lowercase. Keeping XHTML as is.\n    transformCaseFunc = PARSER_MEDIA_TYPE === 'application/xhtml+xml' ? stringToString : stringToLowerCase;\n    /* Set configuration parameters */\n    ALLOWED_TAGS = objectHasOwnProperty(cfg, 'ALLOWED_TAGS') ? addToSet({}, cfg.ALLOWED_TAGS, transformCaseFunc) : DEFAULT_ALLOWED_TAGS;\n    ALLOWED_ATTR = objectHasOwnProperty(cfg, 'ALLOWED_ATTR') ? addToSet({}, cfg.ALLOWED_ATTR, transformCaseFunc) : DEFAULT_ALLOWED_ATTR;\n    ALLOWED_NAMESPACES = objectHasOwnProperty(cfg, 'ALLOWED_NAMESPACES') ? addToSet({}, cfg.ALLOWED_NAMESPACES, stringToString) : DEFAULT_ALLOWED_NAMESPACES;\n    URI_SAFE_ATTRIBUTES = objectHasOwnProperty(cfg, 'ADD_URI_SAFE_ATTR') ? addToSet(clone(DEFAULT_URI_SAFE_ATTRIBUTES), cfg.ADD_URI_SAFE_ATTR, transformCaseFunc) : DEFAULT_URI_SAFE_ATTRIBUTES;\n    DATA_URI_TAGS = objectHasOwnProperty(cfg, 'ADD_DATA_URI_TAGS') ? addToSet(clone(DEFAULT_DATA_URI_TAGS), cfg.ADD_DATA_URI_TAGS, transformCaseFunc) : DEFAULT_DATA_URI_TAGS;\n    FORBID_CONTENTS = objectHasOwnProperty(cfg, 'FORBID_CONTENTS') ? addToSet({}, cfg.FORBID_CONTENTS, transformCaseFunc) : DEFAULT_FORBID_CONTENTS;\n    FORBID_TAGS = objectHasOwnProperty(cfg, 'FORBID_TAGS') ? addToSet({}, cfg.FORBID_TAGS, transformCaseFunc) : {};\n    FORBID_ATTR = objectHasOwnProperty(cfg, 'FORBID_ATTR') ? addToSet({}, cfg.FORBID_ATTR, transformCaseFunc) : {};\n    USE_PROFILES = objectHasOwnProperty(cfg, 'USE_PROFILES') ? cfg.USE_PROFILES : false;\n    ALLOW_ARIA_ATTR = cfg.ALLOW_ARIA_ATTR !== false; // Default true\n    ALLOW_DATA_ATTR = cfg.ALLOW_DATA_ATTR !== false; // Default true\n    ALLOW_UNKNOWN_PROTOCOLS = cfg.ALLOW_UNKNOWN_PROTOCOLS || false; // Default false\n    ALLOW_SELF_CLOSE_IN_ATTR = cfg.ALLOW_SELF_CLOSE_IN_ATTR !== false; // Default true\n    SAFE_FOR_TEMPLATES = cfg.SAFE_FOR_TEMPLATES || false; // Default false\n    SAFE_FOR_XML = cfg.SAFE_FOR_XML !== false; // Default true\n    WHOLE_DOCUMENT = cfg.WHOLE_DOCUMENT || false; // Default false\n    RETURN_DOM = cfg.RETURN_DOM || false; // Default false\n    RETURN_DOM_FRAGMENT = cfg.RETURN_DOM_FRAGMENT || false; // Default false\n    RETURN_TRUSTED_TYPE = cfg.RETURN_TRUSTED_TYPE || false; // Default false\n    FORCE_BODY = cfg.FORCE_BODY || false; // Default false\n    SANITIZE_DOM = cfg.SANITIZE_DOM !== false; // Default true\n    SANITIZE_NAMED_PROPS = cfg.SANITIZE_NAMED_PROPS || false; // Default false\n    KEEP_CONTENT = cfg.KEEP_CONTENT !== false; // Default true\n    IN_PLACE = cfg.IN_PLACE || false; // Default false\n    IS_ALLOWED_URI$1 = cfg.ALLOWED_URI_REGEXP || IS_ALLOWED_URI;\n    NAMESPACE = cfg.NAMESPACE || HTML_NAMESPACE;\n    MATHML_TEXT_INTEGRATION_POINTS = cfg.MATHML_TEXT_INTEGRATION_POINTS || MATHML_TEXT_INTEGRATION_POINTS;\n    HTML_INTEGRATION_POINTS = cfg.HTML_INTEGRATION_POINTS || HTML_INTEGRATION_POINTS;\n    CUSTOM_ELEMENT_HANDLING = cfg.CUSTOM_ELEMENT_HANDLING || {};\n    if (cfg.CUSTOM_ELEMENT_HANDLING && isRegexOrFunction(cfg.CUSTOM_ELEMENT_HANDLING.tagNameCheck)) {\n      CUSTOM_ELEMENT_HANDLING.tagNameCheck = cfg.CUSTOM_ELEMENT_HANDLING.tagNameCheck;\n    }\n    if (cfg.CUSTOM_ELEMENT_HANDLING && isRegexOrFunction(cfg.CUSTOM_ELEMENT_HANDLING.attributeNameCheck)) {\n      CUSTOM_ELEMENT_HANDLING.attributeNameCheck = cfg.CUSTOM_ELEMENT_HANDLING.attributeNameCheck;\n    }\n    if (cfg.CUSTOM_ELEMENT_HANDLING && typeof cfg.CUSTOM_ELEMENT_HANDLING.allowCustomizedBuiltInElements === 'boolean') {\n      CUSTOM_ELEMENT_HANDLING.allowCustomizedBuiltInElements = cfg.CUSTOM_ELEMENT_HANDLING.allowCustomizedBuiltInElements;\n    }\n    if (SAFE_FOR_TEMPLATES) {\n      ALLOW_DATA_ATTR = false;\n    }\n    if (RETURN_DOM_FRAGMENT) {\n      RETURN_DOM = true;\n    }\n    /* Parse profile info */\n    if (USE_PROFILES) {\n      ALLOWED_TAGS = addToSet({}, text);\n      ALLOWED_ATTR = [];\n      if (USE_PROFILES.html === true) {\n        addToSet(ALLOWED_TAGS, html$1);\n        addToSet(ALLOWED_ATTR, html);\n      }\n      if (USE_PROFILES.svg === true) {\n        addToSet(ALLOWED_TAGS, svg$1);\n        addToSet(ALLOWED_ATTR, svg);\n        addToSet(ALLOWED_ATTR, xml);\n      }\n      if (USE_PROFILES.svgFilters === true) {\n        addToSet(ALLOWED_TAGS, svgFilters);\n        addToSet(ALLOWED_ATTR, svg);\n        addToSet(ALLOWED_ATTR, xml);\n      }\n      if (USE_PROFILES.mathMl === true) {\n        addToSet(ALLOWED_TAGS, mathMl$1);\n        addToSet(ALLOWED_ATTR, mathMl);\n        addToSet(ALLOWED_ATTR, xml);\n      }\n    }\n    /* Merge configuration parameters */\n    if (cfg.ADD_TAGS) {\n      if (ALLOWED_TAGS === DEFAULT_ALLOWED_TAGS) {\n        ALLOWED_TAGS = clone(ALLOWED_TAGS);\n      }\n      addToSet(ALLOWED_TAGS, cfg.ADD_TAGS, transformCaseFunc);\n    }\n    if (cfg.ADD_ATTR) {\n      if (ALLOWED_ATTR === DEFAULT_ALLOWED_ATTR) {\n        ALLOWED_ATTR = clone(ALLOWED_ATTR);\n      }\n      addToSet(ALLOWED_ATTR, cfg.ADD_ATTR, transformCaseFunc);\n    }\n    if (cfg.ADD_URI_SAFE_ATTR) {\n      addToSet(URI_SAFE_ATTRIBUTES, cfg.ADD_URI_SAFE_ATTR, transformCaseFunc);\n    }\n    if (cfg.FORBID_CONTENTS) {\n      if (FORBID_CONTENTS === DEFAULT_FORBID_CONTENTS) {\n        FORBID_CONTENTS = clone(FORBID_CONTENTS);\n      }\n      addToSet(FORBID_CONTENTS, cfg.FORBID_CONTENTS, transformCaseFunc);\n    }\n    /* Add #text in case KEEP_CONTENT is set to true */\n    if (KEEP_CONTENT) {\n      ALLOWED_TAGS['#text'] = true;\n    }\n    /* Add html, head and body to ALLOWED_TAGS in case WHOLE_DOCUMENT is true */\n    if (WHOLE_DOCUMENT) {\n      addToSet(ALLOWED_TAGS, ['html', 'head', 'body']);\n    }\n    /* Add tbody to ALLOWED_TAGS in case tables are permitted, see #286, #365 */\n    if (ALLOWED_TAGS.table) {\n      addToSet(ALLOWED_TAGS, ['tbody']);\n      delete FORBID_TAGS.tbody;\n    }\n    if (cfg.TRUSTED_TYPES_POLICY) {\n      if (typeof cfg.TRUSTED_TYPES_POLICY.createHTML !== 'function') {\n        throw typeErrorCreate('TRUSTED_TYPES_POLICY configuration option must provide a \"createHTML\" hook.');\n      }\n      if (typeof cfg.TRUSTED_TYPES_POLICY.createScriptURL !== 'function') {\n        throw typeErrorCreate('TRUSTED_TYPES_POLICY configuration option must provide a \"createScriptURL\" hook.');\n      }\n      // Overwrite existing TrustedTypes policy.\n      trustedTypesPolicy = cfg.TRUSTED_TYPES_POLICY;\n      // Sign local variables required by `sanitize`.\n      emptyHTML = trustedTypesPolicy.createHTML('');\n    } else {\n      // Uninitialized policy, attempt to initialize the internal dompurify policy.\n      if (trustedTypesPolicy === undefined) {\n        trustedTypesPolicy = _createTrustedTypesPolicy(trustedTypes, currentScript);\n      }\n      // If creating the internal policy succeeded sign internal variables.\n      if (trustedTypesPolicy !== null && typeof emptyHTML === 'string') {\n        emptyHTML = trustedTypesPolicy.createHTML('');\n      }\n    }\n    // Prevent further manipulation of configuration.\n    // Not available in IE8, Safari 5, etc.\n    if (freeze) {\n      freeze(cfg);\n    }\n    CONFIG = cfg;\n  };\n  /* Keep track of all possible SVG and MathML tags\n   * so that we can perform the namespace checks\n   * correctly. */\n  const ALL_SVG_TAGS = addToSet({}, [...svg$1, ...svgFilters, ...svgDisallowed]);\n  const ALL_MATHML_TAGS = addToSet({}, [...mathMl$1, ...mathMlDisallowed]);\n  /**\n   * @param element a DOM element whose namespace is being checked\n   * @returns Return false if the element has a\n   *  namespace that a spec-compliant parser would never\n   *  return. Return true otherwise.\n   */\n  const _checkValidNamespace = function _checkValidNamespace(element) {\n    let parent = getParentNode(element);\n    // In JSDOM, if we're inside shadow DOM, then parentNode\n    // can be null. We just simulate parent in this case.\n    if (!parent || !parent.tagName) {\n      parent = {\n        namespaceURI: NAMESPACE,\n        tagName: 'template'\n      };\n    }\n    const tagName = stringToLowerCase(element.tagName);\n    const parentTagName = stringToLowerCase(parent.tagName);\n    if (!ALLOWED_NAMESPACES[element.namespaceURI]) {\n      return false;\n    }\n    if (element.namespaceURI === SVG_NAMESPACE) {\n      // The only way to switch from HTML namespace to SVG\n      // is via <svg>. If it happens via any other tag, then\n      // it should be killed.\n      if (parent.namespaceURI === HTML_NAMESPACE) {\n        return tagName === 'svg';\n      }\n      // The only way to switch from MathML to SVG is via`\n      // svg if parent is either <annotation-xml> or MathML\n      // text integration points.\n      if (parent.namespaceURI === MATHML_NAMESPACE) {\n        return tagName === 'svg' && (parentTagName === 'annotation-xml' || MATHML_TEXT_INTEGRATION_POINTS[parentTagName]);\n      }\n      // We only allow elements that are defined in SVG\n      // spec. All others are disallowed in SVG namespace.\n      return Boolean(ALL_SVG_TAGS[tagName]);\n    }\n    if (element.namespaceURI === MATHML_NAMESPACE) {\n      // The only way to switch from HTML namespace to MathML\n      // is via <math>. If it happens via any other tag, then\n      // it should be killed.\n      if (parent.namespaceURI === HTML_NAMESPACE) {\n        return tagName === 'math';\n      }\n      // The only way to switch from SVG to MathML is via\n      // <math> and HTML integration points\n      if (parent.namespaceURI === SVG_NAMESPACE) {\n        return tagName === 'math' && HTML_INTEGRATION_POINTS[parentTagName];\n      }\n      // We only allow elements that are defined in MathML\n      // spec. All others are disallowed in MathML namespace.\n      return Boolean(ALL_MATHML_TAGS[tagName]);\n    }\n    if (element.namespaceURI === HTML_NAMESPACE) {\n      // The only way to switch from SVG to HTML is via\n      // HTML integration points, and from MathML to HTML\n      // is via MathML text integration points\n      if (parent.namespaceURI === SVG_NAMESPACE && !HTML_INTEGRATION_POINTS[parentTagName]) {\n        return false;\n      }\n      if (parent.namespaceURI === MATHML_NAMESPACE && !MATHML_TEXT_INTEGRATION_POINTS[parentTagName]) {\n        return false;\n      }\n      // We disallow tags that are specific for MathML\n      // or SVG and should never appear in HTML namespace\n      return !ALL_MATHML_TAGS[tagName] && (COMMON_SVG_AND_HTML_ELEMENTS[tagName] || !ALL_SVG_TAGS[tagName]);\n    }\n    // For XHTML and XML documents that support custom namespaces\n    if (PARSER_MEDIA_TYPE === 'application/xhtml+xml' && ALLOWED_NAMESPACES[element.namespaceURI]) {\n      return true;\n    }\n    // The code should never reach this place (this means\n    // that the element somehow got namespace that is not\n    // HTML, SVG, MathML or allowed via ALLOWED_NAMESPACES).\n    // Return false just in case.\n    return false;\n  };\n  /**\n   * _forceRemove\n   *\n   * @param node a DOM node\n   */\n  const _forceRemove = function _forceRemove(node) {\n    arrayPush(DOMPurify.removed, {\n      element: node\n    });\n    try {\n      // eslint-disable-next-line unicorn/prefer-dom-node-remove\n      getParentNode(node).removeChild(node);\n    } catch (_) {\n      remove(node);\n    }\n  };\n  /**\n   * _removeAttribute\n   *\n   * @param name an Attribute name\n   * @param element a DOM node\n   */\n  const _removeAttribute = function _removeAttribute(name, element) {\n    try {\n      arrayPush(DOMPurify.removed, {\n        attribute: element.getAttributeNode(name),\n        from: element\n      });\n    } catch (_) {\n      arrayPush(DOMPurify.removed, {\n        attribute: null,\n        from: element\n      });\n    }\n    element.removeAttribute(name);\n    // We void attribute values for unremovable \"is\" attributes\n    if (name === 'is') {\n      if (RETURN_DOM || RETURN_DOM_FRAGMENT) {\n        try {\n          _forceRemove(element);\n        } catch (_) {}\n      } else {\n        try {\n          element.setAttribute(name, '');\n        } catch (_) {}\n      }\n    }\n  };\n  /**\n   * _initDocument\n   *\n   * @param dirty - a string of dirty markup\n   * @return a DOM, filled with the dirty markup\n   */\n  const _initDocument = function _initDocument(dirty) {\n    /* Create a HTML document */\n    let doc = null;\n    let leadingWhitespace = null;\n    if (FORCE_BODY) {\n      dirty = '<remove></remove>' + dirty;\n    } else {\n      /* If FORCE_BODY isn't used, leading whitespace needs to be preserved manually */\n      const matches = stringMatch(dirty, /^[\\r\\n\\t ]+/);\n      leadingWhitespace = matches && matches[0];\n    }\n    if (PARSER_MEDIA_TYPE === 'application/xhtml+xml' && NAMESPACE === HTML_NAMESPACE) {\n      // Root of XHTML doc must contain xmlns declaration (see https://www.w3.org/TR/xhtml1/normative.html#strict)\n      dirty = '<html xmlns=\"http://www.w3.org/1999/xhtml\"><head></head><body>' + dirty + '</body></html>';\n    }\n    const dirtyPayload = trustedTypesPolicy ? trustedTypesPolicy.createHTML(dirty) : dirty;\n    /*\n     * Use the DOMParser API by default, fallback later if needs be\n     * DOMParser not work for svg when has multiple root element.\n     */\n    if (NAMESPACE === HTML_NAMESPACE) {\n      try {\n        doc = new DOMParser().parseFromString(dirtyPayload, PARSER_MEDIA_TYPE);\n      } catch (_) {}\n    }\n    /* Use createHTMLDocument in case DOMParser is not available */\n    if (!doc || !doc.documentElement) {\n      doc = implementation.createDocument(NAMESPACE, 'template', null);\n      try {\n        doc.documentElement.innerHTML = IS_EMPTY_INPUT ? emptyHTML : dirtyPayload;\n      } catch (_) {\n        // Syntax error if dirtyPayload is invalid xml\n      }\n    }\n    const body = doc.body || doc.documentElement;\n    if (dirty && leadingWhitespace) {\n      body.insertBefore(document.createTextNode(leadingWhitespace), body.childNodes[0] || null);\n    }\n    /* Work on whole document or just its body */\n    if (NAMESPACE === HTML_NAMESPACE) {\n      return getElementsByTagName.call(doc, WHOLE_DOCUMENT ? 'html' : 'body')[0];\n    }\n    return WHOLE_DOCUMENT ? doc.documentElement : body;\n  };\n  /**\n   * Creates a NodeIterator object that you can use to traverse filtered lists of nodes or elements in a document.\n   *\n   * @param root The root element or node to start traversing on.\n   * @return The created NodeIterator\n   */\n  const _createNodeIterator = function _createNodeIterator(root) {\n    return createNodeIterator.call(root.ownerDocument || root, root,\n    // eslint-disable-next-line no-bitwise\n    NodeFilter.SHOW_ELEMENT | NodeFilter.SHOW_COMMENT | NodeFilter.SHOW_TEXT | NodeFilter.SHOW_PROCESSING_INSTRUCTION | NodeFilter.SHOW_CDATA_SECTION, null);\n  };\n  /**\n   * _isClobbered\n   *\n   * @param element element to check for clobbering attacks\n   * @return true if clobbered, false if safe\n   */\n  const _isClobbered = function _isClobbered(element) {\n    return element instanceof HTMLFormElement && (typeof element.nodeName !== 'string' || typeof element.textContent !== 'string' || typeof element.removeChild !== 'function' || !(element.attributes instanceof NamedNodeMap) || typeof element.removeAttribute !== 'function' || typeof element.setAttribute !== 'function' || typeof element.namespaceURI !== 'string' || typeof element.insertBefore !== 'function' || typeof element.hasChildNodes !== 'function');\n  };\n  /**\n   * Checks whether the given object is a DOM node.\n   *\n   * @param value object to check whether it's a DOM node\n   * @return true is object is a DOM node\n   */\n  const _isNode = function _isNode(value) {\n    return typeof Node === 'function' && value instanceof Node;\n  };\n  function _executeHooks(hooks, currentNode, data) {\n    arrayForEach(hooks, hook => {\n      hook.call(DOMPurify, currentNode, data, CONFIG);\n    });\n  }\n  /**\n   * _sanitizeElements\n   *\n   * @protect nodeName\n   * @protect textContent\n   * @protect removeChild\n   * @param currentNode to check for permission to exist\n   * @return true if node was killed, false if left alive\n   */\n  const _sanitizeElements = function _sanitizeElements(currentNode) {\n    let content = null;\n    /* Execute a hook if present */\n    _executeHooks(hooks.beforeSanitizeElements, currentNode, null);\n    /* Check if element is clobbered or can clobber */\n    if (_isClobbered(currentNode)) {\n      _forceRemove(currentNode);\n      return true;\n    }\n    /* Now let's check the element's type and name */\n    const tagName = transformCaseFunc(currentNode.nodeName);\n    /* Execute a hook if present */\n    _executeHooks(hooks.uponSanitizeElement, currentNode, {\n      tagName,\n      allowedTags: ALLOWED_TAGS\n    });\n    /* Detect mXSS attempts abusing namespace confusion */\n    if (currentNode.hasChildNodes() && !_isNode(currentNode.firstElementChild) && regExpTest(/<[/\\w]/g, currentNode.innerHTML) && regExpTest(/<[/\\w]/g, currentNode.textContent)) {\n      _forceRemove(currentNode);\n      return true;\n    }\n    /* Remove any occurrence of processing instructions */\n    if (currentNode.nodeType === NODE_TYPE.progressingInstruction) {\n      _forceRemove(currentNode);\n      return true;\n    }\n    /* Remove any kind of possibly harmful comments */\n    if (SAFE_FOR_XML && currentNode.nodeType === NODE_TYPE.comment && regExpTest(/<[/\\w]/g, currentNode.data)) {\n      _forceRemove(currentNode);\n      return true;\n    }\n    /* Remove element if anything forbids its presence */\n    if (!ALLOWED_TAGS[tagName] || FORBID_TAGS[tagName]) {\n      /* Check if we have a custom element to handle */\n      if (!FORBID_TAGS[tagName] && _isBasicCustomElement(tagName)) {\n        if (CUSTOM_ELEMENT_HANDLING.tagNameCheck instanceof RegExp && regExpTest(CUSTOM_ELEMENT_HANDLING.tagNameCheck, tagName)) {\n          return false;\n        }\n        if (CUSTOM_ELEMENT_HANDLING.tagNameCheck instanceof Function && CUSTOM_ELEMENT_HANDLING.tagNameCheck(tagName)) {\n          return false;\n        }\n      }\n      /* Keep content except for bad-listed elements */\n      if (KEEP_CONTENT && !FORBID_CONTENTS[tagName]) {\n        const parentNode = getParentNode(currentNode) || currentNode.parentNode;\n        const childNodes = getChildNodes(currentNode) || currentNode.childNodes;\n        if (childNodes && parentNode) {\n          const childCount = childNodes.length;\n          for (let i = childCount - 1; i >= 0; --i) {\n            const childClone = cloneNode(childNodes[i], true);\n            childClone.__removalCount = (currentNode.__removalCount || 0) + 1;\n            parentNode.insertBefore(childClone, getNextSibling(currentNode));\n          }\n        }\n      }\n      _forceRemove(currentNode);\n      return true;\n    }\n    /* Check whether element has a valid namespace */\n    if (currentNode instanceof Element && !_checkValidNamespace(currentNode)) {\n      _forceRemove(currentNode);\n      return true;\n    }\n    /* Make sure that older browsers don't get fallback-tag mXSS */\n    if ((tagName === 'noscript' || tagName === 'noembed' || tagName === 'noframes') && regExpTest(/<\\/no(script|embed|frames)/i, currentNode.innerHTML)) {\n      _forceRemove(currentNode);\n      return true;\n    }\n    /* Sanitize element content to be template-safe */\n    if (SAFE_FOR_TEMPLATES && currentNode.nodeType === NODE_TYPE.text) {\n      /* Get the element's text content */\n      content = currentNode.textContent;\n      arrayForEach([MUSTACHE_EXPR, ERB_EXPR, TMPLIT_EXPR], expr => {\n        content = stringReplace(content, expr, ' ');\n      });\n      if (currentNode.textContent !== content) {\n        arrayPush(DOMPurify.removed, {\n          element: currentNode.cloneNode()\n        });\n        currentNode.textContent = content;\n      }\n    }\n    /* Execute a hook if present */\n    _executeHooks(hooks.afterSanitizeElements, currentNode, null);\n    return false;\n  };\n  /**\n   * _isValidAttribute\n   *\n   * @param lcTag Lowercase tag name of containing element.\n   * @param lcName Lowercase attribute name.\n   * @param value Attribute value.\n   * @return Returns true if `value` is valid, otherwise false.\n   */\n  // eslint-disable-next-line complexity\n  const _isValidAttribute = function _isValidAttribute(lcTag, lcName, value) {\n    /* Make sure attribute cannot clobber */\n    if (SANITIZE_DOM && (lcName === 'id' || lcName === 'name') && (value in document || value in formElement)) {\n      return false;\n    }\n    /* Allow valid data-* attributes: At least one character after \"-\"\n        (https://html.spec.whatwg.org/multipage/dom.html#embedding-custom-non-visible-data-with-the-data-*-attributes)\n        XML-compatible (https://html.spec.whatwg.org/multipage/infrastructure.html#xml-compatible and http://www.w3.org/TR/xml/#d0e804)\n        We don't need to check the value; it's always URI safe. */\n    if (ALLOW_DATA_ATTR && !FORBID_ATTR[lcName] && regExpTest(DATA_ATTR, lcName)) ; else if (ALLOW_ARIA_ATTR && regExpTest(ARIA_ATTR, lcName)) ; else if (!ALLOWED_ATTR[lcName] || FORBID_ATTR[lcName]) {\n      if (\n      // First condition does a very basic check if a) it's basically a valid custom element tagname AND\n      // b) if the tagName passes whatever the user has configured for CUSTOM_ELEMENT_HANDLING.tagNameCheck\n      // and c) if the attribute name passes whatever the user has configured for CUSTOM_ELEMENT_HANDLING.attributeNameCheck\n      _isBasicCustomElement(lcTag) && (CUSTOM_ELEMENT_HANDLING.tagNameCheck instanceof RegExp && regExpTest(CUSTOM_ELEMENT_HANDLING.tagNameCheck, lcTag) || CUSTOM_ELEMENT_HANDLING.tagNameCheck instanceof Function && CUSTOM_ELEMENT_HANDLING.tagNameCheck(lcTag)) && (CUSTOM_ELEMENT_HANDLING.attributeNameCheck instanceof RegExp && regExpTest(CUSTOM_ELEMENT_HANDLING.attributeNameCheck, lcName) || CUSTOM_ELEMENT_HANDLING.attributeNameCheck instanceof Function && CUSTOM_ELEMENT_HANDLING.attributeNameCheck(lcName)) ||\n      // Alternative, second condition checks if it's an `is`-attribute, AND\n      // the value passes whatever the user has configured for CUSTOM_ELEMENT_HANDLING.tagNameCheck\n      lcName === 'is' && CUSTOM_ELEMENT_HANDLING.allowCustomizedBuiltInElements && (CUSTOM_ELEMENT_HANDLING.tagNameCheck instanceof RegExp && regExpTest(CUSTOM_ELEMENT_HANDLING.tagNameCheck, value) || CUSTOM_ELEMENT_HANDLING.tagNameCheck instanceof Function && CUSTOM_ELEMENT_HANDLING.tagNameCheck(value))) ; else {\n        return false;\n      }\n      /* Check value is safe. First, is attr inert? If so, is safe */\n    } else if (URI_SAFE_ATTRIBUTES[lcName]) ; else if (regExpTest(IS_ALLOWED_URI$1, stringReplace(value, ATTR_WHITESPACE, ''))) ; else if ((lcName === 'src' || lcName === 'xlink:href' || lcName === 'href') && lcTag !== 'script' && stringIndexOf(value, 'data:') === 0 && DATA_URI_TAGS[lcTag]) ; else if (ALLOW_UNKNOWN_PROTOCOLS && !regExpTest(IS_SCRIPT_OR_DATA, stringReplace(value, ATTR_WHITESPACE, ''))) ; else if (value) {\n      return false;\n    } else ;\n    return true;\n  };\n  /**\n   * _isBasicCustomElement\n   * checks if at least one dash is included in tagName, and it's not the first char\n   * for more sophisticated checking see https://github.com/sindresorhus/validate-element-name\n   *\n   * @param tagName name of the tag of the node to sanitize\n   * @returns Returns true if the tag name meets the basic criteria for a custom element, otherwise false.\n   */\n  const _isBasicCustomElement = function _isBasicCustomElement(tagName) {\n    return tagName !== 'annotation-xml' && stringMatch(tagName, CUSTOM_ELEMENT);\n  };\n  /**\n   * _sanitizeAttributes\n   *\n   * @protect attributes\n   * @protect nodeName\n   * @protect removeAttribute\n   * @protect setAttribute\n   *\n   * @param currentNode to sanitize\n   */\n  const _sanitizeAttributes = function _sanitizeAttributes(currentNode) {\n    /* Execute a hook if present */\n    _executeHooks(hooks.beforeSanitizeAttributes, currentNode, null);\n    const {\n      attributes\n    } = currentNode;\n    /* Check if we have attributes; if not we might have a text node */\n    if (!attributes || _isClobbered(currentNode)) {\n      return;\n    }\n    const hookEvent = {\n      attrName: '',\n      attrValue: '',\n      keepAttr: true,\n      allowedAttributes: ALLOWED_ATTR,\n      forceKeepAttr: undefined\n    };\n    let l = attributes.length;\n    /* Go backwards over all attributes; safely remove bad ones */\n    while (l--) {\n      const attr = attributes[l];\n      const {\n        name,\n        namespaceURI,\n        value: attrValue\n      } = attr;\n      const lcName = transformCaseFunc(name);\n      let value = name === 'value' ? attrValue : stringTrim(attrValue);\n      /* Execute a hook if present */\n      hookEvent.attrName = lcName;\n      hookEvent.attrValue = value;\n      hookEvent.keepAttr = true;\n      hookEvent.forceKeepAttr = undefined; // Allows developers to see this is a property they can set\n      _executeHooks(hooks.uponSanitizeAttribute, currentNode, hookEvent);\n      value = hookEvent.attrValue;\n      /* Full DOM Clobbering protection via namespace isolation,\n       * Prefix id and name attributes with `user-content-`\n       */\n      if (SANITIZE_NAMED_PROPS && (lcName === 'id' || lcName === 'name')) {\n        // Remove the attribute with this value\n        _removeAttribute(name, currentNode);\n        // Prefix the value and later re-create the attribute with the sanitized value\n        value = SANITIZE_NAMED_PROPS_PREFIX + value;\n      }\n      /* Work around a security issue with comments inside attributes */\n      if (SAFE_FOR_XML && regExpTest(/((--!?|])>)|<\\/(style|title)/i, value)) {\n        _removeAttribute(name, currentNode);\n        continue;\n      }\n      /* Did the hooks approve of the attribute? */\n      if (hookEvent.forceKeepAttr) {\n        continue;\n      }\n      /* Remove attribute */\n      _removeAttribute(name, currentNode);\n      /* Did the hooks approve of the attribute? */\n      if (!hookEvent.keepAttr) {\n        continue;\n      }\n      /* Work around a security issue in jQuery 3.0 */\n      if (!ALLOW_SELF_CLOSE_IN_ATTR && regExpTest(/\\/>/i, value)) {\n        _removeAttribute(name, currentNode);\n        continue;\n      }\n      /* Sanitize attribute content to be template-safe */\n      if (SAFE_FOR_TEMPLATES) {\n        arrayForEach([MUSTACHE_EXPR, ERB_EXPR, TMPLIT_EXPR], expr => {\n          value = stringReplace(value, expr, ' ');\n        });\n      }\n      /* Is `value` valid for this attribute? */\n      const lcTag = transformCaseFunc(currentNode.nodeName);\n      if (!_isValidAttribute(lcTag, lcName, value)) {\n        continue;\n      }\n      /* Handle attributes that require Trusted Types */\n      if (trustedTypesPolicy && typeof trustedTypes === 'object' && typeof trustedTypes.getAttributeType === 'function') {\n        if (namespaceURI) ; else {\n          switch (trustedTypes.getAttributeType(lcTag, lcName)) {\n            case 'TrustedHTML':\n              {\n                value = trustedTypesPolicy.createHTML(value);\n                break;\n              }\n            case 'TrustedScriptURL':\n              {\n                value = trustedTypesPolicy.createScriptURL(value);\n                break;\n              }\n          }\n        }\n      }\n      /* Handle invalid data-* attribute set by try-catching it */\n      try {\n        if (namespaceURI) {\n          currentNode.setAttributeNS(namespaceURI, name, value);\n        } else {\n          /* Fallback to setAttribute() for browser-unrecognized namespaces e.g. \"x-schema\". */\n          currentNode.setAttribute(name, value);\n        }\n        if (_isClobbered(currentNode)) {\n          _forceRemove(currentNode);\n        } else {\n          arrayPop(DOMPurify.removed);\n        }\n      } catch (_) {}\n    }\n    /* Execute a hook if present */\n    _executeHooks(hooks.afterSanitizeAttributes, currentNode, null);\n  };\n  /**\n   * _sanitizeShadowDOM\n   *\n   * @param fragment to iterate over recursively\n   */\n  const _sanitizeShadowDOM = function _sanitizeShadowDOM(fragment) {\n    let shadowNode = null;\n    const shadowIterator = _createNodeIterator(fragment);\n    /* Execute a hook if present */\n    _executeHooks(hooks.beforeSanitizeShadowDOM, fragment, null);\n    while (shadowNode = shadowIterator.nextNode()) {\n      /* Execute a hook if present */\n      _executeHooks(hooks.uponSanitizeShadowNode, shadowNode, null);\n      /* Sanitize tags and elements */\n      _sanitizeElements(shadowNode);\n      /* Check attributes next */\n      _sanitizeAttributes(shadowNode);\n      /* Deep shadow DOM detected */\n      if (shadowNode.content instanceof DocumentFragment) {\n        _sanitizeShadowDOM(shadowNode.content);\n      }\n    }\n    /* Execute a hook if present */\n    _executeHooks(hooks.afterSanitizeShadowDOM, fragment, null);\n  };\n  // eslint-disable-next-line complexity\n  DOMPurify.sanitize = function (dirty) {\n    let cfg = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : {};\n    let body = null;\n    let importedNode = null;\n    let currentNode = null;\n    let returnNode = null;\n    /* Make sure we have a string to sanitize.\n      DO NOT return early, as this will return the wrong type if\n      the user has requested a DOM object rather than a string */\n    IS_EMPTY_INPUT = !dirty;\n    if (IS_EMPTY_INPUT) {\n      dirty = '<!-->';\n    }\n    /* Stringify, in case dirty is an object */\n    if (typeof dirty !== 'string' && !_isNode(dirty)) {\n      if (typeof dirty.toString === 'function') {\n        dirty = dirty.toString();\n        if (typeof dirty !== 'string') {\n          throw typeErrorCreate('dirty is not a string, aborting');\n        }\n      } else {\n        throw typeErrorCreate('toString is not a function');\n      }\n    }\n    /* Return dirty HTML if DOMPurify cannot run */\n    if (!DOMPurify.isSupported) {\n      return dirty;\n    }\n    /* Assign config vars */\n    if (!SET_CONFIG) {\n      _parseConfig(cfg);\n    }\n    /* Clean up removed elements */\n    DOMPurify.removed = [];\n    /* Check if dirty is correctly typed for IN_PLACE */\n    if (typeof dirty === 'string') {\n      IN_PLACE = false;\n    }\n    if (IN_PLACE) {\n      /* Do some early pre-sanitization to avoid unsafe root nodes */\n      if (dirty.nodeName) {\n        const tagName = transformCaseFunc(dirty.nodeName);\n        if (!ALLOWED_TAGS[tagName] || FORBID_TAGS[tagName]) {\n          throw typeErrorCreate('root node is forbidden and cannot be sanitized in-place');\n        }\n      }\n    } else if (dirty instanceof Node) {\n      /* If dirty is a DOM element, append to an empty document to avoid\n         elements being stripped by the parser */\n      body = _initDocument('<!---->');\n      importedNode = body.ownerDocument.importNode(dirty, true);\n      if (importedNode.nodeType === NODE_TYPE.element && importedNode.nodeName === 'BODY') {\n        /* Node is already a body, use as is */\n        body = importedNode;\n      } else if (importedNode.nodeName === 'HTML') {\n        body = importedNode;\n      } else {\n        // eslint-disable-next-line unicorn/prefer-dom-node-append\n        body.appendChild(importedNode);\n      }\n    } else {\n      /* Exit directly if we have nothing to do */\n      if (!RETURN_DOM && !SAFE_FOR_TEMPLATES && !WHOLE_DOCUMENT &&\n      // eslint-disable-next-line unicorn/prefer-includes\n      dirty.indexOf('<') === -1) {\n        return trustedTypesPolicy && RETURN_TRUSTED_TYPE ? trustedTypesPolicy.createHTML(dirty) : dirty;\n      }\n      /* Initialize the document to work on */\n      body = _initDocument(dirty);\n      /* Check we have a DOM node from the data */\n      if (!body) {\n        return RETURN_DOM ? null : RETURN_TRUSTED_TYPE ? emptyHTML : '';\n      }\n    }\n    /* Remove first element node (ours) if FORCE_BODY is set */\n    if (body && FORCE_BODY) {\n      _forceRemove(body.firstChild);\n    }\n    /* Get node iterator */\n    const nodeIterator = _createNodeIterator(IN_PLACE ? dirty : body);\n    /* Now start iterating over the created document */\n    while (currentNode = nodeIterator.nextNode()) {\n      /* Sanitize tags and elements */\n      _sanitizeElements(currentNode);\n      /* Check attributes next */\n      _sanitizeAttributes(currentNode);\n      /* Shadow DOM detected, sanitize it */\n      if (currentNode.content instanceof DocumentFragment) {\n        _sanitizeShadowDOM(currentNode.content);\n      }\n    }\n    /* If we sanitized `dirty` in-place, return it. */\n    if (IN_PLACE) {\n      return dirty;\n    }\n    /* Return sanitized string or DOM */\n    if (RETURN_DOM) {\n      if (RETURN_DOM_FRAGMENT) {\n        returnNode = createDocumentFragment.call(body.ownerDocument);\n        while (body.firstChild) {\n          // eslint-disable-next-line unicorn/prefer-dom-node-append\n          returnNode.appendChild(body.firstChild);\n        }\n      } else {\n        returnNode = body;\n      }\n      if (ALLOWED_ATTR.shadowroot || ALLOWED_ATTR.shadowrootmode) {\n        /*\n          AdoptNode() is not used because internal state is not reset\n          (e.g. the past names map of a HTMLFormElement), this is safe\n          in theory but we would rather not risk another attack vector.\n          The state that is cloned by importNode() is explicitly defined\n          by the specs.\n        */\n        returnNode = importNode.call(originalDocument, returnNode, true);\n      }\n      return returnNode;\n    }\n    let serializedHTML = WHOLE_DOCUMENT ? body.outerHTML : body.innerHTML;\n    /* Serialize doctype if allowed */\n    if (WHOLE_DOCUMENT && ALLOWED_TAGS['!doctype'] && body.ownerDocument && body.ownerDocument.doctype && body.ownerDocument.doctype.name && regExpTest(DOCTYPE_NAME, body.ownerDocument.doctype.name)) {\n      serializedHTML = '<!DOCTYPE ' + body.ownerDocument.doctype.name + '>\\n' + serializedHTML;\n    }\n    /* Sanitize final string template-safe */\n    if (SAFE_FOR_TEMPLATES) {\n      arrayForEach([MUSTACHE_EXPR, ERB_EXPR, TMPLIT_EXPR], expr => {\n        serializedHTML = stringReplace(serializedHTML, expr, ' ');\n      });\n    }\n    return trustedTypesPolicy && RETURN_TRUSTED_TYPE ? trustedTypesPolicy.createHTML(serializedHTML) : serializedHTML;\n  };\n  DOMPurify.setConfig = function () {\n    let cfg = arguments.length > 0 && arguments[0] !== undefined ? arguments[0] : {};\n    _parseConfig(cfg);\n    SET_CONFIG = true;\n  };\n  DOMPurify.clearConfig = function () {\n    CONFIG = null;\n    SET_CONFIG = false;\n  };\n  DOMPurify.isValidAttribute = function (tag, attr, value) {\n    /* Initialize shared config vars if necessary. */\n    if (!CONFIG) {\n      _parseConfig({});\n    }\n    const lcTag = transformCaseFunc(tag);\n    const lcName = transformCaseFunc(attr);\n    return _isValidAttribute(lcTag, lcName, value);\n  };\n  DOMPurify.addHook = function (entryPoint, hookFunction) {\n    if (typeof hookFunction !== 'function') {\n      return;\n    }\n    arrayPush(hooks[entryPoint], hookFunction);\n  };\n  DOMPurify.removeHook = function (entryPoint, hookFunction) {\n    if (hookFunction !== undefined) {\n      const index = arrayLastIndexOf(hooks[entryPoint], hookFunction);\n      return index === -1 ? undefined : arraySplice(hooks[entryPoint], index, 1)[0];\n    }\n    return arrayPop(hooks[entryPoint]);\n  };\n  DOMPurify.removeHooks = function (entryPoint) {\n    hooks[entryPoint] = [];\n  };\n  DOMPurify.removeAllHooks = function () {\n    hooks = _createHooksMap();\n  };\n  return DOMPurify;\n}\nvar purify = createDOMPurify();\n\nexport { purify as default };\n//# sourceMappingURL=purify.es.mjs.map\n", "import createDOMPurify from \"dompurify\";\nimport { JSDOM } from \"jsdom\";\n\nconst windowEmulator: any = new JSDOM(\"\").window;\nconst DOMPurify = createDOMPurify(windowEmulator);\n\n/*\n * DOMPurify prevents all XSS attacks by default. With these settings, it also\n * prevents \"deception\" attacks. If an attacker could put <div style=\"...\">\n * into the site's admin banner, they could make give the banner any appearance,\n * overlaid anywhere on the page. For example, a fake \"session expired\" modal\n * with a malicious link. Thus, this very strict DOMPurify config.\n */\nDOMPurify.setConfig({\n  // Only these tags will be allowed through\n  ALLOWED_TAGS: [\"ul\", \"ol\", \"li\", \"a\", \"#text\"],\n  // On those tags, only these attributes are allowed\n  ALLOWED_ATTR: [\"href\", \"alt\"],\n  // If a tag is removed, so will all its child elements & text\n  KEEP_CONTENT: false,\n});\n\n// sanitize string\nexport const sanitizeString = (string: string) => {\n  if (DOMPurify.isSupported) {\n    return DOMPurify.sanitize(string);\n  }\n};\n\n// iterates over array items, sanitizing items recursively\nexport const sanitizeArray = (array: unknown[]): unknown[] =>\n  array.map((entry: unknown) => sanitizeEntry(entry));\n\n// iterates over object key-value pairs, sanitizing values recursively\nexport const sanitizeObject = (object: { [key: string]: unknown }) => {\n  if (object) {\n    const entries = Object.entries(object);\n    const sanitizedEntries = entries.map((entry: [string, unknown]) => {\n      const [key, value] = entry;\n      return [key, sanitizeEntry(value)];\n    });\n    return Object.fromEntries(sanitizedEntries);\n  }\n};\n\nconst sanitizerMap: any = {\n  string: sanitizeString,\n  array: sanitizeArray,\n  object: sanitizeObject,\n};\n\n// return sanitized entry, or if safe type, return entry\nconst sanitizeEntry = (entry: unknown) => {\n  const entryType = Array.isArray(entry) ? \"array\" : typeof entry;\n  const sanitizer = sanitizerMap[entryType];\n  return sanitizer?.(entry) || entry;\n};\n", "// utils\nimport * as logger from \"../utils/debugging/debug-lib\";\nimport { isAuthenticated } from \"../utils/auth/authorization\";\nimport {\n  HttpResponse,\n  internalServerError,\n  unauthenticated,\n} from \"../utils/responses/response-lib\";\nimport { error } from \"../utils/constants/constants\";\nimport { sanitizeObject } from \"../utils/sanitize/sanitize\";\n// types\nimport { APIGatewayProxyEvent } from \"../utils/types\";\n\ntype LambdaFunction = (\n  event: APIGatewayProxyEvent, // eslint-disable-line no-unused-vars\n  context: any // eslint-disable-line no-unused-vars\n) => Promise<HttpResponse>;\n\nexport default function handler(lambda: LambdaFunction) {\n  return async function (event: APIGatewayProxyEvent, context: any) {\n    // Start debugger\n    logger.init();\n    logger.debug(\"API event: %O\", {\n      body: event.body,\n      pathParameters: event.pathParameters,\n      queryStringParameters: event.queryStringParameters,\n    });\n    if (await isAuthenticated(event)) {\n      try {\n        if (event.body) {\n          const newEventBody = sanitizeObject(JSON.parse(event.body));\n          event.body = JSON.stringify(newEventBody);\n        }\n        return await lambda(event, context);\n      } catch (error: any) {\n        logger.error(\"Error: %O\", error);\n\n        const body = { error: error.message };\n        return internalServerError(body);\n      } finally {\n        logger.flush();\n      }\n    } else {\n      return unauthenticated(error.UNAUTHORIZED);\n    }\n  };\n}\n", "import { DeleteCommand, paginateScan, PutCommand } from \"@aws-sdk/lib-dynamodb\";\nimport { createClient } from \"./dynamodb-lib\";\nimport { AdminBannerData } from \"../utils/types/banner\";\nimport { AnyObject } from \"../utils/types\";\n\nconst bannerTableName = process.env.BannerTable!;\nconst client = createClient();\n\nexport const putBanner = async (banner: AdminBannerData) => {\n  await client.send(\n    new PutCommand({\n      TableName: bannerTableName,\n      Item: banner,\n    })\n  );\n};\n\nexport const getBanners = async () => {\n  let items: AnyObject[] = [];\n  const params = {\n    TableName: bannerTableName,\n  };\n\n  for await (const page of paginateScan({ client }, params)) {\n    items = items.concat(page.Items ?? []);\n  }\n\n  return items as AdminBannerData[] | undefined;\n};\n\nexport const deleteBanner = async (bannerId: string) => {\n  await client.send(\n    new DeleteCommand({\n      TableName: bannerTableName,\n      Key: {\n        key: bannerId,\n      },\n    })\n  );\n};\n", "import {\n  DynamoDBClient,\n  QueryCommandOutput,\n  ScanCommandOutput,\n} from \"@aws-sdk/client-dynamodb\";\nimport { DynamoDBDocumentClient, Paginator } from \"@aws-sdk/lib-dynamodb\";\n// utils\nimport { logger } from \"../utils/debugging/debug-lib\";\n\nconst localConfig = {\n  endpoint: process.env.DYNAMODB_URL,\n  region: \"localhost\",\n  credentials: {\n    accessKeyId: \"LOCALFAKEKEY\", // pragma: allowlist secret\n    secretAccessKey: \"LOCALFAKESECRET\", // pragma: allowlist secret\n  },\n  logger,\n};\n\nconst awsConfig = {\n  region: \"us-east-1\",\n  logger,\n};\n\nconst getConfig = () => {\n  return process.env.DYNAMODB_URL ? localConfig : awsConfig;\n};\n\nexport const createClient = () => {\n  return DynamoDBDocumentClient.from(new DynamoDBClient(getConfig()));\n};\n\nexport const collectPageItems = async <\n  T extends QueryCommandOutput | ScanCommandOutput\n>(\n  paginator: Paginator<T>\n) => {\n  let items: Record<string, any>[] = [];\n  for await (let page of paginator) {\n    items = items.concat(page.Items ?? []);\n  }\n  return items;\n};\n", "import { object } from \"yup\";\nimport { error } from \"../constants/constants\";\n// types\nimport { AnyObject } from \"../types\";\n// utils\nimport { nested, endDate, schemaMap } from \"./schemaMap\";\n\n// compare payload data against validation schema\nexport const validateData = async (\n  validationSchema: AnyObject,\n  data: AnyObject,\n  options?: AnyObject\n) => {\n  try {\n    // returns valid data to be passed through API\n    return await validationSchema.validate(data, {\n      stripUnknown: true,\n      ...options,\n    });\n  } catch {\n    throw new Error(error.INVALID_DATA);\n  }\n};\n\n// filter field validation to just what's needed for the passed fields\nexport const filterValidationSchema = (\n  validationObject: AnyObject,\n  data: AnyObject\n): AnyObject => {\n  const validationEntries = Object.entries(validationObject);\n  const dataKeys = Object.keys(data);\n  const filteredEntries = validationEntries.filter(\n    (entry: [string, string | AnyObject]) => {\n      const [entryKey] = entry;\n      return dataKeys.includes(entryKey);\n    }\n  );\n  return Object.fromEntries(filteredEntries);\n};\n\n// map field validation types to validation schema\nexport const mapValidationTypesToSchema = (fieldValidationTypes: AnyObject) => {\n  let validationSchema: AnyObject = {};\n  // for each field to be validated,\n  Object.entries(fieldValidationTypes).forEach(\n    (fieldValidationType: [string, string | AnyObject]) => {\n      const [key, fieldValidation] = fieldValidationType;\n      // if standard validation type, set corresponding schema from map\n      if (typeof fieldValidation === \"string\") {\n        const correspondingSchema = schemaMap[fieldValidation];\n        if (correspondingSchema) {\n          validationSchema[key] = correspondingSchema;\n        }\n      }\n      // else if nested validation type, make and set nested schema\n      else if (fieldValidation.nested) {\n        validationSchema[key] = makeNestedFieldSchema(fieldValidation);\n        // else if not nested, make and set other dependent field types\n      } else if (fieldValidation.type === \"endDate\") {\n        validationSchema[key] = makeEndDateFieldSchema(fieldValidation);\n      }\n    }\n  );\n  return validationSchema;\n};\n\n// return created endDate schema\nexport const makeEndDateFieldSchema = (fieldValidationObject: AnyObject) => {\n  const { dependentFieldName } = fieldValidationObject;\n  return endDate(dependentFieldName);\n};\n\n// return created nested field schema\nexport const makeNestedFieldSchema = (fieldValidationObject: AnyObject) => {\n  const { type, parentFieldName, parentOptionId } = fieldValidationObject;\n  if (fieldValidationObject.type === \"endDate\") {\n    return nested(\n      () => makeEndDateFieldSchema(fieldValidationObject),\n      parentFieldName,\n      parentOptionId\n    );\n  } else {\n    const fieldValidationSchema = schemaMap[type];\n    return nested(() => fieldValidationSchema, parentFieldName, parentOptionId);\n  }\n};\n\nexport const validateFieldData = async (\n  validationJson: AnyObject,\n  unvalidatedFieldData: AnyObject\n) => {\n  let validatedFieldData: AnyObject | undefined = undefined;\n  // filter field validation to just what's needed for the passed fields\n  const filteredFieldDataValidationJson = filterValidationSchema(\n    validationJson,\n    unvalidatedFieldData\n  );\n  // transform field validation instructions to yup validation schema\n  const fieldDataValidationSchema = object().shape(\n    mapValidationTypesToSchema(filteredFieldDataValidationJson)\n  );\n  if (fieldDataValidationSchema) {\n    validatedFieldData = await validateData(\n      fieldDataValidationSchema,\n      unvalidatedFieldData\n    );\n  }\n  return validatedFieldData;\n};\n", "import { array, boolean, mixed, object, string } from \"yup\";\nimport { Choice } from \"../types/index\";\nimport {\n  checkRatioInputAgainstRegexes,\n  checkStandardIntegerInputAgainstRegexes,\n  checkStandardNumberInputAgainstRegexes,\n} from \"./checkInputValidity\";\n\nconst error = {\n  REQUIRED_GENERIC: \"A response is required\",\n  REQUIRED_CHECKBOX: \"Select at least one response\",\n  INVALID_GENERIC: \"Response must be valid\",\n  INVALID_EMAIL: \"Response must be a valid email address\",\n  INVALID_URL: \"Response must be a valid hyperlink/URL\",\n  INVALID_DATE: \"Response must be a valid date\",\n  INVALID_END_DATE: \"End date can't be before start date\",\n  NUMBER_LESS_THAN_ZERO: \"Response must be greater than or equal to zero\",\n  INVALID_NUMBER: \"Response must be a valid number\",\n  INVALID_NUMBER_OR_NA: 'Response must be a valid number or \"N/A\"',\n  INVALID_RATIO: \"Response must be a valid ratio\",\n};\n\n// TEXT - Helpers\nconst isWhitespaceString = (value?: string) => value?.trim().length === 0;\n\n// TEXT\nexport const text = () =>\n  string()\n    .typeError(error.INVALID_GENERIC)\n    .required(error.REQUIRED_GENERIC)\n    .test({\n      test: (value) => !isWhitespaceString(value),\n      message: error.REQUIRED_GENERIC,\n    });\nexport const textOptional = () => string().typeError(error.INVALID_GENERIC);\n\n// NUMBER - Helpers\nconst validNAValues = [\"N/A\", \"Data not available\"];\n// const validNumberRegex = /^\\.$|[0-9]/;\nconst validIntegerRegex = /^[0-9\\s,$%]+$/;\n\n// NUMBER - Number or Valid Strings\nexport const numberSchema = () =>\n  string()\n    .test({\n      message: error.INVALID_NUMBER_OR_NA,\n      test: (value) => {\n        if (value) {\n          const isValidStringValue = validNAValues.includes(value);\n          const isValidNumberValue =\n            checkStandardNumberInputAgainstRegexes(value);\n          return isValidStringValue || isValidNumberValue;\n        } else return true;\n      },\n    })\n    .test({\n      test: (value) => {\n        if (checkStandardNumberInputAgainstRegexes(value!)) {\n          return parseFloat(value!) >= 0;\n        } else return true;\n      },\n      message: error.NUMBER_LESS_THAN_ZERO,\n    });\n\nexport const number = () =>\n  numberSchema()\n    .required(error.REQUIRED_GENERIC)\n    .test({\n      test: (value) => !isWhitespaceString(value),\n      message: error.REQUIRED_GENERIC,\n    });\n\nexport const numberOptional = () => numberSchema().notRequired().nullable();\n\n// Integer or Valid Strings\nexport const validIntegerSchema = () =>\n  string()\n    .test({\n      message: error.INVALID_NUMBER_OR_NA,\n      test: (value) => {\n        if (value) {\n          const isValidStringValue = validNAValues.includes(value);\n          const isValidIntegerValue = validIntegerRegex.test(value);\n          return isValidStringValue || isValidIntegerValue;\n        } else return true;\n      },\n    })\n    .test({\n      test: (value) => {\n        if (checkStandardIntegerInputAgainstRegexes(value!)) {\n          return parseFloat(value!) >= 0;\n        } else return true;\n      },\n      message: error.NUMBER_LESS_THAN_ZERO,\n    });\n\nexport const validInteger = () =>\n  validIntegerSchema().required(error.REQUIRED_GENERIC);\n\nexport const validIntegerOptional = () =>\n  validIntegerSchema().notRequired().nullable();\n\n// Number - Ratio\nexport const ratio = () =>\n  mixed()\n    .test({\n      message: error.REQUIRED_GENERIC,\n      test: (val) => val != \"\",\n    })\n    .required(error.REQUIRED_GENERIC)\n    .test({\n      message: error.INVALID_RATIO,\n      test: (val) => {\n        return checkRatioInputAgainstRegexes(val).isValid;\n      },\n    });\n\n// EMAIL\nexport const email = () => text().email(error.INVALID_EMAIL);\nexport const emailOptional = () => email().notRequired();\n\n// URL\nexport const url = () => text().url(error.INVALID_URL);\nexport const urlOptional = () => url().notRequired();\n\n// DATE\nexport const date = () =>\n  string()\n    .required(error.REQUIRED_GENERIC)\n    .matches(dateFormatRegex, error.INVALID_DATE)\n    .test({\n      message: error.REQUIRED_GENERIC,\n      test: (value) => !isWhitespaceString(value),\n    });\n\nexport const dateOptional = () =>\n  string()\n    .typeError(error.INVALID_GENERIC)\n    .test({\n      message: error.INVALID_DATE,\n      test: (value) => dateFormatRegex.test(value!),\n    });\n\nexport const endDate = (startDateField: string) =>\n  date()\n    .typeError(error.INVALID_DATE)\n    .test({\n      message: error.INVALID_END_DATE,\n      test: (endDateString, context) => {\n        return isEndDateAfterStartDate(\n          context.parent[startDateField],\n          endDateString as string\n        );\n      },\n    });\n\nexport const isEndDateAfterStartDate = (\n  startDateString: string,\n  endDateString: string\n) => {\n  const startDate = new Date(startDateString);\n  const endDate = new Date(endDateString!);\n  return endDate >= startDate;\n};\n\n// DROPDOWN\nexport const dropdown = () =>\n  object({ label: text(), value: text() }).required(error.REQUIRED_GENERIC);\n\n// CHECKBOX\nexport const checkbox = () =>\n  array()\n    .min(1, error.REQUIRED_CHECKBOX)\n    .of(object({ key: text(), value: text() }))\n    .required(error.REQUIRED_CHECKBOX);\nexport const checkboxOptional = () =>\n  array().notRequired().typeError(error.INVALID_GENERIC);\nexport const checkboxSingle = () => boolean();\n\n// RADIO\nexport const radio = () =>\n  array()\n    .min(1, error.REQUIRED_GENERIC)\n    .of(object({ key: text(), value: text() }))\n    .required(error.REQUIRED_GENERIC);\nexport const radioOptional = () => radio().notRequired();\n\n// DYNAMIC\nexport const dynamic = () =>\n  array()\n    .min(1)\n    .of(\n      object().shape({\n        id: text(),\n        name: text(),\n      })\n    )\n    .required(error.REQUIRED_GENERIC);\nexport const dynamicOptional = () => dynamic().notRequired();\n\n// NESTED\nexport const nested = (\n  fieldSchema: Function,\n  parentFieldName: string,\n  parentOptionId: string\n) => {\n  const fieldTypeMap = {\n    array: array(),\n    string: string(),\n    date: date(),\n    object: object(),\n  };\n  const fieldType: keyof typeof fieldTypeMap = fieldSchema().type;\n  const baseSchema: any = fieldTypeMap[fieldType];\n  return baseSchema.when(parentFieldName, {\n    is: (value: Choice[]) =>\n      // look for parentOptionId in checked choices\n      value?.find((option: Choice) => option.key.endsWith(parentOptionId)),\n    then: () => fieldSchema(), // returns standard field schema (required)\n    otherwise: () => baseSchema, // returns not-required Yup base schema\n  });\n};\n\n// OBJECT ARRAY\nexport const objectArray = () => array().of(mixed());\n\n// REGEX\nexport const dateFormatRegex =\n  /^((0[1-9]|1[0-2])\\/(0[1-9]|1\\d|2\\d|3[01])\\/(19|20)\\d{2})|((0[1-9]|1[0-2])(0[1-9]|1\\d|2\\d|3[01])(19|20)\\d{2})$/;\n\n// SCHEMA MAP\nexport const schemaMap: any = {\n  checkbox: checkbox(),\n  checkboxOptional: checkboxOptional(),\n  checkboxSingle: checkboxSingle(),\n  date: date(),\n  dateOptional: dateOptional(),\n  dropdown: dropdown(),\n  dynamic: dynamic(),\n  dynamicOptional: dynamicOptional(),\n  email: email(),\n  emailOptional: emailOptional(),\n  number: number(),\n  numberOptional: numberOptional(),\n  objectArray: objectArray(),\n  radio: radio(),\n  radioOptional: radioOptional(),\n  ratio: ratio(),\n  text: text(),\n  textOptional: textOptional(),\n  url: url(),\n  urlOptional: urlOptional(),\n  validInteger: validInteger(),\n  validIntegerOptional: validIntegerOptional(),\n};\n", "// REGEX\n\n// basic check for all possible characters -- standard number\nconst validCharactersStandardNumberRegex = /^[0-9\\s.,$%-]+$/;\n// basic check for all possible characters -- standard number\nconst validCharactersStandardIntegerRegex = /^[0-9\\s,$%]+$/;\n// basic check for all possible characters -- ratio\nconst validCharactersRatioNumberRegex = /^[0-9.,-]+$/;\n// at most 1 decimal point\nconst atMost1DecimalPointRegex = /^[^.]*\\.?[^.]*$/;\n// commas only exist before decimal point\nconst validCommaLocationRegex = /^[0-9,$-]*\\.?[0-9%]*$/;\n// at most 1 $%\nconst atMost1SpecialCharacterRegex = /^([^$%]*\\$[^$%]*|[^$%]*%[^$%]*|[^$%]*)$/;\n// at most 1 $ at the beginning of the input\nconst validDollarSignPlacementRegex = /^[$]?[^$%]+$/;\n// at most 1 % at the end of the input\nconst validPercentSignPlacementRegex = /^[^%$]+[%]?$/;\n// at most 1 - at the beginning of the input (but after any potential $s)\nconst validNegativeSignPlacementRegex = /^[$]?[-]?[^$-]+[%]?$/;\n// exactly one ratio character in between other characters\nconst exactlyOneRatioCharacterRegex = /^[^:]+:[^:]+$/;\n\nexport const checkStandardNumberInputAgainstRegexes = (\n  value: string\n): boolean => {\n  if (\n    !validCharactersStandardNumberRegex.test(value) ||\n    !atMost1DecimalPointRegex.test(value) ||\n    !validCommaLocationRegex.test(value) ||\n    !atMost1SpecialCharacterRegex.test(value) ||\n    !(\n      validDollarSignPlacementRegex.test(value) ||\n      validPercentSignPlacementRegex.test(value)\n    ) ||\n    !validNegativeSignPlacementRegex.test(value)\n  )\n    return false;\n  return true;\n};\n\nexport const checkStandardIntegerInputAgainstRegexes = (\n  value: string\n): boolean => {\n  if (\n    !validCharactersStandardIntegerRegex.test(value) ||\n    !validCommaLocationRegex.test(value) ||\n    !atMost1SpecialCharacterRegex.test(value) ||\n    !(\n      validDollarSignPlacementRegex.test(value) ||\n      validPercentSignPlacementRegex.test(value)\n    )\n  )\n    return false;\n  return true;\n};\n\nexport const checkRatioInputAgainstRegexes = (\n  value: string\n): { isValid: boolean; leftSide: string; rightSide: string } => {\n  if (!exactlyOneRatioCharacterRegex.test(value))\n    return { isValid: false, leftSide: \"\", rightSide: \"\" };\n\n  // Grab the left and right side of the ratio sign\n  let values = value.split(\":\");\n\n  // Check left and right side for valid inputs\n  if (\n    !checkASideOfRatioAgainstRegexes(values[0]) ||\n    !checkASideOfRatioAgainstRegexes(values[1])\n  )\n    return { isValid: false, leftSide: values[0], rightSide: values[1] };\n\n  return { isValid: true, leftSide: values[0], rightSide: values[1] };\n};\n\nexport const checkASideOfRatioAgainstRegexes = (value: string): boolean => {\n  if (\n    !validCharactersRatioNumberRegex.test(value) ||\n    !atMost1DecimalPointRegex.test(value) ||\n    !validCommaLocationRegex.test(value) ||\n    !validNegativeSignPlacementRegex.test(value)\n  )\n    return false;\n  return true;\n};\n"],
./.cdk/cdk.out/asset.0c6f11c3a2e840ff62061000d965b21e30b050d2b3415021acf6a5103fd17cc8/index.js.map:4: TODO: remove\n\n\n  get _type() {\n    return this.type;\n  }\n\n  _typeCheck(_value) {\n    return true;\n  }\n\n  clone(spec) {\n    if (this._mutate) {\n      if (spec) Object.assign(this.spec, spec);\n      return this;\n    } // if the nested value is a schema we can skip cloning, since\n    // they are already immutable\n\n\n    const next = Object.create(Object.getPrototypeOf(this)); // @ts-expect-error this is readonly\n\n    next.type = this.type;\n    next._typeError = this._typeError;\n    next._whitelistError = this._whitelistError;\n    next._blacklistError = this._blacklistError;\n    next._whitelist = this._whitelist.clone();\n    next._blacklist = this._blacklist.clone();\n    next.exclusiveTests = _extends({}, this.exclusiveTests); // @ts-expect-error this is readonly\n\n    next.deps = [...this.deps];\n    next.conditions = [...this.conditions];\n    next.tests = [...this.tests];\n    next.transforms = [...this.transforms];\n    next.spec = (0, _nanoclone.default)(_extends({}, this.spec, spec));\n    return next;\n  }\n\n  label(label) {\n    let next = this.clone();\n    next.spec.label = label;\n    return next;\n  }\n\n  meta(...args) {\n    if (args.length === 0) return this.spec.meta;\n    let next = this.clone();\n    next.spec.meta = Object.assign(next.spec.meta || {}, args[0]);\n    return next;\n  } // withContext<TContext extends AnyObject>(): BaseSchema<\n  //   TCast,\n  //   TContext,\n  //   TOutput\n  // > {\n  //   return this as any;\n  // }\n\n\n  withMutation(fn) {\n    let before = this._mutate;\n    this._mutate = true;\n    let result = fn(this);\n    this._mutate = before;\n    return result;\n  }\n\n  concat(schema) {\n    if (!schema || schema === this) return this;\n    if (schema.type !== this.type && this.type !== 'mixed') throw new TypeError(`You cannot \\`concat()\\` schema's of different types: ${this.type} and ${schema.type}`);\n    let base = this;\n    let combined = schema.clone();\n\n    const mergedSpec = _extends({}, base.spec, combined.spec); // if (combined.spec.nullable === UNSET)\n    //   mergedSpec.nullable = base.spec.nullable;\n    // if (combined.spec.presence === UNSET)\n    //   mergedSpec.presence = base.spec.presence;\n\n\n    combined.spec = mergedSpec;\n    combined._typeError || (combined._typeError = base._typeError);\n    combined._whitelistError || (combined._whitelistError = base._whitelistError);\n    combined._blacklistError || (combined._blacklistError = base._blacklistError); // manually merge the blacklist/whitelist (the other `schema` takes\n    // precedence in case of conflicts)\n\n    combined._whitelist = base._whitelist.merge(schema._whitelist, schema._blacklist);\n    combined._blacklist = base._blacklist.merge(schema._blacklist, schema._whitelist); // start with the current tests\n\n    combined.tests = base.tests;\n    combined.exclusiveTests = base.exclusiveTests; // manually add the new tests to ensure\n    // the deduping logic is consistent\n\n    combined.withMutation(next => {\n      schema.tests.forEach(fn => {\n        next.test(fn.OPTIONS);\n      });\n    });\n    combined.transforms = [...base.transforms, ...combined.transforms];\n    return combined;\n  }\n\n  isType(v) {\n    if (this.spec.nullable && v === null) return true;\n    return this._typeCheck(v);\n  }\n\n  resolve(options) {\n    let schema = this;\n\n    if (schema.conditions.length) {\n      let conditions = schema.conditions;\n      schema = schema.clone();\n      schema.conditions = [];\n      schema = conditions.reduce((schema, condition) => condition.resolve(schema, options), schema);\n      schema = schema.resolve(options);\n    }\n\n    return schema;\n  }\n  /**\n   *\n   * @param {*} value\n   * @param {Object} options\n   * @param {*=} options.parent\n   * @param {*=} options.context\n   */\n\n\n  cast(value, options = {}) {\n    let resolvedSchema = this.resolve(_extends({\n      value\n    }, options));\n\n    let result = resolvedSchema._cast(value, options);\n\n    if (value !== undefined && options.assert !== false && resolvedSchema.isType(result) !== true) {\n      let formattedValue = (0, _printValue.default)(value);\n      let formattedResult = (0, _printValue.default)(result);\n      throw new TypeError(`The value of ${options.path || 'field'} could not be cast to a value ` + `that satisfies the schema type: \"${resolvedSchema._type}\". \\n\\n` + `attempted value: ${formattedValue} \\n` + (formattedResult !== formattedValue ? `result of cast: ${formattedResult}` : ''));\n    }\n\n    return result;\n  }\n\n  _cast(rawValue, _options) {\n    let value = rawValue === undefined ? rawValue : this.transforms.reduce((value, fn) => fn.call(this, value, rawValue, this), rawValue);\n\n    if (value === undefined) {\n      value = this.getDefault();\n    }\n\n    return value;\n  }\n\n  _validate(_value, options = {}, cb) {\n    let {\n      sync,\n      path,\n      from = [],\n      originalValue = _value,\n      strict = this.spec.strict,\n      abortEarly = this.spec.abortEarly\n    } = options;\n    let value = _value;\n\n    if (!strict) {\n      // this._validating = true;\n      value = this._cast(value, _extends({\n        assert: false\n      }, options)); // this._validating = false;\n    } // value is cast, we can check if it meets type requirements\n\n\n    let args = {\n      value,\n      path,\n      options,\n      originalValue,\n      schema: this,\n      label: this.spec.label,\n      sync,\n      from\n    };\n    let initialTests = [];\n    if (this._typeError) initialTests.push(this._typeError);\n    let finalTests = [];\n    if (this._whitelistError) finalTests.push(this._whitelistError);\n    if (this._blacklistError) finalTests.push(this._blacklistError);\n    (0, _runTests.default)({\n      args,\n      value,\n      path,\n      sync,\n      tests: initialTests,\n      endEarly: abortEarly\n    }, err => {\n      if (err) return void cb(err, value);\n      (0, _runTests.default)({\n        tests: this.tests.concat(finalTests),\n        args,\n        path,\n        sync,\n        value,\n        endEarly: abortEarly\n      }, cb);\n    });\n  }\n\n  validate(value, options, maybeCb) {\n    let schema = this.resolve(_extends({}, options, {\n      value\n    })); // callback case is for nested validations\n\n    return typeof maybeCb === 'function' ? schema._validate(value, options, maybeCb) : new Promise((resolve, reject) => schema._validate(value, options, (err, value) => {\n      if (err) reject(err);else resolve(value);\n    }));\n  }\n\n  validateSync(value, options) {\n    let schema = this.resolve(_extends({}, options, {\n      value\n    }));\n    let result;\n\n    schema._validate(value, _extends({}, options, {\n      sync: true\n    }), (err, value) => {\n      if (err) throw err;\n      result = value;\n    });\n\n    return result;\n  }\n\n  isValid(value, options) {\n    return this.validate(value, options).then(() => true, err => {\n      if (_ValidationError.default.isError(err)) return false;\n      throw err;\n    });\n  }\n\n  isValidSync(value, options) {\n    try {\n      this.validateSync(value, options);\n      return true;\n    } catch (err) {\n      if (_ValidationError.default.isError(err)) return false;\n      throw err;\n    }\n  }\n\n  _getDefault() {\n    let defaultValue = this.spec.default;\n\n    if (defaultValue == null) {\n      return defaultValue;\n    }\n\n    return typeof defaultValue === 'function' ? defaultValue.call(this) : (0, _nanoclone.default)(defaultValue);\n  }\n\n  getDefault(options) {\n    let schema = this.resolve(options || {});\n    return schema._getDefault();\n  }\n\n  default(def) {\n    if (arguments.length === 0) {\n      return this._getDefault();\n    }\n\n    let next = this.clone({\n      default: def\n    });\n    return next;\n  }\n\n  strict(isStrict = true) {\n    let next = this.clone();\n    next.spec.strict = isStrict;\n    return next;\n  }\n\n  _isPresent(value) {\n    return value != null;\n  }\n\n  defined(message = _locale.mixed.defined) {\n    return this.test({\n      message,\n      name: 'defined',\n      exclusive: true,\n\n      test(value) {\n        return value !== undefined;\n      }\n\n    });\n  }\n\n  required(message = _locale.mixed.required) {\n    return this.clone({\n      presence: 'required'\n    }).withMutation(s => s.test({\n      message,\n      name: 'required',\n      exclusive: true,\n\n      test(value) {\n        return this.schema._isPresent(value);\n      }\n\n    }));\n  }\n\n  notRequired() {\n    let next = this.clone({\n      presence: 'optional'\n    });\n    next.tests = next.tests.filter(test => test.OPTIONS.name !== 'required');\n    return next;\n  }\n\n  nullable(isNullable = true) {\n    let next = this.clone({\n      nullable: isNullable !== false\n    });\n    return next;\n  }\n\n  transform(fn) {\n    let next = this.clone();\n    next.transforms.push(fn);\n    return next;\n  }\n  /**\n   * Adds a test function to the schema's queue of tests.\n   * tests can be exclusive or non-exclusive.\n   *\n   * - exclusive tests, will replace any existing tests of the same name.\n   * - non-exclusive: can be stacked\n   *\n   * If a non-exclusive test is added to a schema with an exclusive test of the same name\n   * the exclusive test is removed and further tests of the same name will be stacked.\n   *\n   * If an exclusive test is added to a schema with non-exclusive tests of the same name\n   * the previous tests are removed and further tests of the same name will replace each other.\n   */\n\n\n  test(...args) {\n    let opts;\n\n    if (args.length === 1) {\n      if (typeof args[0] === 'function') {\n        opts = {\n          test: args[0]\n        };\n      } else {\n        opts = args[0];\n      }\n    } else if (args.length === 2) {\n      opts = {\n        name: args[0],\n        test: args[1]\n      };\n    } else {\n      opts = {\n        name: args[0],\n        message: args[1],\n        test: args[2]\n      };\n    }\n\n    if (opts.message === undefined) opts.message = _locale.mixed.default;\n    if (typeof opts.test !== 'function') throw new TypeError('`test` is a required parameters');\n    let next = this.clone();\n    let validate = (0, _createValidation.default)(opts);\n    let isExclusive = opts.exclusive || opts.name && next.exclusiveTests[opts.name] === true;\n\n    if (opts.exclusive) {\n      if (!opts.name) throw new TypeError('Exclusive tests must provide a unique `name` identifying the test');\n    }\n\n    if (opts.name) next.exclusiveTests[opts.name] = !!opts.exclusive;\n    next.tests = next.tests.filter(fn => {\n      if (fn.OPTIONS.name === opts.name) {\n        if (isExclusive) return false;\n        if (fn.OPTIONS.test === validate.OPTIONS.test) return false;\n      }\n\n      return true;\n    });\n    next.tests.push(validate);\n    return next;\n  }\n\n  when(keys, options) {\n    if (!Array.isArray(keys) && typeof keys !== 'string') {\n      options = keys;\n      keys = '.';\n    }\n\n    let next = this.clone();\n    let deps = (0, _toArray.default)(keys).map(key => new _Reference.default(key));\n    deps.forEach(dep => {\n      // @ts-ignore\n      if (dep.isSibling) next.deps.push(dep.key);\n    });\n    next.conditions.push(new _Condition.default(deps, options));\n    return next;\n  }\n\n  typeError(message) {\n    let next = this.clone();\n    next._typeError = (0, _createValidation.default)({\n      message,\n      name: 'typeError',\n\n      test(value) {\n        if (value !== undefined && !this.schema.isType(value)) return this.createError({\n          params: {\n            type: this.schema._type\n          }\n        });\n        return true;\n      }\n\n    });\n    return next;\n  }\n\n  oneOf(enums, message = _locale.mixed.oneOf) {\n    let next = this.clone();\n    enums.forEach(val => {\n      next._whitelist.add(val);\n\n      next._blacklist.delete(val);\n    });\n    next._whitelistError = (0, _createValidation.default)({\n      message,\n      name: 'oneOf',\n\n      test(value) {\n        if (value === undefined) return true;\n        let valids = this.schema._whitelist;\n        let resolved = valids.resolveAll(this.resolve);\n        return resolved.includes(value) ? true : this.createError({\n          params: {\n            values: valids.toArray().join(', '),\n            resolved\n          }\n        });\n      }\n\n    });\n    return next;\n  }\n\n  notOneOf(enums, message = _locale.mixed.notOneOf) {\n    let next = this.clone();\n    enums.forEach(val => {\n      next._blacklist.add(val);\n\n      next._whitelist.delete(val);\n    });\n    next._blacklistError = (0, _createValidation.default)({\n      message,\n      name: 'notOneOf',\n\n      test(value) {\n        let invalids = this.schema._blacklist;\n        let resolved = invalids.resolveAll(this.resolve);\n        if (resolved.includes(value)) return this.createError({\n          params: {\n            values: invalids.toArray().join(', '),\n            resolved\n          }\n        });\n        return true;\n      }\n\n    });\n    return next;\n  }\n\n  strip(strip = true) {\n    let next = this.clone();\n    next.spec.strip = strip;\n    return next;\n  }\n\n  describe() {\n    const next = this.clone();\n    const {\n      label,\n      meta\n    } = next.spec;\n    const description = {\n      meta,\n      label,\n      type: next.type,\n      oneOf: next._whitelist.describe(),\n      notOneOf: next._blacklist.describe(),\n      tests: next.tests.map(fn => ({\n        name: fn.OPTIONS.name,\n        params: fn.OPTIONS.params\n      })).filter((n, idx, list) => list.findIndex(c => c.name === n.name) === idx)\n    };\n    return description;\n  }\n\n} // eslint-disable-next-line @typescript-eslint/no-unused-vars\n\n\nexports.default = BaseSchema;\n// @ts-expect-error\nBaseSchema.prototype.__isYupSchema__ = true;\n\nfor (const method of ['validate', 'validateSync']) BaseSchema.prototype[`${method}At`] = function (path, value, options = {}) {\n  const {\n    parent,\n    parentPath,\n    schema\n  } = (0, _reach.getIn)(this, path, value, options.context);\n  return schema[method](parent && parent[parentPath], _extends({}, options, {\n    parent,\n    path\n  }));\n};\n\nfor (const alias of ['equals', 'is']) BaseSchema.prototype[alias] = BaseSchema.prototype.oneOf;\n\nfor (const alias of ['not', 'nope']) BaseSchema.prototype[alias] = BaseSchema.prototype.notOneOf;\n\nBaseSchema.prototype.optional = BaseSchema.prototype.notRequired;", "\"use strict\";\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\nexports.create = create;\nexports.default = void 0;\n\nvar _schema = _interopRequireDefault(require(\"./schema\"));\n\nfunction _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { default: obj }; }\n\nconst Mixed = _schema.default;\nvar _default = Mixed;\nexports.default = _default;\n\nfunction create() {\n  return new Mixed();\n} // XXX: this is using the Base schema so that `addMethod(mixed)` works as a base class\n\n\ncreate.prototype = Mixed.prototype;", "\"use strict\";\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\nexports.default = void 0;\n\nconst isAbsent = value => value == null;\n\nvar _default = isAbsent;\nexports.default = _default;", "\"use strict\";\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\nexports.create = create;\nexports.default = void 0;\n\nvar _schema = _interopRequireDefault(require(\"./schema\"));\n\nvar _locale = require(\"./locale\");\n\nvar _isAbsent = _interopRequireDefault(require(\"./util/isAbsent\"));\n\nfunction _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { default: obj }; }\n\nfunction create() {\n  return new BooleanSchema();\n}\n\nclass BooleanSchema extends _schema.default {\n  constructor() {\n    super({\n      type: 'boolean'\n    });\n    this.withMutation(() => {\n      this.transform(function (value) {\n        if (!this.isType(value)) {\n          if (/^(true|1)$/i.test(String(value))) return true;\n          if (/^(false|0)$/i.test(String(value))) return false;\n        }\n\n        return value;\n      });\n    });\n  }\n\n  _typeCheck(v) {\n    if (v instanceof Boolean) v = v.valueOf();\n    return typeof v === 'boolean';\n  }\n\n  isTrue(message = _locale.boolean.isValue) {\n    return this.test({\n      message,\n      name: 'is-value',\n      exclusive: true,\n      params: {\n        value: 'true'\n      },\n\n      test(value) {\n        return (0, _isAbsent.default)(value) || value === true;\n      }\n\n    });\n  }\n\n  isFalse(message = _locale.boolean.isValue) {\n    return this.test({\n      message,\n      name: 'is-value',\n      exclusive: true,\n      params: {\n        value: 'false'\n      },\n\n      test(value) {\n        return (0, _isAbsent.default)(value) || value === false;\n      }\n\n    });\n  }\n\n}\n\nexports.default = BooleanSchema;\ncreate.prototype = BooleanSchema.prototype;", "\"use strict\";\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\nexports.create = create;\nexports.default = void 0;\n\nvar _locale = require(\"./locale\");\n\nvar _isAbsent = _interopRequireDefault(require(\"./util/isAbsent\"));\n\nvar _schema = _interopRequireDefault(require(\"./schema\"));\n\nfunction _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { default: obj }; }\n\n// eslint-disable-next-line\nlet rEmail = /^((([a-z]|\\d|[!#\\$%&'\\*\\+\\-\\/=\\?\\^_`{\\|}~]|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])+(\\.([a-z]|\\d|[!#\\$%&'\\*\\+\\-\\/=\\?\\^_`{\\|}~]|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])+)*)|((\\x22)((((\\x20|\\x09)*(\\x0d\\x0a))?(\\x20|\\x09)+)?(([\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x7f]|\\x21|[\\x23-\\x5b]|[\\x5d-\\x7e]|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])|(\\\\([\\x01-\\x09\\x0b\\x0c\\x0d-\\x7f]|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF]))))*(((\\x20|\\x09)*(\\x0d\\x0a))?(\\x20|\\x09)+)?(\\x22)))@((([a-z]|\\d|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])|(([a-z]|\\d|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])([a-z]|\\d|-|\\.|_|~|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])*([a-z]|\\d|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])))\\.)+(([a-z]|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])|(([a-z]|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])([a-z]|\\d|-|\\.|_|~|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])*([a-z]|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])))$/i; // eslint-disable-next-line\n\nlet rUrl = /^((https?|ftp):)?\\/\\/(((([a-z]|\\d|-|\\.|_|~|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])|(%[\\da-f]{2})|[!\\$&'\\(\\)\\*\\+,;=]|:)*@)?(((\\d|[1-9]\\d|1\\d\\d|2[0-4]\\d|25[0-5])\\.(\\d|[1-9]\\d|1\\d\\d|2[0-4]\\d|25[0-5])\\.(\\d|[1-9]\\d|1\\d\\d|2[0-4]\\d|25[0-5])\\.(\\d|[1-9]\\d|1\\d\\d|2[0-4]\\d|25[0-5]))|((([a-z]|\\d|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])|(([a-z]|\\d|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])([a-z]|\\d|-|\\.|_|~|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])*([a-z]|\\d|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])))\\.)+(([a-z]|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])|(([a-z]|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])([a-z]|\\d|-|\\.|_|~|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])*([a-z]|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])))\\.?)(:\\d*)?)(\\/((([a-z]|\\d|-|\\.|_|~|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])|(%[\\da-f]{2})|[!\\$&'\\(\\)\\*\\+,;=]|:|@)+(\\/(([a-z]|\\d|-|\\.|_|~|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])|(%[\\da-f]{2})|[!\\$&'\\(\\)\\*\\+,;=]|:|@)*)*)?)?(\\?((([a-z]|\\d|-|\\.|_|~|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])|(%[\\da-f]{2})|[!\\$&'\\(\\)\\*\\+,;=]|:|@)|[\\uE000-\\uF8FF]|\\/|\\?)*)?(\\#((([a-z]|\\d|-|\\.|_|~|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])|(%[\\da-f]{2})|[!\\$&'\\(\\)\\*\\+,;=]|:|@)|\\/|\\?)*)?$/i; // eslint-disable-next-line\n\nlet rUUID = /^(?:[0-9a-f]{8}-[0-9a-f]{4}-[1-5][0-9a-f]{3}-[89ab][0-9a-f]{3}-[0-9a-f]{12}|00000000-0000-0000-0000-000000000000)$/i;\n\nlet isTrimmed = value => (0, _isAbsent.default)(value) || value === value.trim();\n\nlet objStringTag = {}.toString();\n\nfunction create() {\n  return new StringSchema();\n}\n\nclass StringSchema extends _schema.default {\n  constructor() {\n    super({\n      type: 'string'\n    });\n    this.withMutation(() => {\n      this.transform(function (value) {\n        if (this.isType(value)) return value;\n        if (Array.isArray(value)) return value;\n        const strValue = value != null && value.toString ? value.toString() : value;\n        if (strValue === objStringTag) return value;\n        return strValue;\n      });\n    });\n  }\n\n  _typeCheck(value) {\n    if (value instanceof String) value = value.valueOf();\n    return typeof value === 'string';\n  }\n\n  _isPresent(value) {\n    return super._isPresent(value) && !!value.length;\n  }\n\n  length(length, message = _locale.string.length) {\n    return this.test({\n      message,\n      name: 'length',\n      exclusive: true,\n      params: {\n        length\n      },\n\n      test(value) {\n        return (0, _isAbsent.default)(value) || value.length === this.resolve(length);\n      }\n\n    });\n  }\n\n  min(min, message = _locale.string.min) {\n    return this.test({\n      message,\n      name: 'min',\n      exclusive: true,\n      params: {\n        min\n      },\n\n      test(value) {\n        return (0, _isAbsent.default)(value) || value.length >= this.resolve(min);\n      }\n\n    });\n  }\n\n  max(max, message = _locale.string.max) {\n    return this.test({\n      name: 'max',\n      exclusive: true,\n      message,\n      params: {\n        max\n      },\n\n      test(value) {\n        return (0, _isAbsent.default)(value) || value.length <= this.resolve(max);\n      }\n\n    });\n  }\n\n  matches(regex, options) {\n    let excludeEmptyString = false;\n    let message;\n    let name;\n\n    if (options) {\n      if (typeof options === 'object') {\n        ({\n          excludeEmptyString = false,\n          message,\n          name\n        } = options);\n      } else {\n        message = options;\n      }\n    }\n\n    return this.test({\n      name: name || 'matches',\n      message: message || _locale.string.matches,\n      params: {\n        regex\n      },\n      test: value => (0, _isAbsent.default)(value) || value === '' && excludeEmptyString || value.search(regex) !== -1\n    });\n  }\n\n  email(message = _locale.string.email) {\n    return this.matches(rEmail, {\n      name: 'email',\n      message,\n      excludeEmptyString: true\n    });\n  }\n\n  url(message = _locale.string.url) {\n    return this.matches(rUrl, {\n      name: 'url',\n      message,\n      excludeEmptyString: true\n    });\n  }\n\n  uuid(message = _locale.string.uuid) {\n    return this.matches(rUUID, {\n      name: 'uuid',\n      message,\n      excludeEmptyString: false\n    });\n  } //-- transforms --\n\n\n  ensure() {\n    return this.default('').transform(val => val === null ? '' : val);\n  }\n\n  trim(message = _locale.string.trim) {\n    return this.transform(val => val != null ? val.trim() : val).test({\n      message,\n      name: 'trim',\n      test: isTrimmed\n    });\n  }\n\n  lowercase(message = _locale.string.lowercase) {\n    return this.transform(value => !(0, _isAbsent.default)(value) ? value.toLowerCase() : value).test({\n      message,\n      name: 'string_case',\n      exclusive: true,\n      test: value => (0, _isAbsent.default)(value) || value === value.toLowerCase()\n    });\n  }\n\n  uppercase(message = _locale.string.uppercase) {\n    return this.transform(value => !(0, _isAbsent.default)(value) ? value.toUpperCase() : value).test({\n      message,\n      name: 'string_case',\n      exclusive: true,\n      test: value => (0, _isAbsent.default)(value) || value === value.toUpperCase()\n    });\n  }\n\n}\n\nexports.default = StringSchema;\ncreate.prototype = StringSchema.prototype; //\n// String Interfaces\n//", "\"use strict\";\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\nexports.create = create;\nexports.default = void 0;\n\nvar _locale = require(\"./locale\");\n\nvar _isAbsent = _interopRequireDefault(require(\"./util/isAbsent\"));\n\nvar _schema = _interopRequireDefault(require(\"./schema\"));\n\nfunction _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { default: obj }; }\n\nlet isNaN = value => value != +value;\n\nfunction create() {\n  return new NumberSchema();\n}\n\nclass NumberSchema extends _schema.default {\n  constructor() {\n    super({\n      type: 'number'\n    });\n    this.withMutation(() => {\n      this.transform(function (value) {\n        let parsed = value;\n\n        if (typeof parsed === 'string') {\n          parsed = parsed.replace(/\\s/g, '');\n          if (parsed === '') return NaN; // don't use parseFloat to avoid positives on alpha-numeric strings\n\n          parsed = +parsed;\n        }\n\n        if (this.isType(parsed)) return parsed;\n        return parseFloat(parsed);\n      });\n    });\n  }\n\n  _typeCheck(value) {\n    if (value instanceof Number) value = value.valueOf();\n    return typeof value === 'number' && !isNaN(value);\n  }\n\n  min(min, message = _locale.number.min) {\n    return this.test({\n      message,\n      name: 'min',\n      exclusive: true,\n      params: {\n        min\n      },\n\n      test(value) {\n        return (0, _isAbsent.default)(value) || value >= this.resolve(min);\n      }\n\n    });\n  }\n\n  max(max, message = _locale.number.max) {\n    return this.test({\n      message,\n      name: 'max',\n      exclusive: true,\n      params: {\n        max\n      },\n\n      test(value) {\n        return (0, _isAbsent.default)(value) || value <= this.resolve(max);\n      }\n\n    });\n  }\n\n  lessThan(less, message = _locale.number.lessThan) {\n    return this.test({\n      message,\n      name: 'max',\n      exclusive: true,\n      params: {\n        less\n      },\n\n      test(value) {\n        return (0, _isAbsent.default)(value) || value < this.resolve(less);\n      }\n\n    });\n  }\n\n  moreThan(more, message = _locale.number.moreThan) {\n    return this.test({\n      message,\n      name: 'min',\n      exclusive: true,\n      params: {\n        more\n      },\n\n      test(value) {\n        return (0, _isAbsent.default)(value) || value > this.resolve(more);\n      }\n\n    });\n  }\n\n  positive(msg = _locale.number.positive) {\n    return this.moreThan(0, msg);\n  }\n\n  negative(msg = _locale.number.negative) {\n    return this.lessThan(0, msg);\n  }\n\n  integer(message = _locale.number.integer) {\n    return this.test({\n      name: 'integer',\n      message,\n      test: val => (0, _isAbsent.default)(val) || Number.isInteger(val)\n    });\n  }\n\n  truncate() {\n    return this.transform(value => !(0, _isAbsent.default)(value) ? value | 0 : value);\n  }\n\n  round(method) {\n    var _method;\n\n    let avail = ['ceil', 'floor', 'round', 'trunc'];\n    method = ((_method = method) == null ? void 0 : _method.toLowerCase()) || 'round'; // this exists for symemtry with the new Math.trunc\n\n    if (method === 'trunc') return this.truncate();\n    if (avail.indexOf(method.toLowerCase()) === -1) throw new TypeError('Only valid options for round() are: ' + avail.join(', '));\n    return this.transform(value => !(0, _isAbsent.default)(value) ? Math[method](value) : value);\n  }\n\n}\n\nexports.default = NumberSchema;\ncreate.prototype = NumberSchema.prototype; //\n// Number Interfaces\n//", "\"use strict\";\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\nexports.default = parseIsoDate;\n\n/* eslint-disable */\n\n/**\n *\n * Date.parse with progressive enhancement for ISO 8601 <https://github.com/csnover/js-iso8601>\n * NON-CONFORMANT EDITION.\n * \u00A9 2011 Colin Snover <http://zetafleet.com>\n * Released under MIT license.\n */\n//              1 YYYY                 2 MM        3 DD              4 HH     5 mm        6 ss            7 msec         8 Z 9 \u00B1    10 tzHH    11 tzmm\nvar isoReg = /^(\\d{4}|[+\\-]\\d{6})(?:-?(\\d{2})(?:-?(\\d{2}))?)?(?:[ T]?(\\d{2}):?(\\d{2})(?::?(\\d{2})(?:[,\\.](\\d{1,}))?)?(?:(Z)|([+\\-])(\\d{2})(?::?(\\d{2}))?)?)?$/;\n\nfunction parseIsoDate(date) {\n  var numericKeys = [1, 4, 5, 6, 7, 10, 11],\n      minutesOffset = 0,\n      timestamp,\n      struct;\n\n  if (struct = isoReg.exec(date)) {\n    // avoid NaN timestamps caused by \u201Cundefined\u201D values being passed to Date.UTC\n    for (var i = 0, k; k = numericKeys[i]; ++i) struct[k] = +struct[k] || 0; // allow undefined days and months\n\n\n    struct[2] = (+struct[2] || 1) - 1;\n    struct[3] = +struct[3] || 1; // allow arbitrary sub-second precision beyond milliseconds\n\n    struct[7] = struct[7] ? String(struct[7]).substr(0, 3) : 0; // timestamps without timezone identifiers should be considered local time\n\n    if ((struct[8] === undefined || struct[8] === '') && (struct[9] === undefined || struct[9] === '')) timestamp = +new Date(struct[1], struct[2], struct[3], struct[4], struct[5], struct[6], struct[7]);else {\n      if (struct[8] !== 'Z' && struct[9] !== undefined) {\n        minutesOffset = struct[10] * 60 + struct[11];\n        if (struct[9] === '+') minutesOffset = 0 - minutesOffset;\n      }\n\n      timestamp = Date.UTC(struct[1], struct[2], struct[3], struct[4], struct[5] + minutesOffset, struct[6], struct[7]);\n    }\n  } else timestamp = Date.parse ? Date.parse(date) : NaN;\n\n  return timestamp;\n}", "\"use strict\";\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\nexports.create = create;\nexports.default = void 0;\n\nvar _isodate = _interopRequireDefault(require(\"./util/isodate\"));\n\nvar _locale = require(\"./locale\");\n\nvar _isAbsent = _interopRequireDefault(require(\"./util/isAbsent\"));\n\nvar _Reference = _interopRequireDefault(require(\"./Reference\"));\n\nvar _schema = _interopRequireDefault(require(\"./schema\"));\n\nfunction _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { default: obj }; }\n\n// @ts-ignore\nlet invalidDate = new Date('');\n\nlet isDate = obj => Object.prototype.toString.call(obj) === '[object Date]';\n\nfunction create() {\n  return new DateSchema();\n}\n\nclass DateSchema extends _schema.default {\n  constructor() {\n    super({\n      type: 'date'\n    });\n    this.withMutation(() => {\n      this.transform(function (value) {\n        if (this.isType(value)) return value;\n        value = (0, _isodate.default)(value); // 0 is a valid timestamp equivalent to 1970-01-01T00:00:00Z(unix epoch) or before.\n\n        return !isNaN(value) ? new Date(value) : invalidDate;\n      });\n    });\n  }\n\n  _typeCheck(v) {\n    return isDate(v) && !isNaN(v.getTime());\n  }\n\n  prepareParam(ref, name) {\n    let param;\n\n    if (!_Reference.default.isRef(ref)) {\n      let cast = this.cast(ref);\n      if (!this._typeCheck(cast)) throw new TypeError(`\\`${name}\\` must be a Date or a value that can be \\`cast()\\` to a Date`);\n      param = cast;\n    } else {\n      param = ref;\n    }\n\n    return param;\n  }\n\n  min(min, message = _locale.date.min) {\n    let limit = this.prepareParam(min, 'min');\n    return this.test({\n      message,\n      name: 'min',\n      exclusive: true,\n      params: {\n        min\n      },\n\n      test(value) {\n        return (0, _isAbsent.default)(value) || value >= this.resolve(limit);\n      }\n\n    });\n  }\n\n  max(max, message = _locale.date.max) {\n    let limit = this.prepareParam(max, 'max');\n    return this.test({\n      message,\n      name: 'max',\n      exclusive: true,\n      params: {\n        max\n      },\n\n      test(value) {\n        return (0, _isAbsent.default)(value) || value <= this.resolve(limit);\n      }\n\n    });\n  }\n\n}\n\nexports.default = DateSchema;\nDateSchema.INVALID_DATE = invalidDate;\ncreate.prototype = DateSchema.prototype;\ncreate.INVALID_DATE = invalidDate;", "/**\n * A specialized version of `_.reduce` for arrays without support for\n * iteratee shorthands.\n *\n * @private\n * @param {Array} [array] The array to iterate over.\n * @param {Function} iteratee The function invoked per iteration.\n * @param {*} [accumulator] The initial value.\n * @param {boolean} [initAccum] Specify using the first element of `array` as\n *  the initial value.\n * @returns {*} Returns the accumulated value.\n */\nfunction arrayReduce(array, iteratee, accumulator, initAccum) {\n  var index = -1,\n      length = array == null ? 0 : array.length;\n\n  if (initAccum && length) {\n    accumulator = array[++index];\n  }\n  while (++index < length) {\n    accumulator = iteratee(accumulator, array[index], index, array);\n  }\n  return accumulator;\n}\n\nmodule.exports = arrayReduce;\n", "/**\n * The base implementation of `_.propertyOf` without support for deep paths.\n *\n * @private\n * @param {Object} object The object to query.\n * @returns {Function} Returns the new accessor function.\n */\nfunction basePropertyOf(object) {\n  return function(key) {\n    return object == null ? undefined : object[key];\n  };\n}\n\nmodule.exports = basePropertyOf;\n", "var basePropertyOf = require('./_basePropertyOf');\n\n/** Used to map Latin Unicode letters to basic Latin letters. */\nvar deburredLetters = {\n  // Latin-1 Supplement block.\n  '\\xc0': 'A',  '\\xc1': 'A', '\\xc2': 'A', '\\xc3': 'A', '\\xc4': 'A', '\\xc5': 'A',\n  '\\xe0': 'a',  '\\xe1': 'a', '\\xe2': 'a', '\\xe3': 'a', '\\xe4': 'a', '\\xe5': 'a',\n  '\\xc7': 'C',  '\\xe7': 'c',\n  '\\xd0': 'D',  '\\xf0': 'd',\n  '\\xc8': 'E',  '\\xc9': 'E', '\\xca': 'E', '\\xcb': 'E',\n  '\\xe8': 'e',  '\\xe9': 'e', '\\xea': 'e', '\\xeb': 'e',\n  '\\xcc': 'I',  '\\xcd': 'I', '\\xce': 'I', '\\xcf': 'I',\n  '\\xec': 'i',  '\\xed': 'i', '\\xee': 'i', '\\xef': 'i',\n  '\\xd1': 'N',  '\\xf1': 'n',\n  '\\xd2': 'O',  '\\xd3': 'O', '\\xd4': 'O', '\\xd5': 'O', '\\xd6': 'O', '\\xd8': 'O',\n  '\\xf2': 'o',  '\\xf3': 'o', '\\xf4': 'o', '\\xf5': 'o', '\\xf6': 'o', '\\xf8': 'o',\n  '\\xd9': 'U',  '\\xda': 'U', '\\xdb': 'U', '\\xdc': 'U',\n  '\\xf9': 'u',  '\\xfa': 'u', '\\xfb': 'u', '\\xfc': 'u',\n  '\\xdd': 'Y',  '\\xfd': 'y', '\\xff': 'y',\n  '\\xc6': 'Ae', '\\xe6': 'ae',\n  '\\xde': 'Th', '\\xfe': 'th',\n  '\\xdf': 'ss',\n  // Latin Extended-A block.\n  '\\u0100': 'A',  '\\u0102': 'A', '\\u0104': 'A',\n  '\\u0101': 'a',  '\\u0103': 'a', '\\u0105': 'a',\n  '\\u0106': 'C',  '\\u0108': 'C', '\\u010a': 'C', '\\u010c': 'C',\n  '\\u0107': 'c',  '\\u0109': 'c', '\\u010b': 'c', '\\u010d': 'c',\n  '\\u010e': 'D',  '\\u0110': 'D', '\\u010f': 'd', '\\u0111': 'd',\n  '\\u0112': 'E',  '\\u0114': 'E', '\\u0116': 'E', '\\u0118': 'E', '\\u011a': 'E',\n  '\\u0113': 'e',  '\\u0115': 'e', '\\u0117': 'e', '\\u0119': 'e', '\\u011b': 'e',\n  '\\u011c': 'G',  '\\u011e': 'G', '\\u0120': 'G', '\\u0122': 'G',\n  '\\u011d': 'g',  '\\u011f': 'g', '\\u0121': 'g', '\\u0123': 'g',\n  '\\u0124': 'H',  '\\u0126': 'H', '\\u0125': 'h', '\\u0127': 'h',\n  '\\u0128': 'I',  '\\u012a': 'I', '\\u012c': 'I', '\\u012e': 'I', '\\u0130': 'I',\n  '\\u0129': 'i',  '\\u012b': 'i', '\\u012d': 'i', '\\u012f': 'i', '\\u0131': 'i',\n  '\\u0134': 'J',  '\\u0135': 'j',\n  '\\u0136': 'K',  '\\u0137': 'k', '\\u0138': 'k',\n  '\\u0139': 'L',  '\\u013b': 'L', '\\u013d': 'L', '\\u013f': 'L', '\\u0141': 'L',\n  '\\u013a': 'l',  '\\u013c': 'l', '\\u013e': 'l', '\\u0140': 'l', '\\u0142': 'l',\n  '\\u0143': 'N',  '\\u0145': 'N', '\\u0147': 'N', '\\u014a': 'N',\n  '\\u0144': 'n',  '\\u0146': 'n', '\\u0148': 'n', '\\u014b': 'n',\n  '\\u014c': 'O',  '\\u014e': 'O', '\\u0150': 'O',\n  '\\u014d': 'o',  '\\u014f': 'o', '\\u0151': 'o',\n  '\\u0154': 'R',  '\\u0156': 'R', '\\u0158': 'R',\n  '\\u0155': 'r',  '\\u0157': 'r', '\\u0159': 'r',\n  '\\u015a': 'S',  '\\u015c': 'S', '\\u015e': 'S', '\\u0160': 'S',\n  '\\u015b': 's',  '\\u015d': 's', '\\u015f': 's', '\\u0161': 's',\n  '\\u0162': 'T',  '\\u0164': 'T', '\\u0166': 'T',\n  '\\u0163': 't',  '\\u0165': 't', '\\u0167': 't',\n  '\\u0168': 'U',  '\\u016a': 'U', '\\u016c': 'U', '\\u016e': 'U', '\\u0170': 'U', '\\u0172': 'U',\n  '\\u0169': 'u',  '\\u016b': 'u', '\\u016d': 'u', '\\u016f': 'u', '\\u0171': 'u', '\\u0173': 'u',\n  '\\u0174': 'W',  '\\u0175': 'w',\n  '\\u0176': 'Y',  '\\u0177': 'y', '\\u0178': 'Y',\n  '\\u0179': 'Z',  '\\u017b': 'Z', '\\u017d': 'Z',\n  '\\u017a': 'z',  '\\u017c': 'z', '\\u017e': 'z',\n  '\\u0132': 'IJ', '\\u0133': 'ij',\n  '\\u0152': 'Oe', '\\u0153': 'oe',\n  '\\u0149': \"'n\", '\\u017f': 's'\n};\n\n/**\n * Used by `_.deburr` to convert Latin-1 Supplement and Latin Extended-A\n * letters to basic Latin letters.\n *\n * @private\n * @param {string} letter The matched letter to deburr.\n * @returns {string} Returns the deburred letter.\n */\nvar deburrLetter = basePropertyOf(deburredLetters);\n\nmodule.exports = deburrLetter;\n", "var deburrLetter = require('./_deburrLetter'),\n    toString = require('./toString');\n\n/** Used to match Latin Unicode letters (excluding mathematical operators). */\nvar reLatin = /[\\xc0-\\xd6\\xd8-\\xf6\\xf8-\\xff\\u0100-\\u017f]/g;\n\n/** Used to compose unicode character classes. */\nvar rsComboMarksRange = '\\\\u0300-\\\\u036f',\n    reComboHalfMarksRange = '\\\\ufe20-\\\\ufe2f',\n    rsComboSymbolsRange = '\\\\u20d0-\\\\u20ff',\n    rsComboRange = rsComboMarksRange + reComboHalfMarksRange + rsComboSymbolsRange;\n\n/** Used to compose unicode capture groups. */\nvar rsCombo = '[' + rsComboRange + ']';\n\n/**\n * Used to match [combining diacritical marks](https://en.wikipedia.org/wiki/Combining_Diacritical_Marks) and\n * [combining diacritical marks for symbols](https://en.wikipedia.org/wiki/Combining_Diacritical_Marks_for_Symbols).\n */\nvar reComboMark = RegExp(rsCombo, 'g');\n\n/**\n * Deburrs `string` by converting\n * [Latin-1 Supplement](https://en.wikipedia.org/wiki/Latin-1_Supplement_(Unicode_block)#Character_table)\n * and [Latin Extended-A](https://en.wikipedia.org/wiki/Latin_Extended-A)\n * letters to basic Latin letters and removing\n * [combining diacritical marks](https://en.wikipedia.org/wiki/Combining_Diacritical_Marks).\n *\n * @static\n * @memberOf _\n * @since 3.0.0\n * @category String\n * @param {string} [string=''] The string to deburr.\n * @returns {string} Returns the deburred string.\n * @example\n *\n * _.deburr('d\u00E9j\u00E0 vu');\n * // => 'deja vu'\n */\nfunction deburr(string) {\n  string = toString(string);\n  return string && string.replace(reLatin, deburrLetter).replace(reComboMark, '');\n}\n\nmodule.exports = deburr;\n", "/** Used to match words composed of alphanumeric characters. */\nvar reAsciiWord = /[^\\x00-\\x2f\\x3a-\\x40\\x5b-\\x60\\x7b-\\x7f]+/g;\n\n/**\n * Splits an ASCII `string` into an array of its words.\n *\n * @private\n * @param {string} The string to inspect.\n * @returns {Array} Returns the words of `string`.\n */\nfunction asciiWords(string) {\n  return string.match(reAsciiWord) || [];\n}\n\nmodule.exports = asciiWords;\n", "/** Used to detect strings that need a more robust regexp to match words. */\nvar reHasUnicodeWord = /[a-z][A-Z]|[A-Z]{2}[a-z]|[0-9][a-zA-Z]|[a-zA-Z][0-9]|[^a-zA-Z0-9 ]/;\n\n/**\n * Checks if `string` contains a word composed of Unicode symbols.\n *\n * @private\n * @param {string} string The string to inspect.\n * @returns {boolean} Returns `true` if a word is found, else `false`.\n */\nfunction hasUnicodeWord(string) {\n  return reHasUnicodeWord.test(string);\n}\n\nmodule.exports = hasUnicodeWord;\n", "/** Used to compose unicode character classes. */\nvar rsAstralRange = '\\\\ud800-\\\\udfff',\n    rsComboMarksRange = '\\\\u0300-\\\\u036f',\n    reComboHalfMarksRange = '\\\\ufe20-\\\\ufe2f',\n    rsComboSymbolsRange = '\\\\u20d0-\\\\u20ff',\n    rsComboRange = rsComboMarksRange + reComboHalfMarksRange + rsComboSymbolsRange,\n    rsDingbatRange = '\\\\u2700-\\\\u27bf',\n    rsLowerRange = 'a-z\\\\xdf-\\\\xf6\\\\xf8-\\\\xff',\n    rsMathOpRange = '\\\\xac\\\\xb1\\\\xd7\\\\xf7',\n    rsNonCharRange = '\\\\x00-\\\\x2f\\\\x3a-\\\\x40\\\\x5b-\\\\x60\\\\x7b-\\\\xbf',\n    rsPunctuationRange = '\\\\u2000-\\\\u206f',\n    rsSpaceRange = ' \\\\t\\\\x0b\\\\f\\\\xa0\\\\ufeff\\\\n\\\\r\\\\u2028\\\\u2029\\\\u1680\\\\u180e\\\\u2000\\\\u2001\\\\u2002\\\\u2003\\\\u2004\\\\u2005\\\\u2006\\\\u2007\\\\u2008\\\\u2009\\\\u200a\\\\u202f\\\\u205f\\\\u3000',\n    rsUpperRange = 'A-Z\\\\xc0-\\\\xd6\\\\xd8-\\\\xde',\n    rsVarRange = '\\\\ufe0e\\\\ufe0f',\n    rsBreakRange = rsMathOpRange + rsNonCharRange + rsPunctuationRange + rsSpaceRange;\n\n/** Used to compose unicode capture groups. */\nvar rsApos = \"['\\u2019]\",\n    rsBreak = '[' + rsBreakRange + ']',\n    rsCombo = '[' + rsComboRange + ']',\n    rsDigits = '\\\\d+',\n    rsDingbat = '[' + rsDingbatRange + ']',\n    rsLower = '[' + rsLowerRange + ']',\n    rsMisc = '[^' + rsAstralRange + rsBreakRange + rsDigits + rsDingbatRange + rsLowerRange + rsUpperRange + ']',\n    rsFitz = '\\\\ud83c[\\\\udffb-\\\\udfff]',\n    rsModifier = '(?:' + rsCombo + '|' + rsFitz + ')',\n    rsNonAstral = '[^' + rsAstralRange + ']',\n    rsRegional = '(?:\\\\ud83c[\\\\udde6-\\\\uddff]){2}',\n    rsSurrPair = '[\\\\ud800-\\\\udbff][\\\\udc00-\\\\udfff]',\n    rsUpper = '[' + rsUpperRange + ']',\n    rsZWJ = '\\\\u200d';\n\n/** Used to compose unicode regexes. */\nvar rsMiscLower = '(?:' + rsLower + '|' + rsMisc + ')',\n    rsMiscUpper = '(?:' + rsUpper + '|' + rsMisc + ')',\n    rsOptContrLower = '(?:' + rsApos + '(?:d|ll|m|re|s|t|ve))?',\n    rsOptContrUpper = '(?:' + rsApos + '(?:D|LL|M|RE|S|T|VE))?',\n    reOptMod = rsModifier + '?',\n    rsOptVar = '[' + rsVarRange + ']?',\n    rsOptJoin = '(?:' + rsZWJ + '(?:' + [rsNonAstral, rsRegional, rsSurrPair].join('|') + ')' + rsOptVar + reOptMod + ')*',\n    rsOrdLower = '\\\\d*(?:1st|2nd|3rd|(?![123])\\\\dth)(?=\\\\b|[A-Z_])',\n    rsOrdUpper = '\\\\d*(?:1ST|2ND|3RD|(?![123])\\\\dTH)(?=\\\\b|[a-z_])',\n    rsSeq = rsOptVar + reOptMod + rsOptJoin,\n    rsEmoji = '(?:' + [rsDingbat, rsRegional, rsSurrPair].join('|') + ')' + rsSeq;\n\n/** Used to match complex or compound words. */\nvar reUnicodeWord = RegExp([\n  rsUpper + '?' + rsLower + '+' + rsOptContrLower + '(?=' + [rsBreak, rsUpper, '$'].join('|') + ')',\n  rsMiscUpper + '+' + rsOptContrUpper + '(?=' + [rsBreak, rsUpper + rsMiscLower, '$'].join('|') + ')',\n  rsUpper + '?' + rsMiscLower + '+' + rsOptContrLower,\n  rsUpper + '+' + rsOptContrUpper,\n  rsOrdUpper,\n  rsOrdLower,\n  rsDigits,\n  rsEmoji\n].join('|'), 'g');\n\n/**\n * Splits a Unicode `string` into an array of its words.\n *\n * @private\n * @param {string} The string to inspect.\n * @returns {Array} Returns the words of `string`.\n */\nfunction unicodeWords(string) {\n  return string.match(reUnicodeWord) || [];\n}\n\nmodule.exports = unicodeWords;\n", "var asciiWords = require('./_asciiWords'),\n    hasUnicodeWord = require('./_hasUnicodeWord'),\n    toString = require('./toString'),\n    unicodeWords = require('./_unicodeWords');\n\n/**\n * Splits `string` into an array of its words.\n *\n * @static\n * @memberOf _\n * @since 3.0.0\n * @category String\n * @param {string} [string=''] The string to inspect.\n * @param {RegExp|string} [pattern] The pattern to match words.\n * @param- {Object} [guard] Enables use as an iteratee for methods like `_.map`.\n * @returns {Array} Returns the words of `string`.\n * @example\n *\n * _.words('fred, barney, & pebbles');\n * // => ['fred', 'barney', 'pebbles']\n *\n * _.words('fred, barney, & pebbles', /[^, ]+/g);\n * // => ['fred', 'barney', '&', 'pebbles']\n */\nfunction words(string, pattern, guard) {\n  string = toString(string);\n  pattern = guard ? undefined : pattern;\n\n  if (pattern === undefined) {\n    return hasUnicodeWord(string) ? unicodeWords(string) : asciiWords(string);\n  }\n  return string.match(pattern) || [];\n}\n\nmodule.exports = words;\n", "var arrayReduce = require('./_arrayReduce'),\n    deburr = require('./deburr'),\n    words = require('./words');\n\n/** Used to compose unicode capture groups. */\nvar rsApos = \"['\\u2019]\";\n\n/** Used to match apostrophes. */\nvar reApos = RegExp(rsApos, 'g');\n\n/**\n * Creates a function like `_.camelCase`.\n *\n * @private\n * @param {Function} callback The function to combine each word.\n * @returns {Function} Returns the new compounder function.\n */\nfunction createCompounder(callback) {\n  return function(string) {\n    return arrayReduce(words(deburr(string).replace(reApos, '')), callback, '');\n  };\n}\n\nmodule.exports = createCompounder;\n", "var createCompounder = require('./_createCompounder');\n\n/**\n * Converts `string` to\n * [snake case](https://en.wikipedia.org/wiki/Snake_case).\n *\n * @static\n * @memberOf _\n * @since 3.0.0\n * @category String\n * @param {string} [string=''] The string to convert.\n * @returns {string} Returns the snake cased string.\n * @example\n *\n * _.snakeCase('Foo Bar');\n * // => 'foo_bar'\n *\n * _.snakeCase('fooBar');\n * // => 'foo_bar'\n *\n * _.snakeCase('--FOO-BAR--');\n * // => 'foo_bar'\n */\nvar snakeCase = createCompounder(function(result, word, index) {\n  return result + (index ? '_' : '') + word.toLowerCase();\n});\n\nmodule.exports = snakeCase;\n", "/**\n * The base implementation of `_.slice` without an iteratee call guard.\n *\n * @private\n * @param {Array} array The array to slice.\n * @param {number} [start=0] The start position.\n * @param {number} [end=array.length] The end position.\n * @returns {Array} Returns the slice of `array`.\n */\nfunction baseSlice(array, start, end) {\n  var index = -1,\n      length = array.length;\n\n  if (start < 0) {\n    start = -start > length ? 0 : (length + start);\n  }\n  end = end > length ? length : end;\n  if (end < 0) {\n    end += length;\n  }\n  length = start > end ? 0 : ((end - start) >>> 0);\n  start >>>= 0;\n\n  var result = Array(length);\n  while (++index < length) {\n    result[index] = array[index + start];\n  }\n  return result;\n}\n\nmodule.exports = baseSlice;\n", "var baseSlice = require('./_baseSlice');\n\n/**\n * Casts `array` to a slice if it's needed.\n *\n * @private\n * @param {Array} array The array to inspect.\n * @param {number} start The start position.\n * @param {number} [end=array.length] The end position.\n * @returns {Array} Returns the cast slice.\n */\nfunction castSlice(array, start, end) {\n  var length = array.length;\n  end = end === undefined ? length : end;\n  return (!start && end >= length) ? array : baseSlice(array, start, end);\n}\n\nmodule.exports = castSlice;\n", "/** Used to compose unicode character classes. */\nvar rsAstralRange = '\\\\ud800-\\\\udfff',\n    rsComboMarksRange = '\\\\u0300-\\\\u036f',\n    reComboHalfMarksRange = '\\\\ufe20-\\\\ufe2f',\n    rsComboSymbolsRange = '\\\\u20d0-\\\\u20ff',\n    rsComboRange = rsComboMarksRange + reComboHalfMarksRange + rsComboSymbolsRange,\n    rsVarRange = '\\\\ufe0e\\\\ufe0f';\n\n/** Used to compose unicode capture groups. */\nvar rsZWJ = '\\\\u200d';\n\n/** Used to detect strings with [zero-width joiners or code points from the astral planes](http://eev.ee/blog/2015/09/12/dark-corners-of-unicode/). */\nvar reHasUnicode = RegExp('[' + rsZWJ + rsAstralRange  + rsComboRange + rsVarRange + ']');\n\n/**\n * Checks if `string` contains Unicode symbols.\n *\n * @private\n * @param {string} string The string to inspect.\n * @returns {boolean} Returns `true` if a symbol is found, else `false`.\n */\nfunction hasUnicode(string) {\n  return reHasUnicode.test(string);\n}\n\nmodule.exports = hasUnicode;\n", "/**\n * Converts an ASCII `string` to an array.\n *\n * @private\n * @param {string} string The string to convert.\n * @returns {Array} Returns the converted array.\n */\nfunction asciiToArray(string) {\n  return string.split('');\n}\n\nmodule.exports = asciiToArray;\n", "/** Used to compose unicode character classes. */\nvar rsAstralRange = '\\\\ud800-\\\\udfff',\n    rsComboMarksRange = '\\\\u0300-\\\\u036f',\n    reComboHalfMarksRange = '\\\\ufe20-\\\\ufe2f',\n    rsComboSymbolsRange = '\\\\u20d0-\\\\u20ff',\n    rsComboRange = rsComboMarksRange + reComboHalfMarksRange + rsComboSymbolsRange,\n    rsVarRange = '\\\\ufe0e\\\\ufe0f';\n\n/** Used to compose unicode capture groups. */\nvar rsAstral = '[' + rsAstralRange + ']',\n    rsCombo = '[' + rsComboRange + ']',\n    rsFitz = '\\\\ud83c[\\\\udffb-\\\\udfff]',\n    rsModifier = '(?:' + rsCombo + '|' + rsFitz + ')',\n    rsNonAstral = '[^' + rsAstralRange + ']',\n    rsRegional = '(?:\\\\ud83c[\\\\udde6-\\\\uddff]){2}',\n    rsSurrPair = '[\\\\ud800-\\\\udbff][\\\\udc00-\\\\udfff]',\n    rsZWJ = '\\\\u200d';\n\n/** Used to compose unicode regexes. */\nvar reOptMod = rsModifier + '?',\n    rsOptVar = '[' + rsVarRange + ']?',\n    rsOptJoin = '(?:' + rsZWJ + '(?:' + [rsNonAstral, rsRegional, rsSurrPair].join('|') + ')' + rsOptVar + reOptMod + ')*',\n    rsSeq = rsOptVar + reOptMod + rsOptJoin,\n    rsSymbol = '(?:' + [rsNonAstral + rsCombo + '?', rsCombo, rsRegional, rsSurrPair, rsAstral].join('|') + ')';\n\n/** Used to match [string symbols](https://mathiasbynens.be/notes/javascript-unicode). */\nvar reUnicode = RegExp(rsFitz + '(?=' + rsFitz + ')|' + rsSymbol + rsSeq, 'g');\n\n/**\n * Converts a Unicode `string` to an array.\n *\n * @private\n * @param {string} string The string to convert.\n * @returns {Array} Returns the converted array.\n */\nfunction unicodeToArray(string) {\n  return string.match(reUnicode) || [];\n}\n\nmodule.exports = unicodeToArray;\n", "var asciiToArray = require('./_asciiToArray'),\n    hasUnicode = require('./_hasUnicode'),\n    unicodeToArray = require('./_unicodeToArray');\n\n/**\n * Converts `string` to an array.\n *\n * @private\n * @param {string} string The string to convert.\n * @returns {Array} Returns the converted array.\n */\nfunction stringToArray(string) {\n  return hasUnicode(string)\n    ? unicodeToArray(string)\n    : asciiToArray(string);\n}\n\nmodule.exports = stringToArray;\n", "var castSlice = require('./_castSlice'),\n    hasUnicode = require('./_hasUnicode'),\n    stringToArray = require('./_stringToArray'),\n    toString = require('./toString');\n\n/**\n * Creates a function like `_.lowerFirst`.\n *\n * @private\n * @param {string} methodName The name of the `String` case method to use.\n * @returns {Function} Returns the new case function.\n */\nfunction createCaseFirst(methodName) {\n  return function(string) {\n    string = toString(string);\n\n    var strSymbols = hasUnicode(string)\n      ? stringToArray(string)\n      : undefined;\n\n    var chr = strSymbols\n      ? strSymbols[0]\n      : string.charAt(0);\n\n    var trailing = strSymbols\n      ? castSlice(strSymbols, 1).join('')\n      : string.slice(1);\n\n    return chr[methodName]() + trailing;\n  };\n}\n\nmodule.exports = createCaseFirst;\n", "var createCaseFirst = require('./_createCaseFirst');\n\n/**\n * Converts the first character of `string` to upper case.\n *\n * @static\n * @memberOf _\n * @since 4.0.0\n * @category String\n * @param {string} [string=''] The string to convert.\n * @returns {string} Returns the converted string.\n * @example\n *\n * _.upperFirst('fred');\n * // => 'Fred'\n *\n * _.upperFirst('FRED');\n * // => 'FRED'\n */\nvar upperFirst = createCaseFirst('toUpperCase');\n\nmodule.exports = upperFirst;\n", "var toString = require('./toString'),\n    upperFirst = require('./upperFirst');\n\n/**\n * Converts the first character of `string` to upper case and the remaining\n * to lower case.\n *\n * @static\n * @memberOf _\n * @since 3.0.0\n * @category String\n * @param {string} [string=''] The string to capitalize.\n * @returns {string} Returns the capitalized string.\n * @example\n *\n * _.capitalize('FRED');\n * // => 'Fred'\n */\nfunction capitalize(string) {\n  return upperFirst(toString(string).toLowerCase());\n}\n\nmodule.exports = capitalize;\n", "var capitalize = require('./capitalize'),\n    createCompounder = require('./_createCompounder');\n\n/**\n * Converts `string` to [camel case](https://en.wikipedia.org/wiki/CamelCase).\n *\n * @static\n * @memberOf _\n * @since 3.0.0\n * @category String\n * @param {string} [string=''] The string to convert.\n * @returns {string} Returns the camel cased string.\n * @example\n *\n * _.camelCase('Foo Bar');\n * // => 'fooBar'\n *\n * _.camelCase('--foo-bar--');\n * // => 'fooBar'\n *\n * _.camelCase('__FOO_BAR__');\n * // => 'fooBar'\n */\nvar camelCase = createCompounder(function(result, word, index) {\n  word = word.toLowerCase();\n  return result + (index ? capitalize(word) : word);\n});\n\nmodule.exports = camelCase;\n", "var baseAssignValue = require('./_baseAssignValue'),\n    baseForOwn = require('./_baseForOwn'),\n    baseIteratee = require('./_baseIteratee');\n\n/**\n * The opposite of `_.mapValues`; this method creates an object with the\n * same values as `object` and keys generated by running each own enumerable\n * string keyed property of `object` thru `iteratee`. The iteratee is invoked\n * with three arguments: (value, key, object).\n *\n * @static\n * @memberOf _\n * @since 3.8.0\n * @category Object\n * @param {Object} object The object to iterate over.\n * @param {Function} [iteratee=_.identity] The function invoked per iteration.\n * @returns {Object} Returns the new mapped object.\n * @see _.mapValues\n * @example\n *\n * _.mapKeys({ 'a': 1, 'b': 2 }, function(value, key) {\n *   return key + value;\n * });\n * // => { 'a1': 1, 'b2': 2 }\n */\nfunction mapKeys(object, iteratee) {\n  var result = {};\n  iteratee = baseIteratee(iteratee, 3);\n\n  baseForOwn(object, function(value, key, object) {\n    baseAssignValue(result, iteratee(value, key, object), value);\n  });\n  return result;\n}\n\nmodule.exports = mapKeys;\n", "\n/**\n * Topological sorting function\n *\n * @param {Array} edges\n * @returns {Array}\n */\n\nmodule.exports = function(edges) {\n  return toposort(uniqueNodes(edges), edges)\n}\n\nmodule.exports.array = toposort\n\nfunction toposort(nodes, edges) {\n  var cursor = nodes.length\n    , sorted = new Array(cursor)\n    , visited = {}\n    , i = cursor\n    // Better data structures make algorithm much faster.\n    , outgoingEdges = makeOutgoingEdges(edges)\n    , nodesHash = makeNodesHash(nodes)\n\n  // check for unknown nodes\n  edges.forEach(function(edge) {\n    if (!nodesHash.has(edge[0]) || !nodesHash.has(edge[1])) {\n      throw new Error('Unknown node. There is an unknown node in the supplied edges.')\n    }\n  })\n\n  while (i--) {\n    if (!visited[i]) visit(nodes[i], i, new Set())\n  }\n\n  return sorted\n\n  function visit(node, i, predecessors) {\n    if(predecessors.has(node)) {\n      var nodeRep\n      try {\n        nodeRep = \", node was:\" + JSON.stringify(node)\n      } catch(e) {\n        nodeRep = \"\"\n      }\n      throw new Error('Cyclic dependency' + nodeRep)\n    }\n\n    if (!nodesHash.has(node)) {\n      throw new Error('Found unknown node. Make sure to provided all involved nodes. Unknown node: '+JSON.stringify(node))\n    }\n\n    if (visited[i]) return;\n    visited[i] = true\n\n    var outgoing = outgoingEdges.get(node) || new Set()\n    outgoing = Array.from(outgoing)\n\n    if (i = outgoing.length) {\n      predecessors.add(node)\n      do {\n        var child = outgoing[--i]\n        visit(child, nodesHash.get(child), predecessors)\n      } while (i)\n      predecessors.delete(node)\n    }\n\n    sorted[--cursor] = node\n  }\n}\n\nfunction uniqueNodes(arr){\n  var res = new Set()\n  for (var i = 0, len = arr.length; i < len; i++) {\n    var edge = arr[i]\n    res.add(edge[0])\n    res.add(edge[1])\n  }\n  return Array.from(res)\n}\n\nfunction makeOutgoingEdges(arr){\n  var edges = new Map()\n  for (var i = 0, len = arr.length; i < len; i++) {\n    var edge = arr[i]\n    if (!edges.has(edge[0])) edges.set(edge[0], new Set())\n    if (!edges.has(edge[1])) edges.set(edge[1], new Set())\n    edges.get(edge[0]).add(edge[1])\n  }\n  return edges\n}\n\nfunction makeNodesHash(arr){\n  var res = new Map()\n  for (var i = 0, len = arr.length; i < len; i++) {\n    res.set(arr[i], i)\n  }\n  return res\n}\n", "\"use strict\";\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\nexports.default = sortFields;\n\nvar _has = _interopRequireDefault(require(\"lodash/has\"));\n\nvar _toposort = _interopRequireDefault(require(\"toposort\"));\n\nvar _propertyExpr = require(\"property-expr\");\n\nvar _Reference = _interopRequireDefault(require(\"../Reference\"));\n\nvar _isSchema = _interopRequireDefault(require(\"./isSchema\"));\n\nfunction _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { default: obj }; }\n\n// @ts-expect-error\nfunction sortFields(fields, excludedEdges = []) {\n  let edges = [];\n  let nodes = new Set();\n  let excludes = new Set(excludedEdges.map(([a, b]) => `${a}-${b}`));\n\n  function addNode(depPath, key) {\n    let node = (0, _propertyExpr.split)(depPath)[0];\n    nodes.add(node);\n    if (!excludes.has(`${key}-${node}`)) edges.push([key, node]);\n  }\n\n  for (const key in fields) if ((0, _has.default)(fields, key)) {\n    let value = fields[key];\n    nodes.add(key);\n    if (_Reference.default.isRef(value) && value.isSibling) addNode(value.path, key);else if ((0, _isSchema.default)(value) && 'deps' in value) value.deps.forEach(path => addNode(path, key));\n  }\n\n  return _toposort.default.array(Array.from(nodes), edges).reverse();\n}", "\"use strict\";\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\nexports.default = sortByKeyOrder;\n\nfunction findIndex(arr, err) {\n  let idx = Infinity;\n  arr.some((key, ii) => {\n    var _err$path;\n\n    if (((_err$path = err.path) == null ? void 0 : _err$path.indexOf(key)) !== -1) {\n      idx = ii;\n      return true;\n    }\n  });\n  return idx;\n}\n\nfunction sortByKeyOrder(keys) {\n  return (a, b) => {\n    return findIndex(keys, a) - findIndex(keys, b);\n  };\n}", "\"use strict\";\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\nexports.create = create;\nexports.default = void 0;\n\nvar _has = _interopRequireDefault(require(\"lodash/has\"));\n\nvar _snakeCase = _interopRequireDefault(require(\"lodash/snakeCase\"));\n\nvar _camelCase = _interopRequireDefault(require(\"lodash/camelCase\"));\n\nvar _mapKeys = _interopRequireDefault(require(\"lodash/mapKeys\"));\n\nvar _mapValues = _interopRequireDefault(require(\"lodash/mapValues\"));\n\nvar _propertyExpr = require(\"property-expr\");\n\nvar _locale = require(\"./locale\");\n\nvar _sortFields = _interopRequireDefault(require(\"./util/sortFields\"));\n\nvar _sortByKeyOrder = _interopRequireDefault(require(\"./util/sortByKeyOrder\"));\n\nvar _runTests = _interopRequireDefault(require(\"./util/runTests\"));\n\nvar _ValidationError = _interopRequireDefault(require(\"./ValidationError\"));\n\nvar _schema = _interopRequireDefault(require(\"./schema\"));\n\nfunction _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { default: obj }; }\n\nfunction _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nlet isObject = obj => Object.prototype.toString.call(obj) === '[object Object]';\n\nfunction unknown(ctx, value) {\n  let known = Object.keys(ctx.fields);\n  return Object.keys(value).filter(key => known.indexOf(key) === -1);\n}\n\nconst defaultSort = (0, _sortByKeyOrder.default)([]);\n\nclass ObjectSchema extends _schema.default {\n  constructor(spec) {\n    super({\n      type: 'object'\n    });\n    this.fields = Object.create(null);\n    this._sortErrors = defaultSort;\n    this._nodes = [];\n    this._excludedEdges = [];\n    this.withMutation(() => {\n      this.transform(function coerce(value) {\n        if (typeof value === 'string') {\n          try {\n            value = JSON.parse(value);\n          } catch (err) {\n            value = null;\n          }\n        }\n\n        if (this.isType(value)) return value;\n        return null;\n      });\n\n      if (spec) {\n        this.shape(spec);\n      }\n    });\n  }\n\n  _typeCheck(value) {\n    return isObject(value) || typeof value === 'function';\n  }\n\n  _cast(_value, options = {}) {\n    var _options$stripUnknown;\n\n    let value = super._cast(_value, options); //should ignore nulls here\n\n\n    if (value === undefined) return this.getDefault();\n    if (!this._typeCheck(value)) return value;\n    let fields = this.fields;\n    let strip = (_options$stripUnknown = options.stripUnknown) != null ? _options$stripUnknown : this.spec.noUnknown;\n\n    let props = this._nodes.concat(Object.keys(value).filter(v => this._nodes.indexOf(v) === -1));\n\n    let intermediateValue = {}; // is filled during the transform below\n\n    let innerOptions = _extends({}, options, {\n      parent: intermediateValue,\n      __validating: options.__validating || false\n    });\n\n    let isChanged = false;\n\n    for (const prop of props) {\n      let field = fields[prop];\n      let exists = (0, _has.default)(value, prop);\n\n      if (field) {\n        let fieldValue;\n        let inputValue = value[prop]; // safe to mutate since this is fired in sequence\n\n        innerOptions.path = (options.path ? `${options.path}.` : '') + prop; // innerOptions.value = value[prop];\n\n        field = field.resolve({\n          value: inputValue,\n          context: options.context,\n          parent: intermediateValue\n        });\n        let fieldSpec = 'spec' in field ? field.spec : undefined;\n        let strict = fieldSpec == null ? void 0 : fieldSpec.strict;\n\n        if (fieldSpec == null ? void 0 : fieldSpec.strip) {\n          isChanged = isChanged || prop in value;\n          continue;\n        }\n\n        fieldValue = !options.__validating || !strict ? // TODO: use _cast, this is double resolving\n        field.cast(value[prop], innerOptions) : value[prop];\n\n        if (fieldValue !== undefined) {\n          intermediateValue[prop] = fieldValue;\n        }\n      } else if (exists && !strip) {\n        intermediateValue[prop] = value[prop];\n      }\n\n      if (intermediateValue[prop] !== value[prop]) {\n        isChanged = true;\n      }\n    }\n\n    return isChanged ? intermediateValue : value;\n  }\n\n  _validate(_value, opts = {}, callback) {\n    let errors = [];\n    let {\n      sync,\n      from = [],\n      originalValue = _value,\n      abortEarly = this.spec.abortEarly,\n      recursive = this.spec.recursive\n    } = opts;\n    from = [{\n      schema: this,\n      value: originalValue\n    }, ...from]; // this flag is needed for handling `strict` correctly in the context of\n    // validation vs just casting. e.g strict() on a field is only used when validating\n\n    opts.__validating = true;\n    opts.originalValue = originalValue;\n    opts.from = from;\n\n    super._validate(_value, opts, (err, value) => {\n      if (err) {\n        if (!_ValidationError.default.isError(err) || abortEarly) {\n          return void callback(err, value);\n        }\n\n        errors.push(err);\n      }\n\n      if (!recursive || !isObject(value)) {\n        callback(errors[0] || null, value);\n        return;\n      }\n\n      originalValue = originalValue || value;\n\n      let tests = this._nodes.map(key => (_, cb) => {\n        let path = key.indexOf('.') === -1 ? (opts.path ? `${opts.path}.` : '') + key : `${opts.path || ''}[\"${key}\"]`;\n        let field = this.fields[key];\n\n        if (field && 'validate' in field) {\n          field.validate(value[key], _extends({}, opts, {\n            // @ts-ignore\n            path,\n            from,\n            // inner fields are always strict:\n            // 1. this isn't strict so the casting will also have cast inner values\n            // 2. this is strict in which case the nested values weren't cast either\n            strict: true,\n            parent: value,\n            originalValue: originalValue[key]\n          }), cb);\n          return;\n        }\n\n        cb(null);\n      });\n\n      (0, _runTests.default)({\n        sync,\n        tests,\n        value,\n        errors,\n        endEarly: abortEarly,\n        sort: this._sortErrors,\n        path: opts.path\n      }, callback);\n    });\n  }\n\n  clone(spec) {\n    const next = super.clone(spec);\n    next.fields = _extends({}, this.fields);\n    next._nodes = this._nodes;\n    next._excludedEdges = this._excludedEdges;\n    next._sortErrors = this._sortErrors;\n    return next;\n  }\n\n  concat(schema) {\n    let next = super.concat(schema);\n    let nextFields = next.fields;\n\n    for (let [field, schemaOrRef] of Object.entries(this.fields)) {\n      const target = nextFields[field];\n\n      if (target === undefined) {\n        nextFields[field] = schemaOrRef;\n      } else if (target instanceof _schema.default && schemaOrRef instanceof _schema.default) {\n        nextFields[field] = schemaOrRef.concat(target);\n      }\n    }\n\n    return next.withMutation(() => next.shape(nextFields, this._excludedEdges));\n  }\n\n  getDefaultFromShape() {\n    let dft = {};\n\n    this._nodes.forEach(key => {\n      const field = this.fields[key];\n      dft[key] = 'default' in field ? field.getDefault() : undefined;\n    });\n\n    return dft;\n  }\n\n  _getDefault() {\n    if ('default' in this.spec) {\n      return super._getDefault();\n    } // if there is no default set invent one\n\n\n    if (!this._nodes.length) {\n      return undefined;\n    }\n\n    return this.getDefaultFromShape();\n  }\n\n  shape(additions, excludes = []) {\n    let next = this.clone();\n    let fields = Object.assign(next.fields, additions);\n    next.fields = fields;\n    next._sortErrors = (0, _sortByKeyOrder.default)(Object.keys(fields));\n\n    if (excludes.length) {\n      // this is a convenience for when users only supply a single pair\n      if (!Array.isArray(excludes[0])) excludes = [excludes];\n      next._excludedEdges = [...next._excludedEdges, ...excludes];\n    }\n\n    next._nodes = (0, _sortFields.default)(fields, next._excludedEdges);\n    return next;\n  }\n\n  pick(keys) {\n    const picked = {};\n\n    for (const key of keys) {\n      if (this.fields[key]) picked[key] = this.fields[key];\n    }\n\n    return this.clone().withMutation(next => {\n      next.fields = {};\n      return next.shape(picked);\n    });\n  }\n\n  omit(keys) {\n    const next = this.clone();\n    const fields = next.fields;\n    next.fields = {};\n\n    for (const key of keys) {\n      delete fields[key];\n    }\n\n    return next.withMutation(() => next.shape(fields));\n  }\n\n  from(from, to, alias) {\n    let fromGetter = (0, _propertyExpr.getter)(from, true);\n    return this.transform(obj => {\n      if (obj == null) return obj;\n      let newObj = obj;\n\n      if ((0, _has.default)(obj, from)) {\n        newObj = _extends({}, obj);\n        if (!alias) delete newObj[from];\n        newObj[to] = fromGetter(obj);\n      }\n\n      return newObj;\n    });\n  }\n\n  noUnknown(noAllow = true, message = _locale.object.noUnknown) {\n    if (typeof noAllow === 'string') {\n      message = noAllow;\n      noAllow = true;\n    }\n\n    let next = this.test({\n      name: 'noUnknown',\n      exclusive: true,\n      message: message,\n\n      test(value) {\n        if (value == null) return true;\n        const unknownKeys = unknown(this.schema, value);\n        return !noAllow || unknownKeys.length === 0 || this.createError({\n          params: {\n            unknown: unknownKeys.join(', ')\n          }\n        });\n      }\n\n    });\n    next.spec.noUnknown = noAllow;\n    return next;\n  }\n\n  unknown(allow = true, message = _locale.object.noUnknown) {\n    return this.noUnknown(!allow, message);\n  }\n\n  transformKeys(fn) {\n    return this.transform(obj => obj && (0, _mapKeys.default)(obj, (_, key) => fn(key)));\n  }\n\n  camelCase() {\n    return this.transformKeys(_camelCase.default);\n  }\n\n  snakeCase() {\n    return this.transformKeys(_snakeCase.default);\n  }\n\n  constantCase() {\n    return this.transformKeys(key => (0, _snakeCase.default)(key).toUpperCase());\n  }\n\n  describe() {\n    let base = super.describe();\n    base.fields = (0, _mapValues.default)(this.fields, value => value.describe());\n    return base;\n  }\n\n}\n\nexports.default = ObjectSchema;\n\nfunction create(spec) {\n  return new ObjectSchema(spec);\n}\n\ncreate.prototype = ObjectSchema.prototype;", "\"use strict\";\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\nexports.create = create;\nexports.default = void 0;\n\nvar _isAbsent = _interopRequireDefault(require(\"./util/isAbsent\"));\n\nvar _isSchema = _interopRequireDefault(require(\"./util/isSchema\"));\n\nvar _printValue = _interopRequireDefault(require(\"./util/printValue\"));\n\nvar _locale = require(\"./locale\");\n\nvar _runTests = _interopRequireDefault(require(\"./util/runTests\"));\n\nvar _ValidationError = _interopRequireDefault(require(\"./ValidationError\"));\n\nvar _schema = _interopRequireDefault(require(\"./schema\"));\n\nfunction _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { default: obj }; }\n\nfunction _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction create(type) {\n  return new ArraySchema(type);\n}\n\nclass ArraySchema extends _schema.default {\n  constructor(type) {\n    super({\n      type: 'array'\n    }); // `undefined` specifically means uninitialized, as opposed to\n    // \"no subtype\"\n\n    this.innerType = void 0;\n    this.innerType = type;\n    this.withMutation(() => {\n      this.transform(function (values) {\n        if (typeof values === 'string') try {\n          values = JSON.parse(values);\n        } catch (err) {\n          values = null;\n        }\n        return this.isType(values) ? values : null;\n      });\n    });\n  }\n\n  _typeCheck(v) {\n    return Array.isArray(v);\n  }\n\n  get _subType() {\n    return this.innerType;\n  }\n\n  _cast(_value, _opts) {\n    const value = super._cast(_value, _opts); //should ignore nulls here\n\n\n    if (!this._typeCheck(value) || !this.innerType) return value;\n    let isChanged = false;\n    const castArray = value.map((v, idx) => {\n      const castElement = this.innerType.cast(v, _extends({}, _opts, {\n        path: `${_opts.path || ''}[${idx}]`\n      }));\n\n      if (castElement !== v) {\n        isChanged = true;\n      }\n\n      return castElement;\n    });\n    return isChanged ? castArray : value;\n  }\n\n  _validate(_value, options = {}, callback) {\n    var _options$abortEarly, _options$recursive;\n\n    let errors = [];\n    let sync = options.sync;\n    let path = options.path;\n    let innerType = this.innerType;\n    let endEarly = (_options$abortEarly = options.abortEarly) != null ? _options$abortEarly : this.spec.abortEarly;\n    let recursive = (_options$recursive = options.recursive) != null ? _options$recursive : this.spec.recursive;\n    let originalValue = options.originalValue != null ? options.originalValue : _value;\n\n    super._validate(_value, options, (err, value) => {\n      if (err) {\n        if (!_ValidationError.default.isError(err) || endEarly) {\n          return void callback(err, value);\n        }\n\n        errors.push(err);\n      }\n\n      if (!recursive || !innerType || !this._typeCheck(value)) {\n        callback(errors[0] || null, value);\n        return;\n      }\n\n      originalValue = originalValue || value; // #950 Ensure that sparse array empty slots are validated\n\n      let tests = new Array(value.length);\n\n      for (let idx = 0; idx < value.length; idx++) {\n        let item = value[idx];\n        let path = `${options.path || ''}[${idx}]`; // object._validate note for isStrict explanation\n\n        let innerOptions = _extends({}, options, {\n          path,\n          strict: true,\n          parent: value,\n          index: idx,\n          originalValue: originalValue[idx]\n        });\n\n        tests[idx] = (_, cb) => innerType.validate(item, innerOptions, cb);\n      }\n\n      (0, _runTests.default)({\n        sync,\n        path,\n        value,\n        errors,\n        endEarly,\n        tests\n      }, callback);\n    });\n  }\n\n  clone(spec) {\n    const next = super.clone(spec);\n    next.innerType = this.innerType;\n    return next;\n  }\n\n  concat(schema) {\n    let next = super.concat(schema);\n    next.innerType = this.innerType;\n    if (schema.innerType) next.innerType = next.innerType ? // @ts-expect-error Lazy doesn't have concat()\n    next.innerType.concat(schema.innerType) : schema.innerType;\n    return next;\n  }\n\n  of(schema) {\n    // FIXME: this should return a new instance of array without the default to be\n    let next = this.clone();\n    if (!(0, _isSchema.default)(schema)) throw new TypeError('`array.of()` sub-schema must be a valid yup schema not: ' + (0, _printValue.default)(schema)); // FIXME(ts):\n\n    next.innerType = schema;\n    return next;\n  }\n\n  length(length, message = _locale.array.length) {\n    return this.test({\n      message,\n      name: 'length',\n      exclusive: true,\n      params: {\n        length\n      },\n\n      test(value) {\n        return (0, _isAbsent.default)(value) || value.length === this.resolve(length);\n      }\n\n    });\n  }\n\n  min(min, message) {\n    message = message || _locale.array.min;\n    return this.test({\n      message,\n      name: 'min',\n      exclusive: true,\n      params: {\n        min\n      },\n\n      // FIXME(ts): Array<typeof T>\n      test(value) {\n        return (0, _isAbsent.default)(value) || value.length >= this.resolve(min);\n      }\n\n    });\n  }\n\n  max(max, message) {\n    message = message || _locale.array.max;\n    return this.test({\n      message,\n      name: 'max',\n      exclusive: true,\n      params: {\n        max\n      },\n\n      test(value) {\n        return (0, _isAbsent.default)(value) || value.length <= this.resolve(max);\n      }\n\n    });\n  }\n\n  ensure() {\n    return this.default(() => []).transform((val, original) => {\n      // We don't want to return `null` for nullable schema\n      if (this._typeCheck(val)) return val;\n      return original == null ? [] : [].concat(original);\n    });\n  }\n\n  compact(rejector) {\n    let reject = !rejector ? v => !!v : (v, i, a) => !rejector(v, i, a);\n    return this.transform(values => values != null ? values.filter(reject) : values);\n  }\n\n  describe() {\n    let base = super.describe();\n    if (this.innerType) base.innerType = this.innerType.describe();\n    return base;\n  }\n\n  nullable(isNullable = true) {\n    return super.nullable(isNullable);\n  }\n\n  defined() {\n    return super.defined();\n  }\n\n  required(msg) {\n    return super.required(msg);\n  }\n\n}\n\nexports.default = ArraySchema;\ncreate.prototype = ArraySchema.prototype; //\n// Interfaces\n//", "\"use strict\";\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\nexports.create = create;\nexports.default = void 0;\n\nvar _isSchema = _interopRequireDefault(require(\"./util/isSchema\"));\n\nfunction _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { default: obj }; }\n\nfunction create(builder) {\n  return new Lazy(builder);\n}\n\nclass Lazy {\n  constructor(builder) {\n    this.type = 'lazy';\n    this.__isYupSchema__ = true;\n    this.__inputType = void 0;\n    this.__outputType = void 0;\n\n    this._resolve = (value, options = {}) => {\n      let schema = this.builder(value, options);\n      if (!(0, _isSchema.default)(schema)) throw new TypeError('lazy() functions must return a valid schema');\n      return schema.resolve(options);\n    };\n\n    this.builder = builder;\n  }\n\n  resolve(options) {\n    return this._resolve(options.value, options);\n  }\n\n  cast(value, options) {\n    return this._resolve(value, options).cast(value, options);\n  }\n\n  validate(value, options, maybeCb) {\n    // @ts-expect-error missing public callback on type\n    return this._resolve(value, options).validate(value, options, maybeCb);\n  }\n\n  validateSync(value, options) {\n    return this._resolve(value, options).validateSync(value, options);\n  }\n\n  validateAt(path, value, options) {\n    return this._resolve(value, options).validateAt(path, value, options);\n  }\n\n  validateSyncAt(path, value, options) {\n    return this._resolve(value, options).validateSyncAt(path, value, options);\n  }\n\n  describe() {\n    return null;\n  }\n\n  isValid(value, options) {\n    return this._resolve(value, options).isValid(value, options);\n  }\n\n  isValidSync(value, options) {\n    return this._resolve(value, options).isValidSync(value, options);\n  }\n\n}\n\nvar _default = Lazy;\nexports.default = _default;", "\"use strict\";\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\nexports.default = setLocale;\n\nvar _locale = _interopRequireDefault(require(\"./locale\"));\n\nfunction _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { default: obj }; }\n\nfunction setLocale(custom) {\n  Object.keys(custom).forEach(type => {\n    // @ts-ignore\n    Object.keys(custom[type]).forEach(method => {\n      // @ts-ignore\n      _locale.default[type][method] = custom[type][method];\n    });\n  });\n}", "\"use strict\";\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\nObject.defineProperty(exports, \"ArraySchema\", {\n  enumerable: true,\n  get: function () {\n    return _array.default;\n  }\n});\nObject.defineProperty(exports, \"BaseSchema\", {\n  enumerable: true,\n  get: function () {\n    return _schema.default;\n  }\n});\nObject.defineProperty(exports, \"BooleanSchema\", {\n  enumerable: true,\n  get: function () {\n    return _boolean.default;\n  }\n});\nObject.defineProperty(exports, \"DateSchema\", {\n  enumerable: true,\n  get: function () {\n    return _date.default;\n  }\n});\nObject.defineProperty(exports, \"MixedSchema\", {\n  enumerable: true,\n  get: function () {\n    return _mixed.default;\n  }\n});\nObject.defineProperty(exports, \"NumberSchema\", {\n  enumerable: true,\n  get: function () {\n    return _number.default;\n  }\n});\nObject.defineProperty(exports, \"ObjectSchema\", {\n  enumerable: true,\n  get: function () {\n    return _object.default;\n  }\n});\nObject.defineProperty(exports, \"StringSchema\", {\n  enumerable: true,\n  get: function () {\n    return _string.default;\n  }\n});\nObject.defineProperty(exports, \"ValidationError\", {\n  enumerable: true,\n  get: function () {\n    return _ValidationError.default;\n  }\n});\nexports.addMethod = addMethod;\nObject.defineProperty(exports, \"array\", {\n  enumerable: true,\n  get: function () {\n    return _array.create;\n  }\n});\nObject.defineProperty(exports, \"bool\", {\n  enumerable: true,\n  get: function () {\n    return _boolean.create;\n  }\n});\nObject.defineProperty(exports, \"boolean\", {\n  enumerable: true,\n  get: function () {\n    return _boolean.create;\n  }\n});\nObject.defineProperty(exports, \"date\", {\n  enumerable: true,\n  get: function () {\n    return _date.create;\n  }\n});\nObject.defineProperty(exports, \"isSchema\", {\n  enumerable: true,\n  get: function () {\n    return _isSchema.default;\n  }\n});\nObject.defineProperty(exports, \"lazy\", {\n  enumerable: true,\n  get: function () {\n    return _Lazy.create;\n  }\n});\nObject.defineProperty(exports, \"mixed\", {\n  enumerable: true,\n  get: function () {\n    return _mixed.create;\n  }\n});\nObject.defineProperty(exports, \"number\", {\n  enumerable: true,\n  get: function () {\n    return _number.create;\n  }\n});\nObject.defineProperty(exports, \"object\", {\n  enumerable: true,\n  get: function () {\n    return _object.create;\n  }\n});\nObject.defineProperty(exports, \"reach\", {\n  enumerable: true,\n  get: function () {\n    return _reach.default;\n  }\n});\nObject.defineProperty(exports, \"ref\", {\n  enumerable: true,\n  get: function () {\n    return _Reference.create;\n  }\n});\nObject.defineProperty(exports, \"setLocale\", {\n  enumerable: true,\n  get: function () {\n    return _setLocale.default;\n  }\n});\nObject.defineProperty(exports, \"string\", {\n  enumerable: true,\n  get: function () {\n    return _string.create;\n  }\n});\n\nvar _mixed = _interopRequireWildcard(require(\"./mixed\"));\n\nvar _boolean = _interopRequireWildcard(require(\"./boolean\"));\n\nvar _string = _interopRequireWildcard(require(\"./string\"));\n\nvar _number = _interopRequireWildcard(require(\"./number\"));\n\nvar _date = _interopRequireWildcard(require(\"./date\"));\n\nvar _object = _interopRequireWildcard(require(\"./object\"));\n\nvar _array = _interopRequireWildcard(require(\"./array\"));\n\nvar _Reference = require(\"./Reference\");\n\nvar _Lazy = require(\"./Lazy\");\n\nvar _ValidationError = _interopRequireDefault(require(\"./ValidationError\"));\n\nvar _reach = _interopRequireDefault(require(\"./util/reach\"));\n\nvar _isSchema = _interopRequireDefault(require(\"./util/isSchema\"));\n\nvar _setLocale = _interopRequireDefault(require(\"./setLocale\"));\n\nvar _schema = _interopRequireDefault(require(\"./schema\"));\n\nfunction _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { default: obj }; }\n\nfunction _getRequireWildcardCache(nodeInterop) { if (typeof WeakMap !== \"function\") return null; var cacheBabelInterop = new WeakMap(); var cacheNodeInterop = new WeakMap(); return (_getRequireWildcardCache = function (nodeInterop) { return nodeInterop ? cacheNodeInterop : cacheBabelInterop; })(nodeInterop); }\n\nfunction _interopRequireWildcard(obj, nodeInterop) { if (!nodeInterop && obj && obj.__esModule) { return obj; } if (obj === null || typeof obj !== \"object\" && typeof obj !== \"function\") { return { default: obj }; } var cache = _getRequireWildcardCache(nodeInterop); if (cache && cache.has(obj)) { return cache.get(obj); } var newObj = {}; var hasPropertyDescriptor = Object.defineProperty && Object.getOwnPropertyDescriptor; for (var key in obj) { if (key !== \"default\" && Object.prototype.hasOwnProperty.call(obj, key)) { var desc = hasPropertyDescriptor ? Object.getOwnPropertyDescriptor(obj, key) : null; if (desc && (desc.get || desc.set)) { Object.defineProperty(newObj, key, desc); } else { newObj[key] = obj[key]; } } } newObj.default = obj; if (cache) { cache.set(obj, newObj); } return newObj; }\n\nfunction addMethod(schemaType, name, fn) {\n  if (!schemaType || !(0, _isSchema.default)(schemaType.prototype)) throw new TypeError('You must provide a yup schema constructor function');\n  if (typeof name !== 'string') throw new TypeError('A Method name must be provided');\n  if (typeof fn !== 'function') throw new TypeError('Method function must be provided');\n  schemaType.prototype[name] = fn;\n}", "import handler from \"../handler-lib\";\n// utils\nimport { hasPermissions } from \"../../utils/auth/authorization\";\nimport { parseSpecificReportParameters } from \"../../utils/auth/parameters\";\nimport {\n  validateData,\n  validateFieldData,\n} from \"../../utils/validation/validation\";\nimport { metadataValidationSchema } from \"../../utils/validation/schemas\";\nimport { error } from \"../../utils/constants/constants\";\nimport {\n  calculateCompletionStatus,\n  isComplete,\n} from \"../../utils/validation/completionStatus\";\n// types\nimport { UserRoles } from \"../../utils/types\";\nimport { removeNotApplicablePopsFromInitiatives } from \"../../utils/data/data\";\nimport {\n  getReportFieldData,\n  getReportFormTemplate,\n  getReportMetadata,\n  putReportFieldData,\n  putReportMetadata,\n} from \"../../storage/reports\";\nimport {\n  badRequest,\n  forbidden,\n  internalServerError,\n  notFound,\n  ok,\n} from \"../../utils/responses/response-lib\";\n\nexport const updateReport = handler(async (event) => {\n  const { allParamsValid, reportType, state, id } =\n    parseSpecificReportParameters(event);\n  if (!allParamsValid) {\n    return badRequest(error.NO_KEY);\n  }\n\n  // If request body is missing, return a 400 error.\n  if (!event?.body) {\n    return badRequest(error.MISSING_DATA);\n  }\n\n  // Blocklisted keys\n  const metadataBlocklist = [\n    \"submittedBy\",\n    \"submittedOnDate\",\n    \"locked\",\n    \"archive\",\n  ];\n  const fieldDataBlocklist = [\n    \"submitterName\",\n    \"submitterEmailAddress\",\n    \"reportSubmissionDate\",\n  ];\n\n  // This parse is guaranteed to succeed, because handler-lib already did it.\n  const eventBody = JSON.parse(event.body);\n  if (\n    (eventBody.metadata &&\n      Object.keys(eventBody.metadata).some((_) =>\n        metadataBlocklist.includes(_)\n      )) ||\n    (eventBody.fieldData &&\n      Object.keys(eventBody.fieldData).some((_) =>\n        fieldDataBlocklist.includes(_)\n      ))\n  ) {\n    return badRequest(error.INVALID_DATA);\n  }\n\n  // Ensure user has correct permissions to update a report.\n  if (!hasPermissions(event, [UserRoles.STATE_USER], state)) {\n    return forbidden(error.UNAUTHORIZED);\n  }\n\n  const currentReport = await getReportMetadata(reportType, state, id);\n  if (!currentReport) {\n    return notFound(error.NO_MATCHING_RECORD);\n  }\n\n  if (currentReport.archived || currentReport.locked) {\n    return forbidden(error.UNAUTHORIZED);\n  }\n\n  const formTemplate = await getReportFormTemplate(currentReport);\n  if (!formTemplate) {\n    return notFound(error.MISSING_DATA);\n  }\n\n  const existingFieldData = await getReportFieldData(currentReport);\n  if (!existingFieldData) {\n    return notFound(error.MISSING_DATA);\n  }\n\n  // Parse the passed payload.\n  const unvalidatedPayload = JSON.parse(event.body);\n\n  const { metadata: unvalidatedMetadata, fieldData: unvalidatedFieldData } =\n    unvalidatedPayload;\n\n  if (!unvalidatedFieldData) {\n    return badRequest(error.MISSING_DATA);\n  }\n\n  // Validation JSON should be there\u2014if it's not, there's an issue.\n  if (!formTemplate.validationJson) {\n    return internalServerError(error.MISSING_FORM_TEMPLATE);\n  }\n\n  // Validate passed field data\n  let validatedFieldData;\n  try {\n    validatedFieldData = await validateFieldData(\n      formTemplate.validationJson,\n      unvalidatedFieldData\n    );\n  } catch {\n    return badRequest(error.INVALID_DATA);\n  }\n\n  // Finalize fieldData to be sent to s3\n  const fieldData = {\n    ...existingFieldData,\n    ...validatedFieldData,\n  };\n\n  const cleanedFieldData = removeNotApplicablePopsFromInitiatives(fieldData);\n  try {\n    await putReportFieldData(currentReport, cleanedFieldData);\n  } catch {\n    return internalServerError(error.S3_OBJECT_UPDATE_ERROR);\n  }\n\n  const completionStatus = await calculateCompletionStatus(\n    fieldData,\n    formTemplate\n  );\n\n  // validate report metadata\n  let validatedMetadata;\n  try {\n    validatedMetadata = await validateData(metadataValidationSchema, {\n      ...unvalidatedMetadata,\n      completionStatus,\n    });\n  } catch {\n    // If metadata fails validation, return 400\n    return badRequest(error.INVALID_DATA);\n  }\n\n  // Update record in report metadata table\n  const updatedMetadata = {\n    ...currentReport,\n    ...validatedMetadata,\n    isComplete: isComplete(completionStatus),\n    lastAltered: Date.now(),\n  };\n\n  try {\n    await putReportMetadata(updatedMetadata);\n  } catch {\n    return internalServerError(error.DYNAMO_UPDATE_ERROR);\n  }\n\n  return ok({\n    ...updatedMetadata,\n    fieldData,\n    formTemplate,\n  });\n});\n", "import util from \"util\";\nimport { Logger } from \"@smithy/types\";\n\ntype LogLevel = \"trace\" | \"debug\" | \"info\" | \"warn\" | \"error\";\ntype LogEntry = {\n  date: Date;\n  level: LogLevel;\n  string: string;\n};\n\nconst logs: LogEntry[] = [];\n\nconst buildLoggerForLevel = (level: LogLevel) => {\n  return function (...content: any[]) {\n    logs.push({\n      date: new Date(),\n      level: level,\n      string: util.format.apply(null, content),\n    });\n\n    /*\n     * If we have a function logging thousands of messages,\n     * better to take the console performance hit mid-operation\n     * than to let memory usage run away as well.\n     */\n    if (logs.length > 99) {\n      flush();\n    }\n  };\n};\n\n/*\n * Individual functions are exported to support handler-lib;\n * This integrates SDK client logging with lambda logging.\n */\nexport const trace = buildLoggerForLevel(\"trace\");\nexport const debug = buildLoggerForLevel(\"debug\");\nexport const info = buildLoggerForLevel(\"info\");\nexport const warn = buildLoggerForLevel(\"warn\");\nexport const error = buildLoggerForLevel(\"error\");\n\nexport function flush() {\n  while (logs.length > 0) {\n    const { date, level, string } = logs.shift()!;\n    // eslint-disable-next-line no-console\n    console[level](date, string);\n  }\n}\n\n/*\n * This is only called at the beginning of a lambda handler,\n * so the log buffer should be empty anyway. But it doesn't\n * hurt to make sure!\n */\nexport const init = flush;\n\n/**\n * A logger suitable for passing to any AWS client constructor.\n * Note that the `trace` log level is excluded.\n *\n * This logger accumulates log messages in an internal buffer,\n * eventually flushing them to the console.\n */\nexport const logger: Logger = { debug, info, warn, error };\n", "import { SSMClient, GetParameterCommand } from \"@aws-sdk/client-ssm\";\nimport jwt_decode from \"jwt-decode\";\nimport { CognitoJwtVerifier } from \"aws-jwt-verify\";\n// types\nimport { APIGatewayProxyEvent, UserRoles } from \"../types\";\nimport { logger } from \"../debugging/debug-lib\";\n\ninterface DecodedToken {\n  \"custom:cms_roles\": UserRoles;\n  \"custom:cms_state\": string | undefined;\n}\n\nconst loadCognitoValues = async () => {\n  if (\n    process.env.COGNITO_USER_POOL_ID &&\n    process.env.COGNITO_USER_POOL_CLIENT_ID\n  ) {\n    return {\n      userPoolId: process.env.COGNITO_USER_POOL_ID,\n      userPoolClientId: process.env.COGNITO_USER_POOL_CLIENT_ID,\n    };\n  } else {\n    const ssmClient = new SSMClient({ logger });\n    const stage = process.env.stage!;\n    const getParam = async (identifier: string) => {\n      const command = new GetParameterCommand({\n        Name: `/${stage}/ui-auth/${identifier}`,\n      });\n      const result = await ssmClient.send(command);\n      return result.Parameter?.Value;\n    };\n    const userPoolId = await getParam(\"cognito_user_pool_id\");\n    const userPoolClientId = await getParam(\"cognito_user_pool_client_id\");\n    if (userPoolId && userPoolClientId) {\n      process.env[\"COGNITO_USER_POOL_ID\"] = userPoolId;\n      process.env[\"COGNITO_USER_POOL_CLIENT_ID\"] = userPoolClientId;\n      return { userPoolId, userPoolClientId };\n    } else {\n      throw new Error(\"cannot load cognito values\");\n    }\n  }\n};\n\nexport const isAuthenticated = async (event: APIGatewayProxyEvent) => {\n  const isLocalStack = event.requestContext.accountId === \"000000000000\";\n  if (isLocalStack) {\n    return true;\n  }\n\n  const cognitoValues = await loadCognitoValues();\n\n  // Verifier that expects valid access tokens:\n  const verifier = CognitoJwtVerifier.create({\n    userPoolId: cognitoValues.userPoolId,\n    tokenUse: \"id\",\n    clientId: cognitoValues.userPoolClientId,\n  });\n\n  try {\n    await verifier.verify(event?.headers?.[\"x-api-key\"]!);\n    return true;\n  } catch {\n    return false;\n  }\n};\n\nexport const hasPermissions = (\n  event: APIGatewayProxyEvent,\n  allowedRoles: UserRoles[],\n  state?: string\n) => {\n  let isAllowed = false;\n  // decode the idToken\n  if (event?.headers?.[\"x-api-key\"]) {\n    const decoded = jwt_decode(event.headers[\"x-api-key\"]) as DecodedToken;\n    const idmUserRoles = decoded[\"custom:cms_roles\"];\n    const idmUserState = decoded[\"custom:cms_state\"];\n    const mfpUserRole = idmUserRoles\n      ?.split(\",\")\n      .find((role) => role.includes(\"mdctmfp\")) as UserRoles;\n\n    isAllowed =\n      allowedRoles.includes(mfpUserRole) &&\n      (!state || idmUserState?.includes(state))!;\n  }\n\n  return isAllowed;\n};\n\nexport const isAuthorizedToFetchState = (\n  event: APIGatewayProxyEvent,\n  state: string\n) => {\n  // If this is a state user for the matching state, authorize them.\n  if (hasPermissions(event, [UserRoles.STATE_USER], state)) {\n    return true;\n  }\n\n  const nonStateUserRoles = Object.values(UserRoles).filter(\n    (role) => role !== UserRoles.STATE_USER\n  );\n\n  // If they are any other user type, they don't need to belong to this state.\n  return hasPermissions(event, nonStateUserRoles);\n};\n", "// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n// SPDX-License-Identifier: Apache-2.0\n/**\n * Base Error for all other errors in this file\n */\nexport class JwtBaseError extends Error {\n}\n/**\n * An error that is raised because an actual value does not match with the expected value\n */\nexport class FailedAssertionError extends JwtBaseError {\n    constructor(msg, actual, expected) {\n        super(msg);\n        this.failedAssertion = {\n            actual,\n            expected,\n        };\n    }\n}\n/**\n * JWT errors\n */\nexport class JwtParseError extends JwtBaseError {\n    constructor(msg, error) {\n        const message = error != null ? `${msg}: ${error}` : msg;\n        super(message);\n    }\n}\nexport class ParameterValidationError extends JwtBaseError {\n}\nexport class JwtInvalidSignatureError extends JwtBaseError {\n}\nexport class JwtInvalidSignatureAlgorithmError extends FailedAssertionError {\n}\nexport class JwtInvalidClaimError extends FailedAssertionError {\n    withRawJwt({ header, payload }) {\n        this.rawJwt = {\n            header,\n            payload,\n        };\n        return this;\n    }\n}\nexport class JwtInvalidIssuerError extends JwtInvalidClaimError {\n}\nexport class JwtInvalidAudienceError extends JwtInvalidClaimError {\n}\nexport class JwtInvalidScopeError extends JwtInvalidClaimError {\n}\nexport class JwtExpiredError extends JwtInvalidClaimError {\n}\nexport class JwtNotBeforeError extends JwtInvalidClaimError {\n}\n/**\n * Amazon Cognito specific erros\n */\nexport class CognitoJwtInvalidGroupError extends JwtInvalidClaimError {\n}\nexport class CognitoJwtInvalidTokenUseError extends JwtInvalidClaimError {\n}\nexport class CognitoJwtInvalidClientIdError extends JwtInvalidClaimError {\n}\n/**\n * ASN.1 errors\n */\nexport class Asn1DecodingError extends JwtBaseError {\n}\n/**\n * JWK errors\n */\nexport class JwksValidationError extends JwtBaseError {\n}\nexport class JwkValidationError extends JwtBaseError {\n}\nexport class JwtWithoutValidKidError extends JwtBaseError {\n}\nexport class KidNotFoundInJwksError extends JwtBaseError {\n}\nexport class WaitPeriodNotYetEndedJwkError extends JwtBaseError {\n}\nexport class JwksNotAvailableInCacheError extends JwtBaseError {\n}\nexport class JwkInvalidUseError extends FailedAssertionError {\n}\nexport class JwkInvalidKtyError extends FailedAssertionError {\n}\n/**\n * HTTPS fetch errors\n */\nexport class FetchError extends JwtBaseError {\n    constructor(uri, msg) {\n        super(`Failed to fetch ${uri}: ${msg}`);\n    }\n}\nexport class NonRetryableFetchError extends FetchError {\n}\n/**\n * Web compatibility errors\n */\nexport class NotSupportedError extends JwtBaseError {\n}\n", "// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n// SPDX-License-Identifier: Apache-2.0\n//\n// Node.js implementations for the node-web-compatibility layer\nimport { createPublicKey, createVerify } from \"crypto\";\nimport { constructPublicKeyInDerFormat } from \"./asn1.js\";\nimport { fetchJson } from \"./https-node.js\";\n/**\n * Enum to map supported JWT signature algorithms with OpenSSL message digest algorithm names\n */\nvar JwtSignatureAlgorithms;\n(function (JwtSignatureAlgorithms) {\n    JwtSignatureAlgorithms[\"RS256\"] = \"RSA-SHA256\";\n    JwtSignatureAlgorithms[\"RS384\"] = \"RSA-SHA384\";\n    JwtSignatureAlgorithms[\"RS512\"] = \"RSA-SHA512\";\n})(JwtSignatureAlgorithms || (JwtSignatureAlgorithms = {}));\nexport const nodeWebCompat = {\n    fetchJson,\n    transformJwkToKeyObjectSync: (jwk) => createPublicKey({\n        key: constructPublicKeyInDerFormat(Buffer.from(jwk.n, \"base64\"), Buffer.from(jwk.e, \"base64\")),\n        format: \"der\",\n        type: \"spki\",\n    }),\n    transformJwkToKeyObjectAsync: async (jwk) => createPublicKey({\n        key: constructPublicKeyInDerFormat(Buffer.from(jwk.n, \"base64\"), Buffer.from(jwk.e, \"base64\")),\n        format: \"der\",\n        type: \"spki\",\n    }),\n    parseB64UrlString: (b64) => Buffer.from(b64, \"base64\").toString(\"utf8\"),\n    verifySignatureSync: ({ alg, keyObject, jwsSigningInput, signature }) => \n    // eslint-disable-next-line security/detect-object-injection\n    createVerify(JwtSignatureAlgorithms[alg])\n        .update(jwsSigningInput)\n        .verify(keyObject, signature, \"base64\"),\n    verifySignatureAsync: async ({ alg, keyObject, jwsSigningInput, signature, }) => \n    // eslint-disable-next-line security/detect-object-injection\n    createVerify(JwtSignatureAlgorithms[alg])\n        .update(jwsSigningInput)\n        .verify(keyObject, signature, \"base64\"),\n    defaultFetchTimeouts: {\n        socketIdle: 500,\n        response: 1500,\n    },\n    setTimeoutUnref: (...args) => setTimeout(...args).unref(),\n};\n", "// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n// SPDX-License-Identifier: Apache-2.0\n//\n// Utility to encode RSA public keys (a pair of modulus (n) and exponent (e)) into DER-encoding, per ASN.1 specification.\nimport { Asn1DecodingError } from \"./error.js\";\n/** Enum with possible values for supported ASN.1 classes */\nvar Asn1Class;\n(function (Asn1Class) {\n    Asn1Class[Asn1Class[\"Universal\"] = 0] = \"Universal\";\n})(Asn1Class || (Asn1Class = {}));\n/** Enum with possible values for supported ASN.1 encodings */\nvar Asn1Encoding;\n(function (Asn1Encoding) {\n    Asn1Encoding[Asn1Encoding[\"Primitive\"] = 0] = \"Primitive\";\n    Asn1Encoding[Asn1Encoding[\"Constructed\"] = 1] = \"Constructed\";\n})(Asn1Encoding || (Asn1Encoding = {}));\n/** Enum with possible values for supported ASN.1 tags */\nvar Asn1Tag;\n(function (Asn1Tag) {\n    Asn1Tag[Asn1Tag[\"BitString\"] = 3] = \"BitString\";\n    Asn1Tag[Asn1Tag[\"ObjectIdentifier\"] = 6] = \"ObjectIdentifier\";\n    Asn1Tag[Asn1Tag[\"Sequence\"] = 16] = \"Sequence\";\n    Asn1Tag[Asn1Tag[\"Null\"] = 5] = \"Null\";\n    Asn1Tag[Asn1Tag[\"Integer\"] = 2] = \"Integer\";\n})(Asn1Tag || (Asn1Tag = {}));\n/**\n * Encode an ASN.1 identifier per ASN.1 spec (DER-encoding)\n * See https://www.itu.int/ITU-T/studygroups/com17/languages/X.690-0207.pdf chapter 8.1.2\n *\n * @param identifier - The ASN.1 identifier\n * @returns The buffer\n */\nfunction encodeIdentifier(identifier) {\n    const identifierAsNumber = (identifier.class << 7) |\n        (identifier.primitiveOrConstructed << 5) |\n        identifier.tag;\n    return Buffer.from([identifierAsNumber]);\n}\n/**\n * Encode the length of an ASN.1 type per ASN.1 spec (DER-encoding)\n * See https://www.itu.int/ITU-T/studygroups/com17/languages/X.690-0207.pdf chapter 8.1.3\n *\n * @param length - The length of the ASN.1 type\n * @returns The buffer\n */\nfunction encodeLength(length) {\n    if (length < 128) {\n        return Buffer.from([length]);\n    }\n    const integers = [];\n    while (length > 0) {\n        integers.push(length % 256);\n        length = length >> 8;\n    }\n    integers.reverse();\n    return Buffer.from([128 | integers.length, ...integers]);\n}\n/**\n * Encode a buffer (that represent an integer) as integer per ASN.1 spec (DER-encoding)\n * See https://www.itu.int/ITU-T/studygroups/com17/languages/X.690-0207.pdf chapter 8.3\n *\n * @param buffer - The buffer that represent an integer to encode\n * @returns The buffer\n */\nfunction encodeBufferAsInteger(buffer) {\n    return Buffer.concat([\n        encodeIdentifier({\n            class: Asn1Class.Universal,\n            primitiveOrConstructed: Asn1Encoding.Primitive,\n            tag: Asn1Tag.Integer,\n        }),\n        encodeLength(buffer.length),\n        buffer,\n    ]);\n}\n/**\n * Encode an object identifier (a string such as \"1.2.840.113549.1.1.1\") per ASN.1 spec (DER-encoding)\n * See https://www.itu.int/ITU-T/studygroups/com17/languages/X.690-0207.pdf chapter 8.19\n *\n * @param oid - The object identifier to encode\n * @returns The buffer\n */\nfunction encodeObjectIdentifier(oid) {\n    const oidComponents = oid.split(\".\").map((i) => parseInt(i));\n    const firstSubidentifier = oidComponents[0] * 40 + oidComponents[1];\n    const subsequentSubidentifiers = oidComponents\n        .slice(2)\n        .reduce((expanded, component) => {\n        const bytes = [];\n        do {\n            bytes.push(component % 128);\n            component = component >> 7;\n        } while (component);\n        return expanded.concat(bytes.map((b, index) => (index ? b + 128 : b)).reverse());\n    }, []);\n    const oidBuffer = Buffer.from([\n        firstSubidentifier,\n        ...subsequentSubidentifiers,\n    ]);\n    return Buffer.concat([\n        encodeIdentifier({\n            class: Asn1Class.Universal,\n            primitiveOrConstructed: Asn1Encoding.Primitive,\n            tag: Asn1Tag.ObjectIdentifier,\n        }),\n        encodeLength(oidBuffer.length),\n        oidBuffer,\n    ]);\n}\n/**\n * Encode a buffer as bit string per ASN.1 spec (DER-encoding)\n * See https://www.itu.int/ITU-T/studygroups/com17/languages/X.690-0207.pdf chapter 8.6\n *\n * @param buffer - The buffer to encode\n * @returns The buffer\n */\nfunction encodeBufferAsBitString(buffer) {\n    const bitString = Buffer.concat([Buffer.from([0]), buffer]);\n    return Buffer.concat([\n        encodeIdentifier({\n            class: Asn1Class.Universal,\n            primitiveOrConstructed: Asn1Encoding.Primitive,\n            tag: Asn1Tag.BitString,\n        }),\n        encodeLength(bitString.length),\n        bitString,\n    ]);\n}\n/**\n * Encode a sequence of DER-encoded items per ASN.1 spec (DER-encoding)\n * See https://www.itu.int/ITU-T/studygroups/com17/languages/X.690-0207.pdf chapter 8.9\n *\n * @param sequenceItems - The sequence of DER-encoded items\n * @returns The buffer\n */\nfunction encodeSequence(sequenceItems) {\n    const concatenated = Buffer.concat(sequenceItems);\n    return Buffer.concat([\n        encodeIdentifier({\n            class: Asn1Class.Universal,\n            primitiveOrConstructed: Asn1Encoding.Constructed,\n            tag: Asn1Tag.Sequence,\n        }),\n        encodeLength(concatenated.length),\n        concatenated,\n    ]);\n}\n/**\n * Encode null per ASN.1 spec (DER-encoding)\n * See https://www.itu.int/ITU-T/studygroups/com17/languages/X.690-0207.pdf chapter 8.8\n *\n * @returns The buffer\n */\nfunction encodeNull() {\n    return Buffer.concat([\n        encodeIdentifier({\n            class: Asn1Class.Universal,\n            primitiveOrConstructed: Asn1Encoding.Primitive,\n            tag: Asn1Tag.Null,\n        }),\n        encodeLength(0),\n    ]);\n}\n/**\n * RSA encryption object identifier constant\n *\n * From: https://tools.ietf.org/html/rfc8017\n *\n * pkcs-1    OBJECT IDENTIFIER ::= {\n *     iso(1) member-body(2) us(840) rsadsi(113549) pkcs(1) 1\n * }\n *\n * -- When rsaEncryption is used in an AlgorithmIdentifier,\n * -- the parameters MUST be present and MUST be NULL.\n * --\n * rsaEncryption    OBJECT IDENTIFIER ::= { pkcs-1 1 }\n *\n * See also: http://www.oid-info.com/get/1.2.840.113549.1.1.1\n */\nconst ALGORITHM_RSA_ENCRYPTION = encodeSequence([\n    encodeObjectIdentifier(\"1.2.840.113549.1.1.1\"),\n    encodeNull(), // parameters\n]);\n/**\n * Transform an RSA public key, which is a pair of modulus (n) and exponent (e),\n *  into a buffer per ASN.1 spec (DER-encoding)\n *\n * @param n - The modulus of the public key as buffer\n * @param e - The exponent of the public key as buffer\n * @returns The buffer, which is the public key encoded per ASN.1 spec (DER-encoding)\n */\nexport function constructPublicKeyInDerFormat(n, e) {\n    return encodeSequence([\n        ALGORITHM_RSA_ENCRYPTION,\n        encodeBufferAsBitString(encodeSequence([encodeBufferAsInteger(n), encodeBufferAsInteger(e)])),\n    ]);\n}\n/**\n * Decode an ASN.1 identifier (a number) into its parts: class, primitiveOrConstructed, tag\n * See https://www.itu.int/ITU-T/studygroups/com17/languages/X.690-0207.pdf chapter 8.1.2\n *\n * @param identifier - The identifier\n * @returns An object with properties class, primitiveOrConstructed, tag\n */\nfunction decodeIdentifier(identifier) {\n    if (identifier >> 3 === 0b11111) {\n        throw new Asn1DecodingError(\"Decoding of identifier with tag > 30 not implemented\");\n    }\n    return {\n        class: identifier >> 6,\n        primitiveOrConstructed: (identifier >> 5) & 0b001,\n        tag: identifier & 0b11111, // bit 1-5\n    };\n}\n/**\n * Decode an ASN.1 block of length value combinations,\n * and return the length and byte range of the first length value combination.\n * See https://www.itu.int/ITU-T/studygroups/com17/languages/X.690-0207.pdf chapter 8.1.3 - 8.1.5\n *\n * @param blockOfLengthValues - The ASN.1 length value\n * @returns The length and byte range of the first included length value\n */\nfunction decodeLengthValue(blockOfLengthValues) {\n    if (!(blockOfLengthValues[0] & 0b10000000)) {\n        return {\n            length: blockOfLengthValues[0],\n            firstByteOffset: 1,\n            lastByteOffset: 1 + blockOfLengthValues[0],\n        };\n    }\n    const nrLengthOctets = blockOfLengthValues[0] & 0b01111111;\n    const length = Buffer.from(blockOfLengthValues.subarray(1, 1 + 1 + nrLengthOctets)).readUIntBE(0, nrLengthOctets);\n    return {\n        length,\n        firstByteOffset: 1 + nrLengthOctets,\n        lastByteOffset: 1 + nrLengthOctets + length,\n    };\n}\n/**\n * Decode an ASN.1 sequence into its constituent parts, each part being an identifier-length-value triplet\n * See https://www.itu.int/ITU-T/studygroups/com17/languages/X.690-0207.pdf chapter 8.9\n *\n * @param sequenceValue - The ASN.1 sequence value\n * @returns Array of identifier-length-value triplets\n */\nfunction decodeSequence(sequence) {\n    const { tag } = decodeIdentifier(sequence[0]);\n    if (tag !== Asn1Tag.Sequence) {\n        throw new Asn1DecodingError(`Expected a sequence to decode, but got tag ${tag}`);\n    }\n    const { firstByteOffset, lastByteOffset } = decodeLengthValue(sequence.subarray(1));\n    const sequenceValue = sequence.subarray(1 + firstByteOffset, 1 + 1 + lastByteOffset);\n    const parts = [];\n    let offset = 0;\n    while (offset < sequenceValue.length) {\n        // Silence false postive: accessing an octet in a Buffer at a particular index\n        // is to be done with index operator: [index]\n        // eslint-disable-next-line security/detect-object-injection\n        const identifier = decodeIdentifier(sequenceValue[offset]);\n        const next = decodeLengthValue(sequenceValue.subarray(offset + 1));\n        const value = sequenceValue.subarray(offset + 1 + next.firstByteOffset, offset + 1 + next.lastByteOffset);\n        parts.push({ identifier, length: next.length, value });\n        offset += 1 + next.lastByteOffset;\n    }\n    return parts;\n}\n/**\n * Decode an ASN.1 sequence that is wrapped in a bit string\n * (Which is the way RSA public keys are encoded in ASN.1 DER-encoding)\n * See https://www.itu.int/ITU-T/studygroups/com17/languages/X.690-0207.pdf chapter 8.6 and 8.9\n *\n * @param bitStringValue - The ASN.1 bit string value\n * @returns Array of identifier-length-value triplets\n */\nfunction decodeBitStringWrappedSequenceValue(bitStringValue) {\n    const wrappedSequence = bitStringValue.subarray(1);\n    return decodeSequence(wrappedSequence);\n}\n/**\n * Decode an ASN.1 DER-encoded public key, into its modulus (n) and exponent (e)\n *\n * @param publicKey - The ASN.1 DER-encoded public key\n * @returns Object with modulus (n) and exponent (e)\n */\nexport function deconstructPublicKeyInDerFormat(publicKey) {\n    const [, pubkeyinfo] = decodeSequence(publicKey);\n    const [n, e] = decodeBitStringWrappedSequenceValue(pubkeyinfo.value);\n    return { n: n.value, e: e.value };\n}\n", "// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n// SPDX-License-Identifier: Apache-2.0\n//\n// NodeJS implementation for fetching JSON documents over HTTPS\nimport { request } from \"https\";\nimport { validateHttpsJsonResponse } from \"./https-common.js\";\nimport { pipeline } from \"stream\";\nimport { TextDecoder } from \"util\";\nimport { safeJsonParse } from \"./safe-json-parse.js\";\nimport { FetchError, NonRetryableFetchError } from \"./error.js\";\n/**\n * Execute a HTTPS request\n * @param uri - The URI\n * @param requestOptions - The RequestOptions to use\n * @param data - Data to send to the URI (e.g. POST data)\n * @returns - The response as parsed JSON\n */\nexport async function fetchJson(uri, requestOptions, data) {\n    let responseTimeout;\n    return new Promise((resolve, reject) => {\n        const req = request(uri, {\n            method: \"GET\",\n            ...requestOptions,\n        }, (response) => {\n            // Capture response data\n            // @types/node is incomplete so cast to any\n            // eslint-disable-next-line @typescript-eslint/no-explicit-any\n            pipeline([\n                response,\n                getJsonDestination(uri, response.statusCode, response.headers),\n            ], done);\n        });\n        if (requestOptions?.responseTimeout) {\n            responseTimeout = setTimeout(() => done(new FetchError(uri, `Response time-out (after ${requestOptions.responseTimeout} ms.)`)), requestOptions.responseTimeout);\n            responseTimeout.unref(); // Don't block Node from exiting\n        }\n        function done(...args) {\n            if (responseTimeout)\n                clearTimeout(responseTimeout);\n            if (args[0] == null) {\n                resolve(args[1]);\n                return;\n            }\n            // In case of errors, let the Agent (if any) know to abandon the socket\n            // This is probably best, because the socket may have become stale\n            /* istanbul ignore next */\n            req.socket?.emit(\"agentRemove\");\n            // Turn error into FetchError so the URI is nicely captured in the message\n            let error = args[0];\n            if (!(error instanceof FetchError)) {\n                error = new FetchError(uri, error.message);\n            }\n            req.destroy();\n            reject(error);\n        }\n        // Handle errors while sending request\n        req.on(\"error\", done);\n        // Signal end of request (include optional data)\n        req.end(data);\n    });\n}\n/**\n * Ensures the HTTPS response contains valid JSON\n *\n * @param uri - The URI you were requesting\n * @param statusCode - The response status code to your HTTPS request\n * @param headers - The response headers to your HTTPS request\n *\n * @returns - Async function that can be used as destination in a stream.pipeline, it will return the JSON, if valid, or throw an error otherwise\n */\nfunction getJsonDestination(uri, statusCode, headers) {\n    return async (responseIterable) => {\n        validateHttpsJsonResponse(uri, statusCode, headers[\"content-type\"]);\n        const collected = [];\n        for await (const chunk of responseIterable) {\n            collected.push(chunk);\n        }\n        try {\n            return safeJsonParse(new TextDecoder(\"utf8\", { fatal: true, ignoreBOM: true }).decode(Buffer.concat(collected)));\n        }\n        catch (err) {\n            throw new NonRetryableFetchError(uri, err);\n        }\n    };\n}\n", "// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n// SPDX-License-Identifier: Apache-2.0\n//\n// Lower-level HTTPS functionalities, common for Node.js and Web\nimport { FetchError, NonRetryableFetchError } from \"./error.js\";\n/**\n * Sanity check a HTTPS response where we expect to get JSON data back\n *\n * @param uri the uri that was being requested\n * @param statusCode the HTTP status code, should be 200\n * @param contentType the value of the \"Content-Type\" header in the response, should start with \"application/json\"\n * @returns void - throws an error if the status code or content type aren't as expected\n */\nexport function validateHttpsJsonResponse(uri, statusCode, contentType) {\n    if (statusCode === 429) {\n        throw new FetchError(uri, \"Too many requests\");\n    }\n    else if (statusCode !== 200) {\n        throw new NonRetryableFetchError(uri, `Status code is ${statusCode}, expected 200`);\n    }\n    if (!contentType ||\n        !contentType.toLowerCase().startsWith(\"application/json\")) {\n        throw new NonRetryableFetchError(uri, `Content-type is \"${contentType}\", expected \"application/json\"`);\n    }\n}\n", "// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n// SPDX-License-Identifier: Apache-2.0\n//\n// Utility to parse JSON safely\n/**\n * Check if a piece of JSON is a JSON object, and not e.g. a mere string or null\n *\n * @param j - the JSON\n */\nexport function isJsonObject(j) {\n    // It is not enough to check that `typeof j === \"object\"`\n    // because in JS `typeof null` is also \"object\", and so is `typeof []`.\n    // So we need to check that j is an object, and not null, and not an array\n    return typeof j === \"object\" && !Array.isArray(j) && j !== null;\n}\n/**\n * Parse a string as JSON, while removing __proto__ and constructor, so JS prototype pollution is prevented\n *\n * @param s - the string to JSON parse\n */\nexport function safeJsonParse(s) {\n    return JSON.parse(s, (_, value) => {\n        if (typeof value === \"object\" && !Array.isArray(value) && value !== null) {\n            delete value.__proto__;\n            delete value.constructor;\n        }\n        return value;\n    });\n}\n", "// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n// SPDX-License-Identifier: Apache-2.0\n//\n// Utilities for fetching the JWKS URI, to get the public keys with which to verify JWTs\nimport { NonRetryableFetchError } from \"./error.js\";\nimport { nodeWebCompat } from \"#node-web-compat\";\n/**\n * Execute a HTTPS request\n * @param uri - The URI\n * @param requestOptions - The RequestOptions to use (depending on the runtime context, either Node.js RequestOptions or Web Fetch init)\n * @param data - Data to send to the URI (e.g. POST data)\n * @returns - The response as parsed JSON\n */\nexport const fetchJson = nodeWebCompat.fetchJson;\n/**\n * HTTPS Fetcher for URIs with JSON body\n *\n * @param defaultRequestOptions - The default RequestOptions to use on individual HTTPS requests\n */\nexport class SimpleJsonFetcher {\n    constructor(props) {\n        this.defaultRequestOptions = {\n            timeout: nodeWebCompat.defaultFetchTimeouts.socketIdle,\n            responseTimeout: nodeWebCompat.defaultFetchTimeouts.response,\n            ...props?.defaultRequestOptions,\n        };\n    }\n    /**\n     * Execute a HTTPS request (with 1 immediate retry in case of errors)\n     * @param uri - The URI\n     * @param requestOptions - The RequestOptions to use\n     * @param data - Data to send to the URI (e.g. POST data)\n     * @returns - The response as parsed JSON\n     */\n    async fetch(uri, requestOptions, data) {\n        requestOptions = { ...this.defaultRequestOptions, ...requestOptions };\n        try {\n            return await fetchJson(uri, requestOptions, data);\n        }\n        catch (err) {\n            if (err instanceof NonRetryableFetchError) {\n                throw err;\n            }\n            // Retry once, immediately\n            return fetchJson(uri, requestOptions, data);\n        }\n    }\n}\n", "// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n// SPDX-License-Identifier: Apache-2.0\n//\n// Utilities to assert that supplied values match with expected values\nimport { FailedAssertionError } from \"./error.js\";\n/**\n * Assert value is a non-empty string and equal to the expected value,\n * or throw an error otherwise\n *\n * @param name - Name for the value being checked\n * @param actual - The value to check\n * @param expected - The expected value\n * @param errorConstructor - Constructor for the concrete error to be thrown\n */\nexport function assertStringEquals(name, actual, expected, errorConstructor = FailedAssertionError) {\n    if (!actual) {\n        throw new errorConstructor(`Missing ${name}. Expected: ${expected}`, actual, expected);\n    }\n    if (typeof actual !== \"string\") {\n        throw new errorConstructor(`${name} is not of type string`, actual, expected);\n    }\n    if (expected !== actual) {\n        throw new errorConstructor(`${name} not allowed: ${actual}. Expected: ${expected}`, actual, expected);\n    }\n}\n/**\n * Assert value is a non-empty string and is indeed one of the expected values,\n * or throw an error otherwise\n *\n * @param name - Name for the value being checked\n * @param actual - The value to check\n * @param expected - The array of expected values. For your convenience you can provide\n * @param errorConstructor - Constructor for the concrete error to be thrown\n * a string here as well, which will mean an array with just that string\n */\nexport function assertStringArrayContainsString(name, actual, expected, errorConstructor = FailedAssertionError) {\n    if (!actual) {\n        throw new errorConstructor(`Missing ${name}. ${expectationMessage(expected)}`, actual, expected);\n    }\n    if (typeof actual !== \"string\") {\n        throw new errorConstructor(`${name} is not of type string`, actual, expected);\n    }\n    return assertStringArraysOverlap(name, actual, expected, errorConstructor);\n}\n/**\n * Assert value is an array of strings, where at least one of the strings is indeed one of the expected values,\n * or throw an error otherwise\n *\n * @param name - Name for the value being checked\n * @param actual - The value to check, must be an array of strings, or a single string (which will be treated\n * as an array with just that string)\n * @param expected - The array of expected values. For your convenience you can provide\n * a string here as well, which will mean an array with just that string\n * @param errorConstructor - Constructor for the concrete error to be thrown\n */\nexport function assertStringArraysOverlap(name, actual, expected, errorConstructor = FailedAssertionError) {\n    if (!actual) {\n        throw new errorConstructor(`Missing ${name}. ${expectationMessage(expected)}`, actual, expected);\n    }\n    const expectedAsSet = new Set(Array.isArray(expected) ? expected : [expected]);\n    if (typeof actual === \"string\") {\n        actual = [actual];\n    }\n    if (!Array.isArray(actual)) {\n        throw new errorConstructor(`${name} is not an array`, actual, expected);\n    }\n    const overlaps = actual.some((actualItem) => {\n        if (typeof actualItem !== \"string\") {\n            throw new errorConstructor(`${name} includes elements that are not of type string`, actual, expected);\n        }\n        return expectedAsSet.has(actualItem);\n    });\n    if (!overlaps) {\n        throw new errorConstructor(`${name} not allowed: ${actual.join(\", \")}. ${expectationMessage(expected)}`, actual, expected);\n    }\n}\n/**\n * Get a nicely readable message regarding an expectation\n *\n * @param expected - The expected value.\n */\nfunction expectationMessage(expected) {\n    if (Array.isArray(expected)) {\n        if (expected.length > 1) {\n            return `Expected one of: ${expected.join(\", \")}`;\n        }\n        return `Expected: ${expected[0]}`;\n    }\n    return `Expected: ${expected}`;\n}\n/**\n * Assert value is not a promise, or throw an error otherwise\n *\n * @param actual - The value to check\n * @param errorFactory - Function that returns the error to be thrown\n */\nexport function assertIsNotPromise(actual, errorFactory) {\n    if (actual && typeof actual.then === \"function\") {\n        throw errorFactory();\n    }\n}\n", "// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n// SPDX-License-Identifier: Apache-2.0\nimport { SimpleJsonFetcher, fetchJson } from \"./https.js\";\nimport { isJsonObject } from \"./safe-json-parse.js\";\nimport { JwkValidationError, JwksNotAvailableInCacheError, JwksValidationError, KidNotFoundInJwksError, WaitPeriodNotYetEndedJwkError, JwtWithoutValidKidError, JwkInvalidUseError, JwkInvalidKtyError, } from \"./error.js\";\nimport { nodeWebCompat } from \"#node-web-compat\";\nimport { assertStringEquals } from \"./assert.js\";\nconst optionalJwkFieldNames = [\n    \"use\",\n    \"alg\",\n    \"kid\",\n    \"n\",\n    \"e\", // https://datatracker.ietf.org/doc/html/rfc7518#section-6.3.1.2\n];\nconst mandatoryJwkFieldNames = [\n    \"kty\", // https://datatracker.ietf.org/doc/html/rfc7517#section-4.1\n];\nexport function findJwkInJwks(jwks, kid) {\n    return jwks.keys.find((jwk) => jwk.kid != null && jwk.kid === kid);\n}\nexport async function fetchJwks(jwksUri) {\n    const jwks = await fetchJson(jwksUri);\n    assertIsJwks(jwks);\n    return jwks;\n}\nexport async function fetchJwk(jwksUri, decomposedJwt) {\n    if (!decomposedJwt.header.kid) {\n        throw new JwtWithoutValidKidError(\"JWT header does not have valid kid claim\");\n    }\n    const jwks = await fetchJwks(jwksUri);\n    const jwk = findJwkInJwks(jwks, decomposedJwt.header.kid);\n    if (!jwk) {\n        throw new KidNotFoundInJwksError(`JWK for kid \"${decomposedJwt.header.kid}\" not found in the JWKS`);\n    }\n    return jwk;\n}\nexport function assertIsJwks(jwks) {\n    if (!jwks) {\n        throw new JwksValidationError(\"JWKS empty\");\n    }\n    if (!isJsonObject(jwks)) {\n        throw new JwksValidationError(\"JWKS should be an object\");\n    }\n    if (!Object.keys(jwks).includes(\"keys\")) {\n        throw new JwksValidationError(\"JWKS does not include keys\");\n    }\n    if (!Array.isArray(jwks.keys)) {\n        throw new JwksValidationError(\"JWKS keys should be an array\");\n    }\n    for (const jwk of jwks.keys) {\n        assertIsJwk(jwk);\n    }\n}\nexport function assertIsRsaSignatureJwk(jwk) {\n    // Check JWK use\n    assertStringEquals(\"JWK use\", jwk.use, \"sig\", JwkInvalidUseError);\n    // Check JWK kty\n    assertStringEquals(\"JWK kty\", jwk.kty, \"RSA\", JwkInvalidKtyError);\n    // Check modulus (n) has a value\n    if (!jwk.n)\n        throw new JwkValidationError(\"Missing modulus (n)\");\n    // Check exponent (e) has a value\n    if (!jwk.e)\n        throw new JwkValidationError(\"Missing exponent (e)\");\n}\nexport function assertIsJwk(jwk) {\n    if (!jwk) {\n        throw new JwkValidationError(\"JWK empty\");\n    }\n    if (!isJsonObject(jwk)) {\n        throw new JwkValidationError(\"JWK should be an object\");\n    }\n    for (const field of mandatoryJwkFieldNames) {\n        // disable eslint rule because `field` is trusted\n        // eslint-disable-next-line security/detect-object-injection\n        if (typeof jwk[field] !== \"string\") {\n            throw new JwkValidationError(`JWK ${field} should be a string`);\n        }\n    }\n    for (const field of optionalJwkFieldNames) {\n        // disable eslint rule because `field` is trusted\n        // eslint-disable-next-line security/detect-object-injection\n        if (field in jwk && typeof jwk[field] !== \"string\") {\n            throw new JwkValidationError(`JWK ${field} should be a string`);\n        }\n    }\n}\nexport function isJwks(jwks) {\n    try {\n        assertIsJwks(jwks);\n        return true;\n    }\n    catch {\n        return false;\n    }\n}\nexport function isJwk(jwk) {\n    try {\n        assertIsJwk(jwk);\n        return true;\n    }\n    catch {\n        return false;\n    }\n}\nexport class SimplePenaltyBox {\n    constructor(props) {\n        this.waitingUris = new Map();\n        this.waitSeconds = props?.waitSeconds ?? 10;\n    }\n    async wait(jwksUri) {\n        // SimplePenaltyBox does not actually wait but bluntly throws an error\n        // Any waiting and retries are expected to be done upstream (e.g. in the browser / app)\n        if (this.waitingUris.has(jwksUri)) {\n            throw new WaitPeriodNotYetEndedJwkError(\"Not allowed to fetch JWKS yet, still waiting for back off period to end\");\n        }\n    }\n    release(jwksUri) {\n        const i = this.waitingUris.get(jwksUri);\n        if (i) {\n            clearTimeout(i);\n            this.waitingUris.delete(jwksUri);\n        }\n    }\n    registerFailedAttempt(jwksUri) {\n        const i = nodeWebCompat.setTimeoutUnref(() => {\n            this.waitingUris.delete(jwksUri);\n        }, this.waitSeconds * 1000);\n        this.waitingUris.set(jwksUri, i);\n    }\n    registerSuccessfulAttempt(jwksUri) {\n        this.release(jwksUri);\n    }\n}\nexport class SimpleJwksCache {\n    constructor(props) {\n        this.jwksCache = new Map();\n        this.fetchingJwks = new Map();\n        this.penaltyBox = props?.penaltyBox ?? new SimplePenaltyBox();\n        this.fetcher = props?.fetcher ?? new SimpleJsonFetcher();\n    }\n    addJwks(jwksUri, jwks) {\n        this.jwksCache.set(jwksUri, jwks);\n    }\n    async getJwks(jwksUri) {\n        const existingFetch = this.fetchingJwks.get(jwksUri);\n        if (existingFetch) {\n            return existingFetch;\n        }\n        const jwksPromise = this.fetcher.fetch(jwksUri).then((res) => {\n            assertIsJwks(res);\n            return res;\n        });\n        this.fetchingJwks.set(jwksUri, jwksPromise);\n        let jwks;\n        try {\n            jwks = await jwksPromise;\n        }\n        finally {\n            this.fetchingJwks.delete(jwksUri);\n        }\n        this.jwksCache.set(jwksUri, jwks);\n        return jwks;\n    }\n    getCachedJwk(jwksUri, decomposedJwt) {\n        if (typeof decomposedJwt.header.kid !== \"string\") {\n            throw new JwtWithoutValidKidError(\"JWT header does not have valid kid claim\");\n        }\n        if (!this.jwksCache.has(jwksUri)) {\n            throw new JwksNotAvailableInCacheError(`JWKS for uri ${jwksUri} not yet available in cache`);\n        }\n        const jwk = findJwkInJwks(this.jwksCache.get(jwksUri), decomposedJwt.header.kid);\n        if (!jwk) {\n            throw new KidNotFoundInJwksError(`JWK for kid ${decomposedJwt.header.kid} not found in the JWKS`);\n        }\n        return jwk;\n    }\n    async getJwk(jwksUri, decomposedJwt) {\n        if (typeof decomposedJwt.header.kid !== \"string\") {\n            throw new JwtWithoutValidKidError(\"JWT header does not have valid kid claim\");\n        }\n        // Try to get JWK from cache:\n        const cachedJwks = this.jwksCache.get(jwksUri);\n        if (cachedJwks) {\n            const cachedJwk = findJwkInJwks(cachedJwks, decomposedJwt.header.kid);\n            if (cachedJwk) {\n                return cachedJwk;\n            }\n        }\n        // Await any wait period that is currently in effect\n        // This prevents us from flooding the JWKS URI with requests\n        await this.penaltyBox.wait(jwksUri, decomposedJwt.header.kid);\n        // Fetch the JWKS and (try to) locate the JWK\n        const jwks = await this.getJwks(jwksUri);\n        const jwk = findJwkInJwks(jwks, decomposedJwt.header.kid);\n        // If the JWK could not be located, someone might be messing around with us\n        // Register the failed attempt with the penaltyBox, so it can enforce a wait period\n        // before trying again next time (instead of flooding the JWKS URI with requests)\n        if (!jwk) {\n            this.penaltyBox.registerFailedAttempt(jwksUri, decomposedJwt.header.kid);\n            throw new KidNotFoundInJwksError(`JWK for kid \"${decomposedJwt.header.kid}\" not found in the JWKS`);\n        }\n        else {\n            this.penaltyBox.registerSuccessfulAttempt(jwksUri, decomposedJwt.header.kid);\n        }\n        return jwk;\n    }\n}\n", "// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n// SPDX-License-Identifier: Apache-2.0\nexport const supportedSignatureAlgorithms = [\n    \"RS256\",\n    \"RS384\",\n    \"RS512\",\n];\n", "// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n// SPDX-License-Identifier: Apache-2.0\nimport { assertStringArrayContainsString, assertStringArraysOverlap, } from \"./assert.js\";\nimport { safeJsonParse, isJsonObject } from \"./safe-json-parse.js\";\nimport { JwtExpiredError, JwtNotBeforeError, JwtInvalidIssuerError, JwtInvalidAudienceError, JwtInvalidScopeError, JwtParseError, ParameterValidationError, } from \"./error.js\";\nimport { nodeWebCompat } from \"#node-web-compat\";\n/**\n * Assert that the argument is a valid JWT header object.\n * Throws an error in case it is not.\n *\n * @param header\n * @returns void\n */\nfunction assertJwtHeader(header) {\n    if (!isJsonObject(header)) {\n        throw new JwtParseError(\"JWT header is not an object\");\n    }\n    if (header.alg !== undefined && typeof header.alg !== \"string\") {\n        throw new JwtParseError(\"JWT header alg claim is not a string\");\n    }\n    if (header.kid !== undefined && typeof header.kid !== \"string\") {\n        throw new JwtParseError(\"JWT header kid claim is not a string\");\n    }\n}\n/**\n * Assert that the argument is a valid JWT payload object.\n * Throws an error in case it is not.\n *\n * @param payload\n * @returns void\n */\nfunction assertJwtPayload(payload) {\n    if (!isJsonObject(payload)) {\n        throw new JwtParseError(\"JWT payload is not an object\");\n    }\n    if (payload.exp !== undefined && !Number.isFinite(payload.exp)) {\n        throw new JwtParseError(\"JWT payload exp claim is not a number\");\n    }\n    if (payload.iss !== undefined && typeof payload.iss !== \"string\") {\n        throw new JwtParseError(\"JWT payload iss claim is not a string\");\n    }\n    if (payload.aud !== undefined &&\n        typeof payload.aud !== \"string\" &&\n        (!Array.isArray(payload.aud) ||\n            payload.aud.some((aud) => typeof aud !== \"string\"))) {\n        throw new JwtParseError(\"JWT payload aud claim is not a string or array of strings\");\n    }\n    if (payload.nbf !== undefined && !Number.isFinite(payload.nbf)) {\n        throw new JwtParseError(\"JWT payload nbf claim is not a number\");\n    }\n    if (payload.iat !== undefined && !Number.isFinite(payload.iat)) {\n        throw new JwtParseError(\"JWT payload iat claim is not a number\");\n    }\n    if (payload.scope !== undefined && typeof payload.scope !== \"string\") {\n        throw new JwtParseError(\"JWT payload scope claim is not a string\");\n    }\n    if (payload.jti !== undefined && typeof payload.jti !== \"string\") {\n        throw new JwtParseError(\"JWT payload jti claim is not a string\");\n    }\n}\n/**\n * Sanity check, decompose and JSON parse a JWT string into its constituent parts:\n * - header object\n * - payload object\n * - signature string\n *\n * @param jwt The JWT (as string)\n * @returns the decomposed JWT\n */\nexport function decomposeJwt(jwt) {\n    // Sanity checks on JWT\n    if (!jwt) {\n        throw new JwtParseError(\"Empty JWT\");\n    }\n    if (typeof jwt !== \"string\") {\n        throw new JwtParseError(\"JWT is not a string\");\n    }\n    if (!jwt.match(/^[A-Za-z0-9_-]+\\.[A-Za-z0-9_-]+\\.[A-Za-z0-9_-]+$/)) {\n        throw new JwtParseError(\"JWT string does not consist of exactly 3 parts (header, payload, signature)\");\n    }\n    const [headerB64, payloadB64, signatureB64] = jwt.split(\".\");\n    // B64 decode header and payload\n    const [headerString, payloadString] = [headerB64, payloadB64].map(nodeWebCompat.parseB64UrlString);\n    // Parse header\n    let header;\n    try {\n        header = safeJsonParse(headerString);\n    }\n    catch (err) {\n        throw new JwtParseError(\"Invalid JWT. Header is not a valid JSON object\", err);\n    }\n    assertJwtHeader(header);\n    // parse payload\n    let payload;\n    try {\n        payload = safeJsonParse(payloadString);\n    }\n    catch (err) {\n        throw new JwtParseError(\"Invalid JWT. Payload is not a valid JSON object\", err);\n    }\n    assertJwtPayload(payload);\n    return {\n        header,\n        headerB64,\n        payload,\n        payloadB64,\n        signatureB64,\n    };\n}\n/**\n * Validate JWT payload fields. Throws an error in case there's any validation issue.\n *\n * @param payload The (JSON parsed) JWT payload\n * @param options The options to use during validation\n * @returns void\n */\nexport function validateJwtFields(payload, options) {\n    // Check expiry\n    if (payload.exp !== undefined) {\n        if (payload.exp + (options.graceSeconds ?? 0) < Date.now() / 1000) {\n            throw new JwtExpiredError(`Token expired at ${new Date(payload.exp * 1000).toISOString()}`, payload.exp);\n        }\n    }\n    // Check not before\n    if (payload.nbf !== undefined) {\n        if (payload.nbf - (options.graceSeconds ?? 0) > Date.now() / 1000) {\n            throw new JwtNotBeforeError(`Token can't be used before ${new Date(payload.nbf * 1000).toISOString()}`, payload.nbf);\n        }\n    }\n    // Check JWT issuer\n    if (options.issuer !== null) {\n        if (options.issuer === undefined) {\n            throw new ParameterValidationError(\"issuer must be provided or set to null explicitly\");\n        }\n        assertStringArrayContainsString(\"Issuer\", payload.iss, options.issuer, JwtInvalidIssuerError);\n    }\n    // Check audience\n    if (options.audience !== null) {\n        if (options.audience === undefined) {\n            throw new ParameterValidationError(\"audience must be provided or set to null explicitly\");\n        }\n        assertStringArraysOverlap(\"Audience\", payload.aud, options.audience, JwtInvalidAudienceError);\n    }\n    // Check scope\n    if (options.scope != null) {\n        assertStringArraysOverlap(\"Scope\", payload.scope?.split(\" \"), options.scope, JwtInvalidScopeError);\n    }\n}\n", "// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n// SPDX-License-Identifier: Apache-2.0\nimport { SimpleJwksCache, isJwk, isJwks, fetchJwk, assertIsRsaSignatureJwk, findJwkInJwks, } from \"./jwk.js\";\nimport { assertIsNotPromise, assertStringArrayContainsString, assertStringEquals, } from \"./assert.js\";\nimport { supportedSignatureAlgorithms, } from \"./jwt-model.js\";\nimport { decomposeJwt, validateJwtFields } from \"./jwt.js\";\nimport { JwtInvalidClaimError, JwtInvalidIssuerError, JwtInvalidSignatureAlgorithmError, JwtInvalidSignatureError, KidNotFoundInJwksError, ParameterValidationError, } from \"./error.js\";\nimport { nodeWebCompat } from \"#node-web-compat\";\n/**\n * Sanity check the JWT header and the selected JWK\n *\n * @param header: the JWT header (decoded and JSON parsed)\n * @param jwk: the JWK\n */\nfunction validateJwtHeaderAndJwk(header, jwk) {\n    // Check that the JWK is in fact a JWK for RSA signatures\n    assertIsRsaSignatureJwk(jwk);\n    // Check that JWT signature algorithm matches JWK\n    if (jwk.alg) {\n        assertStringEquals(\"JWT signature algorithm\", header.alg, jwk.alg, JwtInvalidSignatureAlgorithmError);\n    }\n    // Check JWT signature algorithm is one of the supported signature algorithms\n    assertStringArrayContainsString(\"JWT signature algorithm\", header.alg, supportedSignatureAlgorithms, JwtInvalidSignatureAlgorithmError);\n}\n/**\n * Verify a JWT asynchronously (thus allowing for the JWKS to be fetched from the JWKS URI)\n *\n * @param jwt The JWT\n * @param jwksUri The JWKS URI, where the JWKS can be fetched from\n * @param options Verification options\n * @returns Promise that resolves to the payload of the JWT\u2013\u2013if the JWT is valid, otherwise the promise rejects\n */\nexport async function verifyJwt(jwt, jwksUri, options) {\n    return verifyDecomposedJwt(decomposeJwt(jwt), jwksUri, options);\n}\n/**\n * Verify (asynchronously) a JWT that is already decomposed (by function `decomposeJwt`)\n *\n * @param decomposedJwt The decomposed JWT\n * @param jwksUri The JWKS URI, where the JWKS can be fetched from\n * @param options Verification options\n * @param jwkFetcher A function that can execute the fetch of the JWKS from the JWKS URI\n * @param transformJwkToKeyObjectFn A function that can transform a JWK into a crypto native key object\n * @returns Promise that resolves to the payload of the JWT\u2013\u2013if the JWT is valid, otherwise the promise rejects\n */\nasync function verifyDecomposedJwt(decomposedJwt, jwksUri, options, jwkFetcher = fetchJwk, transformJwkToKeyObjectFn = nodeWebCompat.transformJwkToKeyObjectAsync) {\n    const { header, headerB64, payload, payloadB64, signatureB64 } = decomposedJwt;\n    const jwk = await jwkFetcher(jwksUri, decomposedJwt);\n    validateJwtHeaderAndJwk(decomposedJwt.header, jwk);\n    // Transform the JWK to native key format, that can be used with verifySignature\n    const keyObject = await transformJwkToKeyObjectFn(jwk, header.alg, payload.iss);\n    // Verify the JWT signature\n    const valid = await nodeWebCompat.verifySignatureAsync({\n        jwsSigningInput: `${headerB64}.${payloadB64}`,\n        signature: signatureB64,\n        alg: header.alg,\n        keyObject,\n    });\n    if (!valid) {\n        throw new JwtInvalidSignatureError(\"Invalid signature\");\n    }\n    try {\n        validateJwtFields(payload, options);\n        if (options.customJwtCheck) {\n            await options.customJwtCheck({ header, payload, jwk });\n        }\n    }\n    catch (err) {\n        if (options.includeRawJwtInErrors && err instanceof JwtInvalidClaimError) {\n            throw err.withRawJwt(decomposedJwt);\n        }\n        throw err;\n    }\n    return payload;\n}\n/**\n * Verify a JWT synchronously, using a JWKS or JWK that has already been fetched\n *\n * @param jwt The JWT\n * @param jwkOrJwks The JWKS that includes the right JWK (indexed by kid). Alternatively, provide the right JWK directly\n * @param options Verification options\n * @param transformJwkToKeyObjectFn A function that can transform a JWK into a crypto native key object\n * @returns The (JSON parsed) payload of the JWT\u2013\u2013if the JWT is valid, otherwise an error is thrown\n */\nexport function verifyJwtSync(jwt, jwkOrJwks, options, transformJwkToKeyObjectFn = nodeWebCompat.transformJwkToKeyObjectSync) {\n    return verifyDecomposedJwtSync(decomposeJwt(jwt), jwkOrJwks, options, transformJwkToKeyObjectFn);\n}\n/**\n * Verify (synchronously) a JWT that is already decomposed (by function `decomposeJwt`)\n *\n * @param decomposedJwt The decomposed JWT\n * @param jwkOrJwks The JWKS that includes the right JWK (indexed by kid). Alternatively, provide the right JWK directly\n * @param options Verification options\n * @param transformJwkToKeyObjectFn A function that can transform a JWK into a crypto native key object\n * @returns The (JSON parsed) payload of the JWT\u2013\u2013if the JWT is valid, otherwise an error is thrown\n */\nfunction verifyDecomposedJwtSync(decomposedJwt, jwkOrJwks, options, transformJwkToKeyObjectFn) {\n    const { header, headerB64, payload, payloadB64, signatureB64 } = decomposedJwt;\n    let jwk;\n    if (isJwk(jwkOrJwks)) {\n        jwk = jwkOrJwks;\n    }\n    else if (isJwks(jwkOrJwks)) {\n        const locatedJwk = header.kid\n            ? findJwkInJwks(jwkOrJwks, header.kid)\n            : undefined;\n        if (!locatedJwk) {\n            throw new KidNotFoundInJwksError(`JWK for kid ${header.kid} not found in the JWKS`);\n        }\n        jwk = locatedJwk;\n    }\n    else {\n        throw new ParameterValidationError([\n            `Expected a valid JWK or JWKS (parsed as JavaScript object), but received: ${jwkOrJwks}.`,\n            \"If you're passing a JWKS URI, use the async verify() method instead, it will download and parse the JWKS for you\",\n        ].join());\n    }\n    validateJwtHeaderAndJwk(decomposedJwt.header, jwk);\n    // Transform the JWK to native key format, that can be used with verifySignature\n    const keyObject = transformJwkToKeyObjectFn(jwk, header.alg, payload.iss);\n    // Verify the JWT signature (JWS)\n    const valid = nodeWebCompat.verifySignatureSync({\n        jwsSigningInput: `${headerB64}.${payloadB64}`,\n        signature: signatureB64,\n        alg: header.alg,\n        keyObject,\n    });\n    if (!valid) {\n        throw new JwtInvalidSignatureError(\"Invalid signature\");\n    }\n    try {\n        validateJwtFields(payload, options);\n        if (options.customJwtCheck) {\n            const res = options.customJwtCheck({ header, payload, jwk });\n            assertIsNotPromise(res, () => new ParameterValidationError(\"Custom JWT checks must be synchronous but a promise was returned\"));\n        }\n    }\n    catch (err) {\n        if (options.includeRawJwtInErrors && err instanceof JwtInvalidClaimError) {\n            throw err.withRawJwt(decomposedJwt);\n        }\n        throw err;\n    }\n    return payload;\n}\n/**\n * Abstract class representing a verifier for JWTs signed with RSA (e.g. RS256, RS384, RS512)\n *\n * A class is used, because there is state:\n * - The JWKS is fetched (downloaded) from the JWKS URI and cached in memory\n * - Verification properties at verifier level, are used as default options for individual verify calls\n *\n * When instantiating this class, relevant type parameters should be provided, for your concrete case:\n * @param StillToProvide The verification options that you want callers of verify to provide on individual verify calls\n * @param SpecificVerifyProperties The verification options that you'll use\n * @param IssuerConfig The issuer config that you'll use (config options are used as default verification options)\n * @param MultiIssuer Verify multiple issuers (true) or just a single one (false)\n */\nexport class JwtRsaVerifierBase {\n    constructor(verifyProperties, jwksCache = new SimpleJwksCache()) {\n        this.jwksCache = jwksCache;\n        this.issuersConfig = new Map();\n        this.publicKeyCache = new KeyObjectCache();\n        if (Array.isArray(verifyProperties)) {\n            if (!verifyProperties.length) {\n                throw new ParameterValidationError(\"Provide at least one issuer configuration\");\n            }\n            for (const prop of verifyProperties) {\n                if (this.issuersConfig.has(prop.issuer)) {\n                    throw new ParameterValidationError(`issuer ${prop.issuer} supplied multiple times`);\n                }\n                this.issuersConfig.set(prop.issuer, this.withJwksUri(prop));\n            }\n        }\n        else {\n            this.issuersConfig.set(verifyProperties.issuer, this.withJwksUri(verifyProperties));\n        }\n    }\n    get expectedIssuers() {\n        return Array.from(this.issuersConfig.keys());\n    }\n    getIssuerConfig(issuer) {\n        if (!issuer) {\n            if (this.issuersConfig.size !== 1) {\n                throw new ParameterValidationError(\"issuer must be provided\");\n            }\n            issuer = this.issuersConfig.keys().next().value;\n        }\n        const config = this.issuersConfig.get(issuer);\n        if (!config) {\n            throw new ParameterValidationError(`issuer not configured: ${issuer}`);\n        }\n        return config;\n    }\n    /**\n     * This method loads a JWKS that you provide, into the JWKS cache, so that it is\n     * available for JWT verification. Use this method to speed up the first JWT verification\n     * (when the JWKS would otherwise have to be downloaded from the JWKS uri), or to provide the JWKS\n     * in case the JwtVerifier does not have internet access to download the JWKS\n     *\n     * @param jwksThe JWKS\n     * @param issuer The issuer for which you want to cache the JWKS\n     *  Supply this field, if you instantiated the JwtVerifier with multiple issuers\n     * @returns void\n     */\n    cacheJwks(...[jwks, issuer]) {\n        const issuerConfig = this.getIssuerConfig(issuer);\n        this.jwksCache.addJwks(issuerConfig.jwksUri, jwks);\n        this.publicKeyCache.clearCache(issuerConfig.issuer);\n    }\n    /**\n     * Hydrate the JWKS cache for (all of) the configured issuer(s).\n     * This will fetch and cache the latest and greatest JWKS for concerned issuer(s).\n     *\n     * @param issuer The issuer to fetch the JWKS for\n     * @returns void\n     */\n    async hydrate() {\n        const jwksFetches = this.expectedIssuers\n            .map((issuer) => this.getIssuerConfig(issuer).jwksUri)\n            .map((jwksUri) => this.jwksCache.getJwks(jwksUri));\n        await Promise.all(jwksFetches);\n    }\n    /**\n     * Verify (synchronously) a JWT that is signed using RS256 / RS384 / RS512.\n     *\n     * @param jwt The JWT, as string\n     * @param props Verification properties\n     * @returns The payload of the JWT\u2013\u2013if the JWT is valid, otherwise an error is thrown\n     */\n    verifySync(...[jwt, properties]) {\n        const { decomposedJwt, jwksUri, verifyProperties } = this.getVerifyParameters(jwt, properties);\n        return this.verifyDecomposedJwtSync(decomposedJwt, jwksUri, verifyProperties);\n    }\n    /**\n     * Verify (synchronously) an already decomposed JWT, that is signed using RS256 / RS384 / RS512.\n     *\n     * @param decomposedJwt The decomposed Jwt\n     * @param jwk The JWK to verify the JWTs signature with\n     * @param verifyProperties The properties to use for verification\n     * @returns The payload of the JWT\u2013\u2013if the JWT is valid, otherwise an error is thrown\n     */\n    verifyDecomposedJwtSync(decomposedJwt, jwksUri, verifyProperties) {\n        const jwk = this.jwksCache.getCachedJwk(jwksUri, decomposedJwt);\n        return verifyDecomposedJwtSync(decomposedJwt, jwk, verifyProperties, this.publicKeyCache.transformJwkToKeyObjectSync.bind(this.publicKeyCache));\n    }\n    /**\n     * Verify (asynchronously) a JWT that is signed using RS256 / RS384 / RS512.\n     * This call is asynchronous, and the JWKS will be fetched from the JWKS uri,\n     * in case it is not yet available in the cache.\n     *\n     * @param jwt The JWT, as string\n     * @param props Verification properties\n     * @returns Promise that resolves to the payload of the JWT\u2013\u2013if the JWT is valid, otherwise the promise rejects\n     */\n    async verify(...[jwt, properties]) {\n        const { decomposedJwt, jwksUri, verifyProperties } = this.getVerifyParameters(jwt, properties);\n        return this.verifyDecomposedJwt(decomposedJwt, jwksUri, verifyProperties);\n    }\n    /**\n     * Verify (asynchronously) an already decomposed JWT, that is signed using RS256 / RS384 / RS512.\n     *\n     * @param decomposedJwt The decomposed Jwt\n     * @param jwk The JWK to verify the JWTs signature with\n     * @param verifyProperties The properties to use for verification\n     * @returns The payload of the JWT\u2013\u2013if the JWT is valid, otherwise an error is thrown\n     */\n    verifyDecomposedJwt(decomposedJwt, jwksUri, verifyProperties) {\n        return verifyDecomposedJwt(decomposedJwt, jwksUri, verifyProperties, this.jwksCache.getJwk.bind(this.jwksCache), this.publicKeyCache.transformJwkToKeyObjectAsync.bind(this.publicKeyCache));\n    }\n    /**\n     * Get the verification parameters to use, by merging the issuer configuration,\n     * with the overriding properties that are now provided\n     *\n     * @param jwt: the JWT that is going to be verified\n     * @param verifyProperties: the overriding properties, that override the issuer configuration\n     * @returns The merged verification parameters\n     */\n    getVerifyParameters(jwt, verifyProperties) {\n        const decomposedJwt = decomposeJwt(jwt);\n        assertStringArrayContainsString(\"Issuer\", decomposedJwt.payload.iss, this.expectedIssuers, JwtInvalidIssuerError);\n        const issuerConfig = this.getIssuerConfig(decomposedJwt.payload.iss);\n        return {\n            decomposedJwt,\n            jwksUri: issuerConfig.jwksUri,\n            verifyProperties: {\n                ...issuerConfig,\n                ...verifyProperties,\n            },\n        };\n    }\n    /**\n     * Get issuer config with JWKS URI, by adding a default JWKS URI if needed\n     *\n     * @param config: the issuer config.\n     * @returns The config with JWKS URI\n     */\n    withJwksUri(config) {\n        if (config.jwksUri) {\n            return config;\n        }\n        const issuerUri = new URL(config.issuer).pathname.replace(/\\/$/, \"\");\n        return {\n            jwksUri: new URL(`${issuerUri}/.well-known/jwks.json`, config.issuer)\n                .href,\n            ...config,\n        };\n    }\n}\n/**\n * Class representing a verifier for JWTs signed with RSA (e.g. RS256 / RS384 / RS512)\n */\nexport class JwtRsaVerifier extends JwtRsaVerifierBase {\n    // eslint-disable-next-line @typescript-eslint/explicit-module-boundary-types\n    static create(verifyProperties, additionalProperties) {\n        return new this(verifyProperties, additionalProperties?.jwksCache);\n    }\n}\n/**\n * Class representing a cache of RSA public keys in native key object format\n *\n * Because it takes a bit of compute time to turn a JWK into native key object format,\n * we want to cache this computation.\n */\nexport class KeyObjectCache {\n    constructor(transformJwkToKeyObjectSyncFn = nodeWebCompat.transformJwkToKeyObjectSync, transformJwkToKeyObjectAsyncFn = nodeWebCompat.transformJwkToKeyObjectAsync) {\n        this.transformJwkToKeyObjectSyncFn = transformJwkToKeyObjectSyncFn;\n        this.transformJwkToKeyObjectAsyncFn = transformJwkToKeyObjectAsyncFn;\n        this.publicKeys = new Map();\n    }\n    /**\n     * Transform the JWK into an RSA public key in native key object format.\n     * If the transformed JWK is already in the cache, it is returned from the cache instead.\n     *\n     * @param jwk: the JWK\n     * @param jwtHeaderAlg: the alg from the JWT header (used if absent on JWK)\n     * @param issuer: the issuer that uses the JWK for signing JWTs (used for caching the transformation)\n     * @returns the RSA public key in native key object format\n     */\n    transformJwkToKeyObjectSync(jwk, jwtHeaderAlg, issuer) {\n        const alg = jwk.alg ?? jwtHeaderAlg;\n        if (!issuer || !jwk.kid || !alg) {\n            return this.transformJwkToKeyObjectSyncFn(jwk, alg, issuer);\n        }\n        const fromCache = this.publicKeys.get(issuer)?.get(jwk.kid)?.get(alg);\n        if (fromCache)\n            return fromCache;\n        const publicKey = this.transformJwkToKeyObjectSyncFn(jwk, alg, issuer);\n        this.putKeyObjectInCache(issuer, jwk.kid, alg, publicKey);\n        return publicKey;\n    }\n    /**\n     * Transform the JWK into an RSA public key in native key object format (async).\n     * If the transformed JWK is already in the cache, it is returned from the cache instead.\n     *\n     * @param jwk: the JWK\n     * @param jwtHeaderAlg: the alg from the JWT header (used if absent on JWK)\n     * @param issuer: the issuer that uses the JWK for signing JWTs (used for caching the transformation)\n     * @returns the RSA public key in native key object format\n     */\n    async transformJwkToKeyObjectAsync(jwk, jwtHeaderAlg, issuer) {\n        const alg = jwk.alg ?? jwtHeaderAlg;\n        if (!issuer || !jwk.kid || !alg) {\n            return this.transformJwkToKeyObjectAsyncFn(jwk, alg, issuer);\n        }\n        const fromCache = this.publicKeys.get(issuer)?.get(jwk.kid)?.get(alg);\n        if (fromCache)\n            return fromCache;\n        const publicKey = await this.transformJwkToKeyObjectAsyncFn(jwk, alg, issuer);\n        this.putKeyObjectInCache(issuer, jwk.kid, alg, publicKey);\n        return publicKey;\n    }\n    putKeyObjectInCache(issuer, kid, alg, publicKey) {\n        const cachedIssuer = this.publicKeys.get(issuer);\n        const cachedIssuerKid = cachedIssuer?.get(kid);\n        if (cachedIssuerKid) {\n            cachedIssuerKid.set(alg, publicKey);\n        }\n        else if (cachedIssuer) {\n            cachedIssuer.set(kid, new Map([[alg, publicKey]]));\n        }\n        else {\n            this.publicKeys.set(issuer, new Map([[kid, new Map([[alg, publicKey]])]]));\n        }\n    }\n    clearCache(issuer) {\n        this.publicKeys.delete(issuer);\n    }\n}\n", "// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n// SPDX-License-Identifier: Apache-2.0\nimport { CognitoJwtInvalidClientIdError, CognitoJwtInvalidGroupError, CognitoJwtInvalidTokenUseError, JwtInvalidClaimError, ParameterValidationError, } from \"./error.js\";\nimport { JwtRsaVerifierBase } from \"./jwt-rsa.js\";\nimport { assertStringArrayContainsString, assertStringEquals, assertStringArraysOverlap, } from \"./assert.js\";\n/**\n * Validate claims of a decoded Cognito JWT.\n * This function throws an error in case there's any validation issue.\n *\n * @param payload - The JSON parsed payload of the Cognito JWT\n * @param options - Validation options\n * @param options.groups - The cognito groups, of which at least one must be present in the JWT's cognito:groups claim\n * @param options.tokenUse - The required token use of the JWT: \"id\" or \"access\"\n * @param options.clientId - The required clientId of the JWT. May be an array of string, of which at least one must match\n * @returns void\n */\nexport function validateCognitoJwtFields(payload, options) {\n    // Check groups\n    if (options.groups != null) {\n        assertStringArraysOverlap(\"Cognito group\", payload[\"cognito:groups\"], options.groups, CognitoJwtInvalidGroupError);\n    }\n    // Check token use\n    assertStringArrayContainsString(\"Token use\", payload.token_use, [\"id\", \"access\"], CognitoJwtInvalidTokenUseError);\n    if (options.tokenUse !== null) {\n        if (options.tokenUse === undefined) {\n            throw new ParameterValidationError(\"tokenUse must be provided or set to null explicitly\");\n        }\n        assertStringEquals(\"Token use\", payload.token_use, options.tokenUse, CognitoJwtInvalidTokenUseError);\n    }\n    // Check clientId aka audience\n    if (options.clientId !== null) {\n        if (options.clientId === undefined) {\n            throw new ParameterValidationError(\"clientId must be provided or set to null explicitly\");\n        }\n        if (payload.token_use === \"id\") {\n            assertStringArrayContainsString('Client ID (\"audience\")', payload.aud, options.clientId, CognitoJwtInvalidClientIdError);\n        }\n        else {\n            assertStringArrayContainsString(\"Client ID\", payload.client_id, options.clientId, CognitoJwtInvalidClientIdError);\n        }\n    }\n}\n/**\n * Class representing a verifier for JWTs signed by Amazon Cognito\n */\nexport class CognitoJwtVerifier extends JwtRsaVerifierBase {\n    constructor(props, jwksCache) {\n        const issuerConfig = Array.isArray(props)\n            ? props.map((p) => ({\n                ...p,\n                ...CognitoJwtVerifier.parseUserPoolId(p.userPoolId),\n                audience: null, // checked instead by validateCognitoJwtFields\n            }))\n            : {\n                ...props,\n                ...CognitoJwtVerifier.parseUserPoolId(props.userPoolId),\n                audience: null, // checked instead by validateCognitoJwtFields\n            };\n        super(issuerConfig, jwksCache);\n    }\n    /**\n     * Parse a User Pool ID, to extract the issuer and JWKS URI\n     *\n     * @param userPoolId The User Pool ID\n     * @returns The issuer and JWKS URI for the User Pool\n     */\n    static parseUserPoolId(userPoolId) {\n        // Disable safe regexp check as userPoolId is provided by developer, i.e. is not user input\n        // eslint-disable-next-line security/detect-unsafe-regex\n        const match = userPoolId.match(/^(?<region>(\\w+-)?\\w+-\\w+-\\d)+_\\w+$/);\n        if (!match) {\n            throw new ParameterValidationError(`Invalid Cognito User Pool ID: ${userPoolId}`);\n        }\n        const region = match.groups.region;\n        const issuer = `https://cognito-idp.${region}.amazonaws.com/${userPoolId}`;\n        return {\n            issuer,\n            jwksUri: `${issuer}/.well-known/jwks.json`,\n        };\n    }\n    // eslint-disable-next-line @typescript-eslint/explicit-module-boundary-types\n    static create(verifyProperties, additionalProperties) {\n        return new this(verifyProperties, additionalProperties?.jwksCache);\n    }\n    /**\n     * Verify (synchronously) a JWT that is signed by Amazon Cognito.\n     *\n     * @param jwt The JWT, as string\n     * @param props Verification properties\n     * @returns The payload of the JWT\u2013\u2013if the JWT is valid, otherwise an error is thrown\n     */\n    verifySync(...[jwt, properties]) {\n        const { decomposedJwt, jwksUri, verifyProperties } = this.getVerifyParameters(jwt, properties);\n        this.verifyDecomposedJwtSync(decomposedJwt, jwksUri, verifyProperties);\n        try {\n            validateCognitoJwtFields(decomposedJwt.payload, verifyProperties);\n        }\n        catch (err) {\n            if (verifyProperties.includeRawJwtInErrors &&\n                err instanceof JwtInvalidClaimError) {\n                throw err.withRawJwt(decomposedJwt);\n            }\n            throw err;\n        }\n        return decomposedJwt.payload;\n    }\n    /**\n     * Verify (asynchronously) a JWT that is signed by Amazon Cognito.\n     * This call is asynchronous, and the JWKS will be fetched from the JWKS uri,\n     * in case it is not yet available in the cache.\n     *\n     * @param jwt The JWT, as string\n     * @param props Verification properties\n     * @returns Promise that resolves to the payload of the JWT\u2013\u2013if the JWT is valid, otherwise the promise rejects\n     */\n    async verify(...[jwt, properties]) {\n        const { decomposedJwt, jwksUri, verifyProperties } = this.getVerifyParameters(jwt, properties);\n        await this.verifyDecomposedJwt(decomposedJwt, jwksUri, verifyProperties);\n        try {\n            validateCognitoJwtFields(decomposedJwt.payload, verifyProperties);\n        }\n        catch (err) {\n            if (verifyProperties.includeRawJwtInErrors &&\n                err instanceof JwtInvalidClaimError) {\n                throw err.withRawJwt(decomposedJwt);\n            }\n            throw err;\n        }\n        return decomposedJwt.payload;\n    }\n    /**\n     * This method loads a JWKS that you provide, into the JWKS cache, so that it is\n     * available for JWT verification. Use this method to speed up the first JWT verification\n     * (when the JWKS would otherwise have to be downloaded from the JWKS uri), or to provide the JWKS\n     * in case the JwtVerifier does not have internet access to download the JWKS\n     *\n     * @param jwks The JWKS\n     * @param userPoolId The userPoolId for which you want to cache the JWKS\n     *  Supply this field, if you instantiated the CognitoJwtVerifier with multiple userPoolIds\n     * @returns void\n     */\n    cacheJwks(...[jwks, userPoolId]) {\n        let issuer;\n        if (userPoolId !== undefined) {\n            issuer = CognitoJwtVerifier.parseUserPoolId(userPoolId).issuer;\n        }\n        else if (this.expectedIssuers.length > 1) {\n            throw new ParameterValidationError(\"userPoolId must be provided\");\n        }\n        const issuerConfig = this.getIssuerConfig(issuer);\n        super.cacheJwks(jwks, issuerConfig.issuer);\n    }\n}\n", "// GLOBAL\n\nexport interface AnyObject {\n  [key: string]: any;\n}\n\nexport interface CompletionData {\n  [key: string]: boolean | CompletionData;\n}\n\n/**\n * Abridged copy of the type used by `aws-lambda@1.0.7` (from `@types/aws-lambda@8.10.88`)\n * We only this package for these types, and we use only a subset of the\n * properties. Since `aws-lambda` depends on `aws-sdk` (that is, SDK v2),\n * we can save ourselves a big dependency with this small redundancy.\n */\n\nexport interface APIGatewayProxyEventPathParameters {\n  [name: string]: string | undefined;\n}\n\nexport interface APIGatewayProxyEvent {\n  body: string | null;\n  headers: Record<string, string | undefined>;\n  multiValueHeaders: Record<string, string | undefined>;\n  httpMethod: string;\n  isBase64Encoded: boolean;\n  path: string;\n  pathParameters: Record<string, string | undefined> | null;\n  queryStringParameters: Record<string, string | undefined> | null;\n  multiValueQueryStringParameters: Record<string, string | undefined> | null;\n  stageVariables: Record<string, string | undefined> | null;\n  /** The context is complicated, and we don't (as of 2023) use it at all. */\n  requestContext: any;\n  resource: string;\n}\n\n// ALERTS\n\nexport enum AlertTypes {\n  ERROR = \"error\",\n  INFO = \"info\",\n  SUCCESS = \"success\",\n  WARNING = \"warning\",\n}\n\n// TIME\n\nexport interface DateShape {\n  year: number;\n  month: number;\n  day: number;\n}\n\nexport interface TimeShape {\n  hour: number;\n  minute: number;\n  second: number;\n}\n\n// OTHER\n\nexport interface CustomHtmlElement {\n  type: string;\n  content: string | any;\n  as?: string;\n  props?: AnyObject;\n}\n\nexport interface ErrorVerbiage {\n  title: string;\n  description: string | CustomHtmlElement[];\n}\n\nconst states = [\n  \"AL\",\n  \"AK\",\n  \"AS\", // American Samoa\n  \"AZ\",\n  \"AR\",\n  \"CA\",\n  \"CO\",\n  \"CT\",\n  \"DE\",\n  \"DC\",\n  \"FL\",\n  \"FM\", // Federated States of Micronesia\n  \"GA\",\n  \"GU\", // Guam\n  \"HI\",\n  \"ID\",\n  \"IL\",\n  \"IN\",\n  \"IA\",\n  \"KS\",\n  \"KY\",\n  \"LA\",\n  \"ME\",\n  \"MH\", // Marshall Islands\n  \"MD\",\n  \"MA\",\n  \"MI\",\n  \"MN\",\n  \"MS\",\n  \"MO\",\n  \"MP\", // Northern Mariana Islands\n  \"MT\",\n  \"NE\",\n  \"NV\",\n  \"NH\",\n  \"NJ\",\n  \"NM\",\n  \"NY\",\n  \"NC\",\n  \"ND\",\n  \"OH\",\n  \"OK\",\n  \"OR\",\n  \"PA\",\n  \"PR\",\n  \"PW\", // Palau\n  \"RI\",\n  \"SC\",\n  \"SD\",\n  \"TN\",\n  \"TX\",\n  \"UT\",\n  \"VT\",\n  \"VA\",\n  \"VI\", // Virgin Islands\n  \"WA\",\n  \"WV\",\n  \"WI\",\n  \"WY\",\n] as const;\nexport type State = typeof states[number];\n\nexport const isState = (state: unknown): state is State => {\n  return states.includes(state as State);\n};\n\nexport interface FormTemplateVersion {\n  md5Hash: string;\n  versionNumber: number;\n  id: string;\n  lastAltered: string;\n  reportType: string;\n}\n\nexport const enum TemplateKeys {\n  WP = \"templates/MFP-Work-Plan-Help-File.pdf\",\n  SAR = \"templates/MFP-Semi-Annual-Rprt-Help-File.pdf\",\n}\n\n/**\n * S3Create event\n * https://docs.aws.amazon.com/AmazonS3/latest/dev/notification-content-structure.html\n */\n\nexport interface S3EventRecordGlacierRestoreEventData {\n  lifecycleRestorationExpiryTime: string;\n  lifecycleRestoreStorageClass: string;\n}\nexport interface S3EventRecordGlacierEventData {\n  restoreEventData: S3EventRecordGlacierRestoreEventData;\n}\n\nexport interface S3EventRecord {\n  eventVersion: string;\n  eventSource: string;\n  awsRegion: string;\n  eventTime: string;\n  eventName: string;\n  userIdentity: {\n    principalId: string;\n  };\n  requestParameters: {\n    sourceIPAddress: string;\n  };\n  responseElements: {\n    \"x-amz-request-id\": string;\n    \"x-amz-id-2\": string;\n  };\n  s3: {\n    s3SchemaVersion: string;\n    configurationId: string;\n    bucket: {\n      name: string;\n      ownerIdentity: {\n        principalId: string;\n      };\n      arn: string;\n    };\n    object: {\n      key: string;\n      size: number;\n      eTag: string;\n      versionId?: string | undefined;\n      sequencer: string;\n    };\n  };\n  glacierEventData?: S3EventRecordGlacierEventData | undefined;\n}\n\n/**\n * Use this type to create a type guard for filtering arrays of objects\n * by the presence of certain attributes.\n *\n * @example\n * interface Foo {\n *    bar: string;\n *    baz?: string;\n *    buzz?: string;\n *    bizz?: string;\n * }\n * type RequireBaz = SomeRequired<Foo, 'baz'>\n * const array: Foo[] = [\n *  { bar: 'always here' },\n *  { bar: 'always here', baz: 'sometimes here' }\n * ]\n * array.filter((f): f is RequireBaz => typeof f.baz !== 'undefined' )\n * // `array`'s type now shows bar and baz as required.\n * array.map((f) => return f.baz)\n */\nexport type SomeRequired<T, K extends keyof T> = Required<Pick<T, K>> &\n  Omit<T, K>;\n\n/**\n * Instructs Typescript to complain if it detects that this function may be reachable.\n * Useful for the default branch of a switch statement that verifiably covers every case.\n */\nexport const assertExhaustive = (_: never): void => {};\n", "import { FormJson } from \"./formFields\";\nimport { AnyObject, CustomHtmlElement } from \"./other\";\n\n// REPORT STRUCTURE\n\nexport interface ReportJson {\n  id?: string;\n  type: ReportType;\n  name: string;\n  basePath: string;\n  routes: ReportRoute[];\n  validationSchema?: AnyObject;\n  /**\n   * The validationJson property is populated at the moment any form template\n   * is stored in S3 for the first time. It will be populated from that moment on.\n   */\n  validationJson?: AnyObject;\n}\n\nexport type ReportRoute = ReportRouteWithForm | ReportRouteWithoutForm;\n\nexport interface ReportRouteBase {\n  name: string;\n  path: string;\n  pageType?: string;\n  conditionallyRender?: string;\n}\n\nexport type ReportRouteWithForm =\n  | StandardReportPageShape\n  | DrawerReportPageShape\n  | ModalDrawerReportPageShape\n  | ModalOverlayReportPageShape\n  | OverlayModalPageShape\n  | EntityDetailsOverlayShape\n  | DynamicModalOverlayReportPageShape;\n\nexport interface ReportPageShapeBase extends ReportRouteBase {\n  children?: never;\n  verbiage: ReportPageVerbiage;\n}\n\nexport interface StandardReportPageShape extends ReportPageShapeBase {\n  form: FormJson;\n  dashboard?: never;\n  modalForm?: never;\n  drawerForm?: never;\n  entityType?: never;\n  entitySteps?: never;\n  template?: never;\n  initiatives?: never;\n  objectiveCards?: never;\n}\n\nexport interface DrawerReportPageShape extends ReportPageShapeBase {\n  entityType: string;\n  verbiage: DrawerReportPageVerbiage;\n  drawerForm: FormJson;\n  modalForm?: never;\n  form?: never;\n  entitySteps?: never;\n  dashboard?: never;\n  template?: never;\n  initiatives?: never;\n  objectiveCards?: never;\n}\n\nexport interface ModalDrawerReportPageShape extends ReportPageShapeBase {\n  entityType: string;\n  verbiage: ModalDrawerReportPageVerbiage;\n  modalForm: FormJson;\n  drawerForm: FormJson;\n  form?: never;\n  entitySteps?: never;\n  dashboard?: never;\n  template?: never;\n  initiatives?: never;\n  objectiveCards?: never;\n}\n\nexport interface ModalOverlayReportPageShape extends ReportPageShapeBase {\n  initiativeId: string | undefined;\n  entityType: string;\n  entityInfo?: string[];\n  verbiage: ModalOverlayReportPageVerbiage;\n  modalForm: FormJson;\n  drawerForm?: never;\n  form?: never;\n  dashboard: EntityDetailsDashboardOverlayShape;\n  entitySteps?: (EntityDetailsOverlayShape | OverlayModalPageShape)[];\n  template?: never;\n  initiatives?: never;\n  objectiveCards?: never;\n}\n\nexport interface DynamicModalOverlayReportPageShape\n  extends ReportPageShapeBase {\n  entityType: string;\n  entityInfo: string[];\n  verbiage: ModalOverlayReportPageVerbiage;\n  drawerForm?: never;\n  modalForm?: never;\n  form?: never;\n  entitySteps?: never;\n  dashboard?: never;\n  initiatives: {\n    initiativeId: string;\n    name: string;\n    topic: string;\n    dashboard: FormJson;\n    entitySteps: (EntityDetailsOverlayShape | OverlayModalPageShape)[];\n  }[];\n  objectiveCards?: never;\n  /** Only used during form template transformation; will be absent after transformation */\n  template?: AnyObject;\n}\n\nexport interface OverlayModalPageShape extends ReportPageShapeBase {\n  entityType: string;\n  stepName: string;\n  hint: string;\n  verbiage: ModalOverlayReportPageVerbiage;\n  modalForm: FormJson;\n  drawerForm?: FormJson;\n  form?: never;\n  entitySteps?: never;\n  dashboard?: never;\n  template?: never;\n  initiatives?: never;\n  objectiveCards?: {\n    modalForm?: FormJson;\n  }[];\n}\n\nexport interface EntityDetailsOverlayShape extends ReportPageShapeBase {\n  stepName: string;\n  hint: string;\n  form: FormJson;\n  verbiage: EntityOverlayPageVerbiage;\n  entityType?: never;\n  dashboard?: never;\n  modalForm?: never;\n  drawerForm?: never;\n  entitySteps?: never;\n  template?: never;\n  initiatives?: never;\n  objectiveCards?: {\n    modalForm?: FormJson;\n  }[];\n}\n\nexport interface EntityDetailsDashboardOverlayShape\n  extends ReportPageShapeBase {\n  dashboard?: never;\n  template?: never;\n  initiatives?: never;\n  objectiveCards?: never;\n}\n\nexport interface ReportRouteWithoutForm extends ReportRouteBase {\n  children?: ReportRoute[];\n  pageType?: string;\n  entityType?: never;\n  verbiage?: never;\n  modalForm?: never;\n  drawerForm?: never;\n  form?: never;\n  entitySteps?: never;\n  dashboard?: never;\n  template?: never;\n  initiatives?: never;\n  objectiveCards?: never;\n}\n\nexport interface ReportPageVerbiage {\n  intro: {\n    section: string;\n    subsection?: string;\n    hint?: string;\n    info?: string | CustomHtmlElement[];\n  };\n  closeOutWarning?: AnyObject;\n  closeOutModal?: AnyObject;\n}\n\nexport interface DrawerReportPageVerbiage extends ReportPageVerbiage {\n  dashboardTitle: string;\n  countEntitiesInTitle?: boolean;\n  drawerTitle: string;\n  drawerInfo?: CustomHtmlElement[];\n  missingEntityMessage?: CustomHtmlElement[];\n}\n\nexport interface ModalDrawerReportPageVerbiage\n  extends DrawerReportPageVerbiage {\n  addEntityButtonText: string;\n  editEntityButtonText: string;\n  readOnlyEntityButtonText: string;\n  addEditModalAddTitle: string;\n  addEditModalEditTitle: string;\n  addEditModalMessage: string;\n  deleteEntityButtonAltText: string;\n  deleteModalTitle: string;\n  deleteModalConfirmButtonText: string;\n  deleteModalWarning: string;\n  entityUnfinishedMessage: string;\n  enterEntityDetailsButtonText: string;\n  readOnlyEntityDetailsButtonText: string;\n  editEntityDetailsButtonText: string;\n}\n\nexport interface ModalOverlayReportPageVerbiage\n  extends EntityOverlayPageVerbiage {\n  addEntityButtonText: string;\n  dashboardTitle: string;\n  countEntitiesInTitle: boolean;\n  tableHeader: string;\n  addEditModalHint: string;\n  emptyDashboardText: string;\n}\n\nexport interface EntityOverlayPageVerbiage extends ReportPageVerbiage {\n  closeOutWarning?: {\n    title?: string;\n    description?: string;\n  };\n  closeOutModal?: {\n    closeOutModalButtonText?: string;\n    closeOutModalTitle?: string;\n    closeOutModalBodyText?: string;\n    closeOutModalConfirmButtonText?: string;\n  };\n}\n\nexport enum ReportType {\n  WP = \"WP\",\n  SAR = \"SAR\",\n}\n/**\n * Check if unknown value is a report type\n *\n * @param reportType possible report type value\n * @returns type assertion for value\n */\nexport function isReportType(reportType: unknown): reportType is ReportType {\n  return Object.values(ReportType).includes(reportType as ReportType);\n}\n", "/**\n * The response for a successful request.\n * Should include a body for GET, PUT, or POST.\n * Need not include a body for DELETE\n */\nexport const ok = (body?: Object) => new HttpResponse(StatusCodes.Ok, body);\n\n/**\n * The response for a successful POST or PUT request,\n * which resulted in the creation of a new resource.\n */\nexport const created = (body: Object) =>\n  new HttpResponse(StatusCodes.Created, body);\n\n/**\n * The response for a successful request which returns no content.\n */\nexport const noContent = (body?: Object) =>\n  new HttpResponse(StatusCodes.NoContent, body);\n\n/**\n * The response for a failed request, due to client-side issues.\n * Typically indicates a missing parameter or malformed body.\n */\nexport const badRequest = (body?: Object) =>\n  new HttpResponse(StatusCodes.BadRequest, body);\n\n/**\n * The response for a client without any authorization.\n * Typically indicates an issue with the request's headers or token.\n *\n * Note: The usual name for HTTP 401 is \"Unauthorized\", but that's misleading.\n * Authentication is for identity; authorization is for permissions.\n */\nexport const unauthenticated = (body?: Object) =>\n  new HttpResponse(StatusCodes.Unauthenticated, body);\n\n/**\n * The response for a client without sufficient permissions.\n * This is specific to the requested operation.\n * For example, a regular user requesting an admin-only endpoint.\n */\nexport const forbidden = (body?: Object) =>\n  new HttpResponse(StatusCodes.Forbidden, body);\n\n/**\n * The response for a request that assumes the existence of a missing resource.\n * For example, attempting to submit a report that isn't in the database.\n */\nexport const notFound = (body?: Object) =>\n  new HttpResponse(StatusCodes.NotFound, body);\n\n/**\n * The response for a request that assumes the server is in a different state.\n * For example, attempting to submit a report that's already submitted.\n */\nexport const conflict = (body?: Object) =>\n  new HttpResponse(StatusCodes.Conflict, body);\n\n/**\n * The response for a request that errored out on the server side.\n * Typically indicates there is nothing the client can do to resolve the issue.\n */\nexport const internalServerError = (body?: Object) =>\n  new HttpResponse(StatusCodes.InternalServerError, body);\n\n/**\n * Note: Production code shouldn't need to reference this directly.\n * Use a helper method instead.\n *\n * This enum is listed mainly for the purpose of unit testing.\n */\nexport enum StatusCodes {\n  Ok = 200,\n  Created = 201,\n  NoContent = 204,\n  BadRequest = 400,\n  Unauthenticated = 401,\n  Forbidden = 403,\n  NotFound = 404,\n  Conflict = 409,\n  InternalServerError = 500,\n}\n\n/**\n * Note: Production code shouldn't need to reference this directly.\n * Use a helper method instead.\n */\nexport class HttpResponse {\n  readonly statusCode: number;\n  readonly body: string | undefined;\n  readonly headers = {\n    \"Access-Control-Allow-Origin\": \"*\",\n    \"Access-Control-Allow-Credentials\": true,\n  };\n  constructor(statusCode: number, body?: Object | undefined) {\n    this.statusCode = statusCode;\n    if (body !== undefined) {\n      this.body = JSON.stringify(body);\n    }\n  }\n}\n", "import { ReportType } from \"../types/reports\";\n\nexport const error = {\n  // generic errors\n  UNAUTHORIZED: \"User is not authorized to access this resource.\",\n  NO_KEY: \"Must provide key for table.\",\n  MISSING_DATA: \"Missing required data.\",\n  INVALID_DATA: \"Provided data is not valid.\",\n  NO_MATCHING_RECORD: \"No matching record found.\",\n  SERVER_ERROR: \"An unspecified server error occured.\",\n  // bucket errors\n  S3_OBJECT_CREATION_ERROR: \"Report could not be created due to an S3 error.\",\n  S3_OBJECT_UPDATE_ERROR: \"Report could not be updated due to an S3 error.\",\n  S3_OBJECT_GET_ERROR: \"Error while fetching report.\",\n  // dynamo errors\n  DYNAMO_CREATION_ERROR: \"Report could not be created due to a database error.\",\n  DYNAMO_UPDATE_ERROR: \"Report could not be updated due to a database error.\",\n  // template errors\n  NO_TEMPLATE_NAME: \"Must request template for download.\",\n  INVALID_TEMPLATE_NAME: \"Requested template does not exist or does not match.\",\n  NOT_IN_DATABASE: \"Record not found in database.\",\n  UNABLE_TO_COPY:\n    \"Unable to copy over report if todays date is in the same period and year as a previous report.\",\n  MISSING_FORM_TEMPLATE: \"Form Template not found in S3.\",\n  MISSING_FIELD_DATA: \"Field Data not found in S3.\",\n  NO_WORKPLANS_FOUND: \"No record of Work Plans found in database\",\n  // admin action errors\n  ALREADY_ARCHIVED: \"Cannot update archived report.\",\n  ALREADY_LOCKED: \"Cannot update locked report.\",\n  REPORT_INCOMPLETE: \"Cannot submit incomplete form.\",\n} as const;\n\nexport const buckets = {\n  FORM_TEMPLATE: \"formTemplates\",\n  FIELD_DATA: \"fieldData\",\n};\n\n// STATES\nexport enum States {\n  AL = \"Alabama\",\n  AK = \"Alaska\",\n  AS = \"American Samoa\",\n  AZ = \"Arizona\",\n  AR = \"Arkansas\",\n  CA = \"California\",\n  CO = \"Colorado\",\n  CT = \"Connecticut\",\n  DE = \"Delaware\",\n  DC = \"District of Columbia\",\n  FM = \"Federated States of Micronesia\",\n  FL = \"Florida\",\n  GA = \"Georgia\",\n  GU = \"Guam\",\n  HI = \"Hawaii\",\n  ID = \"Idaho\",\n  IL = \"Illinois\",\n  IN = \"Indiana\",\n  IA = \"Iowa\",\n  KS = \"Kansas\",\n  KY = \"Kentucky\",\n  LA = \"Louisiana\",\n  ME = \"Maine\",\n  MH = \"Marshall Islands\",\n  MD = \"Maryland\",\n  MA = \"Massachusetts\",\n  MI = \"Michigan\",\n  MN = \"Minnesota\",\n  MS = \"Mississippi\",\n  MO = \"Missouri\",\n  MT = \"Montana\",\n  NE = \"Nebraska\",\n  NV = \"Nevada\",\n  NH = \"New Hampshire\",\n  NJ = \"New Jersey\",\n  NM = \"New Mexico\",\n  NY = \"New York\",\n  NC = \"North Carolina\",\n  ND = \"North Dakota\",\n  MP = \"Northern Mariana Islands\",\n  OH = \"Ohio\",\n  OK = \"Oklahoma\",\n  OR = \"Oregon\",\n  PW = \"Palau\",\n  PA = \"Pennsylvania\",\n  PR = \"Puerto Rico\",\n  RI = \"Rhode Island\",\n  SC = \"South Carolina\",\n  SD = \"South Dakota\",\n  TN = \"Tennessee\",\n  TX = \"Texas\",\n  UT = \"Utah\",\n  VT = \"Vermont\",\n  VI = \"Virgin Islands\",\n  VA = \"Virginia\",\n  WA = \"Washington\",\n  WV = \"West Virginia\",\n  WI = \"Wisconsin\",\n  WY = \"Wyoming\",\n}\n\n// REPORTS\n\nexport const reportTables: { [key in ReportType]: string } = {\n  SAR: process.env.SarReportsTable!,\n  WP: process.env.WpReportsTable!,\n};\n\nexport const reportBuckets: { [key in ReportType]: string } = {\n  SAR: process.env.SAR_FORM_BUCKET!,\n  WP: process.env.WP_FORM_BUCKET!,\n};\n\nexport const reportNames: { [key in ReportType]: string } = {\n  SAR: \"SAR\",\n  WP: \"Work Plan\",\n};\n\nexport const tableTopics: { [key in ReportType]: string } = {\n  SAR: \"sar-reports\",\n  WP: \"wp-reports\",\n};\n\nexport const bucketTopics: { [key in ReportType]: string } = {\n  SAR: \"sar-form\",\n  WP: \"wp-form\",\n};\n\nexport const DEFAULT_TARGET_POPULATION_NAMES = [\n  \"Older adults\",\n  \"Individuals with physical disabilities (PD)\",\n  \"Individuals with intellectual and developmental disabilities (I/DD)\",\n  \"Individuals with mental health and substance use disorders (MH/SUD)\",\n  \"HCBS infrastructure/system-level development\",\n];\n", "/*! @license DOMPurify 3.2.4 | (c) Cure53 and other contributors | Released under the Apache license 2.0 and Mozilla Public License 2.0 | github.com/cure53/DOMPurify/blob/3.2.4/LICENSE */\n\nconst {\n  entries,\n  setPrototypeOf,\n  isFrozen,\n  getPrototypeOf,\n  getOwnPropertyDescriptor\n} = Object;\nlet {\n  freeze,\n  seal,\n  create\n} = Object; // eslint-disable-line import/no-mutable-exports\nlet {\n  apply,\n  construct\n} = typeof Reflect !== 'undefined' && Reflect;\nif (!freeze) {\n  freeze = function freeze(x) {\n    return x;\n  };\n}\nif (!seal) {\n  seal = function seal(x) {\n    return x;\n  };\n}\nif (!apply) {\n  apply = function apply(fun, thisValue, args) {\n    return fun.apply(thisValue, args);\n  };\n}\nif (!construct) {\n  construct = function construct(Func, args) {\n    return new Func(...args);\n  };\n}\nconst arrayForEach = unapply(Array.prototype.forEach);\nconst arrayLastIndexOf = unapply(Array.prototype.lastIndexOf);\nconst arrayPop = unapply(Array.prototype.pop);\nconst arrayPush = unapply(Array.prototype.push);\nconst arraySplice = unapply(Array.prototype.splice);\nconst stringToLowerCase = unapply(String.prototype.toLowerCase);\nconst stringToString = unapply(String.prototype.toString);\nconst stringMatch = unapply(String.prototype.match);\nconst stringReplace = unapply(String.prototype.replace);\nconst stringIndexOf = unapply(String.prototype.indexOf);\nconst stringTrim = unapply(String.prototype.trim);\nconst objectHasOwnProperty = unapply(Object.prototype.hasOwnProperty);\nconst regExpTest = unapply(RegExp.prototype.test);\nconst typeErrorCreate = unconstruct(TypeError);\n/**\n * Creates a new function that calls the given function with a specified thisArg and arguments.\n *\n * @param func - The function to be wrapped and called.\n * @returns A new function that calls the given function with a specified thisArg and arguments.\n */\nfunction unapply(func) {\n  return function (thisArg) {\n    for (var _len = arguments.length, args = new Array(_len > 1 ? _len - 1 : 0), _key = 1; _key < _len; _key++) {\n      args[_key - 1] = arguments[_key];\n    }\n    return apply(func, thisArg, args);\n  };\n}\n/**\n * Creates a new function that constructs an instance of the given constructor function with the provided arguments.\n *\n * @param func - The constructor function to be wrapped and called.\n * @returns A new function that constructs an instance of the given constructor function with the provided arguments.\n */\nfunction unconstruct(func) {\n  return function () {\n    for (var _len2 = arguments.length, args = new Array(_len2), _key2 = 0; _key2 < _len2; _key2++) {\n      args[_key2] = arguments[_key2];\n    }\n    return construct(func, args);\n  };\n}\n/**\n * Add properties to a lookup table\n *\n * @param set - The set to which elements will be added.\n * @param array - The array containing elements to be added to the set.\n * @param transformCaseFunc - An optional function to transform the case of each element before adding to the set.\n * @returns The modified set with added elements.\n */\nfunction addToSet(set, array) {\n  let transformCaseFunc = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : stringToLowerCase;\n  if (setPrototypeOf) {\n    // Make 'in' and truthy checks like Boolean(set.constructor)\n    // independent of any properties defined on Object.prototype.\n    // Prevent prototype setters from intercepting set as a this value.\n    setPrototypeOf(set, null);\n  }\n  let l = array.length;\n  while (l--) {\n    let element = array[l];\n    if (typeof element === 'string') {\n      const lcElement = transformCaseFunc(element);\n      if (lcElement !== element) {\n        // Config presets (e.g. tags.js, attrs.js) are immutable.\n        if (!isFrozen(array)) {\n          array[l] = lcElement;\n        }\n        element = lcElement;\n      }\n    }\n    set[element] = true;\n  }\n  return set;\n}\n/**\n * Clean up an array to harden against CSPP\n *\n * @param array - The array to be cleaned.\n * @returns The cleaned version of the array\n */\nfunction cleanArray(array) {\n  for (let index = 0; index < array.length; index++) {\n    const isPropertyExist = objectHasOwnProperty(array, index);\n    if (!isPropertyExist) {\n      array[index] = null;\n    }\n  }\n  return array;\n}\n/**\n * Shallow clone an object\n *\n * @param object - The object to be cloned.\n * @returns A new object that copies the original.\n */\nfunction clone(object) {\n  const newObject = create(null);\n  for (const [property, value] of entries(object)) {\n    const isPropertyExist = objectHasOwnProperty(object, property);\n    if (isPropertyExist) {\n      if (Array.isArray(value)) {\n        newObject[property] = cleanArray(value);\n      } else if (value && typeof value === 'object' && value.constructor === Object) {\n        newObject[property] = clone(value);\n      } else {\n        newObject[property] = value;\n      }\n    }\n  }\n  return newObject;\n}\n/**\n * This method automatically checks if the prop is function or getter and behaves accordingly.\n *\n * @param object - The object to look up the getter function in its prototype chain.\n * @param prop - The property name for which to find the getter function.\n * @returns The getter function found in the prototype chain or a fallback function.\n */\nfunction lookupGetter(object, prop) {\n  while (object !== null) {\n    const desc = getOwnPropertyDescriptor(object, prop);\n    if (desc) {\n      if (desc.get) {\n        return unapply(desc.get);\n      }\n      if (typeof desc.value === 'function') {\n        return unapply(desc.value);\n      }\n    }\n    object = getPrototypeOf(object);\n  }\n  function fallbackValue() {\n    return null;\n  }\n  return fallbackValue;\n}\n\nconst html$1 = freeze(['a', 'abbr', 'acronym', 'address', 'area', 'article', 'aside', 'audio', 'b', 'bdi', 'bdo', 'big', 'blink', 'blockquote', 'body', 'br', 'button', 'canvas', 'caption', 'center', 'cite', 'code', 'col', 'colgroup', 'content', 'data', 'datalist', 'dd', 'decorator', 'del', 'details', 'dfn', 'dialog', 'dir', 'div', 'dl', 'dt', 'element', 'em', 'fieldset', 'figcaption', 'figure', 'font', 'footer', 'form', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'head', 'header', 'hgroup', 'hr', 'html', 'i', 'img', 'input', 'ins', 'kbd', 'label', 'legend', 'li', 'main', 'map', 'mark', 'marquee', 'menu', 'menuitem', 'meter', 'nav', 'nobr', 'ol', 'optgroup', 'option', 'output', 'p', 'picture', 'pre', 'progress', 'q', 'rp', 'rt', 'ruby', 's', 'samp', 'section', 'select', 'shadow', 'small', 'source', 'spacer', 'span', 'strike', 'strong', 'style', 'sub', 'summary', 'sup', 'table', 'tbody', 'td', 'template', 'textarea', 'tfoot', 'th', 'thead', 'time', 'tr', 'track', 'tt', 'u', 'ul', 'var', 'video', 'wbr']);\nconst svg$1 = freeze(['svg', 'a', 'altglyph', 'altglyphdef', 'altglyphitem', 'animatecolor', 'animatemotion', 'animatetransform', 'circle', 'clippath', 'defs', 'desc', 'ellipse', 'filter', 'font', 'g', 'glyph', 'glyphref', 'hkern', 'image', 'line', 'lineargradient', 'marker', 'mask', 'metadata', 'mpath', 'path', 'pattern', 'polygon', 'polyline', 'radialgradient', 'rect', 'stop', 'style', 'switch', 'symbol', 'text', 'textpath', 'title', 'tref', 'tspan', 'view', 'vkern']);\nconst svgFilters = freeze(['feBlend', 'feColorMatrix', 'feComponentTransfer', 'feComposite', 'feConvolveMatrix', 'feDiffuseLighting', 'feDisplacementMap', 'feDistantLight', 'feDropShadow', 'feFlood', 'feFuncA', 'feFuncB', 'feFuncG', 'feFuncR', 'feGaussianBlur', 'feImage', 'feMerge', 'feMergeNode', 'feMorphology', 'feOffset', 'fePointLight', 'feSpecularLighting', 'feSpotLight', 'feTile', 'feTurbulence']);\n// List of SVG elements that are disallowed by default.\n// We still need to know them so that we can do namespace\n// checks properly in case one wants to add them to\n// allow-list.\nconst svgDisallowed = freeze(['animate', 'color-profile', 'cursor', 'discard', 'font-face', 'font-face-format', 'font-face-name', 'font-face-src', 'font-face-uri', 'foreignobject', 'hatch', 'hatchpath', 'mesh', 'meshgradient', 'meshpatch', 'meshrow', 'missing-glyph', 'script', 'set', 'solidcolor', 'unknown', 'use']);\nconst mathMl$1 = freeze(['math', 'menclose', 'merror', 'mfenced', 'mfrac', 'mglyph', 'mi', 'mlabeledtr', 'mmultiscripts', 'mn', 'mo', 'mover', 'mpadded', 'mphantom', 'mroot', 'mrow', 'ms', 'mspace', 'msqrt', 'mstyle', 'msub', 'msup', 'msubsup', 'mtable', 'mtd', 'mtext', 'mtr', 'munder', 'munderover', 'mprescripts']);\n// Similarly to SVG, we want to know all MathML elements,\n// even those that we disallow by default.\nconst mathMlDisallowed = freeze(['maction', 'maligngroup', 'malignmark', 'mlongdiv', 'mscarries', 'mscarry', 'msgroup', 'mstack', 'msline', 'msrow', 'semantics', 'annotation', 'annotation-xml', 'mprescripts', 'none']);\nconst text = freeze(['#text']);\n\nconst html = freeze(['accept', 'action', 'align', 'alt', 'autocapitalize', 'autocomplete', 'autopictureinpicture', 'autoplay', 'background', 'bgcolor', 'border', 'capture', 'cellpadding', 'cellspacing', 'checked', 'cite', 'class', 'clear', 'color', 'cols', 'colspan', 'controls', 'controlslist', 'coords', 'crossorigin', 'datetime', 'decoding', 'default', 'dir', 'disabled', 'disablepictureinpicture', 'disableremoteplayback', 'download', 'draggable', 'enctype', 'enterkeyhint', 'face', 'for', 'headers', 'height', 'hidden', 'high', 'href', 'hreflang', 'id', 'inputmode', 'integrity', 'ismap', 'kind', 'label', 'lang', 'list', 'loading', 'loop', 'low', 'max', 'maxlength', 'media', 'method', 'min', 'minlength', 'multiple', 'muted', 'name', 'nonce', 'noshade', 'novalidate', 'nowrap', 'open', 'optimum', 'pattern', 'placeholder', 'playsinline', 'popover', 'popovertarget', 'popovertargetaction', 'poster', 'preload', 'pubdate', 'radiogroup', 'readonly', 'rel', 'required', 'rev', 'reversed', 'role', 'rows', 'rowspan', 'spellcheck', 'scope', 'selected', 'shape', 'size', 'sizes', 'span', 'srclang', 'start', 'src', 'srcset', 'step', 'style', 'summary', 'tabindex', 'title', 'translate', 'type', 'usemap', 'valign', 'value', 'width', 'wrap', 'xmlns', 'slot']);\nconst svg = freeze(['accent-height', 'accumulate', 'additive', 'alignment-baseline', 'amplitude', 'ascent', 'attributename', 'attributetype', 'azimuth', 'basefrequency', 'baseline-shift', 'begin', 'bias', 'by', 'class', 'clip', 'clippathunits', 'clip-path', 'clip-rule', 'color', 'color-interpolation', 'color-interpolation-filters', 'color-profile', 'color-rendering', 'cx', 'cy', 'd', 'dx', 'dy', 'diffuseconstant', 'direction', 'display', 'divisor', 'dur', 'edgemode', 'elevation', 'end', 'exponent', 'fill', 'fill-opacity', 'fill-rule', 'filter', 'filterunits', 'flood-color', 'flood-opacity', 'font-family', 'font-size', 'font-size-adjust', 'font-stretch', 'font-style', 'font-variant', 'font-weight', 'fx', 'fy', 'g1', 'g2', 'glyph-name', 'glyphref', 'gradientunits', 'gradienttransform', 'height', 'href', 'id', 'image-rendering', 'in', 'in2', 'intercept', 'k', 'k1', 'k2', 'k3', 'k4', 'kerning', 'keypoints', 'keysplines', 'keytimes', 'lang', 'lengthadjust', 'letter-spacing', 'kernelmatrix', 'kernelunitlength', 'lighting-color', 'local', 'marker-end', 'marker-mid', 'marker-start', 'markerheight', 'markerunits', 'markerwidth', 'maskcontentunits', 'maskunits', 'max', 'mask', 'media', 'method', 'mode', 'min', 'name', 'numoctaves', 'offset', 'operator', 'opacity', 'order', 'orient', 'orientation', 'origin', 'overflow', 'paint-order', 'path', 'pathlength', 'patterncontentunits', 'patterntransform', 'patternunits', 'points', 'preservealpha', 'preserveaspectratio', 'primitiveunits', 'r', 'rx', 'ry', 'radius', 'refx', 'refy', 'repeatcount', 'repeatdur', 'restart', 'result', 'rotate', 'scale', 'seed', 'shape-rendering', 'slope', 'specularconstant', 'specularexponent', 'spreadmethod', 'startoffset', 'stddeviation', 'stitchtiles', 'stop-color', 'stop-opacity', 'stroke-dasharray', 'stroke-dashoffset', 'stroke-linecap', 'stroke-linejoin', 'stroke-miterlimit', 'stroke-opacity', 'stroke', 'stroke-width', 'style', 'surfacescale', 'systemlanguage', 'tabindex', 'tablevalues', 'targetx', 'targety', 'transform', 'transform-origin', 'text-anchor', 'text-decoration', 'text-rendering', 'textlength', 'type', 'u1', 'u2', 'unicode', 'values', 'viewbox', 'visibility', 'version', 'vert-adv-y', 'vert-origin-x', 'vert-origin-y', 'width', 'word-spacing', 'wrap', 'writing-mode', 'xchannelselector', 'ychannelselector', 'x', 'x1', 'x2', 'xmlns', 'y', 'y1', 'y2', 'z', 'zoomandpan']);\nconst mathMl = freeze(['accent', 'accentunder', 'align', 'bevelled', 'close', 'columnsalign', 'columnlines', 'columnspan', 'denomalign', 'depth', 'dir', 'display', 'displaystyle', 'encoding', 'fence', 'frame', 'height', 'href', 'id', 'largeop', 'length', 'linethickness', 'lspace', 'lquote', 'mathbackground', 'mathcolor', 'mathsize', 'mathvariant', 'maxsize', 'minsize', 'movablelimits', 'notation', 'numalign', 'open', 'rowalign', 'rowlines', 'rowspacing', 'rowspan', 'rspace', 'rquote', 'scriptlevel', 'scriptminsize', 'scriptsizemultiplier', 'selection', 'separator', 'separators', 'stretchy', 'subscriptshift', 'supscriptshift', 'symmetric', 'voffset', 'width', 'xmlns']);\nconst xml = freeze(['xlink:href', 'xml:id', 'xlink:title', 'xml:space', 'xmlns:xlink']);\n\n// eslint-disable-next-line unicorn/better-regex\nconst MUSTACHE_EXPR = seal(/\\{\\{[\\w\\W]*|[\\w\\W]*\\}\\}/gm); // Specify template detection regex for SAFE_FOR_TEMPLATES mode\nconst ERB_EXPR = seal(/<%[\\w\\W]*|[\\w\\W]*%>/gm);\nconst TMPLIT_EXPR = seal(/\\$\\{[\\w\\W]*/gm); // eslint-disable-line unicorn/better-regex\nconst DATA_ATTR = seal(/^data-[\\-\\w.\\u00B7-\\uFFFF]+$/); // eslint-disable-line no-useless-escape\nconst ARIA_ATTR = seal(/^aria-[\\-\\w]+$/); // eslint-disable-line no-useless-escape\nconst IS_ALLOWED_URI = seal(/^(?:(?:(?:f|ht)tps?|mailto|tel|callto|sms|cid|xmpp):|[^a-z]|[a-z+.\\-]+(?:[^a-z+.\\-:]|$))/i // eslint-disable-line no-useless-escape\n);\nconst IS_SCRIPT_OR_DATA = seal(/^(?:\\w+script|data):/i);\nconst ATTR_WHITESPACE = seal(/[\\u0000-\\u0020\\u00A0\\u1680\\u180E\\u2000-\\u2029\\u205F\\u3000]/g // eslint-disable-line no-control-regex\n);\nconst DOCTYPE_NAME = seal(/^html$/i);\nconst CUSTOM_ELEMENT = seal(/^[a-z][.\\w]*(-[.\\w]+)+$/i);\n\nvar EXPRESSIONS = /*#__PURE__*/Object.freeze({\n  __proto__: null,\n  ARIA_ATTR: ARIA_ATTR,\n  ATTR_WHITESPACE: ATTR_WHITESPACE,\n  CUSTOM_ELEMENT: CUSTOM_ELEMENT,\n  DATA_ATTR: DATA_ATTR,\n  DOCTYPE_NAME: DOCTYPE_NAME,\n  ERB_EXPR: ERB_EXPR,\n  IS_ALLOWED_URI: IS_ALLOWED_URI,\n  IS_SCRIPT_OR_DATA: IS_SCRIPT_OR_DATA,\n  MUSTACHE_EXPR: MUSTACHE_EXPR,\n  TMPLIT_EXPR: TMPLIT_EXPR\n});\n\n/* eslint-disable @typescript-eslint/indent */\n// https://developer.mozilla.org/en-US/docs/Web/API/Node/nodeType\nconst NODE_TYPE = {\n  element: 1,\n  attribute: 2,\n  text: 3,\n  cdataSection: 4,\n  entityReference: 5,\n  // Deprecated\n  entityNode: 6,\n  // Deprecated\n  progressingInstruction: 7,\n  comment: 8,\n  document: 9,\n  documentType: 10,\n  documentFragment: 11,\n  notation: 12 // Deprecated\n};\nconst getGlobal = function getGlobal() {\n  return typeof window === 'undefined' ? null : window;\n};\n/**\n * Creates a no-op policy for internal use only.\n * Don't export this function outside this module!\n * @param trustedTypes The policy factory.\n * @param purifyHostElement The Script element used to load DOMPurify (to determine policy name suffix).\n * @return The policy created (or null, if Trusted Types\n * are not supported or creating the policy failed).\n */\nconst _createTrustedTypesPolicy = function _createTrustedTypesPolicy(trustedTypes, purifyHostElement) {\n  if (typeof trustedTypes !== 'object' || typeof trustedTypes.createPolicy !== 'function') {\n    return null;\n  }\n  // Allow the callers to control the unique policy name\n  // by adding a data-tt-policy-suffix to the script element with the DOMPurify.\n  // Policy creation with duplicate names throws in Trusted Types.\n  let suffix = null;\n  const ATTR_NAME = 'data-tt-policy-suffix';\n  if (purifyHostElement && purifyHostElement.hasAttribute(ATTR_NAME)) {\n    suffix = purifyHostElement.getAttribute(ATTR_NAME);\n  }\n  const policyName = 'dompurify' + (suffix ? '#' + suffix : '');\n  try {\n    return trustedTypes.createPolicy(policyName, {\n      createHTML(html) {\n        return html;\n      },\n      createScriptURL(scriptUrl) {\n        return scriptUrl;\n      }\n    });\n  } catch (_) {\n    // Policy creation failed (most likely another DOMPurify script has\n    // already run). Skip creating the policy, as this will only cause errors\n    // if TT are enforced.\n    console.warn('TrustedTypes policy ' + policyName + ' could not be created.');\n    return null;\n  }\n};\nconst _createHooksMap = function _createHooksMap() {\n  return {\n    afterSanitizeAttributes: [],\n    afterSanitizeElements: [],\n    afterSanitizeShadowDOM: [],\n    beforeSanitizeAttributes: [],\n    beforeSanitizeElements: [],\n    beforeSanitizeShadowDOM: [],\n    uponSanitizeAttribute: [],\n    uponSanitizeElement: [],\n    uponSanitizeShadowNode: []\n  };\n};\nfunction createDOMPurify() {\n  let window = arguments.length > 0 && arguments[0] !== undefined ? arguments[0] : getGlobal();\n  const DOMPurify = root => createDOMPurify(root);\n  DOMPurify.version = '3.2.4';\n  DOMPurify.removed = [];\n  if (!window || !window.document || window.document.nodeType !== NODE_TYPE.document || !window.Element) {\n    // Not running in a browser, provide a factory function\n    // so that you can pass your own Window\n    DOMPurify.isSupported = false;\n    return DOMPurify;\n  }\n  let {\n    document\n  } = window;\n  const originalDocument = document;\n  const currentScript = originalDocument.currentScript;\n  const {\n    DocumentFragment,\n    HTMLTemplateElement,\n    Node,\n    Element,\n    NodeFilter,\n    NamedNodeMap = window.NamedNodeMap || window.MozNamedAttrMap,\n    HTMLFormElement,\n    DOMParser,\n    trustedTypes\n  } = window;\n  const ElementPrototype = Element.prototype;\n  const cloneNode = lookupGetter(ElementPrototype, 'cloneNode');\n  const remove = lookupGetter(ElementPrototype, 'remove');\n  const getNextSibling = lookupGetter(ElementPrototype, 'nextSibling');\n  const getChildNodes = lookupGetter(ElementPrototype, 'childNodes');\n  const getParentNode = lookupGetter(ElementPrototype, 'parentNode');\n  // As per issue #47, the web-components registry is inherited by a\n  // new document created via createHTMLDocument. As per the spec\n  // (http://w3c.github.io/webcomponents/spec/custom/#creating-and-passing-registries)\n  // a new empty registry is used when creating a template contents owner\n  // document, so we use that as our parent document to ensure nothing\n  // is inherited.\n  if (typeof HTMLTemplateElement === 'function') {\n    const template = document.createElement('template');\n    if (template.content && template.content.ownerDocument) {\n      document = template.content.ownerDocument;\n    }\n  }\n  let trustedTypesPolicy;\n  let emptyHTML = '';\n  const {\n    implementation,\n    createNodeIterator,\n    createDocumentFragment,\n    getElementsByTagName\n  } = document;\n  const {\n    importNode\n  } = originalDocument;\n  let hooks = _createHooksMap();\n  /**\n   * Expose whether this browser supports running the full DOMPurify.\n   */\n  DOMPurify.isSupported = typeof entries === 'function' && typeof getParentNode === 'function' && implementation && implementation.createHTMLDocument !== undefined;\n  const {\n    MUSTACHE_EXPR,\n    ERB_EXPR,\n    TMPLIT_EXPR,\n    DATA_ATTR,\n    ARIA_ATTR,\n    IS_SCRIPT_OR_DATA,\n    ATTR_WHITESPACE,\n    CUSTOM_ELEMENT\n  } = EXPRESSIONS;\n  let {\n    IS_ALLOWED_URI: IS_ALLOWED_URI$1\n  } = EXPRESSIONS;\n  /**\n   * We consider the elements and attributes below to be safe. Ideally\n   * don't add any new ones but feel free to remove unwanted ones.\n   */\n  /* allowed element names */\n  let ALLOWED_TAGS = null;\n  const DEFAULT_ALLOWED_TAGS = addToSet({}, [...html$1, ...svg$1, ...svgFilters, ...mathMl$1, ...text]);\n  /* Allowed attribute names */\n  let ALLOWED_ATTR = null;\n  const DEFAULT_ALLOWED_ATTR = addToSet({}, [...html, ...svg, ...mathMl, ...xml]);\n  /*\n   * Configure how DOMPurify should handle custom elements and their attributes as well as customized built-in elements.\n   * @property {RegExp|Function|null} tagNameCheck one of [null, regexPattern, predicate]. Default: `null` (disallow any custom elements)\n   * @property {RegExp|Function|null} attributeNameCheck one of [null, regexPattern, predicate]. Default: `null` (disallow any attributes not on the allow list)\n   * @property {boolean} allowCustomizedBuiltInElements allow custom elements derived from built-ins if they pass CUSTOM_ELEMENT_HANDLING.tagNameCheck. Default: `false`.\n   */\n  let CUSTOM_ELEMENT_HANDLING = Object.seal(create(null, {\n    tagNameCheck: {\n      writable: true,\n      configurable: false,\n      enumerable: true,\n      value: null\n    },\n    attributeNameCheck: {\n      writable: true,\n      configurable: false,\n      enumerable: true,\n      value: null\n    },\n    allowCustomizedBuiltInElements: {\n      writable: true,\n      configurable: false,\n      enumerable: true,\n      value: false\n    }\n  }));\n  /* Explicitly forbidden tags (overrides ALLOWED_TAGS/ADD_TAGS) */\n  let FORBID_TAGS = null;\n  /* Explicitly forbidden attributes (overrides ALLOWED_ATTR/ADD_ATTR) */\n  let FORBID_ATTR = null;\n  /* Decide if ARIA attributes are okay */\n  let ALLOW_ARIA_ATTR = true;\n  /* Decide if custom data attributes are okay */\n  let ALLOW_DATA_ATTR = true;\n  /* Decide if unknown protocols are okay */\n  let ALLOW_UNKNOWN_PROTOCOLS = false;\n  /* Decide if self-closing tags in attributes are allowed.\n   * Usually removed due to a mXSS issue in jQuery 3.0 */\n  let ALLOW_SELF_CLOSE_IN_ATTR = true;\n  /* Output should be safe for common template engines.\n   * This means, DOMPurify removes data attributes, mustaches and ERB\n   */\n  let SAFE_FOR_TEMPLATES = false;\n  /* Output should be safe even for XML used within HTML and alike.\n   * This means, DOMPurify removes comments when containing risky content.\n   */\n  let SAFE_FOR_XML = true;\n  /* Decide if document with <html>... should be returned */\n  let WHOLE_DOCUMENT = false;\n  /* Track whether config is already set on this instance of DOMPurify. */\n  let SET_CONFIG = false;\n  /* Decide if all elements (e.g. style, script) must be children of\n   * document.body. By default, browsers might move them to document.head */\n  let FORCE_BODY = false;\n  /* Decide if a DOM `HTMLBodyElement` should be returned, instead of a html\n   * string (or a TrustedHTML object if Trusted Types are supported).\n   * If `WHOLE_DOCUMENT` is enabled a `HTMLHtmlElement` will be returned instead\n   */\n  let RETURN_DOM = false;\n  /* Decide if a DOM `DocumentFragment` should be returned, instead of a html\n   * string  (or a TrustedHTML object if Trusted Types are supported) */\n  let RETURN_DOM_FRAGMENT = false;\n  /* Try to return a Trusted Type object instead of a string, return a string in\n   * case Trusted Types are not supported  */\n  let RETURN_TRUSTED_TYPE = false;\n  /* Output should be free from DOM clobbering attacks?\n   * This sanitizes markups named with colliding, clobberable built-in DOM APIs.\n   */\n  let SANITIZE_DOM = true;\n  /* Achieve full DOM Clobbering protection by isolating the namespace of named\n   * properties and JS variables, mitigating attacks that abuse the HTML/DOM spec rules.\n   *\n   * HTML/DOM spec rules that enable DOM Clobbering:\n   *   - Named Access on Window (\u00A77.3.3)\n   *   - DOM Tree Accessors (\u00A73.1.5)\n   *   - Form Element Parent-Child Relations (\u00A74.10.3)\n   *   - Iframe srcdoc / Nested WindowProxies (\u00A74.8.5)\n   *   - HTMLCollection (\u00A74.2.10.2)\n   *\n   * Namespace isolation is implemented by prefixing `id` and `name` attributes\n   * with a constant string, i.e., `user-content-`\n   */\n  let SANITIZE_NAMED_PROPS = false;\n  const SANITIZE_NAMED_PROPS_PREFIX = 'user-content-';\n  /* Keep element content when removing element? */\n  let KEEP_CONTENT = true;\n  /* If a `Node` is passed to sanitize(), then performs sanitization in-place instead\n   * of importing it into a new Document and returning a sanitized copy */\n  let IN_PLACE = false;\n  /* Allow usage of profiles like html, svg and mathMl */\n  let USE_PROFILES = {};\n  /* Tags to ignore content of when KEEP_CONTENT is true */\n  let FORBID_CONTENTS = null;\n  const DEFAULT_FORBID_CONTENTS = addToSet({}, ['annotation-xml', 'audio', 'colgroup', 'desc', 'foreignobject', 'head', 'iframe', 'math', 'mi', 'mn', 'mo', 'ms', 'mtext', 'noembed', 'noframes', 'noscript', 'plaintext', 'script', 'style', 'svg', 'template', 'thead', 'title', 'video', 'xmp']);\n  /* Tags that are safe for data: URIs */\n  let DATA_URI_TAGS = null;\n  const DEFAULT_DATA_URI_TAGS = addToSet({}, ['audio', 'video', 'img', 'source', 'image', 'track']);\n  /* Attributes safe for values like \"javascript:\" */\n  let URI_SAFE_ATTRIBUTES = null;\n  const DEFAULT_URI_SAFE_ATTRIBUTES = addToSet({}, ['alt', 'class', 'for', 'id', 'label', 'name', 'pattern', 'placeholder', 'role', 'summary', 'title', 'value', 'style', 'xmlns']);\n  const MATHML_NAMESPACE = 'http://www.w3.org/1998/Math/MathML';\n  const SVG_NAMESPACE = 'http://www.w3.org/2000/svg';\n  const HTML_NAMESPACE = 'http://www.w3.org/1999/xhtml';\n  /* Document namespace */\n  let NAMESPACE = HTML_NAMESPACE;\n  let IS_EMPTY_INPUT = false;\n  /* Allowed XHTML+XML namespaces */\n  let ALLOWED_NAMESPACES = null;\n  const DEFAULT_ALLOWED_NAMESPACES = addToSet({}, [MATHML_NAMESPACE, SVG_NAMESPACE, HTML_NAMESPACE], stringToString);\n  let MATHML_TEXT_INTEGRATION_POINTS = addToSet({}, ['mi', 'mo', 'mn', 'ms', 'mtext']);\n  let HTML_INTEGRATION_POINTS = addToSet({}, ['annotation-xml']);\n  // Certain elements are allowed in both SVG and HTML\n  // namespace. We need to specify them explicitly\n  // so that they don't get erroneously deleted from\n  // HTML namespace.\n  const COMMON_SVG_AND_HTML_ELEMENTS = addToSet({}, ['title', 'style', 'font', 'a', 'script']);\n  /* Parsing of strict XHTML documents */\n  let PARSER_MEDIA_TYPE = null;\n  const SUPPORTED_PARSER_MEDIA_TYPES = ['application/xhtml+xml', 'text/html'];\n  const DEFAULT_PARSER_MEDIA_TYPE = 'text/html';\n  let transformCaseFunc = null;\n  /* Keep a reference to config to pass to hooks */\n  let CONFIG = null;\n  /* Ideally, do not touch anything below this line */\n  /* ______________________________________________ */\n  const formElement = document.createElement('form');\n  const isRegexOrFunction = function isRegexOrFunction(testValue) {\n    return testValue instanceof RegExp || testValue instanceof Function;\n  };\n  /**\n   * _parseConfig\n   *\n   * @param cfg optional config literal\n   */\n  // eslint-disable-next-line complexity\n  const _parseConfig = function _parseConfig() {\n    let cfg = arguments.length > 0 && arguments[0] !== undefined ? arguments[0] : {};\n    if (CONFIG && CONFIG === cfg) {\n      return;\n    }\n    /* Shield configuration object from tampering */\n    if (!cfg || typeof cfg !== 'object') {\n      cfg = {};\n    }\n    /* Shield configuration object from prototype pollution */\n    cfg = clone(cfg);\n    PARSER_MEDIA_TYPE =\n    // eslint-disable-next-line unicorn/prefer-includes\n    SUPPORTED_PARSER_MEDIA_TYPES.indexOf(cfg.PARSER_MEDIA_TYPE) === -1 ? DEFAULT_PARSER_MEDIA_TYPE : cfg.PARSER_MEDIA_TYPE;\n    // HTML tags and attributes are not case-sensitive, converting to lowercase. Keeping XHTML as is.\n    transformCaseFunc = PARSER_MEDIA_TYPE === 'application/xhtml+xml' ? stringToString : stringToLowerCase;\n    /* Set configuration parameters */\n    ALLOWED_TAGS = objectHasOwnProperty(cfg, 'ALLOWED_TAGS') ? addToSet({}, cfg.ALLOWED_TAGS, transformCaseFunc) : DEFAULT_ALLOWED_TAGS;\n    ALLOWED_ATTR = objectHasOwnProperty(cfg, 'ALLOWED_ATTR') ? addToSet({}, cfg.ALLOWED_ATTR, transformCaseFunc) : DEFAULT_ALLOWED_ATTR;\n    ALLOWED_NAMESPACES = objectHasOwnProperty(cfg, 'ALLOWED_NAMESPACES') ? addToSet({}, cfg.ALLOWED_NAMESPACES, stringToString) : DEFAULT_ALLOWED_NAMESPACES;\n    URI_SAFE_ATTRIBUTES = objectHasOwnProperty(cfg, 'ADD_URI_SAFE_ATTR') ? addToSet(clone(DEFAULT_URI_SAFE_ATTRIBUTES), cfg.ADD_URI_SAFE_ATTR, transformCaseFunc) : DEFAULT_URI_SAFE_ATTRIBUTES;\n    DATA_URI_TAGS = objectHasOwnProperty(cfg, 'ADD_DATA_URI_TAGS') ? addToSet(clone(DEFAULT_DATA_URI_TAGS), cfg.ADD_DATA_URI_TAGS, transformCaseFunc) : DEFAULT_DATA_URI_TAGS;\n    FORBID_CONTENTS = objectHasOwnProperty(cfg, 'FORBID_CONTENTS') ? addToSet({}, cfg.FORBID_CONTENTS, transformCaseFunc) : DEFAULT_FORBID_CONTENTS;\n    FORBID_TAGS = objectHasOwnProperty(cfg, 'FORBID_TAGS') ? addToSet({}, cfg.FORBID_TAGS, transformCaseFunc) : {};\n    FORBID_ATTR = objectHasOwnProperty(cfg, 'FORBID_ATTR') ? addToSet({}, cfg.FORBID_ATTR, transformCaseFunc) : {};\n    USE_PROFILES = objectHasOwnProperty(cfg, 'USE_PROFILES') ? cfg.USE_PROFILES : false;\n    ALLOW_ARIA_ATTR = cfg.ALLOW_ARIA_ATTR !== false; // Default true\n    ALLOW_DATA_ATTR = cfg.ALLOW_DATA_ATTR !== false; // Default true\n    ALLOW_UNKNOWN_PROTOCOLS = cfg.ALLOW_UNKNOWN_PROTOCOLS || false; // Default false\n    ALLOW_SELF_CLOSE_IN_ATTR = cfg.ALLOW_SELF_CLOSE_IN_ATTR !== false; // Default true\n    SAFE_FOR_TEMPLATES = cfg.SAFE_FOR_TEMPLATES || false; // Default false\n    SAFE_FOR_XML = cfg.SAFE_FOR_XML !== false; // Default true\n    WHOLE_DOCUMENT = cfg.WHOLE_DOCUMENT || false; // Default false\n    RETURN_DOM = cfg.RETURN_DOM || false; // Default false\n    RETURN_DOM_FRAGMENT = cfg.RETURN_DOM_FRAGMENT || false; // Default false\n    RETURN_TRUSTED_TYPE = cfg.RETURN_TRUSTED_TYPE || false; // Default false\n    FORCE_BODY = cfg.FORCE_BODY || false; // Default false\n    SANITIZE_DOM = cfg.SANITIZE_DOM !== false; // Default true\n    SANITIZE_NAMED_PROPS = cfg.SANITIZE_NAMED_PROPS || false; // Default false\n    KEEP_CONTENT = cfg.KEEP_CONTENT !== false; // Default true\n    IN_PLACE = cfg.IN_PLACE || false; // Default false\n    IS_ALLOWED_URI$1 = cfg.ALLOWED_URI_REGEXP || IS_ALLOWED_URI;\n    NAMESPACE = cfg.NAMESPACE || HTML_NAMESPACE;\n    MATHML_TEXT_INTEGRATION_POINTS = cfg.MATHML_TEXT_INTEGRATION_POINTS || MATHML_TEXT_INTEGRATION_POINTS;\n    HTML_INTEGRATION_POINTS = cfg.HTML_INTEGRATION_POINTS || HTML_INTEGRATION_POINTS;\n    CUSTOM_ELEMENT_HANDLING = cfg.CUSTOM_ELEMENT_HANDLING || {};\n    if (cfg.CUSTOM_ELEMENT_HANDLING && isRegexOrFunction(cfg.CUSTOM_ELEMENT_HANDLING.tagNameCheck)) {\n      CUSTOM_ELEMENT_HANDLING.tagNameCheck = cfg.CUSTOM_ELEMENT_HANDLING.tagNameCheck;\n    }\n    if (cfg.CUSTOM_ELEMENT_HANDLING && isRegexOrFunction(cfg.CUSTOM_ELEMENT_HANDLING.attributeNameCheck)) {\n      CUSTOM_ELEMENT_HANDLING.attributeNameCheck = cfg.CUSTOM_ELEMENT_HANDLING.attributeNameCheck;\n    }\n    if (cfg.CUSTOM_ELEMENT_HANDLING && typeof cfg.CUSTOM_ELEMENT_HANDLING.allowCustomizedBuiltInElements === 'boolean') {\n      CUSTOM_ELEMENT_HANDLING.allowCustomizedBuiltInElements = cfg.CUSTOM_ELEMENT_HANDLING.allowCustomizedBuiltInElements;\n    }\n    if (SAFE_FOR_TEMPLATES) {\n      ALLOW_DATA_ATTR = false;\n    }\n    if (RETURN_DOM_FRAGMENT) {\n      RETURN_DOM = true;\n    }\n    /* Parse profile info */\n    if (USE_PROFILES) {\n      ALLOWED_TAGS = addToSet({}, text);\n      ALLOWED_ATTR = [];\n      if (USE_PROFILES.html === true) {\n        addToSet(ALLOWED_TAGS, html$1);\n        addToSet(ALLOWED_ATTR, html);\n      }\n      if (USE_PROFILES.svg === true) {\n        addToSet(ALLOWED_TAGS, svg$1);\n        addToSet(ALLOWED_ATTR, svg);\n        addToSet(ALLOWED_ATTR, xml);\n      }\n      if (USE_PROFILES.svgFilters === true) {\n        addToSet(ALLOWED_TAGS, svgFilters);\n        addToSet(ALLOWED_ATTR, svg);\n        addToSet(ALLOWED_ATTR, xml);\n      }\n      if (USE_PROFILES.mathMl === true) {\n        addToSet(ALLOWED_TAGS, mathMl$1);\n        addToSet(ALLOWED_ATTR, mathMl);\n        addToSet(ALLOWED_ATTR, xml);\n      }\n    }\n    /* Merge configuration parameters */\n    if (cfg.ADD_TAGS) {\n      if (ALLOWED_TAGS === DEFAULT_ALLOWED_TAGS) {\n        ALLOWED_TAGS = clone(ALLOWED_TAGS);\n      }\n      addToSet(ALLOWED_TAGS, cfg.ADD_TAGS, transformCaseFunc);\n    }\n    if (cfg.ADD_ATTR) {\n      if (ALLOWED_ATTR === DEFAULT_ALLOWED_ATTR) {\n        ALLOWED_ATTR = clone(ALLOWED_ATTR);\n      }\n      addToSet(ALLOWED_ATTR, cfg.ADD_ATTR, transformCaseFunc);\n    }\n    if (cfg.ADD_URI_SAFE_ATTR) {\n      addToSet(URI_SAFE_ATTRIBUTES, cfg.ADD_URI_SAFE_ATTR, transformCaseFunc);\n    }\n    if (cfg.FORBID_CONTENTS) {\n      if (FORBID_CONTENTS === DEFAULT_FORBID_CONTENTS) {\n        FORBID_CONTENTS = clone(FORBID_CONTENTS);\n      }\n      addToSet(FORBID_CONTENTS, cfg.FORBID_CONTENTS, transformCaseFunc);\n    }\n    /* Add #text in case KEEP_CONTENT is set to true */\n    if (KEEP_CONTENT) {\n      ALLOWED_TAGS['#text'] = true;\n    }\n    /* Add html, head and body to ALLOWED_TAGS in case WHOLE_DOCUMENT is true */\n    if (WHOLE_DOCUMENT) {\n      addToSet(ALLOWED_TAGS, ['html', 'head', 'body']);\n    }\n    /* Add tbody to ALLOWED_TAGS in case tables are permitted, see #286, #365 */\n    if (ALLOWED_TAGS.table) {\n      addToSet(ALLOWED_TAGS, ['tbody']);\n      delete FORBID_TAGS.tbody;\n    }\n    if (cfg.TRUSTED_TYPES_POLICY) {\n      if (typeof cfg.TRUSTED_TYPES_POLICY.createHTML !== 'function') {\n        throw typeErrorCreate('TRUSTED_TYPES_POLICY configuration option must provide a \"createHTML\" hook.');\n      }\n      if (typeof cfg.TRUSTED_TYPES_POLICY.createScriptURL !== 'function') {\n        throw typeErrorCreate('TRUSTED_TYPES_POLICY configuration option must provide a \"createScriptURL\" hook.');\n      }\n      // Overwrite existing TrustedTypes policy.\n      trustedTypesPolicy = cfg.TRUSTED_TYPES_POLICY;\n      // Sign local variables required by `sanitize`.\n      emptyHTML = trustedTypesPolicy.createHTML('');\n    } else {\n      // Uninitialized policy, attempt to initialize the internal dompurify policy.\n      if (trustedTypesPolicy === undefined) {\n        trustedTypesPolicy = _createTrustedTypesPolicy(trustedTypes, currentScript);\n      }\n      // If creating the internal policy succeeded sign internal variables.\n      if (trustedTypesPolicy !== null && typeof emptyHTML === 'string') {\n        emptyHTML = trustedTypesPolicy.createHTML('');\n      }\n    }\n    // Prevent further manipulation of configuration.\n    // Not available in IE8, Safari 5, etc.\n    if (freeze) {\n      freeze(cfg);\n    }\n    CONFIG = cfg;\n  };\n  /* Keep track of all possible SVG and MathML tags\n   * so that we can perform the namespace checks\n   * correctly. */\n  const ALL_SVG_TAGS = addToSet({}, [...svg$1, ...svgFilters, ...svgDisallowed]);\n  const ALL_MATHML_TAGS = addToSet({}, [...mathMl$1, ...mathMlDisallowed]);\n  /**\n   * @param element a DOM element whose namespace is being checked\n   * @returns Return false if the element has a\n   *  namespace that a spec-compliant parser would never\n   *  return. Return true otherwise.\n   */\n  const _checkValidNamespace = function _checkValidNamespace(element) {\n    let parent = getParentNode(element);\n    // In JSDOM, if we're inside shadow DOM, then parentNode\n    // can be null. We just simulate parent in this case.\n    if (!parent || !parent.tagName) {\n      parent = {\n        namespaceURI: NAMESPACE,\n        tagName: 'template'\n      };\n    }\n    const tagName = stringToLowerCase(element.tagName);\n    const parentTagName = stringToLowerCase(parent.tagName);\n    if (!ALLOWED_NAMESPACES[element.namespaceURI]) {\n      return false;\n    }\n    if (element.namespaceURI === SVG_NAMESPACE) {\n      // The only way to switch from HTML namespace to SVG\n      // is via <svg>. If it happens via any other tag, then\n      // it should be killed.\n      if (parent.namespaceURI === HTML_NAMESPACE) {\n        return tagName === 'svg';\n      }\n      // The only way to switch from MathML to SVG is via`\n      // svg if parent is either <annotation-xml> or MathML\n      // text integration points.\n      if (parent.namespaceURI === MATHML_NAMESPACE) {\n        return tagName === 'svg' && (parentTagName === 'annotation-xml' || MATHML_TEXT_INTEGRATION_POINTS[parentTagName]);\n      }\n      // We only allow elements that are defined in SVG\n      // spec. All others are disallowed in SVG namespace.\n      return Boolean(ALL_SVG_TAGS[tagName]);\n    }\n    if (element.namespaceURI === MATHML_NAMESPACE) {\n      // The only way to switch from HTML namespace to MathML\n      // is via <math>. If it happens via any other tag, then\n      // it should be killed.\n      if (parent.namespaceURI === HTML_NAMESPACE) {\n        return tagName === 'math';\n      }\n      // The only way to switch from SVG to MathML is via\n      // <math> and HTML integration points\n      if (parent.namespaceURI === SVG_NAMESPACE) {\n        return tagName === 'math' && HTML_INTEGRATION_POINTS[parentTagName];\n      }\n      // We only allow elements that are defined in MathML\n      // spec. All others are disallowed in MathML namespace.\n      return Boolean(ALL_MATHML_TAGS[tagName]);\n    }\n    if (element.namespaceURI === HTML_NAMESPACE) {\n      // The only way to switch from SVG to HTML is via\n      // HTML integration points, and from MathML to HTML\n      // is via MathML text integration points\n      if (parent.namespaceURI === SVG_NAMESPACE && !HTML_INTEGRATION_POINTS[parentTagName]) {\n        return false;\n      }\n      if (parent.namespaceURI === MATHML_NAMESPACE && !MATHML_TEXT_INTEGRATION_POINTS[parentTagName]) {\n        return false;\n      }\n      // We disallow tags that are specific for MathML\n      // or SVG and should never appear in HTML namespace\n      return !ALL_MATHML_TAGS[tagName] && (COMMON_SVG_AND_HTML_ELEMENTS[tagName] || !ALL_SVG_TAGS[tagName]);\n    }\n    // For XHTML and XML documents that support custom namespaces\n    if (PARSER_MEDIA_TYPE === 'application/xhtml+xml' && ALLOWED_NAMESPACES[element.namespaceURI]) {\n      return true;\n    }\n    // The code should never reach this place (this means\n    // that the element somehow got namespace that is not\n    // HTML, SVG, MathML or allowed via ALLOWED_NAMESPACES).\n    // Return false just in case.\n    return false;\n  };\n  /**\n   * _forceRemove\n   *\n   * @param node a DOM node\n   */\n  const _forceRemove = function _forceRemove(node) {\n    arrayPush(DOMPurify.removed, {\n      element: node\n    });\n    try {\n      // eslint-disable-next-line unicorn/prefer-dom-node-remove\n      getParentNode(node).removeChild(node);\n    } catch (_) {\n      remove(node);\n    }\n  };\n  /**\n   * _removeAttribute\n   *\n   * @param name an Attribute name\n   * @param element a DOM node\n   */\n  const _removeAttribute = function _removeAttribute(name, element) {\n    try {\n      arrayPush(DOMPurify.removed, {\n        attribute: element.getAttributeNode(name),\n        from: element\n      });\n    } catch (_) {\n      arrayPush(DOMPurify.removed, {\n        attribute: null,\n        from: element\n      });\n    }\n    element.removeAttribute(name);\n    // We void attribute values for unremovable \"is\" attributes\n    if (name === 'is') {\n      if (RETURN_DOM || RETURN_DOM_FRAGMENT) {\n        try {\n          _forceRemove(element);\n        } catch (_) {}\n      } else {\n        try {\n          element.setAttribute(name, '');\n        } catch (_) {}\n      }\n    }\n  };\n  /**\n   * _initDocument\n   *\n   * @param dirty - a string of dirty markup\n   * @return a DOM, filled with the dirty markup\n   */\n  const _initDocument = function _initDocument(dirty) {\n    /* Create a HTML document */\n    let doc = null;\n    let leadingWhitespace = null;\n    if (FORCE_BODY) {\n      dirty = '<remove></remove>' + dirty;\n    } else {\n      /* If FORCE_BODY isn't used, leading whitespace needs to be preserved manually */\n      const matches = stringMatch(dirty, /^[\\r\\n\\t ]+/);\n      leadingWhitespace = matches && matches[0];\n    }\n    if (PARSER_MEDIA_TYPE === 'application/xhtml+xml' && NAMESPACE === HTML_NAMESPACE) {\n      // Root of XHTML doc must contain xmlns declaration (see https://www.w3.org/TR/xhtml1/normative.html#strict)\n      dirty = '<html xmlns=\"http://www.w3.org/1999/xhtml\"><head></head><body>' + dirty + '</body></html>';\n    }\n    const dirtyPayload = trustedTypesPolicy ? trustedTypesPolicy.createHTML(dirty) : dirty;\n    /*\n     * Use the DOMParser API by default, fallback later if needs be\n     * DOMParser not work for svg when has multiple root element.\n     */\n    if (NAMESPACE === HTML_NAMESPACE) {\n      try {\n        doc = new DOMParser().parseFromString(dirtyPayload, PARSER_MEDIA_TYPE);\n      } catch (_) {}\n    }\n    /* Use createHTMLDocument in case DOMParser is not available */\n    if (!doc || !doc.documentElement) {\n      doc = implementation.createDocument(NAMESPACE, 'template', null);\n      try {\n        doc.documentElement.innerHTML = IS_EMPTY_INPUT ? emptyHTML : dirtyPayload;\n      } catch (_) {\n        // Syntax error if dirtyPayload is invalid xml\n      }\n    }\n    const body = doc.body || doc.documentElement;\n    if (dirty && leadingWhitespace) {\n      body.insertBefore(document.createTextNode(leadingWhitespace), body.childNodes[0] || null);\n    }\n    /* Work on whole document or just its body */\n    if (NAMESPACE === HTML_NAMESPACE) {\n      return getElementsByTagName.call(doc, WHOLE_DOCUMENT ? 'html' : 'body')[0];\n    }\n    return WHOLE_DOCUMENT ? doc.documentElement : body;\n  };\n  /**\n   * Creates a NodeIterator object that you can use to traverse filtered lists of nodes or elements in a document.\n   *\n   * @param root The root element or node to start traversing on.\n   * @return The created NodeIterator\n   */\n  const _createNodeIterator = function _createNodeIterator(root) {\n    return createNodeIterator.call(root.ownerDocument || root, root,\n    // eslint-disable-next-line no-bitwise\n    NodeFilter.SHOW_ELEMENT | NodeFilter.SHOW_COMMENT | NodeFilter.SHOW_TEXT | NodeFilter.SHOW_PROCESSING_INSTRUCTION | NodeFilter.SHOW_CDATA_SECTION, null);\n  };\n  /**\n   * _isClobbered\n   *\n   * @param element element to check for clobbering attacks\n   * @return true if clobbered, false if safe\n   */\n  const _isClobbered = function _isClobbered(element) {\n    return element instanceof HTMLFormElement && (typeof element.nodeName !== 'string' || typeof element.textContent !== 'string' || typeof element.removeChild !== 'function' || !(element.attributes instanceof NamedNodeMap) || typeof element.removeAttribute !== 'function' || typeof element.setAttribute !== 'function' || typeof element.namespaceURI !== 'string' || typeof element.insertBefore !== 'function' || typeof element.hasChildNodes !== 'function');\n  };\n  /**\n   * Checks whether the given object is a DOM node.\n   *\n   * @param value object to check whether it's a DOM node\n   * @return true is object is a DOM node\n   */\n  const _isNode = function _isNode(value) {\n    return typeof Node === 'function' && value instanceof Node;\n  };\n  function _executeHooks(hooks, currentNode, data) {\n    arrayForEach(hooks, hook => {\n      hook.call(DOMPurify, currentNode, data, CONFIG);\n    });\n  }\n  /**\n   * _sanitizeElements\n   *\n   * @protect nodeName\n   * @protect textContent\n   * @protect removeChild\n   * @param currentNode to check for permission to exist\n   * @return true if node was killed, false if left alive\n   */\n  const _sanitizeElements = function _sanitizeElements(currentNode) {\n    let content = null;\n    /* Execute a hook if present */\n    _executeHooks(hooks.beforeSanitizeElements, currentNode, null);\n    /* Check if element is clobbered or can clobber */\n    if (_isClobbered(currentNode)) {\n      _forceRemove(currentNode);\n      return true;\n    }\n    /* Now let's check the element's type and name */\n    const tagName = transformCaseFunc(currentNode.nodeName);\n    /* Execute a hook if present */\n    _executeHooks(hooks.uponSanitizeElement, currentNode, {\n      tagName,\n      allowedTags: ALLOWED_TAGS\n    });\n    /* Detect mXSS attempts abusing namespace confusion */\n    if (currentNode.hasChildNodes() && !_isNode(currentNode.firstElementChild) && regExpTest(/<[/\\w]/g, currentNode.innerHTML) && regExpTest(/<[/\\w]/g, currentNode.textContent)) {\n      _forceRemove(currentNode);\n      return true;\n    }\n    /* Remove any occurrence of processing instructions */\n    if (currentNode.nodeType === NODE_TYPE.progressingInstruction) {\n      _forceRemove(currentNode);\n      return true;\n    }\n    /* Remove any kind of possibly harmful comments */\n    if (SAFE_FOR_XML && currentNode.nodeType === NODE_TYPE.comment && regExpTest(/<[/\\w]/g, currentNode.data)) {\n      _forceRemove(currentNode);\n      return true;\n    }\n    /* Remove element if anything forbids its presence */\n    if (!ALLOWED_TAGS[tagName] || FORBID_TAGS[tagName]) {\n      /* Check if we have a custom element to handle */\n      if (!FORBID_TAGS[tagName] && _isBasicCustomElement(tagName)) {\n        if (CUSTOM_ELEMENT_HANDLING.tagNameCheck instanceof RegExp && regExpTest(CUSTOM_ELEMENT_HANDLING.tagNameCheck, tagName)) {\n          return false;\n        }\n        if (CUSTOM_ELEMENT_HANDLING.tagNameCheck instanceof Function && CUSTOM_ELEMENT_HANDLING.tagNameCheck(tagName)) {\n          return false;\n        }\n      }\n      /* Keep content except for bad-listed elements */\n      if (KEEP_CONTENT && !FORBID_CONTENTS[tagName]) {\n        const parentNode = getParentNode(currentNode) || currentNode.parentNode;\n        const childNodes = getChildNodes(currentNode) || currentNode.childNodes;\n        if (childNodes && parentNode) {\n          const childCount = childNodes.length;\n          for (let i = childCount - 1; i >= 0; --i) {\n            const childClone = cloneNode(childNodes[i], true);\n            childClone.__removalCount = (currentNode.__removalCount || 0) + 1;\n            parentNode.insertBefore(childClone, getNextSibling(currentNode));\n          }\n        }\n      }\n      _forceRemove(currentNode);\n      return true;\n    }\n    /* Check whether element has a valid namespace */\n    if (currentNode instanceof Element && !_checkValidNamespace(currentNode)) {\n      _forceRemove(currentNode);\n      return true;\n    }\n    /* Make sure that older browsers don't get fallback-tag mXSS */\n    if ((tagName === 'noscript' || tagName === 'noembed' || tagName === 'noframes') && regExpTest(/<\\/no(script|embed|frames)/i, currentNode.innerHTML)) {\n      _forceRemove(currentNode);\n      return true;\n    }\n    /* Sanitize element content to be template-safe */\n    if (SAFE_FOR_TEMPLATES && currentNode.nodeType === NODE_TYPE.text) {\n      /* Get the element's text content */\n      content = currentNode.textContent;\n      arrayForEach([MUSTACHE_EXPR, ERB_EXPR, TMPLIT_EXPR], expr => {\n        content = stringReplace(content, expr, ' ');\n      });\n      if (currentNode.textContent !== content) {\n        arrayPush(DOMPurify.removed, {\n          element: currentNode.cloneNode()\n        });\n        currentNode.textContent = content;\n      }\n    }\n    /* Execute a hook if present */\n    _executeHooks(hooks.afterSanitizeElements, currentNode, null);\n    return false;\n  };\n  /**\n   * _isValidAttribute\n   *\n   * @param lcTag Lowercase tag name of containing element.\n   * @param lcName Lowercase attribute name.\n   * @param value Attribute value.\n   * @return Returns true if `value` is valid, otherwise false.\n   */\n  // eslint-disable-next-line complexity\n  const _isValidAttribute = function _isValidAttribute(lcTag, lcName, value) {\n    /* Make sure attribute cannot clobber */\n    if (SANITIZE_DOM && (lcName === 'id' || lcName === 'name') && (value in document || value in formElement)) {\n      return false;\n    }\n    /* Allow valid data-* attributes: At least one character after \"-\"\n        (https://html.spec.whatwg.org/multipage/dom.html#embedding-custom-non-visible-data-with-the-data-*-attributes)\n        XML-compatible (https://html.spec.whatwg.org/multipage/infrastructure.html#xml-compatible and http://www.w3.org/TR/xml/#d0e804)\n        We don't need to check the value; it's always URI safe. */\n    if (ALLOW_DATA_ATTR && !FORBID_ATTR[lcName] && regExpTest(DATA_ATTR, lcName)) ; else if (ALLOW_ARIA_ATTR && regExpTest(ARIA_ATTR, lcName)) ; else if (!ALLOWED_ATTR[lcName] || FORBID_ATTR[lcName]) {\n      if (\n      // First condition does a very basic check if a) it's basically a valid custom element tagname AND\n      // b) if the tagName passes whatever the user has configured for CUSTOM_ELEMENT_HANDLING.tagNameCheck\n      // and c) if the attribute name passes whatever the user has configured for CUSTOM_ELEMENT_HANDLING.attributeNameCheck\n      _isBasicCustomElement(lcTag) && (CUSTOM_ELEMENT_HANDLING.tagNameCheck instanceof RegExp && regExpTest(CUSTOM_ELEMENT_HANDLING.tagNameCheck, lcTag) || CUSTOM_ELEMENT_HANDLING.tagNameCheck instanceof Function && CUSTOM_ELEMENT_HANDLING.tagNameCheck(lcTag)) && (CUSTOM_ELEMENT_HANDLING.attributeNameCheck instanceof RegExp && regExpTest(CUSTOM_ELEMENT_HANDLING.attributeNameCheck, lcName) || CUSTOM_ELEMENT_HANDLING.attributeNameCheck instanceof Function && CUSTOM_ELEMENT_HANDLING.attributeNameCheck(lcName)) ||\n      // Alternative, second condition checks if it's an `is`-attribute, AND\n      // the value passes whatever the user has configured for CUSTOM_ELEMENT_HANDLING.tagNameCheck\n      lcName === 'is' && CUSTOM_ELEMENT_HANDLING.allowCustomizedBuiltInElements && (CUSTOM_ELEMENT_HANDLING.tagNameCheck instanceof RegExp && regExpTest(CUSTOM_ELEMENT_HANDLING.tagNameCheck, value) || CUSTOM_ELEMENT_HANDLING.tagNameCheck instanceof Function && CUSTOM_ELEMENT_HANDLING.tagNameCheck(value))) ; else {\n        return false;\n      }\n      /* Check value is safe. First, is attr inert? If so, is safe */\n    } else if (URI_SAFE_ATTRIBUTES[lcName]) ; else if (regExpTest(IS_ALLOWED_URI$1, stringReplace(value, ATTR_WHITESPACE, ''))) ; else if ((lcName === 'src' || lcName === 'xlink:href' || lcName === 'href') && lcTag !== 'script' && stringIndexOf(value, 'data:') === 0 && DATA_URI_TAGS[lcTag]) ; else if (ALLOW_UNKNOWN_PROTOCOLS && !regExpTest(IS_SCRIPT_OR_DATA, stringReplace(value, ATTR_WHITESPACE, ''))) ; else if (value) {\n      return false;\n    } else ;\n    return true;\n  };\n  /**\n   * _isBasicCustomElement\n   * checks if at least one dash is included in tagName, and it's not the first char\n   * for more sophisticated checking see https://github.com/sindresorhus/validate-element-name\n   *\n   * @param tagName name of the tag of the node to sanitize\n   * @returns Returns true if the tag name meets the basic criteria for a custom element, otherwise false.\n   */\n  const _isBasicCustomElement = function _isBasicCustomElement(tagName) {\n    return tagName !== 'annotation-xml' && stringMatch(tagName, CUSTOM_ELEMENT);\n  };\n  /**\n   * _sanitizeAttributes\n   *\n   * @protect attributes\n   * @protect nodeName\n   * @protect removeAttribute\n   * @protect setAttribute\n   *\n   * @param currentNode to sanitize\n   */\n  const _sanitizeAttributes = function _sanitizeAttributes(currentNode) {\n    /* Execute a hook if present */\n    _executeHooks(hooks.beforeSanitizeAttributes, currentNode, null);\n    const {\n      attributes\n    } = currentNode;\n    /* Check if we have attributes; if not we might have a text node */\n    if (!attributes || _isClobbered(currentNode)) {\n      return;\n    }\n    const hookEvent = {\n      attrName: '',\n      attrValue: '',\n      keepAttr: true,\n      allowedAttributes: ALLOWED_ATTR,\n      forceKeepAttr: undefined\n    };\n    let l = attributes.length;\n    /* Go backwards over all attributes; safely remove bad ones */\n    while (l--) {\n      const attr = attributes[l];\n      const {\n        name,\n        namespaceURI,\n        value: attrValue\n      } = attr;\n      const lcName = transformCaseFunc(name);\n      let value = name === 'value' ? attrValue : stringTrim(attrValue);\n      /* Execute a hook if present */\n      hookEvent.attrName = lcName;\n      hookEvent.attrValue = value;\n      hookEvent.keepAttr = true;\n      hookEvent.forceKeepAttr = undefined; // Allows developers to see this is a property they can set\n      _executeHooks(hooks.uponSanitizeAttribute, currentNode, hookEvent);\n      value = hookEvent.attrValue;\n      /* Full DOM Clobbering protection via namespace isolation,\n       * Prefix id and name attributes with `user-content-`\n       */\n      if (SANITIZE_NAMED_PROPS && (lcName === 'id' || lcName === 'name')) {\n        // Remove the attribute with this value\n        _removeAttribute(name, currentNode);\n        // Prefix the value and later re-create the attribute with the sanitized value\n        value = SANITIZE_NAMED_PROPS_PREFIX + value;\n      }\n      /* Work around a security issue with comments inside attributes */\n      if (SAFE_FOR_XML && regExpTest(/((--!?|])>)|<\\/(style|title)/i, value)) {\n        _removeAttribute(name, currentNode);\n        continue;\n      }\n      /* Did the hooks approve of the attribute? */\n      if (hookEvent.forceKeepAttr) {\n        continue;\n      }\n      /* Remove attribute */\n      _removeAttribute(name, currentNode);\n      /* Did the hooks approve of the attribute? */\n      if (!hookEvent.keepAttr) {\n        continue;\n      }\n      /* Work around a security issue in jQuery 3.0 */\n      if (!ALLOW_SELF_CLOSE_IN_ATTR && regExpTest(/\\/>/i, value)) {\n        _removeAttribute(name, currentNode);\n        continue;\n      }\n      /* Sanitize attribute content to be template-safe */\n      if (SAFE_FOR_TEMPLATES) {\n        arrayForEach([MUSTACHE_EXPR, ERB_EXPR, TMPLIT_EXPR], expr => {\n          value = stringReplace(value, expr, ' ');\n        });\n      }\n      /* Is `value` valid for this attribute? */\n      const lcTag = transformCaseFunc(currentNode.nodeName);\n      if (!_isValidAttribute(lcTag, lcName, value)) {\n        continue;\n      }\n      /* Handle attributes that require Trusted Types */\n      if (trustedTypesPolicy && typeof trustedTypes === 'object' && typeof trustedTypes.getAttributeType === 'function') {\n        if (namespaceURI) ; else {\n          switch (trustedTypes.getAttributeType(lcTag, lcName)) {\n            case 'TrustedHTML':\n              {\n                value = trustedTypesPolicy.createHTML(value);\n                break;\n              }\n            case 'TrustedScriptURL':\n              {\n                value = trustedTypesPolicy.createScriptURL(value);\n                break;\n              }\n          }\n        }\n      }\n      /* Handle invalid data-* attribute set by try-catching it */\n      try {\n        if (namespaceURI) {\n          currentNode.setAttributeNS(namespaceURI, name, value);\n        } else {\n          /* Fallback to setAttribute() for browser-unrecognized namespaces e.g. \"x-schema\". */\n          currentNode.setAttribute(name, value);\n        }\n        if (_isClobbered(currentNode)) {\n          _forceRemove(currentNode);\n        } else {\n          arrayPop(DOMPurify.removed);\n        }\n      } catch (_) {}\n    }\n    /* Execute a hook if present */\n    _executeHooks(hooks.afterSanitizeAttributes, currentNode, null);\n  };\n  /**\n   * _sanitizeShadowDOM\n   *\n   * @param fragment to iterate over recursively\n   */\n  const _sanitizeShadowDOM = function _sanitizeShadowDOM(fragment) {\n    let shadowNode = null;\n    const shadowIterator = _createNodeIterator(fragment);\n    /* Execute a hook if present */\n    _executeHooks(hooks.beforeSanitizeShadowDOM, fragment, null);\n    while (shadowNode = shadowIterator.nextNode()) {\n      /* Execute a hook if present */\n      _executeHooks(hooks.uponSanitizeShadowNode, shadowNode, null);\n      /* Sanitize tags and elements */\n      _sanitizeElements(shadowNode);\n      /* Check attributes next */\n      _sanitizeAttributes(shadowNode);\n      /* Deep shadow DOM detected */\n      if (shadowNode.content instanceof DocumentFragment) {\n        _sanitizeShadowDOM(shadowNode.content);\n      }\n    }\n    /* Execute a hook if present */\n    _executeHooks(hooks.afterSanitizeShadowDOM, fragment, null);\n  };\n  // eslint-disable-next-line complexity\n  DOMPurify.sanitize = function (dirty) {\n    let cfg = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : {};\n    let body = null;\n    let importedNode = null;\n    let currentNode = null;\n    let returnNode = null;\n    /* Make sure we have a string to sanitize.\n      DO NOT return early, as this will return the wrong type if\n      the user has requested a DOM object rather than a string */\n    IS_EMPTY_INPUT = !dirty;\n    if (IS_EMPTY_INPUT) {\n      dirty = '<!-->';\n    }\n    /* Stringify, in case dirty is an object */\n    if (typeof dirty !== 'string' && !_isNode(dirty)) {\n      if (typeof dirty.toString === 'function') {\n        dirty = dirty.toString();\n        if (typeof dirty !== 'string') {\n          throw typeErrorCreate('dirty is not a string, aborting');\n        }\n      } else {\n        throw typeErrorCreate('toString is not a function');\n      }\n    }\n    /* Return dirty HTML if DOMPurify cannot run */\n    if (!DOMPurify.isSupported) {\n      return dirty;\n    }\n    /* Assign config vars */\n    if (!SET_CONFIG) {\n      _parseConfig(cfg);\n    }\n    /* Clean up removed elements */\n    DOMPurify.removed = [];\n    /* Check if dirty is correctly typed for IN_PLACE */\n    if (typeof dirty === 'string') {\n      IN_PLACE = false;\n    }\n    if (IN_PLACE) {\n      /* Do some early pre-sanitization to avoid unsafe root nodes */\n      if (dirty.nodeName) {\n        const tagName = transformCaseFunc(dirty.nodeName);\n        if (!ALLOWED_TAGS[tagName] || FORBID_TAGS[tagName]) {\n          throw typeErrorCreate('root node is forbidden and cannot be sanitized in-place');\n        }\n      }\n    } else if (dirty instanceof Node) {\n      /* If dirty is a DOM element, append to an empty document to avoid\n         elements being stripped by the parser */\n      body = _initDocument('<!---->');\n      importedNode = body.ownerDocument.importNode(dirty, true);\n      if (importedNode.nodeType === NODE_TYPE.element && importedNode.nodeName === 'BODY') {\n        /* Node is already a body, use as is */\n        body = importedNode;\n      } else if (importedNode.nodeName === 'HTML') {\n        body = importedNode;\n      } else {\n        // eslint-disable-next-line unicorn/prefer-dom-node-append\n        body.appendChild(importedNode);\n      }\n    } else {\n      /* Exit directly if we have nothing to do */\n      if (!RETURN_DOM && !SAFE_FOR_TEMPLATES && !WHOLE_DOCUMENT &&\n      // eslint-disable-next-line unicorn/prefer-includes\n      dirty.indexOf('<') === -1) {\n        return trustedTypesPolicy && RETURN_TRUSTED_TYPE ? trustedTypesPolicy.createHTML(dirty) : dirty;\n      }\n      /* Initialize the document to work on */\n      body = _initDocument(dirty);\n      /* Check we have a DOM node from the data */\n      if (!body) {\n        return RETURN_DOM ? null : RETURN_TRUSTED_TYPE ? emptyHTML : '';\n      }\n    }\n    /* Remove first element node (ours) if FORCE_BODY is set */\n    if (body && FORCE_BODY) {\n      _forceRemove(body.firstChild);\n    }\n    /* Get node iterator */\n    const nodeIterator = _createNodeIterator(IN_PLACE ? dirty : body);\n    /* Now start iterating over the created document */\n    while (currentNode = nodeIterator.nextNode()) {\n      /* Sanitize tags and elements */\n      _sanitizeElements(currentNode);\n      /* Check attributes next */\n      _sanitizeAttributes(currentNode);\n      /* Shadow DOM detected, sanitize it */\n      if (currentNode.content instanceof DocumentFragment) {\n        _sanitizeShadowDOM(currentNode.content);\n      }\n    }\n    /* If we sanitized `dirty` in-place, return it. */\n    if (IN_PLACE) {\n      return dirty;\n    }\n    /* Return sanitized string or DOM */\n    if (RETURN_DOM) {\n      if (RETURN_DOM_FRAGMENT) {\n        returnNode = createDocumentFragment.call(body.ownerDocument);\n        while (body.firstChild) {\n          // eslint-disable-next-line unicorn/prefer-dom-node-append\n          returnNode.appendChild(body.firstChild);\n        }\n      } else {\n        returnNode = body;\n      }\n      if (ALLOWED_ATTR.shadowroot || ALLOWED_ATTR.shadowrootmode) {\n        /*\n          AdoptNode() is not used because internal state is not reset\n          (e.g. the past names map of a HTMLFormElement), this is safe\n          in theory but we would rather not risk another attack vector.\n          The state that is cloned by importNode() is explicitly defined\n          by the specs.\n        */\n        returnNode = importNode.call(originalDocument, returnNode, true);\n      }\n      return returnNode;\n    }\n    let serializedHTML = WHOLE_DOCUMENT ? body.outerHTML : body.innerHTML;\n    /* Serialize doctype if allowed */\n    if (WHOLE_DOCUMENT && ALLOWED_TAGS['!doctype'] && body.ownerDocument && body.ownerDocument.doctype && body.ownerDocument.doctype.name && regExpTest(DOCTYPE_NAME, body.ownerDocument.doctype.name)) {\n      serializedHTML = '<!DOCTYPE ' + body.ownerDocument.doctype.name + '>\\n' + serializedHTML;\n    }\n    /* Sanitize final string template-safe */\n    if (SAFE_FOR_TEMPLATES) {\n      arrayForEach([MUSTACHE_EXPR, ERB_EXPR, TMPLIT_EXPR], expr => {\n        serializedHTML = stringReplace(serializedHTML, expr, ' ');\n      });\n    }\n    return trustedTypesPolicy && RETURN_TRUSTED_TYPE ? trustedTypesPolicy.createHTML(serializedHTML) : serializedHTML;\n  };\n  DOMPurify.setConfig = function () {\n    let cfg = arguments.length > 0 && arguments[0] !== undefined ? arguments[0] : {};\n    _parseConfig(cfg);\n    SET_CONFIG = true;\n  };\n  DOMPurify.clearConfig = function () {\n    CONFIG = null;\n    SET_CONFIG = false;\n  };\n  DOMPurify.isValidAttribute = function (tag, attr, value) {\n    /* Initialize shared config vars if necessary. */\n    if (!CONFIG) {\n      _parseConfig({});\n    }\n    const lcTag = transformCaseFunc(tag);\n    const lcName = transformCaseFunc(attr);\n    return _isValidAttribute(lcTag, lcName, value);\n  };\n  DOMPurify.addHook = function (entryPoint, hookFunction) {\n    if (typeof hookFunction !== 'function') {\n      return;\n    }\n    arrayPush(hooks[entryPoint], hookFunction);\n  };\n  DOMPurify.removeHook = function (entryPoint, hookFunction) {\n    if (hookFunction !== undefined) {\n      const index = arrayLastIndexOf(hooks[entryPoint], hookFunction);\n      return index === -1 ? undefined : arraySplice(hooks[entryPoint], index, 1)[0];\n    }\n    return arrayPop(hooks[entryPoint]);\n  };\n  DOMPurify.removeHooks = function (entryPoint) {\n    hooks[entryPoint] = [];\n  };\n  DOMPurify.removeAllHooks = function () {\n    hooks = _createHooksMap();\n  };\n  return DOMPurify;\n}\nvar purify = createDOMPurify();\n\nexport { purify as default };\n//# sourceMappingURL=purify.es.mjs.map\n", "import createDOMPurify from \"dompurify\";\nimport { JSDOM } from \"jsdom\";\n\nconst windowEmulator: any = new JSDOM(\"\").window;\nconst DOMPurify = createDOMPurify(windowEmulator);\n\n/*\n * DOMPurify prevents all XSS attacks by default. With these settings, it also\n * prevents \"deception\" attacks. If an attacker could put <div style=\"...\">\n * into the site's admin banner, they could make give the banner any appearance,\n * overlaid anywhere on the page. For example, a fake \"session expired\" modal\n * with a malicious link. Thus, this very strict DOMPurify config.\n */\nDOMPurify.setConfig({\n  // Only these tags will be allowed through\n  ALLOWED_TAGS: [\"ul\", \"ol\", \"li\", \"a\", \"#text\"],\n  // On those tags, only these attributes are allowed\n  ALLOWED_ATTR: [\"href\", \"alt\"],\n  // If a tag is removed, so will all its child elements & text\n  KEEP_CONTENT: false,\n});\n\n// sanitize string\nexport const sanitizeString = (string: string) => {\n  if (DOMPurify.isSupported) {\n    return DOMPurify.sanitize(string);\n  }\n};\n\n// iterates over array items, sanitizing items recursively\nexport const sanitizeArray = (array: unknown[]): unknown[] =>\n  array.map((entry: unknown) => sanitizeEntry(entry));\n\n// iterates over object key-value pairs, sanitizing values recursively\nexport const sanitizeObject = (object: { [key: string]: unknown }) => {\n  if (object) {\n    const entries = Object.entries(object);\n    const sanitizedEntries = entries.map((entry: [string, unknown]) => {\n      const [key, value] = entry;\n      return [key, sanitizeEntry(value)];\n    });\n    return Object.fromEntries(sanitizedEntries);\n  }\n};\n\nconst sanitizerMap: any = {\n  string: sanitizeString,\n  array: sanitizeArray,\n  object: sanitizeObject,\n};\n\n// return sanitized entry, or if safe type, return entry\nconst sanitizeEntry = (entry: unknown) => {\n  const entryType = Array.isArray(entry) ? \"array\" : typeof entry;\n  const sanitizer = sanitizerMap[entryType];\n  return sanitizer?.(entry) || entry;\n};\n", "// utils\nimport * as logger from \"../utils/debugging/debug-lib\";\nimport { isAuthenticated } from \"../utils/auth/authorization\";\nimport {\n  HttpResponse,\n  internalServerError,\n  unauthenticated,\n} from \"../utils/responses/response-lib\";\nimport { error } from \"../utils/constants/constants\";\nimport { sanitizeObject } from \"../utils/sanitize/sanitize\";\n// types\nimport { APIGatewayProxyEvent } from \"../utils/types\";\n\ntype LambdaFunction = (\n  event: APIGatewayProxyEvent, // eslint-disable-line no-unused-vars\n  context: any // eslint-disable-line no-unused-vars\n) => Promise<HttpResponse>;\n\nexport default function handler(lambda: LambdaFunction) {\n  return async function (event: APIGatewayProxyEvent, context: any) {\n    // Start debugger\n    logger.init();\n    logger.debug(\"API event: %O\", {\n      body: event.body,\n      pathParameters: event.pathParameters,\n      queryStringParameters: event.queryStringParameters,\n    });\n    if (await isAuthenticated(event)) {\n      try {\n        if (event.body) {\n          const newEventBody = sanitizeObject(JSON.parse(event.body));\n          event.body = JSON.stringify(newEventBody);\n        }\n        return await lambda(event, context);\n      } catch (error: any) {\n        logger.error(\"Error: %O\", error);\n\n        const body = { error: error.message };\n        return internalServerError(body);\n      } finally {\n        logger.flush();\n      }\n    } else {\n      return unauthenticated(error.UNAUTHORIZED);\n    }\n  };\n}\n", "import { APIGatewayProxyEvent, isReportType, isState } from \"../types\";\nimport { logger } from \"../debugging/debug-lib\";\n\nexport const parseSpecificReportParameters = (event: APIGatewayProxyEvent) => {\n  const { reportType, state, id } = event.pathParameters ?? {};\n  if (!isReportType(reportType)) {\n    logger.warn(\"Invalid report type in path\");\n    return { allParamsValid: false as const };\n  }\n  if (!isState(state)) {\n    logger.warn(\"Invalid state in path\");\n    return { allParamsValid: false as const };\n  }\n  if (!id) {\n    logger.warn(\"Invalid report ID in path\");\n    return { allParamsValid: false as const };\n  }\n  return { allParamsValid: true as const, reportType, state, id };\n};\n\nexport const parseStateReportParameters = (event: APIGatewayProxyEvent) => {\n  const { reportType, state } = event.pathParameters ?? {};\n  if (!isReportType(reportType)) {\n    logger.warn(\"Invalid report type in path\");\n    return { allParamsValid: false as const };\n  }\n  if (!isState(state)) {\n    logger.warn(\"Invalid state in path\");\n    return { allParamsValid: false as const };\n  }\n  return { allParamsValid: true as const, reportType, state };\n};\n", "import { object } from \"yup\";\nimport { error } from \"../constants/constants\";\n// types\nimport { AnyObject } from \"../types\";\n// utils\nimport { nested, endDate, schemaMap } from \"./schemaMap\";\n\n// compare payload data against validation schema\nexport const validateData = async (\n  validationSchema: AnyObject,\n  data: AnyObject,\n  options?: AnyObject\n) => {\n  try {\n    // returns valid data to be passed through API\n    return await validationSchema.validate(data, {\n      stripUnknown: true,\n      ...options,\n    });\n  } catch {\n    throw new Error(error.INVALID_DATA);\n  }\n};\n\n// filter field validation to just what's needed for the passed fields\nexport const filterValidationSchema = (\n  validationObject: AnyObject,\n  data: AnyObject\n): AnyObject => {\n  const validationEntries = Object.entries(validationObject);\n  const dataKeys = Object.keys(data);\n  const filteredEntries = validationEntries.filter(\n    (entry: [string, string | AnyObject]) => {\n      const [entryKey] = entry;\n      return dataKeys.includes(entryKey);\n    }\n  );\n  return Object.fromEntries(filteredEntries);\n};\n\n// map field validation types to validation schema\nexport const mapValidationTypesToSchema = (fieldValidationTypes: AnyObject) => {\n  let validationSchema: AnyObject = {};\n  // for each field to be validated,\n  Object.entries(fieldValidationTypes).forEach(\n    (fieldValidationType: [string, string | AnyObject]) => {\n      const [key, fieldValidation] = fieldValidationType;\n      // if standard validation type, set corresponding schema from map\n      if (typeof fieldValidation === \"string\") {\n        const correspondingSchema = schemaMap[fieldValidation];\n        if (correspondingSchema) {\n          validationSchema[key] = correspondingSchema;\n        }\n      }\n      // else if nested validation type, make and set nested schema\n      else if (fieldValidation.nested) {\n        validationSchema[key] = makeNestedFieldSchema(fieldValidation);\n        // else if not nested, make and set other dependent field types\n      } else if (fieldValidation.type === \"endDate\") {\n        validationSchema[key] = makeEndDateFieldSchema(fieldValidation);\n      }\n    }\n  );\n  return validationSchema;\n};\n\n// return created endDate schema\nexport const makeEndDateFieldSchema = (fieldValidationObject: AnyObject) => {\n  const { dependentFieldName } = fieldValidationObject;\n  return endDate(dependentFieldName);\n};\n\n// return created nested field schema\nexport const makeNestedFieldSchema = (fieldValidationObject: AnyObject) => {\n  const { type, parentFieldName, parentOptionId } = fieldValidationObject;\n  if (fieldValidationObject.type === \"endDate\") {\n    return nested(\n      () => makeEndDateFieldSchema(fieldValidationObject),\n      parentFieldName,\n      parentOptionId\n    );\n  } else {\n    const fieldValidationSchema = schemaMap[type];\n    return nested(() => fieldValidationSchema, parentFieldName, parentOptionId);\n  }\n};\n\nexport const validateFieldData = async (\n  validationJson: AnyObject,\n  unvalidatedFieldData: AnyObject\n) => {\n  let validatedFieldData: AnyObject | undefined = undefined;\n  // filter field validation to just what's needed for the passed fields\n  const filteredFieldDataValidationJson = filterValidationSchema(\n    validationJson,\n    unvalidatedFieldData\n  );\n  // transform field validation instructions to yup validation schema\n  const fieldDataValidationSchema = object().shape(\n    mapValidationTypesToSchema(filteredFieldDataValidationJson)\n  );\n  if (fieldDataValidationSchema) {\n    validatedFieldData = await validateData(\n      fieldDataValidationSchema,\n      unvalidatedFieldData\n    );\n  }\n  return validatedFieldData;\n};\n", "import { array, boolean, mixed, object, string } from \"yup\";\nimport { Choice } from \"../types/index\";\nimport {\n  checkRatioInputAgainstRegexes,\n  checkStandardIntegerInputAgainstRegexes,\n  checkStandardNumberInputAgainstRegexes,\n} from \"./checkInputValidity\";\n\nconst error = {\n  REQUIRED_GENERIC: \"A response is required\",\n  REQUIRED_CHECKBOX: \"Select at least one response\",\n  INVALID_GENERIC: \"Response must be valid\",\n  INVALID_EMAIL: \"Response must be a valid email address\",\n  INVALID_URL: \"Response must be a valid hyperlink/URL\",\n  INVALID_DATE: \"Response must be a valid date\",\n  INVALID_END_DATE: \"End date can't be before start date\",\n  NUMBER_LESS_THAN_ZERO: \"Response must be greater than or equal to zero\",\n  INVALID_NUMBER: \"Response must be a valid number\",\n  INVALID_NUMBER_OR_NA: 'Response must be a valid number or \"N/A\"',\n  INVALID_RATIO: \"Response must be a valid ratio\",\n};\n\n// TEXT - Helpers\nconst isWhitespaceString = (value?: string) => value?.trim().length === 0;\n\n// TEXT\nexport const text = () =>\n  string()\n    .typeError(error.INVALID_GENERIC)\n    .required(error.REQUIRED_GENERIC)\n    .test({\n      test: (value) => !isWhitespaceString(value),\n      message: error.REQUIRED_GENERIC,\n    });\nexport const textOptional = () => string().typeError(error.INVALID_GENERIC);\n\n// NUMBER - Helpers\nconst validNAValues = [\"N/A\", \"Data not available\"];\n// const validNumberRegex = /^\\.$|[0-9]/;\nconst validIntegerRegex = /^[0-9\\s,$%]+$/;\n\n// NUMBER - Number or Valid Strings\nexport const numberSchema = () =>\n  string()\n    .test({\n      message: error.INVALID_NUMBER_OR_NA,\n      test: (value) => {\n        if (value) {\n          const isValidStringValue = validNAValues.includes(value);\n          const isValidNumberValue =\n            checkStandardNumberInputAgainstRegexes(value);\n          return isValidStringValue || isValidNumberValue;\n        } else return true;\n      },\n    })\n    .test({\n      test: (value) => {\n        if (checkStandardNumberInputAgainstRegexes(value!)) {\n          return parseFloat(value!) >= 0;\n        } else return true;\n      },\n      message: error.NUMBER_LESS_THAN_ZERO,\n    });\n\nexport const number = () =>\n  numberSchema()\n    .required(error.REQUIRED_GENERIC)\n    .test({\n      test: (value) => !isWhitespaceString(value),\n      message: error.REQUIRED_GENERIC,\n    });\n\nexport const numberOptional = () => numberSchema().notRequired().nullable();\n\n// Integer or Valid Strings\nexport const validIntegerSchema = () =>\n  string()\n    .test({\n      message: error.INVALID_NUMBER_OR_NA,\n      test: (value) => {\n        if (value) {\n          const isValidStringValue = validNAValues.includes(value);\n          const isValidIntegerValue = validIntegerRegex.test(value);\n          return isValidStringValue || isValidIntegerValue;\n        } else return true;\n      },\n    })\n    .test({\n      test: (value) => {\n        if (checkStandardIntegerInputAgainstRegexes(value!)) {\n          return parseFloat(value!) >= 0;\n        } else return true;\n      },\n      message: error.NUMBER_LESS_THAN_ZERO,\n    });\n\nexport const validInteger = () =>\n  validIntegerSchema().required(error.REQUIRED_GENERIC);\n\nexport const validIntegerOptional = () =>\n  validIntegerSchema().notRequired().nullable();\n\n// Number - Ratio\nexport const ratio = () =>\n  mixed()\n    .test({\n      message: error.REQUIRED_GENERIC,\n      test: (val) => val != \"\",\n    })\n    .required(error.REQUIRED_GENERIC)\n    .test({\n      message: error.INVALID_RATIO,\n      test: (val) => {\n        return checkRatioInputAgainstRegexes(val).isValid;\n      },\n    });\n\n// EMAIL\nexport const email = () => text().email(error.INVALID_EMAIL);\nexport const emailOptional = () => email().notRequired();\n\n// URL\nexport const url = () => text().url(error.INVALID_URL);\nexport const urlOptional = () => url().notRequired();\n\n// DATE\nexport const date = () =>\n  string()\n    .required(error.REQUIRED_GENERIC)\n    .matches(dateFormatRegex, error.INVALID_DATE)\n    .test({\n      message: error.REQUIRED_GENERIC,\n      test: (value) => !isWhitespaceString(value),\n    });\n\nexport const dateOptional = () =>\n  string()\n    .typeError(error.INVALID_GENERIC)\n    .test({\n      message: error.INVALID_DATE,\n      test: (value) => dateFormatRegex.test(value!),\n    });\n\nexport const endDate = (startDateField: string) =>\n  date()\n    .typeError(error.INVALID_DATE)\n    .test({\n      message: error.INVALID_END_DATE,\n      test: (endDateString, context) => {\n        return isEndDateAfterStartDate(\n          context.parent[startDateField],\n          endDateString as string\n        );\n      },\n    });\n\nexport const isEndDateAfterStartDate = (\n  startDateString: string,\n  endDateString: string\n) => {\n  const startDate = new Date(startDateString);\n  const endDate = new Date(endDateString!);\n  return endDate >= startDate;\n};\n\n// DROPDOWN\nexport const dropdown = () =>\n  object({ label: text(), value: text() }).required(error.REQUIRED_GENERIC);\n\n// CHECKBOX\nexport const checkbox = () =>\n  array()\n    .min(1, error.REQUIRED_CHECKBOX)\n    .of(object({ key: text(), value: text() }))\n    .required(error.REQUIRED_CHECKBOX);\nexport const checkboxOptional = () =>\n  array().notRequired().typeError(error.INVALID_GENERIC);\nexport const checkboxSingle = () => boolean();\n\n// RADIO\nexport const radio = () =>\n  array()\n    .min(1, error.REQUIRED_GENERIC)\n    .of(object({ key: text(), value: text() }))\n    .required(error.REQUIRED_GENERIC);\nexport const radioOptional = () => radio().notRequired();\n\n// DYNAMIC\nexport const dynamic = () =>\n  array()\n    .min(1)\n    .of(\n      object().shape({\n        id: text(),\n        name: text(),\n      })\n    )\n    .required(error.REQUIRED_GENERIC);\nexport const dynamicOptional = () => dynamic().notRequired();\n\n// NESTED\nexport const nested = (\n  fieldSchema: Function,\n  parentFieldName: string,\n  parentOptionId: string\n) => {\n  const fieldTypeMap = {\n    array: array(),\n    string: string(),\n    date: date(),\n    object: object(),\n  };\n  const fieldType: keyof typeof fieldTypeMap = fieldSchema().type;\n  const baseSchema: any = fieldTypeMap[fieldType];\n  return baseSchema.when(parentFieldName, {\n    is: (value: Choice[]) =>\n      // look for parentOptionId in checked choices\n      value?.find((option: Choice) => option.key.endsWith(parentOptionId)),\n    then: () => fieldSchema(), // returns standard field schema (required)\n    otherwise: () => baseSchema, // returns not-required Yup base schema\n  });\n};\n\n// OBJECT ARRAY\nexport const objectArray = () => array().of(mixed());\n\n// REGEX\nexport const dateFormatRegex =\n  /^((0[1-9]|1[0-2])\\/(0[1-9]|1\\d|2\\d|3[01])\\/(19|20)\\d{2})|((0[1-9]|1[0-2])(0[1-9]|1\\d|2\\d|3[01])(19|20)\\d{2})$/;\n\n// SCHEMA MAP\nexport const schemaMap: any = {\n  checkbox: checkbox(),\n  checkboxOptional: checkboxOptional(),\n  checkboxSingle: checkboxSingle(),\n  date: date(),\n  dateOptional: dateOptional(),\n  dropdown: dropdown(),\n  dynamic: dynamic(),\n  dynamicOptional: dynamicOptional(),\n  email: email(),\n  emailOptional: emailOptional(),\n  number: number(),\n  numberOptional: numberOptional(),\n  objectArray: objectArray(),\n  radio: radio(),\n  radioOptional: radioOptional(),\n  ratio: ratio(),\n  text: text(),\n  textOptional: textOptional(),\n  url: url(),\n  urlOptional: urlOptional(),\n  validInteger: validInteger(),\n  validIntegerOptional: validIntegerOptional(),\n};\n", "// REGEX\n\n// basic check for all possible characters -- standard number\nconst validCharactersStandardNumberRegex = /^[0-9\\s.,$%-]+$/;\n// basic check for all possible characters -- standard number\nconst validCharactersStandardIntegerRegex = /^[0-9\\s,$%]+$/;\n// basic check for all possible characters -- ratio\nconst validCharactersRatioNumberRegex = /^[0-9.,-]+$/;\n// at most 1 decimal point\nconst atMost1DecimalPointRegex = /^[^.]*\\.?[^.]*$/;\n// commas only exist before decimal point\nconst validCommaLocationRegex = /^[0-9,$-]*\\.?[0-9%]*$/;\n// at most 1 $%\nconst atMost1SpecialCharacterRegex = /^([^$%]*\\$[^$%]*|[^$%]*%[^$%]*|[^$%]*)$/;\n// at most 1 $ at the beginning of the input\nconst validDollarSignPlacementRegex = /^[$]?[^$%]+$/;\n// at most 1 % at the end of the input\nconst validPercentSignPlacementRegex = /^[^%$]+[%]?$/;\n// at most 1 - at the beginning of the input (but after any potential $s)\nconst validNegativeSignPlacementRegex = /^[$]?[-]?[^$-]+[%]?$/;\n// exactly one ratio character in between other characters\nconst exactlyOneRatioCharacterRegex = /^[^:]+:[^:]+$/;\n\nexport const checkStandardNumberInputAgainstRegexes = (\n  value: string\n): boolean => {\n  if (\n    !validCharactersStandardNumberRegex.test(value) ||\n    !atMost1DecimalPointRegex.test(value) ||\n    !validCommaLocationRegex.test(value) ||\n    !atMost1SpecialCharacterRegex.test(value) ||\n    !(\n      validDollarSignPlacementRegex.test(value) ||\n      validPercentSignPlacementRegex.test(value)\n    ) ||\n    !validNegativeSignPlacementRegex.test(value)\n  )\n    return false;\n  return true;\n};\n\nexport const checkStandardIntegerInputAgainstRegexes = (\n  value: string\n): boolean => {\n  if (\n    !validCharactersStandardIntegerRegex.test(value) ||\n    !validCommaLocationRegex.test(value) ||\n    !atMost1SpecialCharacterRegex.test(value) ||\n    !(\n      validDollarSignPlacementRegex.test(value) ||\n      validPercentSignPlacementRegex.test(value)\n    )\n  )\n    return false;\n  return true;\n};\n\nexport const checkRatioInputAgainstRegexes = (\n  value: string\n): { isValid: boolean; leftSide: string; rightSide: string } => {\n  if (!exactlyOneRatioCharacterRegex.test(value))\n    return { isValid: false, leftSide: \"\", rightSide: \"\" };\n\n  // Grab the left and right side of the ratio sign\n  let values = value.split(\":\");\n\n  // Check left and right side for valid inputs\n  if (\n    !checkASideOfRatioAgainstRegexes(values[0]) ||\n    !checkASideOfRatioAgainstRegexes(values[1])\n  )\n    return { isValid: false, leftSide: values[0], rightSide: values[1] };\n\n  return { isValid: true, leftSide: values[0], rightSide: values[1] };\n};\n\nexport const checkASideOfRatioAgainstRegexes = (value: string): boolean => {\n  if (\n    !validCharactersRatioNumberRegex.test(value) ||\n    !atMost1DecimalPointRegex.test(value) ||\n    !validCommaLocationRegex.test(value) ||\n    !validNegativeSignPlacementRegex.test(value)\n  )\n    return false;\n  return true;\n};\n", "import * as yup from \"yup\";\n\nexport const metadataValidationSchema = yup.object().shape({\n  submissionName: yup.string(),\n  reportPeriod: yup.number(),\n  reportType: yup.string(),\n  locked: yup.bool(),\n  status: yup.string(),\n  lastAlteredBy: yup.string(),\n  submittedBy: yup.string(),\n  submittedOnDate: yup.string(),\n  previousRevisions: yup.array(),\n  submissionCount: yup.number(),\n  completionStatus: yup.mixed(),\n  finalSar: yup.mixed(),\n  populations: yup.mixed(),\n});\n", "import { object } from \"yup\";\nimport { error } from \"../constants/constants\";\n// types\nimport { AnyObject } from \"../types\";\n// utils\nimport { nested, endDate } from \"./completionSchemas\";\nimport { completionSchemaMap as schemaMap } from \"./completionSchemaMap\";\n\n// map field validation types to validation schema\nexport const mapValidationTypesToSchema = (fieldValidationTypes: AnyObject) => {\n  let validationSchema: AnyObject = {};\n  // for each field to be validated,\n  Object.entries(fieldValidationTypes).forEach(\n    (fieldValidationType: [string, string | AnyObject]) => {\n      const [key, fieldValidation] = fieldValidationType;\n\n      // if standard validation type, set corresponding schema from map\n      if (typeof fieldValidation === \"string\") {\n        const correspondingSchema = schemaMap[fieldValidation];\n        if (correspondingSchema) {\n          validationSchema[key] = correspondingSchema;\n        }\n      }\n      // else if nested validation type, make and set nested schema\n      else if (fieldValidation.nested) {\n        validationSchema[key] = makeNestedFieldSchema(fieldValidation);\n        // else if not nested, make and set other dependent field types\n      } else if (fieldValidation.type === \"endDate\") {\n        validationSchema[key] = makeEndDateFieldSchema(fieldValidation);\n      }\n    }\n  );\n  return validationSchema;\n};\n\n// return created endDate schema\nexport const makeEndDateFieldSchema = (fieldValidationObject: AnyObject) => {\n  const { dependentFieldName } = fieldValidationObject;\n  return endDate(dependentFieldName);\n};\n\n// return created nested field schema\nexport const makeNestedFieldSchema = (fieldValidationObject: AnyObject) => {\n  const { type, parentFieldName, parentOptionId } = fieldValidationObject;\n  if (fieldValidationObject.type === \"endDate\") {\n    return nested(\n      () => makeEndDateFieldSchema(fieldValidationObject),\n      parentFieldName,\n      parentOptionId\n    );\n  } else {\n    const fieldValidationSchema = schemaMap[type];\n    return nested(() => fieldValidationSchema, parentFieldName, parentOptionId);\n  }\n};\n\nexport const validateFieldData = async (\n  validationJson: AnyObject,\n  unvalidatedFieldData: AnyObject\n) => {\n  let validatedFieldData: AnyObject | undefined = undefined;\n  // filter field validation to just what's needed for the passed fields\n  const filteredFieldDataValidationJson = filterValidationSchema(\n    validationJson,\n    unvalidatedFieldData\n  );\n  // transform field validation instructions to yup validation schema\n  const fieldDataValidationSchema = object().shape(\n    mapValidationTypesToSchema(filteredFieldDataValidationJson)\n  );\n  if (fieldDataValidationSchema) {\n    validatedFieldData = await validateData(\n      fieldDataValidationSchema,\n      unvalidatedFieldData\n    );\n  }\n  return validatedFieldData;\n};\n\n// filter field validation to just what's needed for the passed fields\nexport const filterValidationSchema = (\n  validationObject: AnyObject,\n  data: AnyObject\n): AnyObject => {\n  const validationEntries = Object.entries(validationObject);\n  const dataKeys = Object.keys(data);\n  const filteredEntries = validationEntries.filter(\n    (entry: [string, string | AnyObject]) => {\n      const [entryKey] = entry;\n      return dataKeys.includes(entryKey);\n    }\n  );\n  return Object.fromEntries(filteredEntries);\n};\n\nexport const validateData = async (\n  validationSchema: AnyObject,\n  data: AnyObject,\n  options?: AnyObject\n) => {\n  try {\n    // returns valid data to be passed through API\n    return await validationSchema.validate(data, {\n      stripUnknown: true,\n      ...options,\n    });\n  } catch {\n    throw new Error(error.INVALID_DATA);\n  }\n};\n", "import {\n  array,\n  boolean,\n  mixed,\n  object,\n  string,\n  number as yupNumber,\n} from \"yup\";\nimport { Choice } from \"../types\";\n\nexport const error = {\n  REQUIRED_GENERIC: \"A response is required\",\n  REQUIRED_CHECKBOX: \"Select at least one response\",\n  INVALID_GENERIC: \"Response must be valid\",\n  INVALID_EMAIL: \"Response must be a valid email address\",\n  INVALID_URL: \"Response must be a valid hyperlink/URL\",\n  INVALID_DATE: \"Response must be a valid date\",\n  INVALID_END_DATE: \"End date can't be before start date\",\n  NUMBER_LESS_THAN_ZERO: \"Response must be greater than or equal to zero\",\n  INVALID_NUMBER: \"Response must be a valid number\",\n  INVALID_NUMBER_OR_NA: 'Response must be a valid number or \"N/A\"',\n  INVALID_RATIO: \"Response must be a valid ratio\",\n};\n\n// TEXT - Helpers\nconst isWhitespaceString = (value?: string) => value?.trim().length === 0;\n\n// TEXT\nconst textSchema = () =>\n  string()\n    .typeError(error.INVALID_GENERIC)\n    .test({\n      test: (value) => !isWhitespaceString(value),\n      message: error.REQUIRED_GENERIC,\n    });\n\nexport const text = () => textSchema().required();\nexport const textOptional = () => textSchema().notRequired().nullable();\n\n// NUMBER - Helpers\nconst validNAValues = [\"N/A\", \"Data not available\"];\n\n/** This regex must be at least as permissive as the one in ui-src */\nconst validNumberRegex = /^\\.$|[0-9]/;\n\nconst validIntegerRegex = /^[0-9\\s,]+$/;\n\n// NUMBER - Number or Valid Strings\nconst numberSchema = () =>\n  string()\n    .test({\n      message: error.INVALID_NUMBER_OR_NA,\n      test: (value) => {\n        if (value) {\n          const isValidStringValue = validNAValues.includes(value);\n          const isValidNumberValue = validNumberRegex.test(value);\n          return isValidStringValue || isValidNumberValue;\n        } else return true;\n      },\n    })\n    .test({\n      test: (value) => !isWhitespaceString(value),\n      message: error.REQUIRED_GENERIC,\n    })\n    .test({\n      test: (value) => {\n        if (validNumberRegex.test(value!)) {\n          return parseFloat(value!) >= 0;\n        } else return true;\n      },\n      message: error.NUMBER_LESS_THAN_ZERO,\n    });\n\nconst valueCleaningNumberSchema = (value: string, charsToReplace: RegExp) => {\n  return yupNumber().transform((_value) => {\n    return Number(value.replace(charsToReplace, \"\"));\n  });\n};\n\nexport const number = () => numberSchema().required();\nexport const numberOptional = () => numberSchema().notRequired().nullable();\n\n// Integer or Valid Strings\nexport const validIntegerSchema = () =>\n  string().test({\n    message: error.INVALID_NUMBER_OR_NA,\n    test: (value) => {\n      if (value) {\n        const isValidStringValue = validNAValues.includes(value);\n        const isValidIntegerValue = validIntegerRegex.test(value);\n        return isValidStringValue || isValidIntegerValue;\n      } else return true;\n    },\n  });\n\nexport const validInteger = () =>\n  validIntegerSchema()\n    .required(error.REQUIRED_GENERIC)\n    .test({\n      test: (value) => !isWhitespaceString(value),\n      message: error.REQUIRED_GENERIC,\n    });\n\nexport const validIntegerOptional = () =>\n  validIntegerSchema().notRequired().nullable();\n\n// Number - Ratio\nexport const ratio = () =>\n  mixed()\n    .test({\n      message: error.REQUIRED_GENERIC,\n      test: (val) => val != \"\",\n    })\n    .required(error.REQUIRED_GENERIC)\n    .test({\n      message: error.INVALID_RATIO,\n      test: (val) => {\n        const replaceCharsRegex = /[,.:]/g;\n        const ratio = val?.split(\":\");\n\n        // Double check and make sure that a ratio contains numbers on both sides\n        if (\n          !ratio ||\n          ratio.length != 2 ||\n          ratio[0].trim().length == 0 ||\n          ratio[1].trim().length == 0\n        ) {\n          return false;\n        }\n\n        // Check if the left side of the ratio is a valid number\n        const firstTest = valueCleaningNumberSchema(\n          ratio[0],\n          replaceCharsRegex\n        ).isValidSync(val);\n\n        // Check if the right side of the ratio is a valid number\n        const secondTest = valueCleaningNumberSchema(\n          ratio[1],\n          replaceCharsRegex\n        ).isValidSync(val);\n\n        // If both sides are valid numbers, return true!\n        return firstTest && secondTest;\n      },\n    });\n\n// EMAIL\n\nexport const email = () => textSchema().email(error.INVALID_EMAIL).required();\nexport const emailOptional = () =>\n  textSchema().email(error.INVALID_EMAIL).notRequired().nullable();\n\n// URL\nexport const url = () => textSchema().url(error.INVALID_URL).required();\nexport const urlOptional = () =>\n  textSchema().url(error.INVALID_URL).notRequired().nullable();\n\n// DATE\nconst dateSchema = () =>\n  string()\n    .matches(dateFormatRegex, error.INVALID_DATE)\n    .test({\n      message: error.REQUIRED_GENERIC,\n      test: (value) => !isWhitespaceString(value),\n    });\n\nexport const date = () => dateSchema().required(error.REQUIRED_GENERIC);\nexport const dateOptional = () => dateSchema().notRequired().nullable();\n\nexport const endDate = (startDateField: string) =>\n  date()\n    .typeError(error.INVALID_DATE)\n    .test({\n      message: error.INVALID_END_DATE,\n      test: (endDateString, context) => {\n        return isEndDateAfterStartDate(\n          context.parent[startDateField],\n          endDateString as string\n        );\n      },\n    });\n\nexport const isEndDateAfterStartDate = (\n  startDateString: string,\n  endDateString: string\n) => {\n  const startDate = new Date(startDateString);\n  const endDate = new Date(endDateString!);\n  return endDate >= startDate;\n};\n\n// DROPDOWN\nexport const dropdown = () =>\n  object({ label: textSchema(), value: textSchema() }).required(\n    error.REQUIRED_GENERIC\n  );\n\n// CHECKBOX\nexport const checkboxSchema = () =>\n  array()\n    .of(object({ key: text(), value: text() }))\n    .required(error.REQUIRED_GENERIC);\nexport const checkbox = () =>\n  checkboxSchema()\n    .min(1, error.REQUIRED_GENERIC)\n    .required(error.REQUIRED_GENERIC);\nexport const checkboxOptional = () =>\n  checkboxSchema().min(0, error.REQUIRED_GENERIC).notRequired().nullable();\nexport const checkboxSingle = () => boolean();\n\n// RADIO\nexport const radioSchema = () =>\n  array()\n    .of(object({ key: textSchema(), value: textSchema() }))\n    .min(0);\n\nexport const radio = () =>\n  radioSchema().min(1, error.REQUIRED_GENERIC).required();\nexport const radioOptional = () => radioSchema().notRequired().nullable();\n\n// DYNAMIC\nexport const dynamic = () =>\n  array()\n    .min(1)\n    .of(\n      object().shape({\n        id: textSchema(),\n        name: textSchema(),\n      })\n    )\n    .required(error.REQUIRED_GENERIC);\nexport const dynamicOptional = () => dynamic().notRequired();\n\n// NESTED\nexport const nested = (\n  fieldSchema: Function,\n  parentFieldName: string,\n  parentOptionId: string\n) => {\n  const fieldTypeMap = {\n    array: array(),\n    string: string(),\n    date: dateSchema(),\n    object: object(),\n  };\n  const fieldType: keyof typeof fieldTypeMap = fieldSchema().type;\n  const baseSchema: any = fieldTypeMap[fieldType];\n  return baseSchema.when(parentFieldName, {\n    is: (value: Choice[]) =>\n      // look for parentOptionId in checked choices\n      value?.find((option: Choice) => option.key.endsWith(parentOptionId)),\n    then: () => fieldSchema(), // returns standard field schema (required)\n    otherwise: () => baseSchema, // returns not-required Yup base schema\n  });\n};\n\n// REGEX\nexport const dateFormatRegex =\n  /^((0[1-9]|1[0-2])\\/(0[1-9]|1\\d|2\\d|3[01])\\/(19|20)\\d{2})|((0[1-9]|1[0-2])(0[1-9]|1\\d|2\\d|3[01])(19|20)\\d{2})$/;\n", "import * as schema from \"./completionSchemas\";\n\nexport const completionSchemaMap: any = {\n  text: schema.text(),\n  textOptional: schema.textOptional(),\n  number: schema.number(),\n  numberOptional: schema.numberOptional(),\n  ratio: schema.ratio(),\n  email: schema.email(),\n  emailOptional: schema.emailOptional(),\n  url: schema.url(),\n  urlOptional: schema.urlOptional(),\n  date: schema.date(),\n  dateOptional: schema.dateOptional(),\n  dropdown: schema.dropdown(),\n  checkbox: schema.checkbox(),\n  checkboxOptional: schema.checkboxOptional(),\n  checkboxSingle: schema.checkboxSingle(),\n  radio: schema.radio(),\n  radioOptional: schema.radioOptional(),\n  dynamic: schema.dynamic(),\n  dynamicOptional: schema.dynamicOptional(),\n  validInteger: schema.validInteger(),\n  validIntegerOptional: schema.validIntegerOptional(),\n};\n", "// types\nimport { DEFAULT_TARGET_POPULATION_NAMES } from \"../constants/constants\";\nimport {\n  AnyObject,\n  CompletionData,\n  FormJson,\n  FieldChoice,\n  Choice,\n  FormField,\n  ReportRoute,\n} from \"../types\";\n// utils\nimport { validateFieldData } from \"./completionValidation\";\n\nexport const isComplete = (completionStatus: CompletionData): boolean => {\n  const flatten = (obj: AnyObject, out: AnyObject) => {\n    Object.keys(obj).forEach((key) => {\n      if (typeof obj[key] == \"object\") {\n        out = flatten(obj[key], out);\n      } else {\n        out[key] = obj[key];\n      }\n    });\n    return out;\n  };\n\n  const flattenedStatus = flatten(completionStatus, {});\n\n  for (const status in flattenedStatus) {\n    if (flattenedStatus[status] === false) {\n      return false;\n    }\n  }\n  return true;\n};\n\n// Entry point for calculating completion status\nexport const calculateCompletionStatus = async (\n  fieldData: AnyObject,\n  formTemplate: AnyObject\n) => {\n  // Parent Dictionary for holding all route completion status\n\n  const validationJson = formTemplate.validationJson;\n\n  const areFieldsValid = async (\n    fieldsToBeValidated: Record<string, string>\n  ) => {\n    let areAllFieldsValid = false;\n    try {\n      // all fields successfully validated if validatedFields is not undefined\n      areAllFieldsValid =\n        (await validateFieldData(validationJson, fieldsToBeValidated)) !==\n        undefined;\n    } catch {\n      // Silently ignore error, will result in false\n    }\n    return areAllFieldsValid;\n  };\n\n  const calculateFormCompletion = async (\n    nestedFormTemplate: FormJson,\n    dataForObject: AnyObject = fieldData\n  ) => {\n    // Build an object of k:v for fields to validate\n    let fieldsToBeValidated: Record<string, string> = {};\n    // Repeat fields can't be validated at same time, so holding their completion status here\n    let repeatersValid = true; //default to true in case of no repeat fields\n\n    const getNestedFields = (\n      fieldChoices: FieldChoice[],\n      selectedChoices: Choice[]\n    ) => {\n      let selectedChoicesIds = selectedChoices\n        .map((choice: Choice) => choice.key)\n        .map((choiceId: string) => choiceId?.split(\"-\").pop());\n      let selectedChoicesWithChildren = fieldChoices?.filter(\n        (fieldChoice: FieldChoice) =>\n          selectedChoicesIds.includes(fieldChoice.id) && fieldChoice.children\n      );\n      let fieldIds: string[] = [];\n      selectedChoicesWithChildren?.forEach((selectedChoice: FieldChoice) => {\n        selectedChoice.children?.forEach((childChoice: FormField) => {\n          fieldIds.push(childChoice.id);\n          if (childChoice.props?.choices && dataForObject?.[childChoice.id]) {\n            let childFields = getNestedFields(\n              childChoice.props?.choices,\n              dataForObject[childChoice.id]\n            );\n            fieldIds.push(...childFields);\n          }\n        });\n      });\n      return fieldIds;\n    };\n    // Iterate over all fields in form\n    for (var formField of nestedFormTemplate?.fields || []) {\n      // Key: Form Field ID, Value: Report Data for field\n      if (Array.isArray(dataForObject[formField.id])) {\n        let nestedFields: string[] = getNestedFields(\n          formField.props?.choices,\n          dataForObject[formField.id]\n        );\n        nestedFields?.forEach((nestedField: string) => {\n          fieldsToBeValidated[nestedField] = dataForObject[nestedField]\n            ? dataForObject[nestedField]\n            : null;\n        });\n      }\n\n      fieldsToBeValidated[formField.id] = dataForObject[formField.id]\n        ? dataForObject[formField.id]\n        : null;\n    }\n    // Validate all fields en masse, passing flag that uses required validation schema\n    return repeatersValid && (await areFieldsValid(fieldsToBeValidated));\n  };\n\n  const isDefaultPopulationApplicable = (targetPopulations: AnyObject[]) => {\n    const filteredPopulations = targetPopulations?.filter((population) => {\n      const isDefault = DEFAULT_TARGET_POPULATION_NAMES.includes(\n        population.transitionBenchmarks_targetPopulationName\n      );\n\n      const isApplicable =\n        population?.transitionBenchmarks_applicableToMfpDemonstration?.[0]\n          ?.value === \"Yes\";\n      return isDefault && isApplicable;\n    });\n    return filteredPopulations.length >= 1;\n  };\n\n  const calculateEntityCompletion = async (\n    nestedFormTemplates: FormJson[],\n    entityType: string\n  ) => {\n    let atLeastOneTargetPopApplicable = false;\n    //value for holding combined result\n    var areAllFormsComplete = true;\n    for (var nestedFormTemplate of nestedFormTemplates) {\n      if (fieldData[entityType] && fieldData[entityType].length > 0) {\n        // if target population, at least one must be applicable to be complete\n        if (\n          entityType === \"targetPopulations\" &&\n          nestedFormTemplate?.id === \"tb-drawer\"\n        ) {\n          atLeastOneTargetPopApplicable = isDefaultPopulationApplicable(\n            fieldData[entityType]\n          );\n        }\n        // iterate over each entity (eg transition benchmark)\n        for (var dataForEntity of fieldData[entityType]) {\n          // get completion status for entity, using the correct form template\n          const isEntityComplete = await calculateFormCompletion(\n            nestedFormTemplate,\n            dataForEntity\n          );\n          // update combined result\n          areAllFormsComplete &&= isEntityComplete;\n        }\n      } else {\n        //Entity not present in report data, so check to see if it is required and update combined result\n        areAllFormsComplete &&=\n          formTemplate.entities && !formTemplate.entities[entityType]?.required;\n      }\n    }\n    if (entityType === \"targetPopulations\" && !atLeastOneTargetPopApplicable) {\n      return false;\n    }\n    return areAllFormsComplete;\n  };\n\n  const calculateEntityWithStepsCompletion = async (\n    stepFormTemplates: any[],\n    entityType: string\n  ) => {\n    if (!fieldData[entityType] || fieldData[entityType].length <= 0)\n      return false;\n\n    var areAllFormsComplete = true;\n    for (let i = 0; i < stepFormTemplates.length; i++) {\n      let stepForm = stepFormTemplates[i];\n      for (var entityFields of fieldData[entityType]) {\n        //modal overlay pages should have an array of key stepType in fieldData, automatic false if it doesn't exist or array is empty\n        if (\n          stepForm.pageType === \"overlayModal\" &&\n          (!entityFields[stepForm.stepType] ||\n            entityFields[stepForm.stepType].length <= 0)\n        ) {\n          areAllFormsComplete &&= false;\n        } else if (stepForm.stepType === \"closeOutInformation\") {\n          //skip over closeOut at the moment until we can make WP copies\n        } else {\n          //detemine which fieldData to match to the stepForm\n          const entityFieldsList = entityFields[stepForm.stepType]\n            ? entityFields[stepForm.stepType]\n            : [entityFields];\n          //loop through all children that belong to that entity and validate the values\n          for (var stepFields of entityFieldsList) {\n            if (stepForm?.objectiveCards) {\n              for (let card of stepForm.objectiveCards) {\n                if (card?.modalForm) {\n                  const nestedFormTemplate = card.modalForm;\n\n                  if (nestedFormTemplate?.objectiveId !== stepFields?.id) {\n                    continue;\n                  }\n                  const isEntityComplete = await calculateFormCompletion(\n                    nestedFormTemplate,\n                    stepFields\n                  );\n                  areAllFormsComplete &&= isEntityComplete;\n                }\n              }\n            } else {\n              const nestedFormTemplate = stepForm.form\n                ? stepForm.form\n                : stepForm.modalForm;\n\n              //WP uses modaloverlay so it doesn't have an initiativeId, only SAR does\n              if (\n                nestedFormTemplate?.initiativeId !== stepFields?.id &&\n                formTemplate.type === \"SAR\"\n              ) {\n                continue;\n              }\n              const isEntityComplete = await calculateFormCompletion(\n                nestedFormTemplate,\n                stepFields\n              );\n              areAllFormsComplete &&= isEntityComplete;\n            }\n          }\n        }\n      }\n    }\n\n    return areAllFormsComplete;\n  };\n\n  const calculateDynamicModalOverlayCompletion = async (\n    initiatives: any[],\n    entityType: string\n  ) => {\n    let areAllFormsComplete = true;\n\n    for (let initiative of initiatives) {\n      const isComplete = await calculateEntityWithStepsCompletion(\n        initiative.entitySteps,\n        entityType\n      );\n      if (!isComplete) {\n        areAllFormsComplete = false;\n        break;\n      }\n    }\n    return areAllFormsComplete;\n  };\n\n  const calculateRouteCompletion = async (route: ReportRoute) => {\n    let routeCompletion;\n    // Determine which type of page we are calculating status for\n    switch (route.pageType) {\n      case \"standard\":\n        if (!route.form) break;\n        // Standard forms use simple validation\n        routeCompletion = {\n          [route.path]: await calculateFormCompletion(route.form),\n        };\n        break;\n      case \"drawer\":\n        if (!route.drawerForm) break;\n        routeCompletion = {\n          [route.path]: await calculateEntityCompletion(\n            [route.drawerForm],\n            route.entityType\n          ),\n        };\n        break;\n      case \"modalDrawer\":\n        if (!route.drawerForm || !route.modalForm) break;\n        routeCompletion = {\n          [route.path]: await calculateEntityCompletion(\n            [route.drawerForm, route.modalForm],\n            route.entityType\n          ),\n        };\n        break;\n      case \"modalOverlay\":\n        if (!route.modalForm) break;\n        if (route.entitySteps) {\n          routeCompletion = {\n            [route.path]: await calculateEntityWithStepsCompletion(\n              route.entitySteps as [],\n              route.entityType\n            ),\n          };\n        } else {\n          routeCompletion = {\n            [route.path]: await calculateEntityCompletion(\n              [route.modalForm],\n              route.entityType\n            ),\n          };\n        }\n        break;\n      case \"dynamicModalOverlay\":\n        if (!route.initiatives) break;\n        routeCompletion = {\n          [route.path]: await calculateDynamicModalOverlayCompletion(\n            route.initiatives as [],\n            route.entityType\n          ),\n        };\n        break;\n      case \"reviewSubmit\":\n        // Don't evaluate the review and submit page\n        break;\n      default:\n        if (!route.children) break;\n        // Default behavior indicates that we are not on a form to be evaluated, which implies we have child routes to evaluate\n        routeCompletion = {\n          [route.path]: await calculateRoutesCompletion(route.children),\n        };\n        break;\n    }\n    return routeCompletion;\n  };\n\n  const calculateRoutesCompletion = async (routes: ReportRoute[]) => {\n    var completionDict: CompletionData = {};\n    // Iterate over each route\n    for (var route of routes || []) {\n      // Determine the status of each child in the route\n      const routeCompletionDict = await calculateRouteCompletion(route);\n      // Add completion status to parent dictionary\n      completionDict = { ...completionDict, ...routeCompletionDict };\n    }\n    return completionDict;\n  };\n\n  return await calculateRoutesCompletion(formTemplate.routes);\n};\n", "import { AnyObject } from \"../types\";\nimport { DEFAULT_TARGET_POPULATION_NAMES } from \"../constants/constants\";\n\n/**\n * This function goes through the fieldData and looks to see if there are target populations\n * attached to any initiatives and removes those target populations from them if the user\n * has marked it as not applicable\n * @param {AnyObject} fieldData - The fieldData of the Form\n * @return {AnyObject} Returns fieldData, with the possible removal of target populations from initiatives\n */\nexport const removeNotApplicablePopsFromInitiatives = (\n  fieldData: AnyObject\n) => {\n  // Gather the data we need\n  const targetPopulations = fieldData.targetPopulations;\n  const initiatives = fieldData.initiative;\n\n  /*\n   * If we can't find any data on targetPopulations or initiatives, we don't\n   * need to do anything\n   */\n  if (!targetPopulations || !initiatives) return fieldData;\n\n  /*\n   * Find any targetPopulations that user has answered \"No\" to when asked\n   * if this is applicable to the MFP demonstration. If none are found,\n   * we don't need to do anything\n   */\n  const isNotApplicable = (population: AnyObject) =>\n    population.transitionBenchmarks_applicableToMfpDemonstration?.[0]?.value ===\n    \"No\";\n\n  const getPopulationName = (population: AnyObject) =>\n    population.isRequired\n      ? population.transitionBenchmarks_targetPopulationName\n      : `Other: ${population.transitionBenchmarks_targetPopulationName}`;\n\n  const notApplicablePopulationNames = targetPopulations\n    .filter(isNotApplicable)\n    .map(getPopulationName);\n\n  if (notApplicablePopulationNames.length === 0) return fieldData;\n\n  /*\n   * Now knowing what target populations a user doesn't feel is applicable\n   * to the MFP demonstration, we need to look through the initiatives\n   * and remove it from the data\n   */\n  for (let initiative of initiatives) {\n    initiative.defineInitiative_targetPopulations =\n      initiative.defineInitiative_targetPopulations?.filter(\n        (initiativePopulation: AnyObject) =>\n          !notApplicablePopulationNames.includes(initiativePopulation.value) ||\n          DEFAULT_TARGET_POPULATION_NAMES.includes(initiativePopulation.value)\n      );\n  }\n\n  // Now set and return the cleaned up data!\n  return fieldData;\n};\n", "import { GetObjectCommand, PutObjectCommand } from \"@aws-sdk/client-s3\";\nimport {\n  GetCommand,\n  paginateQuery,\n  PutCommand,\n  QueryCommand,\n} from \"@aws-sdk/lib-dynamodb\";\nimport {\n  FormTemplateVersion,\n  ReportFieldData,\n  ReportJson,\n  ReportMetadataShape,\n  ReportType,\n  State,\n} from \"../utils/types\";\nimport {\n  createClient as createDynamoClient,\n  collectPageItems,\n} from \"./dynamodb-lib\";\nimport { createClient as createS3Client, parseS3Response } from \"./s3-lib\";\nimport { reportBuckets, reportTables } from \"../utils/constants/constants\";\n\nconst dynamoClient = createDynamoClient();\nconst s3Client = createS3Client();\n\nconst formTemplateVersionTable = process.env.FormTemplateVersionsTable!;\n\n/* METADATA (dynamo) */\n\nexport const putReportMetadata = async (metadata: ReportMetadataShape) => {\n  await dynamoClient.send(\n    new PutCommand({\n      TableName: reportTables[metadata.reportType],\n      Item: metadata,\n    })\n  );\n};\n\nexport const queryReportMetadatasForState = async (\n  reportType: ReportType,\n  state: State\n) => {\n  const table = reportTables[reportType];\n  const responsePages = paginateQuery(\n    { client: dynamoClient },\n    {\n      TableName: table,\n      KeyConditionExpression: \"#state = :state\",\n      ExpressionAttributeNames: { \"#state\": \"state\" },\n      ExpressionAttributeValues: { \":state\": state },\n    }\n  );\n  const metadatas = await collectPageItems(responsePages);\n  return metadatas as ReportMetadataShape[];\n};\n\nexport const getReportMetadata = async (\n  reportType: ReportType,\n  state: State,\n  id: string\n) => {\n  const table = reportTables[reportType];\n  const response = await dynamoClient.send(\n    new GetCommand({\n      TableName: table,\n      Key: { state, id },\n    })\n  );\n  return response.Item as ReportMetadataShape | undefined;\n};\n\n/* FIELD DATA (s3) */\n\nexport const putReportFieldData = async (\n  {\n    reportType,\n    state,\n    fieldDataId,\n  }: Pick<ReportMetadataShape, \"reportType\" | \"state\" | \"fieldDataId\">,\n  fieldData: ReportFieldData\n) => {\n  const bucket = reportBuckets[reportType];\n  await s3Client.send(\n    new PutObjectCommand({\n      Bucket: bucket,\n      ContentType: \"application/json\",\n      Key: `fieldData/${state}/${fieldDataId}.json`,\n      Body: JSON.stringify(fieldData),\n    })\n  );\n};\n\nexport const getReportFieldData = async ({\n  reportType,\n  state,\n  fieldDataId,\n}: Pick<ReportMetadataShape, \"reportType\" | \"state\" | \"fieldDataId\">) => {\n  const bucket = reportBuckets[reportType];\n  const response = await s3Client.send(\n    new GetObjectCommand({\n      Bucket: bucket,\n      Key: `fieldData/${state}/${fieldDataId}.json`,\n    })\n  );\n  const fieldData = await parseS3Response(response);\n  return fieldData as ReportFieldData | undefined;\n};\n\n/* FORM TEMPLATES (s3) */\n\nexport const putReportFormTemplate = async (\n  {\n    reportType,\n    formTemplateId,\n  }: Pick<ReportMetadataShape, \"reportType\" | \"formTemplateId\">,\n  formTemplate: ReportJson\n) => {\n  const bucket = reportBuckets[reportType];\n  await s3Client.send(\n    new PutObjectCommand({\n      Bucket: bucket,\n      ContentType: \"application/json\",\n      Key: `formTemplates/${formTemplateId}.json`,\n      Body: JSON.stringify(formTemplate),\n    })\n  );\n};\n\nexport const getReportFormTemplate = async ({\n  reportType,\n  formTemplateId,\n}: Pick<ReportMetadataShape, \"reportType\" | \"formTemplateId\">) => {\n  const bucket = reportBuckets[reportType];\n  const response = await s3Client.send(\n    new GetObjectCommand({\n      Bucket: bucket,\n      Key: `formTemplates/${formTemplateId}.json`,\n    })\n  );\n  const fieldData = await parseS3Response(response);\n  return fieldData as ReportJson | undefined;\n};\n\n/* FORM TEMPLATE VERSIONS (dynamo) */\n\nexport const putFormTemplateVersion = async (\n  formTemplateVersion: FormTemplateVersion\n) => {\n  await dynamoClient.send(\n    new PutCommand({\n      TableName: formTemplateVersionTable,\n      Item: formTemplateVersion,\n    })\n  );\n};\n\nexport const queryFormTemplateVersionByHash = async (\n  reportType: ReportType,\n  md5Hash: string\n) => {\n  const response = await dynamoClient.send(\n    new QueryCommand({\n      TableName: formTemplateVersionTable,\n      IndexName: \"HashIndex\",\n      KeyConditionExpression: \"reportType = :reportType AND md5Hash = :md5Hash\",\n      ExpressionAttributeValues: {\n        \":reportType\": reportType,\n        \":md5Hash\": md5Hash,\n      },\n      Limit: 1,\n    })\n  );\n  return response.Items?.[0] as FormTemplateVersion | undefined;\n};\n\nexport const queryLatestFormTemplateVersionNumber = async (\n  reportType: ReportType\n) => {\n  const response = await dynamoClient.send(\n    new QueryCommand({\n      TableName: formTemplateVersionTable,\n      KeyConditionExpression: \"reportType = :reportType\",\n      ExpressionAttributeValues: { \":reportType\": reportType },\n      Limit: 1,\n      ScanIndexForward: false, // false -> backwards -> highest version first\n    })\n  );\n  const latestFormTemplate = response.Items?.[0] as\n    | FormTemplateVersion\n    | undefined;\n  return latestFormTemplate?.versionNumber ?? 0;\n};\n", "import {\n  DynamoDBClient,\n  QueryCommandOutput,\n  ScanCommandOutput,\n} from \"@aws-sdk/client-dynamodb\";\nimport { DynamoDBDocumentClient, Paginator } from \"@aws-sdk/lib-dynamodb\";\n// utils\nimport { logger } from \"../utils/debugging/debug-lib\";\n\nconst localConfig = {\n  endpoint: process.env.DYNAMODB_URL,\n  region: \"localhost\",\n  credentials: {\n    accessKeyId: \"LOCALFAKEKEY\", // pragma: allowlist secret\n    secretAccessKey: \"LOCALFAKESECRET\", // pragma: allowlist secret\n  },\n  logger,\n};\n\nconst awsConfig = {\n  region: \"us-east-1\",\n  logger,\n};\n\nconst getConfig = () => {\n  return process.env.DYNAMODB_URL ? localConfig : awsConfig;\n};\n\nexport const createClient = () => {\n  return DynamoDBDocumentClient.from(new DynamoDBClient(getConfig()));\n};\n\nexport const collectPageItems = async <\n  T extends QueryCommandOutput | ScanCommandOutput\n>(\n  paginator: Paginator<T>\n) => {\n  let items: Record<string, any>[] = [];\n  for await (let page of paginator) {\n    items = items.concat(page.Items ?? []);\n  }\n  return items;\n};\n", "import { S3Client, GetObjectCommandOutput } from \"@aws-sdk/client-s3\";\nimport { logger } from \"../utils/debugging/debug-lib\";\n\nconst localConfig = {\n  endpoint: process.env.S3_LOCAL_ENDPOINT,\n  region: \"localhost\",\n  forcePathStyle: true,\n  credentials: {\n    accessKeyId: \"S3RVER\", // pragma: allowlist secret\n    secretAccessKey: \"S3RVER\", // pragma: allowlist secret\n  },\n  logger,\n};\n\nconst awsConfig = {\n  region: \"us-east-1\",\n  logger,\n};\n\nconst getConfig = () => {\n  return process.env.S3_LOCAL_ENDPOINT ? localConfig : awsConfig;\n};\n\nexport const createClient = () => new S3Client(getConfig());\n\nexport const parseS3Response = async (response: GetObjectCommandOutput) => {\n  const stringBody = await response.Body?.transformToString();\n  if (!stringBody) {\n    logger.warn(`Empty response from S3`);\n    return undefined;\n  }\n  return JSON.parse(stringBody);\n};\n"],
./.cdk/cdk.out/asset.1933f2bcc4bcbcde17038dfb72e31c8a0daf544c917d5787bb34a9683da083c0/index.js.map:4: TODO: validate topics not null/empty\n    const fetch = this.lookupRequest(apiKeys.Fetch, requests.Fetch)\n\n    // Shuffle topic-partitions to ensure fair response allocation across partitions (KIP-74)\n    const flattenedTopicPartitions = topics.reduce((topicPartitions, { topic, partitions }) => {\n      partitions.forEach(partition => {\n        topicPartitions.push({ topic, partition })\n      })\n      return topicPartitions\n    }, [])\n\n    const shuffledTopicPartitions = shuffle(flattenedTopicPartitions)\n\n    // Consecutive partitions for the same topic can be combined into a single `topic` entry\n    const consolidatedTopicPartitions = shuffledTopicPartitions.reduce(\n      (topicPartitions, { topic, partition }) => {\n        const last = topicPartitions[topicPartitions.length - 1]\n\n        if (last != null && last.topic === topic) {\n          topicPartitions[topicPartitions.length - 1].partitions.push(partition)\n        } else {\n          topicPartitions.push({ topic, partitions: [partition] })\n        }\n\n        return topicPartitions\n      },\n      []\n    )\n\n    return await this[PRIVATE.SEND_REQUEST](\n      fetch({\n        replicaId,\n        isolationLevel,\n        maxWaitTime,\n        minBytes,\n        maxBytes,\n        topics: consolidatedTopicPartitions,\n        rackId,\n      })\n    )\n  }\n\n  /**\n   * @public\n   * @param {object} request\n   * @param {string} request.groupId The group id\n   * @param {number} request.groupGenerationId The generation of the group\n   * @param {string} request.memberId The member id assigned by the group coordinator\n   * @returns {Promise}\n   */\n  async heartbeat({ groupId, groupGenerationId, memberId }) {\n    const heartbeat = this.lookupRequest(apiKeys.Heartbeat, requests.Heartbeat)\n    return await this[PRIVATE.SEND_REQUEST](heartbeat({ groupId, groupGenerationId, memberId }))\n  }\n\n  /**\n   * @public\n   * @param {object} request\n   * @param {string} request.groupId The unique group id\n   * @param {import(\"../protocol/coordinatorTypes\").CoordinatorType} request.coordinatorType The type of coordinator to find\n   * @returns {Promise}\n   */\n  async findGroupCoordinator({ groupId, coordinatorType }) {\n    // TODO: validate groupId, mandatory\n    const findCoordinator = this.lookupRequest(apiKeys.GroupCoordinator, requests.GroupCoordinator)\n    return await this[PRIVATE.SEND_REQUEST](findCoordinator({ groupId, coordinatorType }))\n  }\n\n  /**\n   * @public\n   * @param {object} request\n   * @param {string} request.groupId The unique group id\n   * @param {number} request.sessionTimeout The coordinator considers the consumer dead if it receives\n   *                                no heartbeat after this timeout in ms\n   * @param {number} request.rebalanceTimeout The maximum time that the coordinator will wait for each member\n   *                                  to rejoin when rebalancing the group\n   * @param {string} [request.memberId=\"\"] The assigned consumer id or an empty string for a new consumer\n   * @param {string} [request.protocolType=\"consumer\"] Unique name for class of protocols implemented by group\n   * @param {Array} request.groupProtocols List of protocols that the member supports (assignment strategy)\n   *                                [{ name: 'AssignerName', metadata: '{\"version\": 1, \"topics\": []}' }]\n   * @returns {Promise}\n   */\n  async joinGroup({\n    groupId,\n    sessionTimeout,\n    rebalanceTimeout,\n    memberId = '',\n    protocolType = 'consumer',\n    groupProtocols,\n  }) {\n    const joinGroup = this.lookupRequest(apiKeys.JoinGroup, requests.JoinGroup)\n    const makeRequest = (assignedMemberId = memberId) =>\n      this[PRIVATE.SEND_REQUEST](\n        joinGroup({\n          groupId,\n          sessionTimeout,\n          rebalanceTimeout,\n          memberId: assignedMemberId,\n          protocolType,\n          groupProtocols,\n        })\n      )\n\n    try {\n      return await makeRequest()\n    } catch (error) {\n      if (error.name === 'KafkaJSMemberIdRequired') {\n        return makeRequest(error.memberId)\n      }\n\n      throw error\n    }\n  }\n\n  /**\n   * @public\n   * @param {object} request\n   * @param {string} request.groupId\n   * @param {string} request.memberId\n   * @returns {Promise}\n   */\n  async leaveGroup({ groupId, memberId }) {\n    const leaveGroup = this.lookupRequest(apiKeys.LeaveGroup, requests.LeaveGroup)\n    return await this[PRIVATE.SEND_REQUEST](leaveGroup({ groupId, memberId }))\n  }\n\n  /**\n   * @public\n   * @param {object} request\n   * @param {string} request.groupId\n   * @param {number} request.generationId\n   * @param {string} request.memberId\n   * @param {object} request.groupAssignment\n   * @returns {Promise}\n   */\n  async syncGroup({ groupId, generationId, memberId, groupAssignment }) {\n    const syncGroup = this.lookupRequest(apiKeys.SyncGroup, requests.SyncGroup)\n    return await this[PRIVATE.SEND_REQUEST](\n      syncGroup({\n        groupId,\n        generationId,\n        memberId,\n        groupAssignment,\n      })\n    )\n  }\n\n  /**\n   * @public\n   * @param {object} request\n   * @param {number} request.replicaId=-1 Broker id of the follower. For normal consumers, use -1\n   * @param {number} request.isolationLevel=1 This setting controls the visibility of transactional records (default READ_COMMITTED, Kafka >0.11 only)\n   * @param {TopicPartitionOffset[]} request.topics e.g:\n   *\n   * @typedef {Object} TopicPartitionOffset\n   * @property {string} topic\n   * @property {PartitionOffset[]} partitions\n   *\n   * @typedef {Object} PartitionOffset\n   * @property {number} partition\n   * @property {number} [timestamp=-1]\n   *\n   *\n   * @returns {Promise}\n   */\n  async listOffsets({ replicaId, isolationLevel, topics }) {\n    const listOffsets = this.lookupRequest(apiKeys.ListOffsets, requests.ListOffsets)\n    const result = await this[PRIVATE.SEND_REQUEST](\n      listOffsets({ replicaId, isolationLevel, topics })\n    )\n\n    // ListOffsets >= v1 will return a single `offset` rather than an array of `offsets` (ListOffsets V0).\n    // Normalize to just return `offset`.\n    for (const response of result.responses) {\n      response.partitions = response.partitions.map(({ offsets, ...partitionData }) => {\n        return offsets ? { ...partitionData, offset: offsets.pop() } : partitionData\n      })\n    }\n\n    return result\n  }\n\n  /**\n   * @public\n   * @param {object} request\n   * @param {string} request.groupId\n   * @param {number} request.groupGenerationId\n   * @param {string} request.memberId\n   * @param {number} [request.retentionTime=-1] -1 signals to the broker that its default configuration\n   *                                    should be used.\n   * @param {object} request.topics Topics to commit offsets, e.g:\n   *                  [\n   *                    {\n   *                      topic: 'topic-name',\n   *                      partitions: [\n   *                        { partition: 0, offset: '11' }\n   *                      ]\n   *                    }\n   *                  ]\n   * @returns {Promise}\n   */\n  async offsetCommit({ groupId, groupGenerationId, memberId, retentionTime, topics }) {\n    const offsetCommit = this.lookupRequest(apiKeys.OffsetCommit, requests.OffsetCommit)\n    return await this[PRIVATE.SEND_REQUEST](\n      offsetCommit({\n        groupId,\n        groupGenerationId,\n        memberId,\n        retentionTime,\n        topics,\n      })\n    )\n  }\n\n  /**\n   * @public\n   * @param {object} request\n   * @param {string} request.groupId\n   * @param {object} request.topics - If the topic array is null fetch offsets for all topics. e.g:\n   *                  [\n   *                    {\n   *                      topic: 'topic-name',\n   *                      partitions: [\n   *                        { partition: 0 }\n   *                      ]\n   *                    }\n   *                  ]\n   * @returns {Promise}\n   */\n  async offsetFetch({ groupId, topics }) {\n    const offsetFetch = this.lookupRequest(apiKeys.OffsetFetch, requests.OffsetFetch)\n    return await this[PRIVATE.SEND_REQUEST](offsetFetch({ groupId, topics }))\n  }\n\n  /**\n   * @public\n   * @param {object} request\n   * @param {Array} request.groupIds\n   * @returns {Promise}\n   */\n  async describeGroups({ groupIds }) {\n    const describeGroups = this.lookupRequest(apiKeys.DescribeGroups, requests.DescribeGroups)\n    return await this[PRIVATE.SEND_REQUEST](describeGroups({ groupIds }))\n  }\n\n  /**\n   * @public\n   * @param {object} request\n   * @param {Array} request.topics e.g:\n   *                 [\n   *                   {\n   *                     topic: 'topic-name',\n   *                     numPartitions: 1,\n   *                     replicationFactor: 1\n   *                   }\n   *                 ]\n   * @param {boolean} [request.validateOnly=false] If this is true, the request will be validated, but the topic\n   *                                       won't be created\n   * @param {number} [request.timeout=5000] The time in ms to wait for a topic to be completely created\n   *                                on the controller node\n   * @returns {Promise}\n   */\n  async createTopics({ topics, validateOnly = false, timeout = 5000 }) {\n    const createTopics = this.lookupRequest(apiKeys.CreateTopics, requests.CreateTopics)\n    return await this[PRIVATE.SEND_REQUEST](createTopics({ topics, validateOnly, timeout }))\n  }\n\n  /**\n   * @public\n   * @param {object} request\n   * @param {Array} request.topicPartitions e.g:\n   *                 [\n   *                   {\n   *                     topic: 'topic-name',\n   *                     count: 3,\n   *                     assignments: []\n   *                   }\n   *                 ]\n   * @param {boolean} [request.validateOnly=false] If this is true, the request will be validated, but the topic\n   *                                       won't be created\n   * @param {number} [request.timeout=5000] The time in ms to wait for a topic to be completely created\n   *                                on the controller node\n   * @returns {Promise<void>}\n   */\n  async createPartitions({ topicPartitions, validateOnly = false, timeout = 5000 }) {\n    const createPartitions = this.lookupRequest(apiKeys.CreatePartitions, requests.CreatePartitions)\n    return await this[PRIVATE.SEND_REQUEST](\n      createPartitions({ topicPartitions, validateOnly, timeout })\n    )\n  }\n\n  /**\n   * @public\n   * @param {object} request\n   * @param {string[]} request.topics An array of topics to be deleted\n   * @param {number} [request.timeout=5000] The time in ms to wait for a topic to be completely deleted on the\n   *                                controller node.\n   * @returns {Promise}\n   */\n  async deleteTopics({ topics, timeout = 5000 }) {\n    const deleteTopics = this.lookupRequest(apiKeys.DeleteTopics, requests.DeleteTopics)\n    return await this[PRIVATE.SEND_REQUEST](deleteTopics({ topics, timeout }))\n  }\n\n  /**\n   * @public\n   * @param {object} request\n   * @param {import(\"../../types\").ResourceConfigQuery[]} request.resources\n   *                                 [{\n   *                                   type: RESOURCE_TYPES.TOPIC,\n   *                                   name: 'topic-name',\n   *                                   configNames: ['compression.type', 'retention.ms']\n   *                                 }]\n   * @param {boolean} [request.includeSynonyms=false]\n   * @returns {Promise}\n   */\n  async describeConfigs({ resources, includeSynonyms = false }) {\n    const describeConfigs = this.lookupRequest(apiKeys.DescribeConfigs, requests.DescribeConfigs)\n    return await this[PRIVATE.SEND_REQUEST](describeConfigs({ resources, includeSynonyms }))\n  }\n\n  /**\n   * @public\n   * @param {object} request\n   * @param {import(\"../../types\").IResourceConfig[]} request.resources\n   *                                 [{\n   *                                  type: RESOURCE_TYPES.TOPIC,\n   *                                  name: 'topic-name',\n   *                                  configEntries: [\n   *                                    {\n   *                                      name: 'cleanup.policy',\n   *                                      value: 'compact'\n   *                                    }\n   *                                  ]\n   *                                 }]\n   * @param {boolean} [request.validateOnly=false]\n   * @returns {Promise}\n   */\n  async alterConfigs({ resources, validateOnly = false }) {\n    const alterConfigs = this.lookupRequest(apiKeys.AlterConfigs, requests.AlterConfigs)\n    return await this[PRIVATE.SEND_REQUEST](alterConfigs({ resources, validateOnly }))\n  }\n\n  /**\n   * Send an `InitProducerId` request to fetch a PID and bump the producer epoch.\n   *\n   * Request should be made to the transaction coordinator.\n   * @public\n   * @param {object} request\n   * @param {number} request.transactionTimeout The time in ms to wait for before aborting idle transactions\n   * @param {number} [request.transactionalId] The transactional id or null if the producer is not transactional\n   * @returns {Promise}\n   */\n  async initProducerId({ transactionalId, transactionTimeout }) {\n    const initProducerId = this.lookupRequest(apiKeys.InitProducerId, requests.InitProducerId)\n    return await this[PRIVATE.SEND_REQUEST](initProducerId({ transactionalId, transactionTimeout }))\n  }\n\n  /**\n   * Send an `AddPartitionsToTxn` request to mark a TopicPartition as participating in the transaction.\n   *\n   * Request should be made to the transaction coordinator.\n   * @public\n   * @param {object} request\n   * @param {string} request.transactionalId The transactional id corresponding to the transaction.\n   * @param {number} request.producerId Current producer id in use by the transactional id.\n   * @param {number} request.producerEpoch Current epoch associated with the producer id.\n   * @param {object[]} request.topics e.g:\n   *                  [\n   *                    {\n   *                      topic: 'topic-name',\n   *                      partitions: [ 0, 1]\n   *                    }\n   *                  ]\n   * @returns {Promise}\n   */\n  async addPartitionsToTxn({ transactionalId, producerId, producerEpoch, topics }) {\n    const addPartitionsToTxn = this.lookupRequest(\n      apiKeys.AddPartitionsToTxn,\n      requests.AddPartitionsToTxn\n    )\n    return await this[PRIVATE.SEND_REQUEST](\n      addPartitionsToTxn({ transactionalId, producerId, producerEpoch, topics })\n    )\n  }\n\n  /**\n   * Send an `AddOffsetsToTxn` request.\n   *\n   * Request should be made to the transaction coordinator.\n   * @public\n   * @param {object} request\n   * @param {string} request.transactionalId The transactional id corresponding to the transaction.\n   * @param {number} request.producerId Current producer id in use by the transactional id.\n   * @param {number} request.producerEpoch Current epoch associated with the producer id.\n   * @param {string} request.groupId The unique group identifier (for the consumer group)\n   * @returns {Promise}\n   */\n  async addOffsetsToTxn({ transactionalId, producerId, producerEpoch, groupId }) {\n    const addOffsetsToTxn = this.lookupRequest(apiKeys.AddOffsetsToTxn, requests.AddOffsetsToTxn)\n    return await this[PRIVATE.SEND_REQUEST](\n      addOffsetsToTxn({ transactionalId, producerId, producerEpoch, groupId })\n    )\n  }\n\n  /**\n   * Send a `TxnOffsetCommit` request to persist the offsets in the `__consumer_offsets` topics.\n   *\n   * Request should be made to the consumer coordinator.\n   * @public\n   * @param {object} request\n   * @param {OffsetCommitTopic[]} request.topics\n   * @param {string} request.transactionalId The transactional id corresponding to the transaction.\n   * @param {string} request.groupId The unique group identifier (for the consumer group)\n   * @param {number} request.producerId Current producer id in use by the transactional id.\n   * @param {number} request.producerEpoch Current epoch associated with the producer id.\n   * @param {OffsetCommitTopic[]} request.topics\n   *\n   * @typedef {Object} OffsetCommitTopic\n   * @property {string} topic\n   * @property {OffsetCommitTopicPartition[]} partitions\n   *\n   * @typedef {Object} OffsetCommitTopicPartition\n   * @property {number} partition\n   * @property {number} offset\n   * @property {string} [metadata]\n   *\n   * @returns {Promise}\n   */\n  async txnOffsetCommit({ transactionalId, groupId, producerId, producerEpoch, topics }) {\n    const txnOffsetCommit = this.lookupRequest(apiKeys.TxnOffsetCommit, requests.TxnOffsetCommit)\n    return await this[PRIVATE.SEND_REQUEST](\n      txnOffsetCommit({ transactionalId, groupId, producerId, producerEpoch, topics })\n    )\n  }\n\n  /**\n   * Send an `EndTxn` request to indicate transaction should be committed or aborted.\n   *\n   * Request should be made to the transaction coordinator.\n   * @public\n   * @param {object} request\n   * @param {string} request.transactionalId The transactional id corresponding to the transaction.\n   * @param {number} request.producerId Current producer id in use by the transactional id.\n   * @param {number} request.producerEpoch Current epoch associated with the producer id.\n   * @param {boolean} request.transactionResult The result of the transaction (false = ABORT, true = COMMIT)\n   * @returns {Promise}\n   */\n  async endTxn({ transactionalId, producerId, producerEpoch, transactionResult }) {\n    const endTxn = this.lookupRequest(apiKeys.EndTxn, requests.EndTxn)\n    return await this[PRIVATE.SEND_REQUEST](\n      endTxn({ transactionalId, producerId, producerEpoch, transactionResult })\n    )\n  }\n\n  /**\n   * Send request for list of groups\n   * @public\n   * @returns {Promise}\n   */\n  async listGroups() {\n    const listGroups = this.lookupRequest(apiKeys.ListGroups, requests.ListGroups)\n    return await this[PRIVATE.SEND_REQUEST](listGroups())\n  }\n\n  /**\n   * Send request to delete groups\n   * @param {string[]} groupIds\n   * @public\n   * @returns {Promise}\n   */\n  async deleteGroups(groupIds) {\n    const deleteGroups = this.lookupRequest(apiKeys.DeleteGroups, requests.DeleteGroups)\n    return await this[PRIVATE.SEND_REQUEST](deleteGroups(groupIds))\n  }\n\n  /**\n   * Send request to delete records\n   * @public\n   * @param {object} request\n   * @param {TopicPartitionRecords[]} request.topics\n   *                          [\n   *                            {\n   *                              topic: 'my-topic-name',\n   *                              partitions: [\n   *                                { partition: 0, offset 2 },\n   *                                { partition: 1, offset 4 },\n   *                              ],\n   *                            }\n   *                          ]\n   * @returns {Promise<Array>} example:\n   *                          {\n   *                            throttleTime: 0\n   *                           [\n   *                              {\n   *                                topic: 'my-topic-name',\n   *                                partitions: [\n   *                                 { partition: 0, lowWatermark: '2n', errorCode: 0 },\n   *                                 { partition: 1, lowWatermark: '4n', errorCode: 0 },\n   *                               ],\n   *                             },\n   *                           ]\n   *                          }\n   *\n   * @typedef {object} TopicPartitionRecords\n   * @property {string} topic\n   * @property {PartitionRecord[]} partitions\n   *\n   * @typedef {object} PartitionRecord\n   * @property {number} partition\n   * @property {number} offset\n   */\n  async deleteRecords({ topics }) {\n    const deleteRecords = this.lookupRequest(apiKeys.DeleteRecords, requests.DeleteRecords)\n    return await this[PRIVATE.SEND_REQUEST](deleteRecords({ topics }))\n  }\n\n  /**\n   * @public\n   * @param {object} request\n   * @param {import(\"../../types\").AclEntry[]} request.acl e.g:\n   *                 [\n   *                   {\n   *                     resourceType: AclResourceTypes.TOPIC,\n   *                     resourceName: 'topic-name',\n   *                     resourcePatternType: ResourcePatternTypes.LITERAL,\n   *                     principal: 'User:bob',\n   *                     host: '*',\n   *                     operation: AclOperationTypes.ALL,\n   *                     permissionType: AclPermissionTypes.DENY,\n   *                   }\n   *                 ]\n   * @returns {Promise<void>}\n   */\n  async createAcls({ acl }) {\n    const createAcls = this.lookupRequest(apiKeys.CreateAcls, requests.CreateAcls)\n    return await this[PRIVATE.SEND_REQUEST](createAcls({ creations: acl }))\n  }\n\n  /**\n   * @public\n   * @param {import(\"../../types\").AclEntry} aclEntry\n   * @returns {Promise<void>}\n   */\n  async describeAcls({\n    resourceType,\n    resourceName,\n    resourcePatternType,\n    principal,\n    host,\n    operation,\n    permissionType,\n  }) {\n    const describeAcls = this.lookupRequest(apiKeys.DescribeAcls, requests.DescribeAcls)\n    return await this[PRIVATE.SEND_REQUEST](\n      describeAcls({\n        resourceType,\n        resourceName,\n        resourcePatternType,\n        principal,\n        host,\n        operation,\n        permissionType,\n      })\n    )\n  }\n\n  /**\n   * @public\n   * @param {Object} request\n   * @param {import(\"../../types\").AclEntry[]} request.filters\n   * @returns {Promise<void>}\n   */\n  async deleteAcls({ filters }) {\n    const deleteAcls = this.lookupRequest(apiKeys.DeleteAcls, requests.DeleteAcls)\n    return await this[PRIVATE.SEND_REQUEST](deleteAcls({ filters }))\n  }\n\n  /**\n   * @public\n   * @param {Object} request\n   * @param {import(\"../../types\").PartitionReassignment[]} request.topics\n   * @param {number} [request.timeout]\n   * @returns {Promise}\n   */\n  async alterPartitionReassignments({ topics, timeout }) {\n    const alterPartitionReassignments = this.lookupRequest(\n      apiKeys.AlterPartitionReassignments,\n      requests.AlterPartitionReassignments\n    )\n    return await this[PRIVATE.SEND_REQUEST](alterPartitionReassignments({ topics, timeout }))\n  }\n\n  /**\n   * @public\n   * @param {Object} request\n   * @param {import(\"../../types\").TopicPartitions[]} request.topics can be null\n   * @param {number} [request.timeout]\n   * @returns {Promise}\n   */\n  async listPartitionReassignments({ topics = null, timeout }) {\n    const listPartitionReassignments = this.lookupRequest(\n      apiKeys.ListPartitionReassignments,\n      requests.ListPartitionReassignments\n    )\n    return await this[PRIVATE.SEND_REQUEST](listPartitionReassignments({ topics, timeout }))\n  }\n\n  /**\n   * @private\n   */\n  async [PRIVATE.SEND_REQUEST](protocolRequest) {\n    try {\n      return await this.connectionPool.send(protocolRequest)\n    } catch (e) {\n      if (e.name === 'KafkaJSConnectionClosedError') {\n        await this.disconnect()\n      }\n\n      throw e\n    }\n  }\n}\n", "module.exports = {\n  maxRetryTime: 1000,\n  initialRetryTime: 50,\n  factor: 0.02, // randomization factor\n  multiplier: 1.5, // exponential factor\n  retries: 15, // max retries\n}\n", "module.exports = {\n  maxRetryTime: 30 * 1000,\n  initialRetryTime: 300,\n  factor: 0.2, // randomization factor\n  multiplier: 2, // exponential factor\n  retries: 5, // max retries\n}\n", "const { KafkaJSNumberOfRetriesExceeded, KafkaJSNonRetriableError } = require('../errors')\n\nconst isTestMode = process.env.NODE_ENV === 'test'\nconst RETRY_DEFAULT = isTestMode ? require('./defaults.test') : require('./defaults')\n\nconst random = (min, max) => {\n  return Math.random() * (max - min) + min\n}\n\nconst randomFromRetryTime = (factor, retryTime) => {\n  const delta = factor * retryTime\n  return Math.ceil(random(retryTime - delta, retryTime + delta))\n}\n\nconst UNRECOVERABLE_ERRORS = ['RangeError', 'ReferenceError', 'SyntaxError', 'TypeError']\nconst isErrorUnrecoverable = e => UNRECOVERABLE_ERRORS.includes(e.name)\nconst isErrorRetriable = error =>\n  (error.retriable || error.retriable !== false) && !isErrorUnrecoverable(error)\n\nconst createRetriable = (configs, resolve, reject, fn) => {\n  let aborted = false\n  const { factor, multiplier, maxRetryTime, retries } = configs\n\n  const bail = error => {\n    aborted = true\n    reject(error || new Error('Aborted'))\n  }\n\n  const calculateExponentialRetryTime = retryTime => {\n    return Math.min(randomFromRetryTime(factor, retryTime) * multiplier, maxRetryTime)\n  }\n\n  const retry = (retryTime, retryCount = 0) => {\n    if (aborted) return\n\n    const nextRetryTime = calculateExponentialRetryTime(retryTime)\n    const shouldRetry = retryCount < retries\n\n    const scheduleRetry = () => {\n      setTimeout(() => retry(nextRetryTime, retryCount + 1), retryTime)\n    }\n\n    fn(bail, retryCount, retryTime)\n      .then(resolve)\n      .catch(e => {\n        if (isErrorRetriable(e)) {\n          if (shouldRetry) {\n            scheduleRetry()\n          } else {\n            reject(\n              new KafkaJSNumberOfRetriesExceeded(e, { retryCount, retryTime, cause: e.cause || e })\n            )\n          }\n        } else {\n          reject(new KafkaJSNonRetriableError(e, { cause: e.cause || e }))\n        }\n      })\n  }\n\n  return retry\n}\n\n/**\n * @typedef {(fn: (bail: (err: Error) => void, retryCount: number, retryTime: number) => any) => Promise<ReturnType<fn>>} Retrier\n */\n\n/**\n * @param {import(\"../../types\").RetryOptions} [opts]\n * @returns {Retrier}\n */\nmodule.exports = (opts = {}) => fn => {\n  return new Promise((resolve, reject) => {\n    const configs = Object.assign({}, RETRY_DEFAULT, opts)\n    const start = createRetriable(configs, resolve, reject, fn)\n    start(randomFromRetryTime(configs.factor, configs.initialRetryTime))\n  })\n}\n", "module.exports = (a, b) => {\n  const result = []\n  const length = a.length\n  let i = 0\n\n  while (i < length) {\n    if (b.indexOf(a[i]) === -1) {\n      result.push(a[i])\n    }\n    i += 1\n  }\n\n  return result\n}\n", "const Broker = require('../broker')\nconst createRetry = require('../retry')\nconst shuffle = require('../utils/shuffle')\nconst arrayDiff = require('../utils/arrayDiff')\nconst { KafkaJSBrokerNotFound, KafkaJSProtocolError } = require('../errors')\n\nconst { keys, assign, values } = Object\nconst hasBrokerBeenReplaced = (broker, { host, port, rack }) =>\n  broker.connectionPool.host !== host ||\n  broker.connectionPool.port !== port ||\n  broker.connectionPool.rack !== rack\n\nmodule.exports = class BrokerPool {\n  /**\n   * @param {object} options\n   * @param {import(\"./connectionPoolBuilder\").ConnectionPoolBuilder} options.connectionPoolBuilder\n   * @param {import(\"../../types\").Logger} options.logger\n   * @param {import(\"../../types\").RetryOptions} [options.retry]\n   * @param {boolean} [options.allowAutoTopicCreation]\n   * @param {number} [options.authenticationTimeout]\n   * @param {number} [options.metadataMaxAge]\n   */\n  constructor({\n    connectionPoolBuilder,\n    logger,\n    retry,\n    allowAutoTopicCreation,\n    authenticationTimeout,\n    metadataMaxAge,\n  }) {\n    this.rootLogger = logger\n    this.connectionPoolBuilder = connectionPoolBuilder\n    this.metadataMaxAge = metadataMaxAge || 0\n    this.logger = logger.namespace('BrokerPool')\n    this.retrier = createRetry(assign({}, retry))\n\n    this.createBroker = options =>\n      new Broker({\n        allowAutoTopicCreation,\n        authenticationTimeout,\n        ...options,\n      })\n\n    this.brokers = {}\n    /** @type {Broker | undefined} */\n    this.seedBroker = undefined\n    /** @type {import(\"../../types\").BrokerMetadata | null} */\n    this.metadata = null\n    this.metadataExpireAt = null\n    this.versions = null\n  }\n\n  /**\n   * @public\n   * @returns {Boolean}\n   */\n  hasConnectedBrokers() {\n    const brokers = values(this.brokers)\n    return (\n      !!brokers.find(broker => broker.isConnected()) ||\n      (this.seedBroker ? this.seedBroker.isConnected() : false)\n    )\n  }\n\n  async createSeedBroker() {\n    if (this.seedBroker) {\n      await this.seedBroker.disconnect()\n    }\n\n    const connectionPool = await this.connectionPoolBuilder.build()\n\n    this.seedBroker = this.createBroker({\n      connectionPool,\n      logger: this.rootLogger,\n    })\n  }\n\n  /**\n   * @public\n   * @returns {Promise<void>}\n   */\n  async connect() {\n    if (this.hasConnectedBrokers()) {\n      return\n    }\n\n    if (!this.seedBroker) {\n      await this.createSeedBroker()\n    }\n\n    return this.retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await this.seedBroker.connect()\n        this.versions = this.seedBroker.versions\n      } catch (e) {\n        if (e.name === 'KafkaJSConnectionError' || e.type === 'ILLEGAL_SASL_STATE') {\n          // Connection builder will always rotate the seed broker\n          await this.createSeedBroker()\n          this.logger.error(\n            `Failed to connect to seed broker, trying another broker from the list: ${e.message}`,\n            { retryCount, retryTime }\n          )\n        } else {\n          this.logger.error(e.message, { retryCount, retryTime })\n        }\n\n        if (e.retriable) throw e\n        bail(e)\n      }\n    })\n  }\n\n  /**\n   * @public\n   * @returns {Promise}\n   */\n  async disconnect() {\n    this.seedBroker && (await this.seedBroker.disconnect())\n    await Promise.all(values(this.brokers).map(broker => broker.disconnect()))\n\n    this.brokers = {}\n    this.metadata = null\n    this.versions = null\n  }\n\n  /**\n   * @public\n   * @param {Object} destination\n   * @param {string} destination.host\n   * @param {number} destination.port\n   */\n  removeBroker({ host, port }) {\n    const removedBroker = values(this.brokers).find(\n      broker => broker.connectionPool.host === host && broker.connectionPool.port === port\n    )\n\n    if (removedBroker) {\n      delete this.brokers[removedBroker.nodeId]\n      this.metadataExpireAt = null\n\n      if (this.seedBroker.nodeId === removedBroker.nodeId) {\n        this.seedBroker = shuffle(values(this.brokers))[0]\n      }\n    }\n  }\n\n  /**\n   * @public\n   * @param {Array<String>} topics\n   * @returns {Promise<null>}\n   */\n  async refreshMetadata(topics) {\n    const broker = await this.findConnectedBroker()\n    const { host: seedHost, port: seedPort } = this.seedBroker.connectionPool\n\n    return this.retrier(async (bail, retryCount, retryTime) => {\n      try {\n        this.metadata = await broker.metadata(topics)\n        this.metadataExpireAt = Date.now() + this.metadataMaxAge\n\n        const replacedBrokers = []\n\n        this.brokers = await this.metadata.brokers.reduce(\n          async (resultPromise, { nodeId, host, port, rack }) => {\n            const result = await resultPromise\n\n            if (result[nodeId]) {\n              if (!hasBrokerBeenReplaced(result[nodeId], { host, port, rack })) {\n                return result\n              }\n\n              replacedBrokers.push(result[nodeId])\n            }\n\n            if (host === seedHost && port === seedPort) {\n              this.seedBroker.nodeId = nodeId\n              this.seedBroker.connectionPool.rack = rack\n              return assign(result, {\n                [nodeId]: this.seedBroker,\n              })\n            }\n\n            return assign(result, {\n              [nodeId]: this.createBroker({\n                logger: this.rootLogger,\n                versions: this.versions,\n                connectionPool: await this.connectionPoolBuilder.build({ host, port, rack }),\n                nodeId,\n              }),\n            })\n          },\n          this.brokers\n        )\n\n        const freshBrokerIds = this.metadata.brokers.map(({ nodeId }) => `${nodeId}`).sort()\n        const currentBrokerIds = keys(this.brokers).sort()\n        const unusedBrokerIds = arrayDiff(currentBrokerIds, freshBrokerIds)\n\n        const brokerDisconnects = unusedBrokerIds.map(nodeId => {\n          const broker = this.brokers[nodeId]\n          return broker.disconnect().then(() => {\n            delete this.brokers[nodeId]\n          })\n        })\n\n        const replacedBrokersDisconnects = replacedBrokers.map(broker => broker.disconnect())\n        await Promise.all([...brokerDisconnects, ...replacedBrokersDisconnects])\n      } catch (e) {\n        if (e.type === 'LEADER_NOT_AVAILABLE') {\n          throw e\n        }\n\n        bail(e)\n      }\n    })\n  }\n\n  /**\n   * Only refreshes metadata if the data is stale according to the `metadataMaxAge` param or does not contain information about the provided topics\n   *\n   * @public\n   * @param {Array<String>} topics\n   * @returns {Promise<null>}\n   */\n  async refreshMetadataIfNecessary(topics) {\n    const shouldRefresh =\n      this.metadata == null ||\n      this.metadataExpireAt == null ||\n      Date.now() > this.metadataExpireAt ||\n      !topics.every(topic =>\n        this.metadata.topicMetadata.some(topicMetadata => topicMetadata.topic === topic)\n      )\n\n    if (shouldRefresh) {\n      return this.refreshMetadata(topics)\n    }\n  }\n\n  /** @type {() => string[]} */\n  getNodeIds() {\n    return keys(this.brokers)\n  }\n\n  /**\n   * @public\n   * @param {object} options\n   * @param {string} options.nodeId\n   * @returns {Promise<Broker>}\n   */\n  async findBroker({ nodeId }) {\n    const broker = this.brokers[nodeId]\n\n    if (!broker) {\n      throw new KafkaJSBrokerNotFound(`Broker ${nodeId} not found in the cached metadata`)\n    }\n\n    await this.connectBroker(broker)\n    return broker\n  }\n\n  /**\n   * @public\n   * @param {(params: { nodeId: string, broker: Broker }) => Promise<T>} callback\n   * @returns {Promise<T>}\n   * @template T\n   */\n  async withBroker(callback) {\n    const brokers = shuffle(keys(this.brokers))\n    if (brokers.length === 0) {\n      throw new KafkaJSBrokerNotFound('No brokers in the broker pool')\n    }\n\n    for (const nodeId of brokers) {\n      const broker = await this.findBroker({ nodeId })\n      try {\n        return await callback({ nodeId, broker })\n      } catch (e) {}\n    }\n\n    return null\n  }\n\n  /**\n   * @public\n   * @returns {Promise<Broker>}\n   */\n  async findConnectedBroker() {\n    const nodeIds = shuffle(keys(this.brokers))\n    const connectedBrokerId = nodeIds.find(nodeId => this.brokers[nodeId].isConnected())\n\n    if (connectedBrokerId) {\n      return await this.findBroker({ nodeId: connectedBrokerId })\n    }\n\n    // Cycle through the nodes until one connects\n    for (const nodeId of nodeIds) {\n      try {\n        return await this.findBroker({ nodeId })\n      } catch (e) {}\n    }\n\n    // Failed to connect to all known brokers, metadata might be old\n    await this.connect()\n    return this.seedBroker\n  }\n\n  /**\n   * @private\n   * @param {Broker} broker\n   * @returns {Promise<null>}\n   */\n  async connectBroker(broker) {\n    if (broker.isConnected()) {\n      return\n    }\n\n    return this.retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await broker.connect()\n      } catch (e) {\n        if (e.name === 'KafkaJSConnectionError' || e.type === 'ILLEGAL_SASL_STATE') {\n          await broker.disconnect()\n        }\n\n        // To avoid reconnecting to an unavailable host, we bail on connection errors\n        // and refresh metadata on a higher level before reconnecting\n        if (e.name === 'KafkaJSConnectionError') {\n          return bail(e)\n        }\n\n        if (e.type === 'ILLEGAL_SASL_STATE') {\n          // Rebuild the connection pool since it can't recover from illegal SASL state\n          broker.connectionPool = await this.connectionPoolBuilder.build({\n            host: broker.connectionPool.host,\n            port: broker.connectionPool.port,\n            rack: broker.connectionPool.rack,\n          })\n\n          this.logger.error(`Failed to connect to broker, reconnecting`, { retryCount, retryTime })\n          throw new KafkaJSProtocolError(e, { retriable: true })\n        }\n\n        if (e.retriable) throw e\n        this.logger.error(e, { retryCount, retryTime, stack: e.stack })\n        bail(e)\n      }\n    })\n  }\n}\n", "/**\n * @template T\n * @param { (...args: any) => Promise<T> } [asyncFunction]\n * Promise returning function that will only ever be invoked sequentially.\n * @returns { (...args: any) => Promise<T> }\n * Function that may invoke asyncFunction if there is not a currently executing invocation.\n * Returns promise from the currently executing invocation.\n */\nmodule.exports = asyncFunction => {\n  let promise = null\n\n  return (...args) => {\n    if (promise == null) {\n      promise = asyncFunction(...args).finally(() => (promise = null))\n    }\n    return promise\n  }\n}\n", "/**\n * @param {Object} options\n * @param {import(\"../../types\").ISocketFactory} options.socketFactory\n * @param {string} options.host\n * @param {number} options.port\n * @param {Object} options.ssl\n * @param {() => void} options.onConnect\n * @param {(data: Buffer) => void} options.onData\n * @param {() => void} options.onEnd\n * @param {(err: Error) => void} options.onError\n * @param {() => void} options.onTimeout\n */\nmodule.exports = ({\n  socketFactory,\n  host,\n  port,\n  ssl,\n  onConnect,\n  onData,\n  onEnd,\n  onError,\n  onTimeout,\n}) => {\n  const socket = socketFactory({ host, port, ssl, onConnect })\n\n  socket.on('data', onData)\n  socket.on('end', onEnd)\n  socket.on('error', onError)\n  socket.on('timeout', onTimeout)\n\n  return socket\n}\n", "const Encoder = require('./encoder')\n\nmodule.exports = async ({ correlationId, clientId, request: { apiKey, apiVersion, encode } }) => {\n  const payload = await encode()\n  const requestPayload = new Encoder()\n    .writeInt16(apiKey)\n    .writeInt16(apiVersion)\n    .writeInt32(correlationId)\n    .writeString(clientId)\n    .writeEncoder(payload)\n\n  return new Encoder().writeInt32(requestPayload.size()).writeEncoder(requestPayload)\n}\n", "const EARLIEST_OFFSET = -2\nconst LATEST_OFFSET = -1\nconst INT_32_MAX_VALUE = Math.pow(2, 31) - 1\n\nmodule.exports = {\n  EARLIEST_OFFSET,\n  LATEST_OFFSET,\n  INT_32_MAX_VALUE,\n}\n", "module.exports = () => ({\n  KAFKAJS_DEBUG_PROTOCOL_BUFFERS: process.env.KAFKAJS_DEBUG_PROTOCOL_BUFFERS,\n  KAFKAJS_DEBUG_EXTENDED_PROTOCOL_BUFFERS: process.env.KAFKAJS_DEBUG_EXTENDED_PROTOCOL_BUFFERS,\n})\n", "/** @type {<T1 extends string>(namespace: T1) => <T2 extends string>(type: T2) => `${T1}.${T2}`} */\nmodule.exports = namespace => type => `${namespace}.${type}`\n", "const InstrumentationEventType = require('../instrumentation/eventType')\nconst eventType = InstrumentationEventType('network')\n\nmodule.exports = {\n  NETWORK_REQUEST: eventType('request'),\n  NETWORK_REQUEST_TIMEOUT: eventType('request_timeout'),\n  NETWORK_REQUEST_QUEUE_SIZE: eventType('request_queue_size'),\n}\n", "const { KafkaJSRequestTimeoutError, KafkaJSNonRetriableError } = require('../../errors')\nconst events = require('../instrumentationEvents')\n\nconst PRIVATE = {\n  STATE: Symbol('private:SocketRequest:state'),\n  EMIT_EVENT: Symbol('private:SocketRequest:emitEvent'),\n}\n\nconst REQUEST_STATE = {\n  PENDING: Symbol('PENDING'),\n  SENT: Symbol('SENT'),\n  COMPLETED: Symbol('COMPLETED'),\n  REJECTED: Symbol('REJECTED'),\n}\n\n/**\n * SocketRequest abstracts the life cycle of a socket request, making it easier to track\n * request durations and to have individual timeouts per request.\n *\n * @typedef {Object} SocketRequest\n * @property {number} createdAt\n * @property {number} sentAt\n * @property {number} pendingDuration\n * @property {number} duration\n * @property {number} requestTimeout\n * @property {string} broker\n * @property {string} clientId\n * @property {RequestEntry} entry\n * @property {boolean} expectResponse\n * @property {Function} send\n * @property {Function} timeout\n *\n * @typedef {Object} RequestEntry\n * @property {string} apiKey\n * @property {string} apiName\n * @property {number} apiVersion\n * @property {number} correlationId\n * @property {Function} resolve\n * @property {Function} reject\n */\nmodule.exports = class SocketRequest {\n  /**\n   * @param {Object} options\n   * @param {number} options.requestTimeout\n   * @param {string} options.broker - e.g: 127.0.0.1:9092\n   * @param {string} options.clientId\n   * @param {RequestEntry} options.entry\n   * @param {boolean} options.expectResponse\n   * @param {Function} options.send\n   * @param {() => void} options.timeout\n   * @param {import(\"../../instrumentation/emitter\")} [options.instrumentationEmitter=null]\n   */\n  constructor({\n    requestTimeout,\n    broker,\n    clientId,\n    entry,\n    expectResponse,\n    send,\n    timeout,\n    instrumentationEmitter = null,\n  }) {\n    this.createdAt = Date.now()\n    this.requestTimeout = requestTimeout\n    this.broker = broker\n    this.clientId = clientId\n    this.entry = entry\n    this.correlationId = entry.correlationId\n    this.expectResponse = expectResponse\n    this.sendRequest = send\n    this.timeoutHandler = timeout\n\n    this.sentAt = null\n    this.duration = null\n    this.pendingDuration = null\n\n    this[PRIVATE.STATE] = REQUEST_STATE.PENDING\n    this[PRIVATE.EMIT_EVENT] = (eventName, payload) =>\n      instrumentationEmitter && instrumentationEmitter.emit(eventName, payload)\n  }\n\n  send() {\n    this.throwIfInvalidState({\n      accepted: [REQUEST_STATE.PENDING],\n      next: REQUEST_STATE.SENT,\n    })\n\n    this.sendRequest()\n    this.sentAt = Date.now()\n    this.pendingDuration = this.sentAt - this.createdAt\n    this[PRIVATE.STATE] = REQUEST_STATE.SENT\n  }\n\n  timeoutRequest() {\n    const { apiName, apiKey, apiVersion } = this.entry\n    const requestInfo = `${apiName}(key: ${apiKey}, version: ${apiVersion})`\n    const eventData = {\n      broker: this.broker,\n      clientId: this.clientId,\n      correlationId: this.correlationId,\n      createdAt: this.createdAt,\n      sentAt: this.sentAt,\n      pendingDuration: this.pendingDuration,\n    }\n\n    this.timeoutHandler()\n    this.rejected(new KafkaJSRequestTimeoutError(`Request ${requestInfo} timed out`, eventData))\n    this[PRIVATE.EMIT_EVENT](events.NETWORK_REQUEST_TIMEOUT, {\n      ...eventData,\n      apiName,\n      apiKey,\n      apiVersion,\n    })\n  }\n\n  completed({ size, payload }) {\n    this.throwIfInvalidState({\n      accepted: [REQUEST_STATE.SENT],\n      next: REQUEST_STATE.COMPLETED,\n    })\n\n    const { entry, correlationId, broker, clientId, createdAt, sentAt, pendingDuration } = this\n\n    this[PRIVATE.STATE] = REQUEST_STATE.COMPLETED\n    this.duration = Date.now() - this.sentAt\n    entry.resolve({ correlationId, entry, size, payload })\n\n    this[PRIVATE.EMIT_EVENT](events.NETWORK_REQUEST, {\n      broker,\n      clientId,\n      correlationId,\n      size,\n      createdAt,\n      sentAt,\n      pendingDuration,\n      duration: this.duration,\n      apiName: entry.apiName,\n      apiKey: entry.apiKey,\n      apiVersion: entry.apiVersion,\n    })\n  }\n\n  rejected(error) {\n    this.throwIfInvalidState({\n      accepted: [REQUEST_STATE.PENDING, REQUEST_STATE.SENT],\n      next: REQUEST_STATE.REJECTED,\n    })\n\n    this[PRIVATE.STATE] = REQUEST_STATE.REJECTED\n    this.duration = Date.now() - this.sentAt\n    this.entry.reject(error)\n  }\n\n  /**\n   * @private\n   */\n  throwIfInvalidState({ accepted, next }) {\n    if (accepted.includes(this[PRIVATE.STATE])) {\n      return\n    }\n\n    const current = this[PRIVATE.STATE].toString()\n\n    throw new KafkaJSNonRetriableError(\n      `Invalid state, can't transition from ${current} to ${next.toString()}`\n    )\n  }\n}\n", "const { EventEmitter } = require('events')\nconst SocketRequest = require('./socketRequest')\nconst events = require('../instrumentationEvents')\nconst { KafkaJSInvariantViolation } = require('../../errors')\n\nconst PRIVATE = {\n  EMIT_QUEUE_SIZE_EVENT: Symbol('private:RequestQueue:emitQueueSizeEvent'),\n  EMIT_REQUEST_QUEUE_EMPTY: Symbol('private:RequestQueue:emitQueueEmpty'),\n}\n\nconst REQUEST_QUEUE_EMPTY = 'requestQueueEmpty'\nconst CHECK_PENDING_REQUESTS_INTERVAL = 10\n\nmodule.exports = class RequestQueue extends EventEmitter {\n  /**\n   * @param {Object} options\n   * @param {number} options.maxInFlightRequests\n   * @param {number} options.requestTimeout\n   * @param {boolean} options.enforceRequestTimeout\n   * @param {string} options.clientId\n   * @param {string} options.broker\n   * @param {import(\"../../../types\").Logger} options.logger\n   * @param {import(\"../../instrumentation/emitter\")} [options.instrumentationEmitter=null]\n   * @param {() => boolean} [options.isConnected]\n   */\n  constructor({\n    instrumentationEmitter = null,\n    maxInFlightRequests,\n    requestTimeout,\n    enforceRequestTimeout,\n    clientId,\n    broker,\n    logger,\n    isConnected = () => true,\n  }) {\n    super()\n    this.instrumentationEmitter = instrumentationEmitter\n    this.maxInFlightRequests = maxInFlightRequests\n    this.requestTimeout = requestTimeout\n    this.enforceRequestTimeout = enforceRequestTimeout\n    this.clientId = clientId\n    this.broker = broker\n    this.logger = logger\n    this.isConnected = isConnected\n\n    this.inflight = new Map()\n    this.pending = []\n\n    /**\n     * Until when this request queue is throttled and shouldn't send requests\n     *\n     * The value represents the timestamp of the end of the throttling in ms-since-epoch. If the value\n     * is smaller than the current timestamp no throttling is active.\n     *\n     * @type {number}\n     */\n    this.throttledUntil = -1\n\n    /**\n     * Timeout id if we have scheduled a check for pending requests due to client-side throttling\n     *\n     * @type {null|NodeJS.Timeout}\n     */\n    this.throttleCheckTimeoutId = null\n\n    this[PRIVATE.EMIT_REQUEST_QUEUE_EMPTY] = () => {\n      if (this.pending.length === 0 && this.inflight.size === 0) {\n        this.emit(REQUEST_QUEUE_EMPTY)\n      }\n    }\n\n    this[PRIVATE.EMIT_QUEUE_SIZE_EVENT] = () => {\n      instrumentationEmitter &&\n        instrumentationEmitter.emit(events.NETWORK_REQUEST_QUEUE_SIZE, {\n          broker: this.broker,\n          clientId: this.clientId,\n          queueSize: this.pending.length,\n        })\n\n      this[PRIVATE.EMIT_REQUEST_QUEUE_EMPTY]()\n    }\n  }\n\n  /**\n   * @public\n   */\n  scheduleRequestTimeoutCheck() {\n    if (this.enforceRequestTimeout) {\n      this.destroy()\n\n      this.requestTimeoutIntervalId = setInterval(() => {\n        this.inflight.forEach(request => {\n          if (Date.now() - request.sentAt > request.requestTimeout) {\n            request.timeoutRequest()\n          }\n        })\n\n        if (!this.isConnected()) {\n          this.destroy()\n        }\n      }, Math.min(this.requestTimeout, 100))\n    }\n  }\n\n  maybeThrottle(clientSideThrottleTime) {\n    if (clientSideThrottleTime !== null && clientSideThrottleTime > 0) {\n      this.logger.debug(`Client side throttling in effect for ${clientSideThrottleTime}ms`)\n      const minimumThrottledUntil = Date.now() + clientSideThrottleTime\n      this.throttledUntil = Math.max(minimumThrottledUntil, this.throttledUntil)\n    }\n  }\n\n  createSocketRequest(pushedRequest) {\n    const { correlationId } = pushedRequest.entry\n    const defaultRequestTimeout = this.requestTimeout\n    const customRequestTimeout = pushedRequest.requestTimeout\n\n    // Some protocol requests have custom request timeouts (e.g JoinGroup, Fetch, etc). The custom\n    // timeouts are influenced by user configurations, which can be lower than the default requestTimeout\n    const requestTimeout = Math.max(defaultRequestTimeout, customRequestTimeout || 0)\n\n    const socketRequest = new SocketRequest({\n      entry: pushedRequest.entry,\n      expectResponse: pushedRequest.expectResponse,\n      broker: this.broker,\n      clientId: this.clientId,\n      instrumentationEmitter: this.instrumentationEmitter,\n      requestTimeout,\n      send: () => {\n        if (this.inflight.has(correlationId)) {\n          throw new KafkaJSInvariantViolation('Correlation id already exists')\n        }\n        this.inflight.set(correlationId, socketRequest)\n        pushedRequest.sendRequest()\n      },\n      timeout: () => {\n        this.inflight.delete(correlationId)\n        this.checkPendingRequests()\n        // Try to emit REQUEST_QUEUE_EMPTY. Otherwise, waitForPendingRequests may stuck forever\n        this[PRIVATE.EMIT_REQUEST_QUEUE_EMPTY]()\n      },\n    })\n\n    return socketRequest\n  }\n\n  /**\n   * @typedef {Object} PushedRequest\n   * @property {import(\"./socketRequest\").RequestEntry} entry\n   * @property {boolean} expectResponse\n   * @property {Function} sendRequest\n   * @property {number} [requestTimeout]\n   *\n   * @public\n   * @param {PushedRequest} pushedRequest\n   */\n  push(pushedRequest) {\n    const { correlationId } = pushedRequest.entry\n    const socketRequest = this.createSocketRequest(pushedRequest)\n\n    if (this.canSendSocketRequestImmediately()) {\n      this.sendSocketRequest(socketRequest)\n      return\n    }\n\n    this.pending.push(socketRequest)\n    this.scheduleCheckPendingRequests()\n\n    this.logger.debug(`Request enqueued`, {\n      clientId: this.clientId,\n      broker: this.broker,\n      correlationId,\n    })\n\n    this[PRIVATE.EMIT_QUEUE_SIZE_EVENT]()\n  }\n\n  /**\n   * @param {SocketRequest} socketRequest\n   */\n  sendSocketRequest(socketRequest) {\n    socketRequest.send()\n\n    if (!socketRequest.expectResponse) {\n      this.logger.debug(`Request does not expect a response, resolving immediately`, {\n        clientId: this.clientId,\n        broker: this.broker,\n        correlationId: socketRequest.correlationId,\n      })\n\n      this.inflight.delete(socketRequest.correlationId)\n      socketRequest.completed({ size: 0, payload: null })\n    }\n  }\n\n  /**\n   * @public\n   * @param {object} response\n   * @param {number} response.correlationId\n   * @param {Buffer} response.payload\n   * @param {number} response.size\n   */\n  fulfillRequest({ correlationId, payload, size }) {\n    const socketRequest = this.inflight.get(correlationId)\n    this.inflight.delete(correlationId)\n    this.checkPendingRequests()\n\n    if (socketRequest) {\n      socketRequest.completed({ size, payload })\n    } else {\n      this.logger.warn(`Response without match`, {\n        clientId: this.clientId,\n        broker: this.broker,\n        correlationId,\n      })\n    }\n\n    this[PRIVATE.EMIT_REQUEST_QUEUE_EMPTY]()\n  }\n\n  /**\n   * @public\n   * @param {Error} error\n   */\n  rejectAll(error) {\n    const requests = [...this.inflight.values(), ...this.pending]\n\n    for (const socketRequest of requests) {\n      socketRequest.rejected(error)\n      this.inflight.delete(socketRequest.correlationId)\n    }\n\n    this.pending = []\n    this.inflight.clear()\n    this[PRIVATE.EMIT_QUEUE_SIZE_EVENT]()\n  }\n\n  /**\n   * @public\n   */\n  waitForPendingRequests() {\n    return new Promise(resolve => {\n      if (this.pending.length === 0 && this.inflight.size === 0) {\n        return resolve()\n      }\n\n      this.logger.debug('Waiting for pending requests', {\n        clientId: this.clientId,\n        broker: this.broker,\n        currentInflightRequests: this.inflight.size,\n        currentPendingQueueSize: this.pending.length,\n      })\n\n      this.once(REQUEST_QUEUE_EMPTY, () => resolve())\n    })\n  }\n\n  /**\n   * @public\n   */\n  destroy() {\n    clearInterval(this.requestTimeoutIntervalId)\n    clearTimeout(this.throttleCheckTimeoutId)\n    this.throttleCheckTimeoutId = null\n  }\n\n  canSendSocketRequestImmediately() {\n    const shouldEnqueue =\n      (this.maxInFlightRequests != null && this.inflight.size >= this.maxInFlightRequests) ||\n      this.throttledUntil > Date.now()\n\n    return !shouldEnqueue\n  }\n\n  /**\n   * Check and process pending requests either now or in the future\n   *\n   * This function will send out as many pending requests as possible taking throttling and\n   * in-flight limits into account.\n   */\n  checkPendingRequests() {\n    while (this.pending.length > 0 && this.canSendSocketRequestImmediately()) {\n      const pendingRequest = this.pending.shift() // first in first out\n      this.sendSocketRequest(pendingRequest)\n\n      this.logger.debug(`Consumed pending request`, {\n        clientId: this.clientId,\n        broker: this.broker,\n        correlationId: pendingRequest.correlationId,\n        pendingDuration: pendingRequest.pendingDuration,\n        currentPendingQueueSize: this.pending.length,\n      })\n\n      this[PRIVATE.EMIT_QUEUE_SIZE_EVENT]()\n    }\n\n    this.scheduleCheckPendingRequests()\n  }\n\n  /**\n   * Ensure that pending requests will be checked in the future\n   *\n   * If there is a client-side throttling in place this will ensure that we will check\n   * the pending request queue eventually.\n   */\n  scheduleCheckPendingRequests() {\n    // If we're throttled: Schedule checkPendingRequests when the throttle\n    // should be resolved. If there is already something scheduled we assume that that\n    // will be fine, and potentially fix up a new timeout if needed at that time.\n    // Note that if we're merely \"overloaded\" by having too many inflight requests\n    // we will anyways check the queue when one of them gets fulfilled.\n    let scheduleAt = this.throttledUntil - Date.now()\n    if (!this.throttleCheckTimeoutId) {\n      if (this.pending.length > 0) {\n        scheduleAt = scheduleAt > 0 ? scheduleAt : CHECK_PENDING_REQUESTS_INTERVAL\n      }\n      this.throttleCheckTimeoutId = setTimeout(() => {\n        this.throttleCheckTimeoutId = null\n        this.checkPendingRequests()\n      }, scheduleAt)\n    }\n  }\n}\n", "const CONNECTION_STATUS = {\n  CONNECTED: 'connected',\n  DISCONNECTING: 'disconnecting',\n  DISCONNECTED: 'disconnected',\n}\n\nconst CONNECTED_STATUS = [CONNECTION_STATUS.CONNECTED, CONNECTION_STATUS.DISCONNECTING]\n\nmodule.exports = {\n  CONNECTION_STATUS,\n  CONNECTED_STATUS,\n}\n", "/**\n * http://www.ietf.org/rfc/rfc2595.txt\n *\n * The mechanism consists of a single message from the client to the\n * server.  The client sends the authorization identity (identity to\n * login as), followed by a US-ASCII NUL character, followed by the\n * authentication identity (identity whose password will be used),\n * followed by a US-ASCII NUL character, followed by the clear-text\n * password.  The client may leave the authorization identity empty to\n * indicate that it is the same as the authentication identity.\n *\n * The server will verify the authentication identity and password with\n * the system authentication database and verify that the authentication\n * credentials permit the client to login as the authorization identity.\n * If both steps succeed, the user is logged in.\n */\n\nconst Encoder = require('../../encoder')\n\nconst US_ASCII_NULL_CHAR = '\\u0000'\n\nmodule.exports = ({ authorizationIdentity = null, username, password }) => ({\n  encode: async () => {\n    return new Encoder().writeBytes(\n      [authorizationIdentity, username, password].join(US_ASCII_NULL_CHAR)\n    ).buffer\n  },\n})\n", "module.exports = {\n  decode: async () => true,\n  parse: async () => true,\n}\n", "module.exports = {\n  request: require('./request'),\n  response: require('./response'),\n}\n", "const { request, response } = require('../../protocol/sasl/plain')\nconst { KafkaJSSASLAuthenticationError } = require('../../errors')\n\nconst plainAuthenticatorProvider = sasl => ({ host, port, logger, saslAuthenticate }) => {\n  return {\n    authenticate: async () => {\n      if (sasl.username == null || sasl.password == null) {\n        throw new KafkaJSSASLAuthenticationError('SASL Plain: Invalid username or password')\n      }\n\n      const broker = `${host}:${port}`\n\n      try {\n        logger.debug('Authenticate with SASL PLAIN', { broker })\n        await saslAuthenticate({ request: request(sasl), response })\n        logger.debug('SASL PLAIN authentication successful', { broker })\n      } catch (e) {\n        const error = new KafkaJSSASLAuthenticationError(\n          `SASL PLAIN authentication failed: ${e.message}`\n        )\n        logger.error(error.message, { broker })\n        throw error\n      }\n    },\n  }\n}\n\nmodule.exports = plainAuthenticatorProvider\n", "/**\n * https://tools.ietf.org/html/rfc5802\n *\n * First, the client sends the \"client-first-message\" containing:\n *\n *  -> a GS2 header consisting of a flag indicating whether channel\n * binding is supported-but-not-used, not supported, or used, and an\n * optional SASL authorization identity;\n *\n *  -> SCRAM username and a random, unique nonce attributes.\n *\n * Note that the client's first message will always start with \"n\", \"y\",\n * or \"p\"; otherwise, the message is invalid and authentication MUST\n * fail.  This is important, as it allows for GS2 extensibility (e.g.,\n * to add support for security layers).\n */\n\nconst Encoder = require('../../../encoder')\n\nmodule.exports = ({ clientFirstMessage }) => ({\n  encode: async () => new Encoder().writeBytes(clientFirstMessage).buffer,\n})\n", "/* eslint no-unused-vars: [\"error\", { \"varsIgnorePattern\": \"_\" }] */\n\nconst Decoder = require('../../../decoder')\n\nconst ENTRY_REGEX = /^([rsiev])=(.*)$/\n\nmodule.exports = {\n  decode: async rawData => {\n    return new Decoder(rawData).readBytes()\n  },\n  parse: async data => {\n    const processed = data\n      .toString()\n      .split(',')\n      .map(str => {\n        const [_, key, value] = str.match(ENTRY_REGEX)\n        return [key, value]\n      })\n      .reduce((obj, entry) => ({ ...obj, [entry[0]]: entry[1] }), {})\n\n    return { original: data.toString(), ...processed }\n  },\n}\n", "const Encoder = require('../../../encoder')\n\nmodule.exports = ({ finalMessage }) => ({\n  encode: async () => new Encoder().writeBytes(finalMessage).buffer,\n})\n", "module.exports = require('../firstMessage/response')\n", "module.exports = {\n  firstMessage: {\n    request: require('./firstMessage/request'),\n    response: require('./firstMessage/response'),\n  },\n  finalMessage: {\n    request: require('./finalMessage/request'),\n    response: require('./finalMessage/response'),\n  },\n}\n", "const crypto = require('crypto')\nconst scram = require('../../protocol/sasl/scram')\nconst { KafkaJSSASLAuthenticationError, KafkaJSNonRetriableError } = require('../../errors')\n\nconst GS2_HEADER = 'n,,'\n\nconst EQUAL_SIGN_REGEX = /=/g\nconst COMMA_SIGN_REGEX = /,/g\n\nconst URLSAFE_BASE64_PLUS_REGEX = /\\+/g\nconst URLSAFE_BASE64_SLASH_REGEX = /\\//g\nconst URLSAFE_BASE64_TRAILING_EQUAL_REGEX = /=+$/\n\nconst HMAC_CLIENT_KEY = 'Client Key'\nconst HMAC_SERVER_KEY = 'Server Key'\n\nconst DIGESTS = {\n  SHA256: {\n    length: 32,\n    type: 'sha256',\n    minIterations: 4096,\n  },\n  SHA512: {\n    length: 64,\n    type: 'sha512',\n    minIterations: 4096,\n  },\n}\n\nconst encode64 = str => Buffer.from(str).toString('base64')\n\nclass SCRAM {\n  /**\n   * From https://tools.ietf.org/html/rfc5802#section-5.1\n   *\n   * The characters ',' or '=' in usernames are sent as '=2C' and\n   * '=3D' respectively.  If the server receives a username that\n   * contains '=' not followed by either '2C' or '3D', then the\n   * server MUST fail the authentication.\n   *\n   * @returns {String}\n   */\n  static sanitizeString(str) {\n    return str.replace(EQUAL_SIGN_REGEX, '=3D').replace(COMMA_SIGN_REGEX, '=2C')\n  }\n\n  /**\n   * In cryptography, a nonce is an arbitrary number that can be used just once.\n   * It is similar in spirit to a nonce * word, hence the name. It is often a random or pseudo-random\n   * number issued in an authentication protocol to * ensure that old communications cannot be reused\n   * in replay attacks.\n   *\n   * @returns {String}\n   */\n  static nonce() {\n    return crypto\n      .randomBytes(16)\n      .toString('base64')\n      .replace(URLSAFE_BASE64_PLUS_REGEX, '-') // make it url safe\n      .replace(URLSAFE_BASE64_SLASH_REGEX, '_')\n      .replace(URLSAFE_BASE64_TRAILING_EQUAL_REGEX, '')\n      .toString('ascii')\n  }\n\n  /**\n   * Hi() is, essentially, PBKDF2 [RFC2898] with HMAC() as the\n   * pseudorandom function (PRF) and with dkLen == output length of\n   * HMAC() == output length of H()\n   *\n   * @returns {Promise<Buffer>}\n   */\n  static hi(password, salt, iterations, digestDefinition) {\n    return new Promise((resolve, reject) => {\n      crypto.pbkdf2(\n        password,\n        salt,\n        iterations,\n        digestDefinition.length,\n        digestDefinition.type,\n        (err, derivedKey) => (err ? reject(err) : resolve(derivedKey))\n      )\n    })\n  }\n\n  /**\n   * Apply the exclusive-or operation to combine the octet string\n   * on the left of this operator with the octet string on the right of\n   * this operator.  The length of the output and each of the two\n   * inputs will be the same for this use\n   *\n   * @returns {Buffer}\n   */\n  static xor(left, right) {\n    const bufferA = Buffer.from(left)\n    const bufferB = Buffer.from(right)\n    const length = Buffer.byteLength(bufferA)\n\n    if (length !== Buffer.byteLength(bufferB)) {\n      throw new KafkaJSNonRetriableError('Buffers must be of the same length')\n    }\n\n    const result = []\n    for (let i = 0; i < length; i++) {\n      result.push(bufferA[i] ^ bufferB[i])\n    }\n\n    return Buffer.from(result)\n  }\n\n  /**\n   * @param {SASLOptions} sasl\n   * @param {Logger} logger\n   * @param {Function} saslAuthenticate\n   * @param {DigestDefinition} digestDefinition\n   */\n  constructor(sasl, host, port, logger, saslAuthenticate, digestDefinition) {\n    this.sasl = sasl\n    this.host = host\n    this.port = port\n    this.logger = logger\n    this.saslAuthenticate = saslAuthenticate\n    this.digestDefinition = digestDefinition\n\n    const digestType = digestDefinition.type.toUpperCase()\n    this.PREFIX = `SASL SCRAM ${digestType} authentication`\n\n    this.currentNonce = SCRAM.nonce()\n  }\n\n  async authenticate() {\n    const { PREFIX } = this\n    const broker = `${this.host}:${this.port}`\n\n    if (this.sasl.username == null || this.sasl.password == null) {\n      throw new KafkaJSSASLAuthenticationError(`${this.PREFIX}: Invalid username or password`)\n    }\n\n    try {\n      this.logger.debug('Exchanging first client message', { broker })\n      const clientMessageResponse = await this.sendClientFirstMessage()\n\n      this.logger.debug('Sending final message', { broker })\n      const finalResponse = await this.sendClientFinalMessage(clientMessageResponse)\n\n      if (finalResponse.e) {\n        throw new Error(finalResponse.e)\n      }\n\n      const serverKey = await this.serverKey(clientMessageResponse)\n      const serverSignature = this.serverSignature(serverKey, clientMessageResponse)\n\n      if (finalResponse.v !== serverSignature) {\n        throw new Error('Invalid server signature in server final message')\n      }\n\n      this.logger.debug(`${PREFIX} successful`, { broker })\n    } catch (e) {\n      const error = new KafkaJSSASLAuthenticationError(`${PREFIX} failed: ${e.message}`)\n      this.logger.error(error.message, { broker })\n      throw error\n    }\n  }\n\n  /**\n   * @private\n   */\n  async sendClientFirstMessage() {\n    const clientFirstMessage = `${GS2_HEADER}${this.firstMessageBare()}`\n    const request = scram.firstMessage.request({ clientFirstMessage })\n    const response = scram.firstMessage.response\n\n    return this.saslAuthenticate({\n      request,\n      response,\n    })\n  }\n\n  /**\n   * @private\n   */\n  async sendClientFinalMessage(clientMessageResponse) {\n    const { PREFIX } = this\n    const iterations = parseInt(clientMessageResponse.i, 10)\n    const { minIterations } = this.digestDefinition\n\n    if (!clientMessageResponse.r.startsWith(this.currentNonce)) {\n      throw new KafkaJSSASLAuthenticationError(\n        `${PREFIX} failed: Invalid server nonce, it does not start with the client nonce`\n      )\n    }\n\n    if (iterations < minIterations) {\n      throw new KafkaJSSASLAuthenticationError(\n        `${PREFIX} failed: Requested iterations ${iterations} is less than the minimum ${minIterations}`\n      )\n    }\n\n    const finalMessageWithoutProof = this.finalMessageWithoutProof(clientMessageResponse)\n    const clientProof = await this.clientProof(clientMessageResponse)\n    const finalMessage = `${finalMessageWithoutProof},p=${clientProof}`\n    const request = scram.finalMessage.request({ finalMessage })\n    const response = scram.finalMessage.response\n\n    return this.saslAuthenticate({\n      request,\n      response,\n    })\n  }\n\n  /**\n   * @private\n   */\n  async clientProof(clientMessageResponse) {\n    const clientKey = await this.clientKey(clientMessageResponse)\n    const storedKey = this.H(clientKey)\n    const clientSignature = this.clientSignature(storedKey, clientMessageResponse)\n    return encode64(SCRAM.xor(clientKey, clientSignature))\n  }\n\n  /**\n   * @private\n   */\n  async clientKey(clientMessageResponse) {\n    const saltedPassword = await this.saltPassword(clientMessageResponse)\n    return this.HMAC(saltedPassword, HMAC_CLIENT_KEY)\n  }\n\n  /**\n   * @private\n   */\n  async serverKey(clientMessageResponse) {\n    const saltedPassword = await this.saltPassword(clientMessageResponse)\n    return this.HMAC(saltedPassword, HMAC_SERVER_KEY)\n  }\n\n  /**\n   * @private\n   */\n  clientSignature(storedKey, clientMessageResponse) {\n    return this.HMAC(storedKey, this.authMessage(clientMessageResponse))\n  }\n\n  /**\n   * @private\n   */\n  serverSignature(serverKey, clientMessageResponse) {\n    return encode64(this.HMAC(serverKey, this.authMessage(clientMessageResponse)))\n  }\n\n  /**\n   * @private\n   */\n  authMessage(clientMessageResponse) {\n    return [\n      this.firstMessageBare(),\n      clientMessageResponse.original,\n      this.finalMessageWithoutProof(clientMessageResponse),\n    ].join(',')\n  }\n\n  /**\n   * @private\n   */\n  async saltPassword(clientMessageResponse) {\n    const salt = Buffer.from(clientMessageResponse.s, 'base64')\n    const iterations = parseInt(clientMessageResponse.i, 10)\n    return SCRAM.hi(this.encodedPassword(), salt, iterations, this.digestDefinition)\n  }\n\n  /**\n   * @private\n   */\n  firstMessageBare() {\n    return `n=${this.encodedUsername()},r=${this.currentNonce}`\n  }\n\n  /**\n   * @private\n   */\n  finalMessageWithoutProof(clientMessageResponse) {\n    const rnonce = clientMessageResponse.r\n    return `c=${encode64(GS2_HEADER)},r=${rnonce}`\n  }\n\n  /**\n   * @private\n   */\n  encodedUsername() {\n    const { username } = this.sasl\n    return SCRAM.sanitizeString(username).toString('utf-8')\n  }\n\n  /**\n   * @private\n   */\n  encodedPassword() {\n    const { password } = this.sasl\n    return password.toString('utf-8')\n  }\n\n  /**\n   * @private\n   */\n  H(data) {\n    return crypto\n      .createHash(this.digestDefinition.type)\n      .update(data)\n      .digest()\n  }\n\n  /**\n   * @private\n   */\n  HMAC(key, data) {\n    return crypto\n      .createHmac(this.digestDefinition.type, key)\n      .update(data)\n      .digest()\n  }\n}\n\nmodule.exports = {\n  DIGESTS,\n  SCRAM,\n}\n", "const { SCRAM, DIGESTS } = require('./scram')\n\nconst scram256AuthenticatorProvider = sasl => ({ host, port, logger, saslAuthenticate }) => {\n  const scram = new SCRAM(sasl, host, port, logger, saslAuthenticate, DIGESTS.SHA256)\n  return {\n    authenticate: async () => await scram.authenticate(),\n  }\n}\n\nmodule.exports = scram256AuthenticatorProvider\n", "const { SCRAM, DIGESTS } = require('./scram')\n\nconst scram512AuthenticatorProvider = sasl => ({ host, port, logger, saslAuthenticate }) => {\n  const scram = new SCRAM(sasl, host, port, logger, saslAuthenticate, DIGESTS.SHA512)\n  return {\n    authenticate: async () => await scram.authenticate(),\n  }\n}\n\nmodule.exports = scram512AuthenticatorProvider\n", "const Encoder = require('../../encoder')\n\nconst US_ASCII_NULL_CHAR = '\\u0000'\n\nmodule.exports = ({ authorizationIdentity, accessKeyId, secretAccessKey, sessionToken = '' }) => ({\n  encode: async () => {\n    return new Encoder().writeBytes(\n      [authorizationIdentity, accessKeyId, secretAccessKey, sessionToken].join(US_ASCII_NULL_CHAR)\n    ).buffer\n  },\n})\n", "module.exports = {\n  decode: async () => true,\n  parse: async () => true,\n}\n", "module.exports = {\n  request: require('./request'),\n  response: require('./response'),\n}\n", "const { request, response } = require('../../protocol/sasl/awsIam')\nconst { KafkaJSSASLAuthenticationError } = require('../../errors')\n\nconst awsIAMAuthenticatorProvider = sasl => ({ host, port, logger, saslAuthenticate }) => {\n  return {\n    authenticate: async () => {\n      if (!sasl.authorizationIdentity) {\n        throw new KafkaJSSASLAuthenticationError('SASL AWS-IAM: Missing authorizationIdentity')\n      }\n      if (!sasl.accessKeyId) {\n        throw new KafkaJSSASLAuthenticationError('SASL AWS-IAM: Missing accessKeyId')\n      }\n      if (!sasl.secretAccessKey) {\n        throw new KafkaJSSASLAuthenticationError('SASL AWS-IAM: Missing secretAccessKey')\n      }\n      if (!sasl.sessionToken) {\n        sasl.sessionToken = ''\n      }\n\n      const broker = `${host}:${port}`\n\n      try {\n        logger.debug('Authenticate with SASL AWS-IAM', { broker })\n        await saslAuthenticate({ request: request(sasl), response })\n        logger.debug('SASL AWS-IAM authentication successful', { broker })\n      } catch (e) {\n        const error = new KafkaJSSASLAuthenticationError(\n          `SASL AWS-IAM authentication failed: ${e.message}`\n        )\n        logger.error(error.message, { broker })\n        throw error\n      }\n    },\n  }\n}\n\nmodule.exports = awsIAMAuthenticatorProvider\n", "/**\n * http://www.ietf.org/rfc/rfc5801.txt\n *\n * See org.apache.kafka.common.security.oauthbearer.internals.OAuthBearerClientInitialResponse\n * for official Java client implementation.\n *\n * The mechanism consists of a message from the client to the server.\n * The client sends the \"n,\"\" GS header, followed by the authorizationIdentitty\n * prefixed by \"a=\" (if present), followed by \",\", followed by a US-ASCII SOH\n * character, followed by \"auth=Bearer \", followed by the token value, followed\n * by US-ASCII SOH character, followed by SASL extensions in OAuth \"friendly\"\n * format and then closed by two additionals US-ASCII SOH characters.\n *\n * SASL extensions are optional an must be expressed as key-value pairs in an\n * object. Each expression is converted as, the extension entry key, followed\n * by \"=\", followed by extension entry value. Each extension is separated by a\n * US-ASCII SOH character. If extensions are not present, their relative part\n * in the message, including the US-ASCII SOH character, is omitted.\n *\n * The client may leave the authorization identity empty to\n * indicate that it is the same as the authentication identity.\n *\n * The server will verify the authentication token and verify that the\n * authentication credentials permit the client to login as the authorization\n * identity. If both steps succeed, the user is logged in.\n */\n\nconst Encoder = require('../../encoder')\n\nconst SEPARATOR = '\\u0001' // SOH - Start Of Header ASCII\n\nfunction formatExtensions(extensions) {\n  let msg = ''\n\n  if (extensions == null) {\n    return msg\n  }\n\n  let prefix = ''\n  for (const k in extensions) {\n    msg += `${prefix}${k}=${extensions[k]}`\n    prefix = SEPARATOR\n  }\n\n  return msg\n}\n\nmodule.exports = async ({ authorizationIdentity = null }, oauthBearerToken) => {\n  const authzid = authorizationIdentity == null ? '' : `\"a=${authorizationIdentity}`\n  let ext = formatExtensions(oauthBearerToken.extensions)\n  if (ext.length > 0) {\n    ext = `${SEPARATOR}${ext}`\n  }\n\n  const oauthMsg = `n,${authzid},${SEPARATOR}auth=Bearer ${oauthBearerToken.value}${ext}${SEPARATOR}${SEPARATOR}`\n\n  return {\n    encode: async () => {\n      return new Encoder().writeBytes(Buffer.from(oauthMsg)).buffer\n    },\n  }\n}\n", "module.exports = {\n  decode: async () => true,\n  parse: async () => true,\n}\n", "module.exports = {\n  request: require('./request'),\n  response: require('./response'),\n}\n", "/**\n * The sasl object must include a property named oauthBearerProvider, an\n * async function that is used to return the OAuth bearer token.\n *\n * The OAuth bearer token must be an object with properties value and\n * (optionally) extensions, that will be sent during the SASL/OAUTHBEARER\n * request.\n *\n * The implementation of the oauthBearerProvider must take care that tokens are\n * reused and refreshed when appropriate.\n */\n\nconst { request } = require('../../protocol/sasl/oauthBearer')\nconst { KafkaJSSASLAuthenticationError } = require('../../errors')\n\nconst oauthBearerAuthenticatorProvider = sasl => ({ host, port, logger, saslAuthenticate }) => {\n  return {\n    authenticate: async () => {\n      const { oauthBearerProvider } = sasl\n\n      if (oauthBearerProvider == null) {\n        throw new KafkaJSSASLAuthenticationError(\n          'SASL OAUTHBEARER: Missing OAuth bearer token provider'\n        )\n      }\n\n      const oauthBearerToken = await oauthBearerProvider()\n\n      if (oauthBearerToken.value == null) {\n        throw new KafkaJSSASLAuthenticationError('SASL OAUTHBEARER: Invalid OAuth bearer token')\n      }\n\n      const broker = `${host}:${port}`\n\n      try {\n        logger.debug('Authenticate with SASL OAUTHBEARER', { broker })\n        await saslAuthenticate({ request: await request(sasl, oauthBearerToken) })\n        logger.debug('SASL OAUTHBEARER authentication successful', { broker })\n      } catch (e) {\n        const error = new KafkaJSSASLAuthenticationError(\n          `SASL OAUTHBEARER authentication failed: ${e.message}`\n        )\n        logger.error(error.message, { broker })\n        throw error\n      }\n    },\n  }\n}\n\nmodule.exports = oauthBearerAuthenticatorProvider\n", "const { requests, lookup } = require('../../protocol/requests')\nconst apiKeys = require('../../protocol/requests/apiKeys')\nconst plainAuthenticatorProvider = require('./plain')\nconst scram256AuthenticatorProvider = require('./scram256')\nconst scram512AuthenticatorProvider = require('./scram512')\nconst awsIAMAuthenticatorProvider = require('./awsIam')\nconst oauthBearerAuthenticatorProvider = require('./oauthBearer')\nconst { KafkaJSSASLAuthenticationError } = require('../../errors')\n\nconst BUILT_IN_AUTHENTICATION_PROVIDERS = {\n  AWS: awsIAMAuthenticatorProvider,\n  PLAIN: plainAuthenticatorProvider,\n  OAUTHBEARER: oauthBearerAuthenticatorProvider,\n  'SCRAM-SHA-256': scram256AuthenticatorProvider,\n  'SCRAM-SHA-512': scram512AuthenticatorProvider,\n}\n\nconst UNLIMITED_SESSION_LIFETIME = '0'\n\nmodule.exports = class SASLAuthenticator {\n  constructor(connection, logger, versions, supportAuthenticationProtocol) {\n    this.connection = connection\n    this.logger = logger\n    this.sessionLifetime = UNLIMITED_SESSION_LIFETIME\n\n    const lookupRequest = lookup(versions)\n    this.saslHandshake = lookupRequest(apiKeys.SaslHandshake, requests.SaslHandshake)\n    this.protocolAuthentication = supportAuthenticationProtocol\n      ? lookupRequest(apiKeys.SaslAuthenticate, requests.SaslAuthenticate)\n      : null\n  }\n\n  async authenticate() {\n    const mechanism = this.connection.sasl.mechanism.toUpperCase()\n    const handshake = await this.connection.send(this.saslHandshake({ mechanism }))\n    if (!handshake.enabledMechanisms.includes(mechanism)) {\n      throw new KafkaJSSASLAuthenticationError(\n        `SASL ${mechanism} mechanism is not supported by the server`\n      )\n    }\n\n    const saslAuthenticate = async ({ request, response }) => {\n      if (this.protocolAuthentication) {\n        const requestAuthBytes = await request.encode()\n        const authResponse = await this.connection.send(\n          this.protocolAuthentication({ authBytes: requestAuthBytes })\n        )\n\n        // `0` is a string because `sessionLifetimeMs` is an int64 encoded as string.\n        // This is not present in SaslAuthenticateV0, so we default to `\"0\"`\n        this.sessionLifetime = authResponse.sessionLifetimeMs || UNLIMITED_SESSION_LIFETIME\n\n        if (!response) {\n          return\n        }\n\n        const { authBytes: responseAuthBytes } = authResponse\n        const payloadDecoded = await response.decode(responseAuthBytes)\n        return response.parse(payloadDecoded)\n      }\n\n      return this.connection.sendAuthRequest({ request, response })\n    }\n\n    if (\n      !this.connection.sasl.authenticationProvider &&\n      Object.keys(BUILT_IN_AUTHENTICATION_PROVIDERS).includes(mechanism)\n    ) {\n      this.connection.sasl.authenticationProvider = BUILT_IN_AUTHENTICATION_PROVIDERS[mechanism](\n        this.connection.sasl\n      )\n    }\n    await this.connection.sasl\n      .authenticationProvider({\n        host: this.connection.host,\n        port: this.connection.port,\n        logger: this.logger.namespace(`SaslAuthenticator-${mechanism}`),\n        saslAuthenticate,\n      })\n      .authenticate()\n  }\n}\n", "const createSocket = require('./socket')\nconst createRequest = require('../protocol/request')\nconst Decoder = require('../protocol/decoder')\nconst { KafkaJSConnectionError, KafkaJSConnectionClosedError } = require('../errors')\nconst { INT_32_MAX_VALUE } = require('../constants')\nconst getEnv = require('../env')\nconst RequestQueue = require('./requestQueue')\nconst { CONNECTION_STATUS, CONNECTED_STATUS } = require('./connectionStatus')\nconst sharedPromiseTo = require('../utils/sharedPromiseTo')\nconst Long = require('../utils/long')\nconst SASLAuthenticator = require('../broker/saslAuthenticator')\nconst apiKeys = require('../protocol/requests/apiKeys')\n\nconst requestInfo = ({ apiName, apiKey, apiVersion }) =>\n  `${apiName}(key: ${apiKey}, version: ${apiVersion})`\n\n/**\n * @param request - request from protocol\n * @returns {boolean}\n */\nconst isAuthenticatedRequest = request => {\n  return ![apiKeys.ApiVersions, apiKeys.SaslHandshake, apiKeys.SaslAuthenticate].includes(\n    request.apiKey\n  )\n}\n\nconst PRIVATE = {\n  SHOULD_REAUTHENTICATE: Symbol('private:Connection:shouldReauthenticate'),\n  AUTHENTICATE: Symbol('private:Connection:authenticate'),\n}\n\nmodule.exports = class Connection {\n  /**\n   * @param {Object} options\n   * @param {string} options.host\n   * @param {number} options.port\n   * @param {import(\"../../types\").Logger} options.logger\n   * @param {import(\"../../types\").ISocketFactory} options.socketFactory\n   * @param {string} [options.clientId='kafkajs']\n   * @param {number} options.requestTimeout The maximum amount of time the client will wait for the response of a request,\n   *                                in milliseconds\n   * @param {string} [options.rack=null]\n   * @param {Object} [options.ssl=null] Options for the TLS Secure Context. It accepts all options,\n   *                            usually \"cert\", \"key\" and \"ca\". More information at\n   *                            https://nodejs.org/api/tls.html#tls_tls_createsecurecontext_options\n   * @param {Object} [options.sasl=null] Attributes used for SASL authentication. Options based on the\n   *                             key \"mechanism\". Connection is not actively using the SASL attributes\n   *                             but acting as a data object for this information\n   * @param {number} [options.reauthenticationThreshold=10000]\n   * @param {number} options.connectionTimeout The connection timeout, in milliseconds\n   * @param {boolean} [options.enforceRequestTimeout]\n   * @param {number} [options.maxInFlightRequests=null] The maximum number of unacknowledged requests on a connection before\n   *                                            enqueuing\n   * @param {import(\"../instrumentation/emitter\")} [options.instrumentationEmitter=null]\n   */\n  constructor({\n    host,\n    port,\n    logger,\n    socketFactory,\n    requestTimeout,\n    reauthenticationThreshold = 10000,\n    rack = null,\n    ssl = null,\n    sasl = null,\n    clientId = 'kafkajs',\n    connectionTimeout,\n    enforceRequestTimeout = true,\n    maxInFlightRequests = null,\n    instrumentationEmitter = null,\n  }) {\n    this.host = host\n    this.port = port\n    this.rack = rack\n    this.clientId = clientId\n    this.broker = `${this.host}:${this.port}`\n    this.logger = logger.namespace('Connection')\n\n    this.socketFactory = socketFactory\n    this.ssl = ssl\n    this.sasl = sasl\n\n    this.requestTimeout = requestTimeout\n    this.connectionTimeout = connectionTimeout\n    this.reauthenticationThreshold = reauthenticationThreshold\n\n    this.bytesBuffered = 0\n    this.bytesNeeded = Decoder.int32Size()\n    this.chunks = []\n\n    this.connectionStatus = CONNECTION_STATUS.DISCONNECTED\n    this.correlationId = 0\n    this.requestQueue = new RequestQueue({\n      instrumentationEmitter,\n      maxInFlightRequests,\n      requestTimeout,\n      enforceRequestTimeout,\n      clientId,\n      broker: this.broker,\n      logger: logger.namespace('RequestQueue'),\n      isConnected: () => this.isConnected(),\n    })\n\n    this.versions = null\n\n    this.authHandlers = null\n    this.authExpectResponse = false\n\n    const log = level => (message, extra = {}) => {\n      const logFn = this.logger[level]\n      logFn(message, { broker: this.broker, clientId, ...extra })\n    }\n\n    this.logDebug = log('debug')\n    this.logError = log('error')\n\n    const env = getEnv()\n    this.shouldLogBuffers = env.KAFKAJS_DEBUG_PROTOCOL_BUFFERS === '1'\n    this.shouldLogFetchBuffer =\n      this.shouldLogBuffers && env.KAFKAJS_DEBUG_EXTENDED_PROTOCOL_BUFFERS === '1'\n\n    this.authenticatedAt = null\n    this.sessionLifetime = Long.ZERO\n    this.supportAuthenticationProtocol = null\n\n    /**\n     * @private\n     * @returns {Promise}\n     */\n    this[PRIVATE.AUTHENTICATE] = sharedPromiseTo(async () => {\n      if (this.sasl && !this.isAuthenticated()) {\n        const authenticator = new SASLAuthenticator(\n          this,\n          this.logger,\n          this.versions,\n          this.supportAuthenticationProtocol\n        )\n\n        await authenticator.authenticate()\n        this.authenticatedAt = process.hrtime()\n        this.sessionLifetime = Long.fromValue(authenticator.sessionLifetime)\n      }\n    })\n  }\n\n  getSupportAuthenticationProtocol() {\n    return this.supportAuthenticationProtocol\n  }\n\n  setSupportAuthenticationProtocol(isSupported) {\n    this.supportAuthenticationProtocol = isSupported\n  }\n\n  setVersions(versions) {\n    this.versions = versions\n  }\n\n  isConnected() {\n    return CONNECTED_STATUS.includes(this.connectionStatus)\n  }\n\n  /**\n   * @public\n   * @returns {Promise}\n   */\n  connect() {\n    return new Promise((resolve, reject) => {\n      if (this.isConnected()) {\n        return resolve(true)\n      }\n\n      this.authenticatedAt = null\n\n      let timeoutId\n\n      const onConnect = () => {\n        clearTimeout(timeoutId)\n        this.connectionStatus = CONNECTION_STATUS.CONNECTED\n        this.requestQueue.scheduleRequestTimeoutCheck()\n        resolve(true)\n      }\n\n      const onData = data => {\n        this.processData(data)\n      }\n\n      const onEnd = async () => {\n        clearTimeout(timeoutId)\n\n        const wasConnected = this.isConnected()\n\n        if (this.authHandlers) {\n          this.authHandlers.onError()\n        } else if (wasConnected) {\n          this.logDebug('Kafka server has closed connection')\n          this.rejectRequests(\n            new KafkaJSConnectionClosedError('Closed connection', {\n              host: this.host,\n              port: this.port,\n            })\n          )\n        }\n\n        await this.disconnect()\n      }\n\n      const onError = async e => {\n        clearTimeout(timeoutId)\n\n        const error = new KafkaJSConnectionError(`Connection error: ${e.message}`, {\n          broker: `${this.host}:${this.port}`,\n          code: e.code,\n        })\n\n        this.logError(error.message, { stack: e.stack })\n        this.rejectRequests(error)\n        await this.disconnect()\n\n        reject(error)\n      }\n\n      const onTimeout = async () => {\n        const error = new KafkaJSConnectionError('Connection timeout', {\n          broker: `${this.host}:${this.port}`,\n        })\n\n        this.logError(error.message)\n        this.rejectRequests(error)\n        await this.disconnect()\n        reject(error)\n      }\n\n      this.logDebug(`Connecting`, {\n        ssl: !!this.ssl,\n        sasl: !!this.sasl,\n      })\n\n      try {\n        timeoutId = setTimeout(onTimeout, this.connectionTimeout)\n        this.socket = createSocket({\n          socketFactory: this.socketFactory,\n          host: this.host,\n          port: this.port,\n          ssl: this.ssl,\n          onConnect,\n          onData,\n          onEnd,\n          onError,\n          onTimeout,\n        })\n      } catch (e) {\n        clearTimeout(timeoutId)\n        reject(\n          new KafkaJSConnectionError(`Failed to connect: ${e.message}`, {\n            broker: `${this.host}:${this.port}`,\n          })\n        )\n      }\n    })\n  }\n\n  /**\n   * @public\n   * @returns {Promise}\n   */\n  async disconnect() {\n    this.authenticatedAt = null\n    this.connectionStatus = CONNECTION_STATUS.DISCONNECTING\n    this.logDebug('disconnecting...')\n\n    await this.requestQueue.waitForPendingRequests()\n    this.requestQueue.destroy()\n\n    if (this.socket) {\n      this.socket.end()\n      this.socket.unref()\n    }\n\n    this.connectionStatus = CONNECTION_STATUS.DISCONNECTED\n    this.logDebug('disconnected')\n    return true\n  }\n\n  /**\n   * @public\n   * @returns {boolean}\n   */\n  isAuthenticated() {\n    return this.authenticatedAt != null && !this[PRIVATE.SHOULD_REAUTHENTICATE]()\n  }\n\n  /***\n   * @private\n   */\n  [PRIVATE.SHOULD_REAUTHENTICATE]() {\n    if (this.sessionLifetime.equals(Long.ZERO)) {\n      return false\n    }\n\n    if (this.authenticatedAt == null) {\n      return true\n    }\n\n    const [secondsSince, remainingNanosSince] = process.hrtime(this.authenticatedAt)\n    const millisSince = Long.fromValue(secondsSince)\n      .multiply(1000)\n      .add(Long.fromValue(remainingNanosSince).divide(1000000))\n\n    const reauthenticateAt = millisSince.add(this.reauthenticationThreshold)\n    return reauthenticateAt.greaterThanOrEqual(this.sessionLifetime)\n  }\n\n  /** @public */\n  async authenticate() {\n    await this[PRIVATE.AUTHENTICATE]()\n  }\n\n  /**\n   * @public\n   * @returns {Promise}\n   */\n  sendAuthRequest({ request, response }) {\n    this.authExpectResponse = !!response\n\n    /**\n     * TODO: rewrite removing the async promise executor\n     */\n\n    /* eslint-disable no-async-promise-executor */\n    return new Promise(async (resolve, reject) => {\n      this.authHandlers = {\n        onSuccess: rawData => {\n          this.authHandlers = null\n          this.authExpectResponse = false\n\n          response\n            .decode(rawData)\n            .then(data => response.parse(data))\n            .then(resolve)\n            .catch(reject)\n        },\n        onError: () => {\n          this.authHandlers = null\n          this.authExpectResponse = false\n\n          reject(\n            new KafkaJSConnectionError('Connection closed by the server', {\n              broker: `${this.host}:${this.port}`,\n            })\n          )\n        },\n      }\n\n      try {\n        const requestPayload = await request.encode()\n\n        this.failIfNotConnected()\n        this.socket.write(requestPayload, 'binary')\n      } catch (e) {\n        reject(e)\n      }\n    })\n  }\n\n  /**\n   * @public\n   * @param {object} protocol\n   * @param {object} protocol.request It is defined by the protocol and consists of an object with \"apiKey\",\n   *                         \"apiVersion\", \"apiName\" and an \"encode\" function. The encode function\n   *                         must return an instance of Encoder\n   *\n   * @param {object} protocol.response It is defined by the protocol and consists of an object with two functions:\n   *                          \"decode\" and \"parse\"\n   *\n   * @param {number} [protocol.requestTimeout=null] Override for the default requestTimeout\n   * @param {boolean} [protocol.logResponseError=true] Whether to log errors\n   * @returns {Promise<data>} where data is the return of \"response#parse\"\n   */\n  async send({ request, response, requestTimeout = null, logResponseError = true }) {\n    if (!this.isAuthenticated() && isAuthenticatedRequest(request)) {\n      await this[PRIVATE.AUTHENTICATE]()\n    }\n\n    this.failIfNotConnected()\n\n    const expectResponse = !request.expectResponse || request.expectResponse()\n    const sendRequest = async () => {\n      const { clientId } = this\n      const correlationId = this.nextCorrelationId()\n\n      const requestPayload = await createRequest({ request, correlationId, clientId })\n      const { apiKey, apiName, apiVersion } = request\n      this.logDebug(`Request ${requestInfo(request)}`, {\n        correlationId,\n        expectResponse,\n        size: Buffer.byteLength(requestPayload.buffer),\n      })\n\n      return new Promise((resolve, reject) => {\n        try {\n          this.failIfNotConnected()\n          const entry = { apiKey, apiName, apiVersion, correlationId, resolve, reject }\n\n          this.requestQueue.push({\n            entry,\n            expectResponse,\n            requestTimeout,\n            sendRequest: () => {\n              this.socket.write(requestPayload.buffer, 'binary')\n            },\n          })\n        } catch (e) {\n          reject(e)\n        }\n      })\n    }\n\n    const { correlationId, size, entry, payload } = await sendRequest()\n\n    if (!expectResponse) {\n      return\n    }\n\n    try {\n      const payloadDecoded = await response.decode(payload)\n\n      /**\n       * @see KIP-219\n       * If the response indicates that the client-side needs to throttle, do that.\n       */\n      this.requestQueue.maybeThrottle(payloadDecoded.clientSideThrottleTime)\n\n      const data = await response.parse(payloadDecoded)\n      const isFetchApi = entry.apiName === 'Fetch'\n      this.logDebug(`Response ${requestInfo(entry)}`, {\n        correlationId,\n        size,\n        data: isFetchApi && !this.shouldLogFetchBuffer ? '[filtered]' : data,\n      })\n\n      return data\n    } catch (e) {\n      if (logResponseError) {\n        this.logError(`Response ${requestInfo(entry)}`, {\n          error: e.message,\n          correlationId,\n          size,\n        })\n      }\n\n      const isBuffer = Buffer.isBuffer(payload)\n      this.logDebug(`Response ${requestInfo(entry)}`, {\n        error: e.message,\n        correlationId,\n        payload:\n          isBuffer && !this.shouldLogBuffers ? { type: 'Buffer', data: '[filtered]' } : payload,\n      })\n\n      throw e\n    }\n  }\n\n  /**\n   * @private\n   */\n  failIfNotConnected() {\n    if (!this.isConnected()) {\n      throw new KafkaJSConnectionError('Not connected', {\n        broker: `${this.host}:${this.port}`,\n      })\n    }\n  }\n\n  /**\n   * @private\n   */\n  nextCorrelationId() {\n    if (this.correlationId >= INT_32_MAX_VALUE) {\n      this.correlationId = 0\n    }\n\n    return this.correlationId++\n  }\n\n  /**\n   * @private\n   */\n  processData(rawData) {\n    if (this.authHandlers && !this.authExpectResponse) {\n      return this.authHandlers.onSuccess(rawData)\n    }\n\n    // Accumulate the new chunk\n    this.chunks.push(rawData)\n    this.bytesBuffered += Buffer.byteLength(rawData)\n\n    // Process data if there are enough bytes to read the expected response size,\n    // otherwise keep buffering\n    while (this.bytesNeeded <= this.bytesBuffered) {\n      const buffer = this.chunks.length > 1 ? Buffer.concat(this.chunks) : this.chunks[0]\n      const decoder = new Decoder(buffer)\n      const expectedResponseSize = decoder.readInt32()\n\n      // Return early if not enough bytes to read the full response\n      if (!decoder.canReadBytes(expectedResponseSize)) {\n        this.chunks = [buffer]\n        this.bytesBuffered = Buffer.byteLength(buffer)\n        this.bytesNeeded = Decoder.int32Size() + expectedResponseSize\n        return\n      }\n\n      const response = new Decoder(decoder.readBytes(expectedResponseSize))\n\n      // Reset the buffered chunks as the rest of the bytes\n      const remainderBuffer = decoder.readAll()\n      this.chunks = [remainderBuffer]\n      this.bytesBuffered = Buffer.byteLength(remainderBuffer)\n      this.bytesNeeded = Decoder.int32Size()\n\n      if (this.authHandlers) {\n        const rawResponseSize = Decoder.int32Size() + expectedResponseSize\n        const rawResponseBuffer = buffer.slice(0, rawResponseSize)\n        return this.authHandlers.onSuccess(rawResponseBuffer)\n      }\n\n      const correlationId = response.readInt32()\n      const payload = response.readAll()\n\n      this.requestQueue.fulfillRequest({\n        size: expectedResponseSize,\n        correlationId,\n        payload,\n      })\n    }\n  }\n\n  /**\n   * @private\n   */\n  rejectRequests(error) {\n    this.requestQueue.rejectAll(error)\n  }\n}\n", "const apiKeys = require('../protocol/requests/apiKeys')\nconst Connection = require('./connection')\n\nmodule.exports = class ConnectionPool {\n  /**\n   * @param {ConstructorParameters<typeof Connection>[0]} options\n   */\n  constructor(options) {\n    this.logger = options.logger.namespace('ConnectionPool')\n    this.connectionTimeout = options.connectionTimeout\n    this.host = options.host\n    this.port = options.port\n    this.rack = options.rack\n    this.ssl = options.ssl\n    this.sasl = options.sasl\n    this.clientId = options.clientId\n    this.socketFactory = options.socketFactory\n\n    this.pool = new Array(2).fill().map(() => new Connection(options))\n  }\n\n  isConnected() {\n    return this.pool.some(c => c.isConnected())\n  }\n\n  isAuthenticated() {\n    return this.pool.some(c => c.isAuthenticated())\n  }\n\n  setSupportAuthenticationProtocol(isSupported) {\n    this.map(c => c.setSupportAuthenticationProtocol(isSupported))\n  }\n\n  setVersions(versions) {\n    this.map(c => c.setVersions(versions))\n  }\n\n  map(callback) {\n    return this.pool.map(c => callback(c))\n  }\n\n  async send(protocolRequest) {\n    const connection = await this.getConnectionByRequest(protocolRequest)\n    return connection.send(protocolRequest)\n  }\n\n  getConnectionByRequest({ request: { apiKey } }) {\n    const index = { [apiKeys.Fetch]: 1 }[apiKey] || 0\n    return this.getConnection(index)\n  }\n\n  async getConnection(index = 0) {\n    const connection = this.pool[index]\n\n    if (!connection.isConnected()) {\n      await connection.connect()\n    }\n\n    return connection\n  }\n\n  async destroy() {\n    await Promise.all(this.map(c => c.disconnect()))\n  }\n}\n", "const { KafkaJSConnectionError, KafkaJSNonRetriableError } = require('../errors')\nconst ConnectionPool = require('../network/connectionPool')\n\n/**\n * @typedef {Object} ConnectionPoolBuilder\n * @property {(destination?: { host?: string, port?: number, rack?: string }) => Promise<ConnectionPool>} build\n */\n\n/**\n * @param {Object} options\n * @param {import(\"../../types\").ISocketFactory} [options.socketFactory]\n * @param {string[]|(() => string[])} options.brokers\n * @param {Object} [options.ssl]\n * @param {Object} [options.sasl]\n * @param {string} options.clientId\n * @param {number} options.requestTimeout\n * @param {boolean} [options.enforceRequestTimeout]\n * @param {number} [options.connectionTimeout]\n * @param {number} [options.maxInFlightRequests]\n * @param {import(\"../../types\").RetryOptions} [options.retry]\n * @param {import(\"../../types\").Logger} options.logger\n * @param {import(\"../instrumentation/emitter\")} [options.instrumentationEmitter]\n * @param {number} [options.reauthenticationThreshold]\n * @returns {ConnectionPoolBuilder}\n */\nmodule.exports = ({\n  socketFactory,\n  brokers,\n  ssl,\n  sasl,\n  clientId,\n  requestTimeout,\n  enforceRequestTimeout,\n  connectionTimeout,\n  maxInFlightRequests,\n  logger,\n  instrumentationEmitter = null,\n  reauthenticationThreshold,\n}) => {\n  let index = 0\n\n  const isValidBroker = broker => {\n    return broker && typeof broker === 'string' && broker.length > 0\n  }\n\n  const validateBrokers = brokers => {\n    if (!brokers) {\n      throw new KafkaJSNonRetriableError(`Failed to connect: brokers should not be null`)\n    }\n\n    if (Array.isArray(brokers)) {\n      if (!brokers.length) {\n        throw new KafkaJSNonRetriableError(`Failed to connect: brokers array is empty`)\n      }\n\n      brokers.forEach((broker, index) => {\n        if (!isValidBroker(broker)) {\n          throw new KafkaJSNonRetriableError(\n            `Failed to connect: broker at index ${index} is invalid \"${typeof broker}\"`\n          )\n        }\n      })\n    }\n  }\n\n  const getBrokers = async () => {\n    let list\n\n    if (typeof brokers === 'function') {\n      try {\n        list = await brokers()\n      } catch (e) {\n        const wrappedError = new KafkaJSConnectionError(\n          `Failed to connect: \"config.brokers\" threw: ${e.message}`\n        )\n        wrappedError.stack = `${wrappedError.name}\\n  Caused by: ${e.stack}`\n        throw wrappedError\n      }\n    } else {\n      list = brokers\n    }\n\n    validateBrokers(list)\n\n    return list\n  }\n\n  return {\n    build: async ({ host, port, rack } = {}) => {\n      if (!host) {\n        const list = await getBrokers()\n\n        const randomBroker = list[index++ % list.length]\n\n        host = randomBroker.split(':')[0]\n        port = Number(randomBroker.split(':')[1])\n      }\n\n      return new ConnectionPool({\n        host,\n        port,\n        rack,\n        sasl,\n        ssl,\n        clientId,\n        socketFactory,\n        connectionTimeout,\n        requestTimeout,\n        enforceRequestTimeout,\n        maxInFlightRequests,\n        instrumentationEmitter,\n        logger,\n        reauthenticationThreshold,\n      })\n    },\n  }\n}\n", "const BrokerPool = require('./brokerPool')\nconst Lock = require('../utils/lock')\nconst sharedPromiseTo = require('../utils/sharedPromiseTo')\nconst createRetry = require('../retry')\nconst connectionPoolBuilder = require('./connectionPoolBuilder')\nconst { EARLIEST_OFFSET, LATEST_OFFSET } = require('../constants')\nconst {\n  KafkaJSError,\n  KafkaJSBrokerNotFound,\n  KafkaJSMetadataNotLoaded,\n  KafkaJSTopicMetadataNotLoaded,\n  KafkaJSGroupCoordinatorNotFound,\n} = require('../errors')\nconst COORDINATOR_TYPES = require('../protocol/coordinatorTypes')\n\nconst { keys } = Object\n\nconst mergeTopics = (obj, { topic, partitions }) => ({\n  ...obj,\n  [topic]: [...(obj[topic] || []), ...partitions],\n})\n\nconst PRIVATE = {\n  CONNECT: Symbol('private:Cluster:connect'),\n  REFRESH_METADATA: Symbol('private:Cluster:refreshMetadata'),\n  REFRESH_METADATA_IF_NECESSARY: Symbol('private:Cluster:refreshMetadataIfNecessary'),\n  FIND_CONTROLLER_BROKER: Symbol('private:Cluster:findControllerBroker'),\n}\n\nmodule.exports = class Cluster {\n  /**\n   * @param {Object} options\n   * @param {Array<string>} options.brokers example: ['127.0.0.1:9092', '127.0.0.1:9094']\n   * @param {Object} options.ssl\n   * @param {Object} options.sasl\n   * @param {string} options.clientId\n   * @param {number} options.connectionTimeout - in milliseconds\n   * @param {number} options.authenticationTimeout - in milliseconds\n   * @param {number} options.reauthenticationThreshold - in milliseconds\n   * @param {number} [options.requestTimeout=30000] - in milliseconds\n   * @param {boolean} [options.enforceRequestTimeout]\n   * @param {number} options.metadataMaxAge - in milliseconds\n   * @param {boolean} options.allowAutoTopicCreation\n   * @param {number} options.maxInFlightRequests\n   * @param {number} options.isolationLevel\n   * @param {import(\"../../types\").RetryOptions} options.retry\n   * @param {import(\"../../types\").Logger} options.logger\n   * @param {import(\"../../types\").ISocketFactory} options.socketFactory\n   * @param {Map} [options.offsets]\n   * @param {import(\"../instrumentation/emitter\")} [options.instrumentationEmitter=null]\n   */\n  constructor({\n    logger: rootLogger,\n    socketFactory,\n    brokers,\n    ssl,\n    sasl,\n    clientId,\n    connectionTimeout,\n    authenticationTimeout,\n    reauthenticationThreshold,\n    requestTimeout = 30000,\n    enforceRequestTimeout,\n    metadataMaxAge,\n    retry,\n    allowAutoTopicCreation,\n    maxInFlightRequests,\n    isolationLevel,\n    instrumentationEmitter = null,\n    offsets = new Map(),\n  }) {\n    this.rootLogger = rootLogger\n    this.logger = rootLogger.namespace('Cluster')\n    this.retrier = createRetry(retry)\n    this.connectionPoolBuilder = connectionPoolBuilder({\n      logger: rootLogger,\n      instrumentationEmitter,\n      socketFactory,\n      brokers,\n      ssl,\n      sasl,\n      clientId,\n      connectionTimeout,\n      requestTimeout,\n      enforceRequestTimeout,\n      maxInFlightRequests,\n      reauthenticationThreshold,\n    })\n\n    this.targetTopics = new Set()\n    this.mutatingTargetTopics = new Lock({\n      description: `updating target topics`,\n      timeout: requestTimeout,\n    })\n    this.isolationLevel = isolationLevel\n    this.brokerPool = new BrokerPool({\n      connectionPoolBuilder: this.connectionPoolBuilder,\n      logger: this.rootLogger,\n      retry,\n      allowAutoTopicCreation,\n      authenticationTimeout,\n      metadataMaxAge,\n    })\n    this.committedOffsetsByGroup = offsets\n\n    this[PRIVATE.CONNECT] = sharedPromiseTo(async () => {\n      return await this.brokerPool.connect()\n    })\n\n    this[PRIVATE.REFRESH_METADATA] = sharedPromiseTo(async () => {\n      return await this.brokerPool.refreshMetadata(Array.from(this.targetTopics))\n    })\n\n    this[PRIVATE.REFRESH_METADATA_IF_NECESSARY] = sharedPromiseTo(async () => {\n      return await this.brokerPool.refreshMetadataIfNecessary(Array.from(this.targetTopics))\n    })\n\n    this[PRIVATE.FIND_CONTROLLER_BROKER] = sharedPromiseTo(async () => {\n      const { metadata } = this.brokerPool\n\n      if (!metadata || metadata.controllerId == null) {\n        throw new KafkaJSMetadataNotLoaded('Topic metadata not loaded')\n      }\n\n      const broker = await this.findBroker({ nodeId: metadata.controllerId })\n\n      if (!broker) {\n        throw new KafkaJSBrokerNotFound(\n          `Controller broker with id ${metadata.controllerId} not found in the cached metadata`\n        )\n      }\n\n      return broker\n    })\n  }\n\n  isConnected() {\n    return this.brokerPool.hasConnectedBrokers()\n  }\n\n  /**\n   * @public\n   * @returns {Promise<void>}\n   */\n  async connect() {\n    await this[PRIVATE.CONNECT]()\n  }\n\n  /**\n   * @public\n   * @returns {Promise<void>}\n   */\n  async disconnect() {\n    await this.brokerPool.disconnect()\n  }\n\n  /**\n   * @public\n   * @param {object} destination\n   * @param {String} destination.host\n   * @param {Number} destination.port\n   */\n  removeBroker({ host, port }) {\n    this.brokerPool.removeBroker({ host, port })\n  }\n\n  /**\n   * @public\n   * @returns {Promise<void>}\n   */\n  async refreshMetadata() {\n    await this[PRIVATE.REFRESH_METADATA]()\n  }\n\n  /**\n   * @public\n   * @returns {Promise<void>}\n   */\n  async refreshMetadataIfNecessary() {\n    await this[PRIVATE.REFRESH_METADATA_IF_NECESSARY]()\n  }\n\n  /**\n   * @public\n   * @returns {Promise<import(\"../../types\").BrokerMetadata>}\n   */\n  async metadata({ topics = [] } = {}) {\n    return this.retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await this.brokerPool.refreshMetadataIfNecessary(topics)\n        return this.brokerPool.withBroker(async ({ broker }) => broker.metadata(topics))\n      } catch (e) {\n        if (e.type === 'LEADER_NOT_AVAILABLE') {\n          throw e\n        }\n\n        bail(e)\n      }\n    })\n  }\n\n  /**\n   * @public\n   * @param {string} topic\n   * @return {Promise}\n   */\n  async addTargetTopic(topic) {\n    return this.addMultipleTargetTopics([topic])\n  }\n\n  /**\n   * @public\n   * @param {string[]} topics\n   * @return {Promise}\n   */\n  async addMultipleTargetTopics(topics) {\n    await this.mutatingTargetTopics.acquire()\n\n    try {\n      const previousSize = this.targetTopics.size\n      const previousTopics = new Set(this.targetTopics)\n      for (const topic of topics) {\n        this.targetTopics.add(topic)\n      }\n\n      const hasChanged = previousSize !== this.targetTopics.size || !this.brokerPool.metadata\n\n      if (hasChanged) {\n        try {\n          await this.refreshMetadata()\n        } catch (e) {\n          if (\n            e.type === 'INVALID_TOPIC_EXCEPTION' ||\n            e.type === 'UNKNOWN_TOPIC_OR_PARTITION' ||\n            e.type === 'TOPIC_AUTHORIZATION_FAILED'\n          ) {\n            this.targetTopics = previousTopics\n          }\n\n          throw e\n        }\n      }\n    } finally {\n      await this.mutatingTargetTopics.release()\n    }\n  }\n\n  /** @type {() => string[]} */\n  getNodeIds() {\n    return this.brokerPool.getNodeIds()\n  }\n\n  /**\n   * @public\n   * @param {object} options\n   * @param {string} options.nodeId\n   * @returns {Promise<import(\"../../types\").Broker>}\n   */\n  async findBroker({ nodeId }) {\n    try {\n      return await this.brokerPool.findBroker({ nodeId })\n    } catch (e) {\n      // The client probably has stale metadata\n      if (\n        e.name === 'KafkaJSBrokerNotFound' ||\n        e.name === 'KafkaJSLockTimeout' ||\n        e.name === 'KafkaJSConnectionError'\n      ) {\n        await this.refreshMetadata()\n      }\n\n      throw e\n    }\n  }\n\n  /**\n   * @public\n   * @returns {Promise<import(\"../../types\").Broker>}\n   */\n  async findControllerBroker() {\n    return await this[PRIVATE.FIND_CONTROLLER_BROKER]()\n  }\n\n  /**\n   * @public\n   * @param {string} topic\n   * @returns {import(\"../../types\").PartitionMetadata[]} Example:\n   *                   [{\n   *                     isr: [2],\n   *                     leader: 2,\n   *                     partitionErrorCode: 0,\n   *                     partitionId: 0,\n   *                     replicas: [2],\n   *                   }]\n   */\n  findTopicPartitionMetadata(topic) {\n    const { metadata } = this.brokerPool\n    if (!metadata || !metadata.topicMetadata) {\n      throw new KafkaJSTopicMetadataNotLoaded('Topic metadata not loaded', { topic })\n    }\n\n    const topicMetadata = metadata.topicMetadata.find(t => t.topic === topic)\n    return topicMetadata ? topicMetadata.partitionMetadata : []\n  }\n\n  /**\n   * @public\n   * @param {string} topic\n   * @param {(number|string)[]} partitions\n   * @returns {Object} Object with leader and partitions. For partitions 0 and 5\n   *                   the result could be:\n   *                     { '0': [0], '2': [5] }\n   *\n   *                   where the key is the nodeId.\n   */\n  findLeaderForPartitions(topic, partitions) {\n    const partitionMetadata = this.findTopicPartitionMetadata(topic)\n    return partitions.reduce((result, id) => {\n      const partitionId = parseInt(id, 10)\n      const metadata = partitionMetadata.find(p => p.partitionId === partitionId)\n\n      if (!metadata) {\n        return result\n      }\n\n      if (metadata.leader === null || metadata.leader === undefined) {\n        throw new KafkaJSError('Invalid partition metadata', { topic, partitionId, metadata })\n      }\n\n      const { leader } = metadata\n      const current = result[leader] || []\n      return { ...result, [leader]: [...current, partitionId] }\n    }, {})\n  }\n\n  /**\n   * @public\n   * @param {object} params\n   * @param {string} params.groupId\n   * @param {import(\"../protocol/coordinatorTypes\").CoordinatorType} [params.coordinatorType=0]\n   * @returns {Promise<import(\"../../types\").Broker>}\n   */\n  async findGroupCoordinator({ groupId, coordinatorType = COORDINATOR_TYPES.GROUP }) {\n    return this.retrier(async (bail, retryCount, retryTime) => {\n      try {\n        const { coordinator } = await this.findGroupCoordinatorMetadata({\n          groupId,\n          coordinatorType,\n        })\n        return await this.findBroker({ nodeId: coordinator.nodeId })\n      } catch (e) {\n        // A new broker can join the cluster before we have the chance\n        // to refresh metadata\n        if (e.name === 'KafkaJSBrokerNotFound' || e.type === 'GROUP_COORDINATOR_NOT_AVAILABLE') {\n          this.logger.debug(`${e.message}, refreshing metadata and trying again...`, {\n            groupId,\n            retryCount,\n            retryTime,\n          })\n\n          await this.refreshMetadata()\n          throw e\n        }\n\n        if (e.code === 'ECONNREFUSED') {\n          // During maintenance the current coordinator can go down; findBroker will\n          // refresh metadata and re-throw the error. findGroupCoordinator has to re-throw\n          // the error to go through the retry cycle.\n          throw e\n        }\n\n        bail(e)\n      }\n    })\n  }\n\n  /**\n   * @public\n   * @param {object} params\n   * @param {string} params.groupId\n   * @param {import(\"../protocol/coordinatorTypes\").CoordinatorType} [params.coordinatorType=0]\n   * @returns {Promise<Object>}\n   */\n  async findGroupCoordinatorMetadata({ groupId, coordinatorType }) {\n    const brokerMetadata = await this.brokerPool.withBroker(async ({ nodeId, broker }) => {\n      return await this.retrier(async (bail, retryCount, retryTime) => {\n        try {\n          const brokerMetadata = await broker.findGroupCoordinator({ groupId, coordinatorType })\n          this.logger.debug('Found group coordinator', {\n            broker: brokerMetadata.host,\n            nodeId: brokerMetadata.coordinator.nodeId,\n          })\n          return brokerMetadata\n        } catch (e) {\n          this.logger.debug('Tried to find group coordinator', {\n            nodeId,\n            groupId,\n            error: e,\n          })\n\n          if (e.type === 'GROUP_COORDINATOR_NOT_AVAILABLE') {\n            this.logger.debug('Group coordinator not available, retrying...', {\n              nodeId,\n              retryCount,\n              retryTime,\n            })\n\n            throw e\n          }\n\n          bail(e)\n        }\n      })\n    })\n\n    if (brokerMetadata) {\n      return brokerMetadata\n    }\n\n    throw new KafkaJSGroupCoordinatorNotFound('Failed to find group coordinator')\n  }\n\n  /**\n   * @param {object} topicConfiguration\n   * @returns {number}\n   */\n  defaultOffset({ fromBeginning }) {\n    return fromBeginning ? EARLIEST_OFFSET : LATEST_OFFSET\n  }\n\n  /**\n   * @public\n   * @param {Array<Object>} topics\n   *                          [\n   *                            {\n   *                              topic: 'my-topic-name',\n   *                              partitions: [{ partition: 0 }],\n   *                              fromBeginning: false\n   *                            }\n   *                          ]\n   * @returns {Promise<import(\"../../types\").TopicOffsets[]>} example:\n   *                          [\n   *                            {\n   *                              topic: 'my-topic-name',\n   *                              partitions: [\n   *                                { partition: 0, offset: '1' },\n   *                                { partition: 1, offset: '2' },\n   *                                { partition: 2, offset: '1' },\n   *                              ],\n   *                            },\n   *                          ]\n   */\n  async fetchTopicsOffset(topics) {\n    const partitionsPerBroker = {}\n    const topicConfigurations = {}\n\n    const addDefaultOffset = topic => partition => {\n      const { timestamp } = topicConfigurations[topic]\n      return { ...partition, timestamp }\n    }\n\n    // Index all topics and partitions per leader (nodeId)\n    for (const topicData of topics) {\n      const { topic, partitions, fromBeginning, fromTimestamp } = topicData\n      const partitionsPerLeader = this.findLeaderForPartitions(\n        topic,\n        partitions.map(p => p.partition)\n      )\n      const timestamp =\n        fromTimestamp != null ? fromTimestamp : this.defaultOffset({ fromBeginning })\n\n      topicConfigurations[topic] = { timestamp }\n\n      keys(partitionsPerLeader).forEach(nodeId => {\n        partitionsPerBroker[nodeId] = partitionsPerBroker[nodeId] || {}\n        partitionsPerBroker[nodeId][topic] = partitions.filter(p =>\n          partitionsPerLeader[nodeId].includes(p.partition)\n        )\n      })\n    }\n\n    // Create a list of requests to fetch the offset of all partitions\n    const requests = keys(partitionsPerBroker).map(async nodeId => {\n      const broker = await this.findBroker({ nodeId })\n      const partitions = partitionsPerBroker[nodeId]\n\n      const { responses: topicOffsets } = await broker.listOffsets({\n        isolationLevel: this.isolationLevel,\n        topics: keys(partitions).map(topic => ({\n          topic,\n          partitions: partitions[topic].map(addDefaultOffset(topic)),\n        })),\n      })\n\n      return topicOffsets\n    })\n\n    // Execute all requests, merge and normalize the responses\n    const responses = await Promise.all(requests)\n    const partitionsPerTopic = responses.flat().reduce(mergeTopics, {})\n\n    return keys(partitionsPerTopic).map(topic => ({\n      topic,\n      partitions: partitionsPerTopic[topic].map(({ partition, offset }) => ({\n        partition,\n        offset,\n      })),\n    }))\n  }\n\n  /**\n   * Retrieve the object mapping for committed offsets for a single consumer group\n   * @param {object} options\n   * @param {string} options.groupId\n   * @returns {Object}\n   */\n  committedOffsets({ groupId }) {\n    if (!this.committedOffsetsByGroup.has(groupId)) {\n      this.committedOffsetsByGroup.set(groupId, {})\n    }\n\n    return this.committedOffsetsByGroup.get(groupId)\n  }\n\n  /**\n   * Mark offset as committed for a single consumer group's topic-partition\n   * @param {object} options\n   * @param {string} options.groupId\n   * @param {string} options.topic\n   * @param {string|number} options.partition\n   * @param {string} options.offset\n   */\n  markOffsetAsCommitted({ groupId, topic, partition, offset }) {\n    const committedOffsets = this.committedOffsets({ groupId })\n\n    committedOffsets[topic] = committedOffsets[topic] || {}\n    committedOffsets[topic][partition] = offset\n  }\n}\n", "/* eslint-disable */\nconst Long = require('../../../utils/long')\n\n// Based on the kafka client 0.10.2 murmur2 implementation\n// https://github.com/apache/kafka/blob/0.10.2/clients/src/main/java/org/apache/kafka/common/utils/Utils.java#L364\n\nconst SEED = Long.fromValue(0x9747b28c)\n\n// 'm' and 'r' are mixing constants generated offline.\n// They're not really 'magic', they just happen to work well.\nconst M = Long.fromValue(0x5bd1e995)\nconst R = Long.fromValue(24)\n\nmodule.exports = key => {\n  const data = Buffer.isBuffer(key) ? key : Buffer.from(String(key))\n  const length = data.length\n\n  // Initialize the hash to a random value\n  let h = Long.fromValue(SEED.xor(length))\n  let length4 = Math.floor(length / 4)\n\n  for (let i = 0; i < length4; i++) {\n    const i4 = i * 4\n    let k =\n      (data[i4 + 0] & 0xff) +\n      ((data[i4 + 1] & 0xff) << 8) +\n      ((data[i4 + 2] & 0xff) << 16) +\n      ((data[i4 + 3] & 0xff) << 24)\n    k = Long.fromValue(k)\n    k = k.multiply(M)\n    k = k.xor(k.toInt() >>> R)\n    k = Long.fromValue(k).multiply(M)\n    h = h.multiply(M)\n    h = h.xor(k)\n  }\n\n  // Handle the last few bytes of the input array\n  switch (length % 4) {\n    case 3:\n      h = h.xor((data[(length & ~3) + 2] & 0xff) << 16)\n    case 2:\n      h = h.xor((data[(length & ~3) + 1] & 0xff) << 8)\n    case 1:\n      h = h.xor(data[length & ~3] & 0xff)\n      h = h.multiply(M)\n  }\n\n  h = h.xor(h.toInt() >>> 13)\n  h = h.multiply(M)\n  h = h.xor(h.toInt() >>> 15)\n\n  return h.toInt()\n}\n", "const { KafkaJSNonRetriableError } = require('../../../errors')\n\nconst toNodeCompatible = crypto => ({\n  randomBytes: size => crypto.getRandomValues(Buffer.allocUnsafe(size)),\n})\n\nlet cryptoImplementation = null\nif (global && global.crypto) {\n  cryptoImplementation =\n    global.crypto.randomBytes === undefined ? toNodeCompatible(global.crypto) : global.crypto\n} else if (global && global.msCrypto) {\n  cryptoImplementation = toNodeCompatible(global.msCrypto)\n} else if (global && !global.crypto) {\n  cryptoImplementation = require('crypto')\n}\n\nconst MAX_BYTES = 65536\n\nmodule.exports = size => {\n  if (size > MAX_BYTES) {\n    throw new KafkaJSNonRetriableError(\n      `Byte length (${size}) exceeds the max number of bytes of entropy available (${MAX_BYTES})`\n    )\n  }\n\n  if (!cryptoImplementation) {\n    throw new KafkaJSNonRetriableError('No available crypto implementation')\n  }\n\n  return cryptoImplementation.randomBytes(size)\n}\n", "const randomBytes = require('./randomBytes')\n\n// Based on the java client 0.10.2\n// https://github.com/apache/kafka/blob/0.10.2/clients/src/main/java/org/apache/kafka/clients/producer/internals/DefaultPartitioner.java\n\n/**\n * A cheap way to deterministically convert a number to a positive value. When the input is\n * positive, the original value is returned. When the input number is negative, the returned\n * positive value is the original value bit AND against 0x7fffffff which is not its absolutely\n * value.\n */\nconst toPositive = x => x & 0x7fffffff\n\n/**\n * The default partitioning strategy:\n *  - If a partition is specified in the message, use it\n *  - If no partition is specified but a key is present choose a partition based on a hash of the key\n *  - If no partition or key is present choose a partition in a round-robin fashion\n */\nmodule.exports = murmur2 => () => {\n  const counters = {}\n\n  return ({ topic, partitionMetadata, message }) => {\n    if (!(topic in counters)) {\n      counters[topic] = randomBytes(32).readUInt32BE(0)\n    }\n    const numPartitions = partitionMetadata.length\n    const availablePartitions = partitionMetadata.filter(p => p.leader >= 0)\n    const numAvailablePartitions = availablePartitions.length\n\n    if (message.partition !== null && message.partition !== undefined) {\n      return message.partition\n    }\n\n    if (message.key !== null && message.key !== undefined) {\n      return toPositive(murmur2(message.key)) % numPartitions\n    }\n\n    if (numAvailablePartitions > 0) {\n      const i = toPositive(++counters[topic]) % numAvailablePartitions\n      return availablePartitions[i].partitionId\n    }\n\n    // no partitions are available, give a non-available partition\n    return toPositive(++counters[topic]) % numPartitions\n  }\n}\n", "const murmur2 = require('./murmur2')\nconst createDefaultPartitioner = require('../legacy/partitioner')\n\nmodule.exports = createDefaultPartitioner(murmur2)\n", "/* eslint-disable */\n\n// Based on the kafka client 0.10.2 murmur2 implementation\n// https://github.com/apache/kafka/blob/0.10.2/clients/src/main/java/org/apache/kafka/common/utils/Utils.java#L364\n\nconst SEED = 0x9747b28c\n\n// 'm' and 'r' are mixing constants generated offline.\n// They're not really 'magic', they just happen to work well.\nconst M = 0x5bd1e995\nconst R = 24\n\nmodule.exports = key => {\n  const data = Buffer.isBuffer(key) ? key : Buffer.from(String(key))\n  const length = data.length\n\n  // Initialize the hash to a random value\n  let h = SEED ^ length\n  let length4 = length / 4\n\n  for (let i = 0; i < length4; i++) {\n    const i4 = i * 4\n    let k =\n      (data[i4 + 0] & 0xff) +\n      ((data[i4 + 1] & 0xff) << 8) +\n      ((data[i4 + 2] & 0xff) << 16) +\n      ((data[i4 + 3] & 0xff) << 24)\n    k *= M\n    k ^= k >>> R\n    k *= M\n    h *= M\n    h ^= k\n  }\n\n  // Handle the last few bytes of the input array\n  switch (length % 4) {\n    case 3:\n      h ^= (data[(length & ~3) + 2] & 0xff) << 16\n    case 2:\n      h ^= (data[(length & ~3) + 1] & 0xff) << 8\n    case 1:\n      h ^= data[length & ~3] & 0xff\n      h *= M\n  }\n\n  h ^= h >>> 13\n  h *= M\n  h ^= h >>> 15\n\n  return h\n}\n", "const murmur2 = require('./murmur2')\nconst createLegacyPartitioner = require('./partitioner')\n\nmodule.exports = createLegacyPartitioner(murmur2)\n", "const DefaultPartitioner = require('./default')\nconst LegacyPartitioner = require('./legacy')\n\nmodule.exports = {\n  DefaultPartitioner,\n  LegacyPartitioner,\n  /**\n   * @deprecated Use DefaultPartitioner instead\n   *\n   * The JavaCompatiblePartitioner was renamed DefaultPartitioner\n   * and made to be the default in 2.0.0.\n   */\n  JavaCompatiblePartitioner: DefaultPartitioner,\n}\n", "module.exports = {\n  UNINITIALIZED: 'UNINITIALIZED',\n  READY: 'READY',\n  TRANSACTING: 'TRANSACTING',\n  COMMITTING: 'COMMITTING',\n  ABORTING: 'ABORTING',\n}\n", "const { EventEmitter } = require('events')\nconst { KafkaJSNonRetriableError } = require('../../errors')\nconst STATES = require('./transactionStates')\n\nconst VALID_STATE_TRANSITIONS = {\n  [STATES.UNINITIALIZED]: [STATES.READY],\n  [STATES.READY]: [STATES.READY, STATES.TRANSACTING],\n  [STATES.TRANSACTING]: [STATES.COMMITTING, STATES.ABORTING],\n  [STATES.COMMITTING]: [STATES.READY],\n  [STATES.ABORTING]: [STATES.READY],\n}\n\nmodule.exports = ({ logger, initialState = STATES.UNINITIALIZED }) => {\n  let currentState = initialState\n\n  const guard = (object, method, { legalStates, async: isAsync = true }) => {\n    if (!object[method]) {\n      throw new KafkaJSNonRetriableError(`Cannot add guard on missing method \"${method}\"`)\n    }\n\n    return (...args) => {\n      const fn = object[method]\n\n      if (!legalStates.includes(currentState)) {\n        const error = new KafkaJSNonRetriableError(\n          `Transaction state exception: Cannot call \"${method}\" in state \"${currentState}\"`\n        )\n\n        if (isAsync) {\n          return Promise.reject(error)\n        } else {\n          throw error\n        }\n      }\n\n      return fn.apply(object, args)\n    }\n  }\n\n  const stateMachine = Object.assign(new EventEmitter(), {\n    /**\n     * Create a clone of \"object\" where we ensure state machine is in correct state\n     * prior to calling any of the configured methods\n     * @param {Object} object The object whose methods we will guard\n     * @param {Object} methodStateMapping Keys are method names on \"object\"\n     * @param {string[]} methodStateMapping.legalStates Legal states for this method\n     * @param {boolean=true} methodStateMapping.async Whether this method is async (throw vs reject)\n     */\n    createGuarded(object, methodStateMapping) {\n      const guardedMethods = Object.keys(methodStateMapping).reduce((guards, method) => {\n        guards[method] = guard(object, method, methodStateMapping[method])\n        return guards\n      }, {})\n\n      return { ...object, ...guardedMethods }\n    },\n    /**\n     * Transition safely to a new state\n     */\n    transitionTo(state) {\n      logger.debug(`Transaction state transition ${currentState} --> ${state}`)\n\n      if (!VALID_STATE_TRANSITIONS[currentState].includes(state)) {\n        throw new KafkaJSNonRetriableError(\n          `Transaction state exception: Invalid transition ${currentState} --> ${state}`\n        )\n      }\n\n      stateMachine.emit('transition', { to: state, from: currentState })\n      currentState = state\n    },\n\n    state() {\n      return currentState\n    },\n  })\n\n  return stateMachine\n}\n", "const createRetry = require('../../retry')\nconst Lock = require('../../utils/lock')\nconst { KafkaJSNonRetriableError } = require('../../errors')\nconst COORDINATOR_TYPES = require('../../protocol/coordinatorTypes')\nconst createStateMachine = require('./transactionStateMachine')\nconst { INT_32_MAX_VALUE } = require('../../constants')\nconst assert = require('assert')\n\nconst STATES = require('./transactionStates')\nconst NO_PRODUCER_ID = -1\nconst SEQUENCE_START = 0\nconst INIT_PRODUCER_RETRIABLE_PROTOCOL_ERRORS = [\n  'NOT_COORDINATOR_FOR_GROUP',\n  'GROUP_COORDINATOR_NOT_AVAILABLE',\n  'GROUP_LOAD_IN_PROGRESS',\n  /**\n   * The producer might have crashed and never committed the transaction; retry the\n   * request so Kafka can abort the current transaction\n   * @see https://github.com/apache/kafka/blob/201da0542726472d954080d54bc585b111aaf86f/clients/src/main/java/org/apache/kafka/clients/producer/internals/TransactionManager.java#L1001-L1002\n   */\n  'CONCURRENT_TRANSACTIONS',\n]\nconst COMMIT_RETRIABLE_PROTOCOL_ERRORS = [\n  'UNKNOWN_TOPIC_OR_PARTITION',\n  'COORDINATOR_LOAD_IN_PROGRESS',\n]\nconst COMMIT_STALE_COORDINATOR_PROTOCOL_ERRORS = ['COORDINATOR_NOT_AVAILABLE', 'NOT_COORDINATOR']\n\n/**\n * @typedef {Object} EosManager\n */\n\n/**\n * Manage behavior for an idempotent producer and transactions.\n *\n * @returns {EosManager}\n */\nmodule.exports = ({\n  logger,\n  cluster,\n  transactionTimeout = 60000,\n  transactional,\n  transactionalId,\n}) => {\n  if (transactional && !transactionalId) {\n    throw new KafkaJSNonRetriableError('Cannot manage transactions without a transactionalId')\n  }\n\n  const retrier = createRetry(cluster.retry)\n\n  /**\n   * Current producer ID\n   */\n  let producerId = NO_PRODUCER_ID\n\n  /**\n   * Current producer epoch\n   */\n  let producerEpoch = 0\n\n  /**\n   * Idempotent production requires that the producer track the sequence number of messages.\n   *\n   * Sequences are sent with every Record Batch and tracked per Topic-Partition\n   */\n  let producerSequence = {}\n\n  /**\n   * Idempotent production requires a mutex lock per broker to serialize requests with sequence number handling\n   */\n  let brokerMutexLocks = {}\n\n  /**\n   * Topic partitions already participating in the transaction\n   */\n  let transactionTopicPartitions = {}\n\n  /**\n   * Offsets have been added to the transaction\n   */\n  let hasOffsetsAddedToTransaction = false\n\n  const stateMachine = createStateMachine({ logger })\n  stateMachine.on('transition', ({ to }) => {\n    if (to === STATES.READY) {\n      transactionTopicPartitions = {}\n      hasOffsetsAddedToTransaction = false\n    }\n  })\n\n  const findTransactionCoordinator = () => {\n    return cluster.findGroupCoordinator({\n      groupId: transactionalId,\n      coordinatorType: COORDINATOR_TYPES.TRANSACTION,\n    })\n  }\n\n  const transactionalGuard = () => {\n    if (!transactional) {\n      throw new KafkaJSNonRetriableError('Method unavailable if non-transactional')\n    }\n  }\n\n  /**\n   * A transaction is ongoing when offsets or partitions added to it\n   *\n   * @returns {boolean}\n   */\n  const isOngoing = () => {\n    return (\n      hasOffsetsAddedToTransaction ||\n      Object.entries(transactionTopicPartitions).some(([, partitions]) => {\n        return Object.entries(partitions).some(\n          ([, isPartitionAddedToTransaction]) => isPartitionAddedToTransaction\n        )\n      })\n    )\n  }\n\n  const eosManager = stateMachine.createGuarded(\n    {\n      /**\n       * Get the current producer id\n       * @returns {number}\n       */\n      getProducerId() {\n        return producerId\n      },\n\n      /**\n       * Get the current producer epoch\n       * @returns {number}\n       */\n      getProducerEpoch() {\n        return producerEpoch\n      },\n\n      getTransactionalId() {\n        return transactionalId\n      },\n\n      /**\n       * Initialize the idempotent producer by making an `InitProducerId` request.\n       * Overwrites any existing state in this transaction manager\n       */\n      async initProducerId() {\n        return retrier(async (bail, retryCount, retryTime) => {\n          try {\n            await cluster.refreshMetadataIfNecessary()\n\n            // If non-transactional we can request the PID from any broker\n            const broker = await (transactional\n              ? findTransactionCoordinator()\n              : cluster.findControllerBroker())\n\n            const result = await broker.initProducerId({\n              transactionalId: transactional ? transactionalId : undefined,\n              transactionTimeout,\n            })\n\n            stateMachine.transitionTo(STATES.READY)\n            producerId = result.producerId\n            producerEpoch = result.producerEpoch\n            producerSequence = {}\n            brokerMutexLocks = {}\n\n            logger.debug('Initialized producer id & epoch', { producerId, producerEpoch })\n          } catch (e) {\n            if (INIT_PRODUCER_RETRIABLE_PROTOCOL_ERRORS.includes(e.type)) {\n              if (e.type === 'CONCURRENT_TRANSACTIONS') {\n                logger.debug('There is an ongoing transaction on this transactionId, retrying', {\n                  error: e.message,\n                  stack: e.stack,\n                  transactionalId,\n                  retryCount,\n                  retryTime,\n                })\n              }\n\n              throw e\n            }\n\n            bail(e)\n          }\n        })\n      },\n\n      /**\n       * Get the current sequence for a given Topic-Partition. Defaults to 0.\n       *\n       * @param {string} topic\n       * @param {string} partition\n       * @returns {number}\n       */\n      getSequence(topic, partition) {\n        if (!eosManager.isInitialized()) {\n          return SEQUENCE_START\n        }\n\n        producerSequence[topic] = producerSequence[topic] || {}\n        producerSequence[topic][partition] = producerSequence[topic][partition] || SEQUENCE_START\n\n        return producerSequence[topic][partition]\n      },\n\n      /**\n       * Update the sequence for a given Topic-Partition.\n       *\n       * Do nothing if not yet initialized (not idempotent)\n       * @param {string} topic\n       * @param {string} partition\n       * @param {number} increment\n       */\n      updateSequence(topic, partition, increment) {\n        if (!eosManager.isInitialized()) {\n          return\n        }\n\n        const previous = eosManager.getSequence(topic, partition)\n        let sequence = previous + increment\n\n        // Sequence is defined as Int32 in the Record Batch,\n        // so theoretically should need to rotate here\n        if (sequence >= INT_32_MAX_VALUE) {\n          logger.debug(\n            `Sequence for ${topic} ${partition} exceeds max value (${sequence}). Rotating to 0.`\n          )\n          sequence = 0\n        }\n\n        producerSequence[topic][partition] = sequence\n      },\n\n      /**\n       * Begin a transaction\n       */\n      beginTransaction() {\n        transactionalGuard()\n        stateMachine.transitionTo(STATES.TRANSACTING)\n      },\n\n      /**\n       * Add partitions to a transaction if they are not already marked as participating.\n       *\n       * Should be called prior to sending any messages during a transaction\n       * @param {TopicData[]} topicData\n       *\n       * @typedef {Object} TopicData\n       * @property {string} topic\n       * @property {object[]} partitions\n       * @property {number} partitions[].partition\n       */\n      async addPartitionsToTransaction(topicData) {\n        transactionalGuard()\n        const newTopicPartitions = {}\n\n        topicData.forEach(({ topic, partitions }) => {\n          transactionTopicPartitions[topic] = transactionTopicPartitions[topic] || {}\n\n          partitions.forEach(({ partition }) => {\n            if (!transactionTopicPartitions[topic][partition]) {\n              newTopicPartitions[topic] = newTopicPartitions[topic] || []\n              newTopicPartitions[topic].push(partition)\n            }\n          })\n        })\n\n        const topics = Object.keys(newTopicPartitions).map(topic => ({\n          topic,\n          partitions: newTopicPartitions[topic],\n        }))\n\n        if (topics.length) {\n          const broker = await findTransactionCoordinator()\n          await broker.addPartitionsToTxn({ transactionalId, producerId, producerEpoch, topics })\n        }\n\n        topics.forEach(({ topic, partitions }) => {\n          partitions.forEach(partition => {\n            transactionTopicPartitions[topic][partition] = true\n          })\n        })\n      },\n\n      /**\n       * Commit the ongoing transaction\n       */\n      async commit() {\n        transactionalGuard()\n        stateMachine.transitionTo(STATES.COMMITTING)\n\n        if (!isOngoing()) {\n          logger.debug('No partitions or offsets registered, not sending EndTxn')\n\n          stateMachine.transitionTo(STATES.READY)\n          return\n        }\n\n        const broker = await findTransactionCoordinator()\n        await broker.endTxn({\n          producerId,\n          producerEpoch,\n          transactionalId,\n          transactionResult: true,\n        })\n\n        stateMachine.transitionTo(STATES.READY)\n      },\n\n      /**\n       * Abort the ongoing transaction\n       */\n      async abort() {\n        transactionalGuard()\n        stateMachine.transitionTo(STATES.ABORTING)\n\n        if (!isOngoing()) {\n          logger.debug('No partitions or offsets registered, not sending EndTxn')\n\n          stateMachine.transitionTo(STATES.READY)\n          return\n        }\n\n        const broker = await findTransactionCoordinator()\n        await broker.endTxn({\n          producerId,\n          producerEpoch,\n          transactionalId,\n          transactionResult: false,\n        })\n\n        stateMachine.transitionTo(STATES.READY)\n      },\n\n      /**\n       * Whether the producer id has already been initialized\n       */\n      isInitialized() {\n        return producerId !== NO_PRODUCER_ID\n      },\n\n      isTransactional() {\n        return transactional\n      },\n\n      isInTransaction() {\n        return stateMachine.state() === STATES.TRANSACTING\n      },\n\n      async acquireBrokerLock(broker) {\n        if (this.isInitialized()) {\n          brokerMutexLocks[broker.nodeId] =\n            brokerMutexLocks[broker.nodeId] || new Lock({ timeout: 0xffff })\n          await brokerMutexLocks[broker.nodeId].acquire()\n        }\n      },\n\n      releaseBrokerLock(broker) {\n        if (this.isInitialized()) brokerMutexLocks[broker.nodeId].release()\n      },\n\n      /**\n       * Mark the provided offsets as participating in the transaction for the given consumer group.\n       *\n       * This allows us to commit an offset as consumed only if the transaction passes.\n       * @param {string} consumerGroupId The unique group identifier\n       * @param {OffsetCommitTopic[]} topics The unique group identifier\n       * @returns {Promise}\n       *\n       * @typedef {Object} OffsetCommitTopic\n       * @property {string} topic\n       * @property {OffsetCommitTopicPartition[]} partitions\n       *\n       * @typedef {Object} OffsetCommitTopicPartition\n       * @property {number} partition\n       * @property {number} offset\n       */\n      async sendOffsets({ consumerGroupId, topics }) {\n        assert(consumerGroupId, 'Missing consumerGroupId')\n        assert(topics, 'Missing offset topics')\n\n        const transactionCoordinator = await findTransactionCoordinator()\n\n        // Do we need to add offsets if we've already done so for this consumer group?\n        await transactionCoordinator.addOffsetsToTxn({\n          transactionalId,\n          producerId,\n          producerEpoch,\n          groupId: consumerGroupId,\n        })\n\n        hasOffsetsAddedToTransaction = true\n\n        let groupCoordinator = await cluster.findGroupCoordinator({\n          groupId: consumerGroupId,\n          coordinatorType: COORDINATOR_TYPES.GROUP,\n        })\n\n        return retrier(async (bail, retryCount, retryTime) => {\n          try {\n            await groupCoordinator.txnOffsetCommit({\n              transactionalId,\n              producerId,\n              producerEpoch,\n              groupId: consumerGroupId,\n              topics,\n            })\n          } catch (e) {\n            if (COMMIT_RETRIABLE_PROTOCOL_ERRORS.includes(e.type)) {\n              logger.debug('Group coordinator is not ready yet, retrying', {\n                error: e.message,\n                stack: e.stack,\n                transactionalId,\n                retryCount,\n                retryTime,\n              })\n\n              throw e\n            }\n\n            if (\n              COMMIT_STALE_COORDINATOR_PROTOCOL_ERRORS.includes(e.type) ||\n              e.code === 'ECONNREFUSED'\n            ) {\n              logger.debug(\n                'Invalid group coordinator, finding new group coordinator and retrying',\n                {\n                  error: e.message,\n                  stack: e.stack,\n                  transactionalId,\n                  retryCount,\n                  retryTime,\n                }\n              )\n\n              groupCoordinator = await cluster.findGroupCoordinator({\n                groupId: consumerGroupId,\n                coordinatorType: COORDINATOR_TYPES.GROUP,\n              })\n\n              throw e\n            }\n\n            bail(e)\n          }\n        })\n      },\n    },\n\n    /**\n     * Transaction state guards\n     */\n    {\n      initProducerId: { legalStates: [STATES.UNINITIALIZED, STATES.READY] },\n      beginTransaction: { legalStates: [STATES.READY], async: false },\n      addPartitionsToTransaction: { legalStates: [STATES.TRANSACTING] },\n      sendOffsets: { legalStates: [STATES.TRANSACTING] },\n      commit: { legalStates: [STATES.TRANSACTING] },\n      abort: { legalStates: [STATES.TRANSACTING] },\n    }\n  )\n\n  return eosManager\n}\n", "module.exports = ({ topic, partitionMetadata, messages, partitioner }) => {\n  if (partitionMetadata.length === 0) {\n    return {}\n  }\n\n  return messages.reduce((result, message) => {\n    const partition = partitioner({ topic, partitionMetadata, message })\n    const current = result[partition] || []\n    return Object.assign(result, { [partition]: [...current, message] })\n  }, {})\n}\n", "module.exports = topicDataForBroker => {\n  return topicDataForBroker.map(\n    ({ topic, partitions, messagesPerPartition, sequencePerPartition }) => ({\n      topic,\n      partitions: partitions.map(partition => ({\n        partition,\n        messages: messagesPerPartition[partition],\n      })),\n    })\n  )\n}\n", "module.exports = ({ topics }) =>\n  topics.flatMap(({ topicName, partitions }) =>\n    partitions.map(partition => ({ topicName, ...partition }))\n  )\n", "const { KafkaJSMetadataNotLoaded } = require('../errors')\nconst { staleMetadata } = require('../protocol/error')\nconst groupMessagesPerPartition = require('./groupMessagesPerPartition')\nconst createTopicData = require('./createTopicData')\nconst responseSerializer = require('./responseSerializer')\n\nconst { keys } = Object\n\n/**\n * @param {Object} options\n * @param {import(\"../../types\").Logger} options.logger\n * @param {import(\"../../types\").Cluster} options.cluster\n * @param {ReturnType<import(\"../../types\").ICustomPartitioner>} options.partitioner\n * @param {import(\"./eosManager\").EosManager} options.eosManager\n * @param {import(\"../retry\").Retrier} options.retrier\n */\nmodule.exports = ({ logger, cluster, partitioner, eosManager, retrier }) => {\n  return async ({ acks, timeout, compression, topicMessages }) => {\n    /** @type {Map<import(\"../../types\").Broker, any[]>} */\n    const responsePerBroker = new Map()\n\n    /** @param {Map<import(\"../../types\").Broker, any[]>} responsePerBroker */\n    const createProducerRequests = async responsePerBroker => {\n      const topicMetadata = new Map()\n\n      await cluster.refreshMetadataIfNecessary()\n\n      for (const { topic, messages } of topicMessages) {\n        const partitionMetadata = cluster.findTopicPartitionMetadata(topic)\n\n        if (partitionMetadata.length === 0) {\n          logger.debug('Producing to topic without metadata', {\n            topic,\n            targetTopics: Array.from(cluster.targetTopics),\n          })\n\n          throw new KafkaJSMetadataNotLoaded('Producing to topic without metadata')\n        }\n\n        const messagesPerPartition = groupMessagesPerPartition({\n          topic,\n          partitionMetadata,\n          messages,\n          partitioner,\n        })\n\n        const partitions = keys(messagesPerPartition)\n        const partitionsPerLeader = cluster.findLeaderForPartitions(topic, partitions)\n        const leaders = keys(partitionsPerLeader)\n\n        topicMetadata.set(topic, {\n          partitionsPerLeader,\n          messagesPerPartition,\n        })\n\n        for (const nodeId of leaders) {\n          const broker = await cluster.findBroker({ nodeId })\n          if (!responsePerBroker.has(broker)) {\n            responsePerBroker.set(broker, null)\n          }\n        }\n      }\n\n      const brokers = Array.from(responsePerBroker.keys())\n      const brokersWithoutResponse = brokers.filter(broker => !responsePerBroker.get(broker))\n\n      return brokersWithoutResponse.map(async broker => {\n        const entries = Array.from(topicMetadata.entries())\n        const topicDataForBroker = entries\n          .filter(([_, { partitionsPerLeader }]) => !!partitionsPerLeader[broker.nodeId])\n          .map(([topic, { partitionsPerLeader, messagesPerPartition, sequencePerPartition }]) => ({\n            topic,\n            partitions: partitionsPerLeader[broker.nodeId],\n            messagesPerPartition,\n          }))\n\n        const topicData = createTopicData(topicDataForBroker)\n\n        await eosManager.acquireBrokerLock(broker)\n        try {\n          if (eosManager.isTransactional()) {\n            await eosManager.addPartitionsToTransaction(topicData)\n          }\n\n          topicData.forEach(({ topic, partitions }) => {\n            partitions.forEach(entry => {\n              entry['firstSequence'] = eosManager.getSequence(topic, entry.partition)\n              eosManager.updateSequence(topic, entry.partition, entry.messages.length)\n            })\n          })\n\n          let response\n          try {\n            response = await broker.produce({\n              transactionalId: eosManager.isTransactional()\n                ? eosManager.getTransactionalId()\n                : undefined,\n              producerId: eosManager.getProducerId(),\n              producerEpoch: eosManager.getProducerEpoch(),\n              acks,\n              timeout,\n              compression,\n              topicData,\n            })\n          } catch (e) {\n            topicData.forEach(({ topic, partitions }) => {\n              partitions.forEach(entry => {\n                eosManager.updateSequence(topic, entry.partition, -entry.messages.length)\n              })\n            })\n            throw e\n          }\n\n          const expectResponse = acks !== 0\n          const formattedResponse = expectResponse ? responseSerializer(response) : []\n\n          responsePerBroker.set(broker, formattedResponse)\n        } catch (e) {\n          responsePerBroker.delete(broker)\n          throw e\n        } finally {\n          await eosManager.releaseBrokerLock(broker)\n        }\n      })\n    }\n\n    return retrier(async (bail, retryCount, retryTime) => {\n      const topics = topicMessages.map(({ topic }) => topic)\n      await cluster.addMultipleTargetTopics(topics)\n\n      try {\n        const requests = await createProducerRequests(responsePerBroker)\n        await Promise.all(requests)\n        return Array.from(responsePerBroker.values()).flat()\n      } catch (e) {\n        if (e.name === 'KafkaJSConnectionClosedError') {\n          cluster.removeBroker({ host: e.host, port: e.port })\n        }\n\n        if (!cluster.isConnected()) {\n          logger.debug(`Cluster has disconnected, reconnecting: ${e.message}`, {\n            retryCount,\n            retryTime,\n          })\n          await cluster.connect()\n          await cluster.refreshMetadata()\n          throw e\n        }\n\n        // This is necessary in case the metadata is stale and the number of partitions\n        // for this topic has increased in the meantime\n        if (\n          staleMetadata(e) ||\n          e.name === 'KafkaJSMetadataNotLoaded' ||\n          e.name === 'KafkaJSConnectionError' ||\n          e.name === 'KafkaJSConnectionClosedError' ||\n          (e.name === 'KafkaJSProtocolError' && e.retriable)\n        ) {\n          logger.error(`Failed to send messages: ${e.message}`, { retryCount, retryTime })\n          await cluster.refreshMetadata()\n          throw e\n        }\n\n        logger.error(`${e.message}`, { retryCount, retryTime })\n        if (e.retriable) throw e\n        bail(e)\n      }\n    })\n  }\n}\n", "const createSendMessages = require('./sendMessages')\nconst { KafkaJSError, KafkaJSNonRetriableError } = require('../errors')\nconst { CONNECTION_STATUS } = require('../network/connectionStatus')\n\nmodule.exports = ({\n  logger,\n  cluster,\n  partitioner,\n  eosManager,\n  idempotent,\n  retrier,\n  getConnectionStatus,\n}) => {\n  const sendMessages = createSendMessages({\n    logger,\n    cluster,\n    retrier,\n    partitioner,\n    eosManager,\n  })\n\n  const validateConnectionStatus = () => {\n    const connectionStatus = getConnectionStatus()\n\n    switch (connectionStatus) {\n      case CONNECTION_STATUS.DISCONNECTING:\n        throw new KafkaJSNonRetriableError(\n          `The producer is disconnecting; therefore, it can't safely accept messages anymore`\n        )\n      case CONNECTION_STATUS.DISCONNECTED:\n        throw new KafkaJSError('The producer is disconnected')\n    }\n  }\n\n  /**\n   * @typedef {Object} TopicMessages\n   * @property {string} topic\n   * @property {Array} messages An array of objects with \"key\" and \"value\", example:\n   *                         [{ key: 'my-key', value: 'my-value'}]\n   *\n   * @typedef {Object} SendBatchRequest\n   * @property {Array<TopicMessages>} topicMessages\n   * @property {number} [acks=-1] Control the number of required acks.\n   *                           -1 = all replicas must acknowledge\n   *                            0 = no acknowledgments\n   *                            1 = only waits for the leader to acknowledge\n   *\n   * @property {number} [timeout=30000] The time to await a response in ms\n   * @property {Compression.Types} [compression=Compression.Types.None] Compression codec\n   *\n   * @param {SendBatchRequest}\n   * @returns {Promise}\n   */\n  const sendBatch = async ({ acks = -1, timeout, compression, topicMessages = [] }) => {\n    if (topicMessages.some(({ topic }) => !topic)) {\n      throw new KafkaJSNonRetriableError(`Invalid topic`)\n    }\n\n    if (idempotent && acks !== -1) {\n      throw new KafkaJSNonRetriableError(\n        `Not requiring ack for all messages invalidates the idempotent producer's EoS guarantees`\n      )\n    }\n\n    for (const { topic, messages } of topicMessages) {\n      if (!messages) {\n        throw new KafkaJSNonRetriableError(\n          `Invalid messages array [${messages}] for topic \"${topic}\"`\n        )\n      }\n\n      const messageWithoutValue = messages.find(message => message.value === undefined)\n      if (messageWithoutValue) {\n        throw new KafkaJSNonRetriableError(\n          `Invalid message without value for topic \"${topic}\": ${JSON.stringify(\n            messageWithoutValue\n          )}`\n        )\n      }\n    }\n\n    validateConnectionStatus()\n    const mergedTopicMessages = topicMessages.reduce((merged, { topic, messages }) => {\n      const index = merged.findIndex(({ topic: mergedTopic }) => topic === mergedTopic)\n\n      if (index === -1) {\n        merged.push({ topic, messages })\n      } else {\n        merged[index].messages = [...merged[index].messages, ...messages]\n      }\n\n      return merged\n    }, [])\n\n    return await sendMessages({\n      acks,\n      timeout,\n      compression,\n      topicMessages: mergedTopicMessages,\n    })\n  }\n\n  /**\n   * @param {ProduceRequest} ProduceRequest\n   * @returns {Promise}\n   *\n   * @typedef {Object} ProduceRequest\n   * @property {string} topic\n   * @property {Array} messages An array of objects with \"key\" and \"value\", example:\n   *                         [{ key: 'my-key', value: 'my-value'}]\n   * @property {number} [acks=-1] Control the number of required acks.\n   *                           -1 = all replicas must acknowledge\n   *                            0 = no acknowledgments\n   *                            1 = only waits for the leader to acknowledge\n   * @property {number} [timeout=30000] The time to await a response in ms\n   * @property {Compression.Types} [compression=Compression.Types.None] Compression codec\n   */\n  const send = async ({ acks, timeout, compression, topic, messages }) => {\n    const topicMessage = { topic, messages }\n    return sendBatch({\n      acks,\n      timeout,\n      compression,\n      topicMessages: [topicMessage],\n    })\n  }\n\n  return {\n    send,\n    sendBatch,\n  }\n}\n", "const { keys } = Object\nmodule.exports = object =>\n  keys(object).reduce((result, key) => ({ ...result, [object[key]]: key }), {})\n", "const swapObject = require('../utils/swapObject')\nconst networkEvents = require('../network/instrumentationEvents')\nconst InstrumentationEventType = require('../instrumentation/eventType')\nconst producerType = InstrumentationEventType('producer')\n\nconst events = {\n  CONNECT: producerType('connect'),\n  DISCONNECT: producerType('disconnect'),\n  REQUEST: producerType(networkEvents.NETWORK_REQUEST),\n  REQUEST_TIMEOUT: producerType(networkEvents.NETWORK_REQUEST_TIMEOUT),\n  REQUEST_QUEUE_SIZE: producerType(networkEvents.NETWORK_REQUEST_QUEUE_SIZE),\n}\n\nconst wrappedEvents = {\n  [events.REQUEST]: networkEvents.NETWORK_REQUEST,\n  [events.REQUEST_TIMEOUT]: networkEvents.NETWORK_REQUEST_TIMEOUT,\n  [events.REQUEST_QUEUE_SIZE]: networkEvents.NETWORK_REQUEST_QUEUE_SIZE,\n}\n\nconst reversedWrappedEvents = swapObject(wrappedEvents)\nconst unwrap = eventName => wrappedEvents[eventName] || eventName\nconst wrap = eventName => reversedWrappedEvents[eventName] || eventName\n\nmodule.exports = {\n  events,\n  wrap,\n  unwrap,\n}\n", "const createRetry = require('../retry')\nconst { CONNECTION_STATUS } = require('../network/connectionStatus')\nconst { DefaultPartitioner } = require('./partitioners/')\nconst InstrumentationEventEmitter = require('../instrumentation/emitter')\nconst createEosManager = require('./eosManager')\nconst createMessageProducer = require('./messageProducer')\nconst { events, wrap: wrapEvent, unwrap: unwrapEvent } = require('./instrumentationEvents')\nconst { KafkaJSNonRetriableError } = require('../errors')\n\nconst { values, keys } = Object\nconst eventNames = values(events)\nconst eventKeys = keys(events)\n  .map(key => `producer.events.${key}`)\n  .join(', ')\n\nconst { CONNECT, DISCONNECT } = events\n\n/**\n *\n * @param {Object} params\n * @param {import('../../types').Cluster} params.cluster\n * @param {import('../../types').Logger} params.logger\n * @param {import('../../types').ICustomPartitioner} [params.createPartitioner]\n * @param {import('../../types').RetryOptions} [params.retry]\n * @param {boolean} [params.idempotent]\n * @param {string} [params.transactionalId]\n * @param {number} [params.transactionTimeout]\n * @param {InstrumentationEventEmitter} [params.instrumentationEmitter]\n *\n * @returns {import('../../types').Producer}\n */\nmodule.exports = ({\n  cluster,\n  logger: rootLogger,\n  createPartitioner = DefaultPartitioner,\n  retry,\n  idempotent = false,\n  transactionalId,\n  transactionTimeout,\n  instrumentationEmitter: rootInstrumentationEmitter,\n}) => {\n  let connectionStatus = CONNECTION_STATUS.DISCONNECTED\n  retry = retry || { retries: idempotent ? Number.MAX_SAFE_INTEGER : 5 }\n\n  if (idempotent && retry.retries < 1) {\n    throw new KafkaJSNonRetriableError(\n      'Idempotent producer must allow retries to protect against transient errors'\n    )\n  }\n\n  const logger = rootLogger.namespace('Producer')\n\n  if (idempotent && retry.retries < Number.MAX_SAFE_INTEGER) {\n    logger.warn('Limiting retries for the idempotent producer may invalidate EoS guarantees')\n  }\n\n  const partitioner = createPartitioner()\n  const retrier = createRetry(Object.assign({}, cluster.retry, retry))\n  const instrumentationEmitter = rootInstrumentationEmitter || new InstrumentationEventEmitter()\n  const idempotentEosManager = createEosManager({\n    logger,\n    cluster,\n    transactionTimeout,\n    transactional: false,\n    transactionalId,\n  })\n\n  const { send, sendBatch } = createMessageProducer({\n    logger,\n    cluster,\n    partitioner,\n    eosManager: idempotentEosManager,\n    idempotent,\n    retrier,\n    getConnectionStatus: () => connectionStatus,\n  })\n\n  let transactionalEosManager\n\n  /** @type {import(\"../../types\").Producer[\"on\"]} */\n  const on = (eventName, listener) => {\n    if (!eventNames.includes(eventName)) {\n      throw new KafkaJSNonRetriableError(`Event name should be one of ${eventKeys}`)\n    }\n\n    return instrumentationEmitter.addListener(unwrapEvent(eventName), event => {\n      event.type = wrapEvent(event.type)\n      Promise.resolve(listener(event)).catch(e => {\n        logger.error(`Failed to execute listener: ${e.message}`, {\n          eventName,\n          stack: e.stack,\n        })\n      })\n    })\n  }\n\n  /**\n   * Begin a transaction. The returned object contains methods to send messages\n   * to the transaction and end the transaction by committing or aborting.\n   *\n   * Only messages sent on the transaction object will participate in the transaction.\n   *\n   * Calling any of the transactional methods after the transaction has ended\n   * will raise an exception (use `isActive` to ascertain if ended).\n   * @returns {Promise<Transaction>}\n   *\n   * @typedef {Object} Transaction\n   * @property {Function} send  Identical to the producer \"send\" method\n   * @property {Function} sendBatch Identical to the producer \"sendBatch\" method\n   * @property {Function} abort Abort the transaction\n   * @property {Function} commit  Commit the transaction\n   * @property {Function} isActive  Whether the transaction is active\n   */\n  const transaction = async () => {\n    if (!transactionalId) {\n      throw new KafkaJSNonRetriableError('Must provide transactional id for transactional producer')\n    }\n\n    let transactionDidEnd = false\n    transactionalEosManager =\n      transactionalEosManager ||\n      createEosManager({\n        logger,\n        cluster,\n        transactionTimeout,\n        transactional: true,\n        transactionalId,\n      })\n\n    if (transactionalEosManager.isInTransaction()) {\n      throw new KafkaJSNonRetriableError(\n        'There is already an ongoing transaction for this producer. Please end the transaction before beginning another.'\n      )\n    }\n\n    // We only initialize the producer id once\n    if (!transactionalEosManager.isInitialized()) {\n      await transactionalEosManager.initProducerId()\n    }\n    transactionalEosManager.beginTransaction()\n\n    const { send: sendTxn, sendBatch: sendBatchTxn } = createMessageProducer({\n      logger,\n      cluster,\n      partitioner,\n      retrier,\n      eosManager: transactionalEosManager,\n      idempotent: true,\n      getConnectionStatus: () => connectionStatus,\n    })\n\n    const isActive = () => transactionalEosManager.isInTransaction() && !transactionDidEnd\n\n    const transactionGuard = fn => (...args) => {\n      if (!isActive()) {\n        return Promise.reject(\n          new KafkaJSNonRetriableError('Cannot continue to use transaction once ended')\n        )\n      }\n\n      return fn(...args)\n    }\n\n    return {\n      sendBatch: transactionGuard(sendBatchTxn),\n      send: transactionGuard(sendTxn),\n      /**\n       * Abort the ongoing transaction.\n       *\n       * @throws {KafkaJSNonRetriableError} If transaction has ended\n       */\n      abort: transactionGuard(async () => {\n        await transactionalEosManager.abort()\n        transactionDidEnd = true\n      }),\n      /**\n       * Commit the ongoing transaction.\n       *\n       * @throws {KafkaJSNonRetriableError} If transaction has ended\n       */\n      commit: transactionGuard(async () => {\n        await transactionalEosManager.commit()\n        transactionDidEnd = true\n      }),\n      /**\n       * Sends a list of specified offsets to the consumer group coordinator, and also marks those offsets as part of the current transaction.\n       *\n       * @throws {KafkaJSNonRetriableError} If transaction has ended\n       */\n      sendOffsets: transactionGuard(async ({ consumerGroupId, topics }) => {\n        await transactionalEosManager.sendOffsets({ consumerGroupId, topics })\n\n        for (const topicOffsets of topics) {\n          const { topic, partitions } = topicOffsets\n          for (const { partition, offset } of partitions) {\n            cluster.markOffsetAsCommitted({\n              groupId: consumerGroupId,\n              topic,\n              partition,\n              offset,\n            })\n          }\n        }\n      }),\n      isActive,\n    }\n  }\n\n  /**\n   * @returns {Object} logger\n   */\n  const getLogger = () => logger\n\n  return {\n    /**\n     * @returns {Promise}\n     */\n    connect: async () => {\n      await cluster.connect()\n      connectionStatus = CONNECTION_STATUS.CONNECTED\n      instrumentationEmitter.emit(CONNECT)\n\n      if (idempotent && !idempotentEosManager.isInitialized()) {\n        await idempotentEosManager.initProducerId()\n      }\n    },\n    /**\n     * @return {Promise}\n     */\n    disconnect: async () => {\n      connectionStatus = CONNECTION_STATUS.DISCONNECTING\n      await cluster.disconnect()\n      connectionStatus = CONNECTION_STATUS.DISCONNECTED\n      instrumentationEmitter.emit(DISCONNECT)\n    },\n    isIdempotent: () => {\n      return idempotent\n    },\n    events,\n    on,\n    send,\n    sendBatch,\n    transaction,\n    logger: getLogger,\n  }\n}\n", "module.exports = timeInMs =>\n  new Promise(resolve => {\n    setTimeout(resolve, timeInMs)\n  })\n", "const Long = require('../../utils/long')\n\nmodule.exports = offset => (!offset && offset !== 0) || Long.fromValue(offset).isNegative()\n", "const isInvalidOffset = require('./isInvalidOffset')\nconst { keys, assign } = Object\n\nconst indexPartitions = (obj, { partition, offset }) => assign(obj, { [partition]: offset })\nconst indexTopics = (obj, { topic, partitions }) =>\n  assign(obj, { [topic]: partitions.reduce(indexPartitions, {}) })\n\nmodule.exports = (consumerOffsets, topicOffsets) => {\n  const indexedConsumerOffsets = consumerOffsets.reduce(indexTopics, {})\n  const indexedTopicOffsets = topicOffsets.reduce(indexTopics, {})\n\n  return keys(indexedConsumerOffsets).map(topic => {\n    const partitions = indexedConsumerOffsets[topic]\n    return {\n      topic,\n      partitions: keys(partitions).map(partition => {\n        const offset = partitions[partition]\n        const resolvedOffset = isInvalidOffset(offset)\n          ? indexedTopicOffsets[topic][partition]\n          : offset\n\n        return { partition: Number(partition), offset: resolvedOffset }\n      }),\n    }\n  })\n}\n", "const swapObject = require('../utils/swapObject')\nconst InstrumentationEventType = require('../instrumentation/eventType')\nconst networkEvents = require('../network/instrumentationEvents')\nconst consumerType = InstrumentationEventType('consumer')\n\n/** @type {import('types').ConsumerEvents} */\nconst events = {\n  HEARTBEAT: consumerType('heartbeat'),\n  COMMIT_OFFSETS: consumerType('commit_offsets'),\n  GROUP_JOIN: consumerType('group_join'),\n  FETCH: consumerType('fetch'),\n  FETCH_START: consumerType('fetch_start'),\n  START_BATCH_PROCESS: consumerType('start_batch_process'),\n  END_BATCH_PROCESS: consumerType('end_batch_process'),\n  CONNECT: consumerType('connect'),\n  DISCONNECT: consumerType('disconnect'),\n  STOP: consumerType('stop'),\n  CRASH: consumerType('crash'),\n  REBALANCING: consumerType('rebalancing'),\n  RECEIVED_UNSUBSCRIBED_TOPICS: consumerType('received_unsubscribed_topics'),\n  REQUEST: consumerType(networkEvents.NETWORK_REQUEST),\n  REQUEST_TIMEOUT: consumerType(networkEvents.NETWORK_REQUEST_TIMEOUT),\n  REQUEST_QUEUE_SIZE: consumerType(networkEvents.NETWORK_REQUEST_QUEUE_SIZE),\n}\n\nconst wrappedEvents = {\n  [events.REQUEST]: networkEvents.NETWORK_REQUEST,\n  [events.REQUEST_TIMEOUT]: networkEvents.NETWORK_REQUEST_TIMEOUT,\n  [events.REQUEST_QUEUE_SIZE]: networkEvents.NETWORK_REQUEST_QUEUE_SIZE,\n}\n\nconst reversedWrappedEvents = swapObject(wrappedEvents)\nconst unwrap = eventName => wrappedEvents[eventName] || eventName\nconst wrap = eventName => reversedWrappedEvents[eventName] || eventName\n\nmodule.exports = {\n  events,\n  wrap,\n  unwrap,\n}\n", "const Long = require('../../utils/long')\nconst isInvalidOffset = require('./isInvalidOffset')\nconst initializeConsumerOffsets = require('./initializeConsumerOffsets')\nconst {\n  events: { COMMIT_OFFSETS },\n} = require('../instrumentationEvents')\n\nconst { keys, assign } = Object\nconst indexTopics = topics => topics.reduce((obj, topic) => assign(obj, { [topic]: {} }), {})\n\nconst PRIVATE = {\n  COMMITTED_OFFSETS: Symbol('private:OffsetManager:committedOffsets'),\n}\nmodule.exports = class OffsetManager {\n  /**\n   * @param {Object} options\n   * @param {import(\"../../../types\").Cluster} options.cluster\n   * @param {import(\"../../../types\").Broker} options.coordinator\n   * @param {import(\"../../../types\").IMemberAssignment} options.memberAssignment\n   * @param {boolean} options.autoCommit\n   * @param {number | null} options.autoCommitInterval\n   * @param {number | null} options.autoCommitThreshold\n   * @param {{[topic: string]: { fromBeginning: boolean }}} options.topicConfigurations\n   * @param {import(\"../../instrumentation/emitter\")} options.instrumentationEmitter\n   * @param {string} options.groupId\n   * @param {number} options.generationId\n   * @param {string} options.memberId\n   */\n  constructor({\n    cluster,\n    coordinator,\n    memberAssignment,\n    autoCommit,\n    autoCommitInterval,\n    autoCommitThreshold,\n    topicConfigurations,\n    instrumentationEmitter,\n    groupId,\n    generationId,\n    memberId,\n  }) {\n    this.cluster = cluster\n    this.coordinator = coordinator\n\n    // memberAssignment format:\n    // {\n    //   'topic1': [0, 1, 2, 3],\n    //   'topic2': [0, 1, 2, 3, 4, 5],\n    // }\n    this.memberAssignment = memberAssignment\n\n    this.topicConfigurations = topicConfigurations\n    this.instrumentationEmitter = instrumentationEmitter\n    this.groupId = groupId\n    this.generationId = generationId\n    this.memberId = memberId\n\n    this.autoCommit = autoCommit\n    this.autoCommitInterval = autoCommitInterval\n    this.autoCommitThreshold = autoCommitThreshold\n    this.lastCommit = Date.now()\n\n    this.topics = keys(memberAssignment)\n    this.clearAllOffsets()\n  }\n\n  /**\n   * @param {string} topic\n   * @param {number} partition\n   * @returns {Long}\n   */\n  nextOffset(topic, partition) {\n    if (!this.resolvedOffsets[topic][partition]) {\n      this.resolvedOffsets[topic][partition] = this.committedOffsets()[topic][partition]\n    }\n\n    let offset = this.resolvedOffsets[topic][partition]\n    if (isInvalidOffset(offset)) {\n      offset = '0'\n    }\n\n    return Long.fromValue(offset)\n  }\n\n  /**\n   * @returns {Promise<import(\"../../../types\").Broker>}\n   */\n  async getCoordinator() {\n    if (!this.coordinator.isConnected()) {\n      this.coordinator = await this.cluster.findBroker(this.coordinator)\n    }\n\n    return this.coordinator\n  }\n\n  /**\n   * @param {import(\"../../../types\").TopicPartition} topicPartition\n   */\n  resetOffset({ topic, partition }) {\n    this.resolvedOffsets[topic][partition] = this.committedOffsets()[topic][partition]\n  }\n\n  /**\n   * @param {import(\"../../../types\").TopicPartitionOffset} topicPartitionOffset\n   */\n  resolveOffset({ topic, partition, offset }) {\n    this.resolvedOffsets[topic][partition] = Long.fromValue(offset)\n      .add(1)\n      .toString()\n  }\n\n  /**\n   * @returns {Long}\n   */\n  countResolvedOffsets() {\n    const committedOffsets = this.committedOffsets()\n\n    const subtractOffsets = (resolvedOffset, committedOffset) => {\n      const resolvedOffsetLong = Long.fromValue(resolvedOffset)\n      return isInvalidOffset(committedOffset)\n        ? resolvedOffsetLong\n        : resolvedOffsetLong.subtract(Long.fromValue(committedOffset))\n    }\n\n    const subtractPartitionOffsets = (resolvedTopicOffsets, committedTopicOffsets) =>\n      keys(resolvedTopicOffsets).map(partition =>\n        subtractOffsets(resolvedTopicOffsets[partition], committedTopicOffsets[partition])\n      )\n\n    const subtractTopicOffsets = topic =>\n      subtractPartitionOffsets(this.resolvedOffsets[topic], committedOffsets[topic])\n\n    const offsetsDiff = this.topics.flatMap(subtractTopicOffsets)\n    return offsetsDiff.reduce((sum, offset) => sum.add(offset), Long.fromValue(0))\n  }\n\n  /**\n   * @param {import(\"../../../types\").TopicPartition} topicPartition\n   */\n  async setDefaultOffset({ topic, partition }) {\n    const { groupId, generationId, memberId } = this\n    const defaultOffset = this.cluster.defaultOffset(this.topicConfigurations[topic])\n    const coordinator = await this.getCoordinator()\n\n    await coordinator.offsetCommit({\n      groupId,\n      memberId,\n      groupGenerationId: generationId,\n      topics: [\n        {\n          topic,\n          partitions: [{ partition, offset: defaultOffset }],\n        },\n      ],\n    })\n\n    this.clearOffsets({ topic, partition })\n  }\n\n  /**\n   * Commit the given offset to the topic/partition. If the consumer isn't assigned to the given\n   * topic/partition this method will be a NO-OP.\n   *\n   * @param {import(\"../../../types\").TopicPartitionOffset} topicPartitionOffset\n   */\n  async seek({ topic, partition, offset }) {\n    if (!this.memberAssignment[topic] || !this.memberAssignment[topic].includes(partition)) {\n      return\n    }\n\n    if (!this.autoCommit) {\n      this.resolveOffset({\n        topic,\n        partition,\n        offset: Long.fromValue(offset)\n          .subtract(1)\n          .toString(),\n      })\n      return\n    }\n\n    const { groupId, generationId, memberId } = this\n    const coordinator = await this.getCoordinator()\n\n    await coordinator.offsetCommit({\n      groupId,\n      memberId,\n      groupGenerationId: generationId,\n      topics: [\n        {\n          topic,\n          partitions: [{ partition, offset }],\n        },\n      ],\n    })\n\n    this.clearOffsets({ topic, partition })\n  }\n\n  async commitOffsetsIfNecessary() {\n    const now = Date.now()\n\n    const timeoutReached =\n      this.autoCommitInterval != null && now >= this.lastCommit + this.autoCommitInterval\n\n    const thresholdReached =\n      this.autoCommitThreshold != null &&\n      this.countResolvedOffsets().gte(Long.fromValue(this.autoCommitThreshold))\n\n    if (timeoutReached || thresholdReached) {\n      return this.commitOffsets()\n    }\n  }\n\n  /**\n   * Return all locally resolved offsets which are not marked as committed, by topic-partition.\n   * @returns {import('../../../types').OffsetsByTopicPartition}\n   */\n  uncommittedOffsets() {\n    const offsets = topic => keys(this.resolvedOffsets[topic])\n    const emptyPartitions = ({ partitions }) => partitions.length > 0\n    const toPartitions = topic => partition => ({\n      partition,\n      offset: this.resolvedOffsets[topic][partition],\n    })\n    const changedOffsets = topic => ({ partition, offset }) => {\n      return (\n        offset !== this.committedOffsets()[topic][partition] &&\n        Long.fromValue(offset).greaterThanOrEqual(0)\n      )\n    }\n\n    // Select and format updated partitions\n    const topicsWithPartitionsToCommit = this.topics\n      .map(topic => ({\n        topic,\n        partitions: offsets(topic)\n          .map(toPartitions(topic))\n          .filter(changedOffsets(topic)),\n      }))\n      .filter(emptyPartitions)\n\n    return { topics: topicsWithPartitionsToCommit }\n  }\n\n  async commitOffsets(offsets = {}) {\n    const { groupId, generationId, memberId } = this\n    const { topics = this.uncommittedOffsets().topics } = offsets\n\n    if (topics.length === 0) {\n      this.lastCommit = Date.now()\n      return\n    }\n\n    const payload = {\n      groupId,\n      memberId,\n      groupGenerationId: generationId,\n      topics,\n    }\n\n    try {\n      const coordinator = await this.getCoordinator()\n      await coordinator.offsetCommit(payload)\n      this.instrumentationEmitter.emit(COMMIT_OFFSETS, payload)\n\n      // Update local reference of committed offsets\n      topics.forEach(({ topic, partitions }) => {\n        const updatedOffsets = partitions.reduce(\n          (obj, { partition, offset }) => assign(obj, { [partition]: offset }),\n          {}\n        )\n\n        this[PRIVATE.COMMITTED_OFFSETS][topic] = assign(\n          {},\n          this.committedOffsets()[topic],\n          updatedOffsets\n        )\n      })\n\n      this.lastCommit = Date.now()\n    } catch (e) {\n      // metadata is stale, the coordinator has changed due to a restart or\n      // broker reassignment\n      if (e.type === 'NOT_COORDINATOR_FOR_GROUP') {\n        await this.cluster.refreshMetadata()\n      }\n\n      throw e\n    }\n  }\n\n  async resolveOffsets() {\n    const { groupId } = this\n    const invalidOffset = topic => partition => {\n      return isInvalidOffset(this.committedOffsets()[topic][partition])\n    }\n\n    const pendingPartitions = this.topics\n      .map(topic => ({\n        topic,\n        partitions: this.memberAssignment[topic]\n          .filter(invalidOffset(topic))\n          .map(partition => ({ partition })),\n      }))\n      .filter(t => t.partitions.length > 0)\n\n    if (pendingPartitions.length === 0) {\n      return\n    }\n\n    const coordinator = await this.getCoordinator()\n    const { responses: consumerOffsets } = await coordinator.offsetFetch({\n      groupId,\n      topics: pendingPartitions,\n    })\n\n    const unresolvedPartitions = consumerOffsets.map(({ topic, partitions }) =>\n      assign(\n        {\n          topic,\n          partitions: partitions\n            .filter(({ offset }) => isInvalidOffset(offset))\n            .map(({ partition }) => assign({ partition })),\n        },\n        this.topicConfigurations[topic]\n      )\n    )\n\n    const indexPartitions = (obj, { partition, offset }) => {\n      return assign(obj, { [partition]: offset })\n    }\n\n    const hasUnresolvedPartitions = () => unresolvedPartitions.some(t => t.partitions.length > 0)\n\n    let offsets = consumerOffsets\n    if (hasUnresolvedPartitions()) {\n      const topicOffsets = await this.cluster.fetchTopicsOffset(unresolvedPartitions)\n      offsets = initializeConsumerOffsets(consumerOffsets, topicOffsets)\n    }\n\n    offsets.forEach(({ topic, partitions }) => {\n      this.committedOffsets()[topic] = partitions.reduce(indexPartitions, {\n        ...this.committedOffsets()[topic],\n      })\n    })\n  }\n\n  /**\n   * @private\n   * @param {import(\"../../../types\").TopicPartition} topicPartition\n   */\n  clearOffsets({ topic, partition }) {\n    delete this.committedOffsets()[topic][partition]\n    delete this.resolvedOffsets[topic][partition]\n  }\n\n  /**\n   * @private\n   */\n  clearAllOffsets() {\n    const committedOffsets = this.committedOffsets()\n\n    for (const topic in committedOffsets) {\n      delete committedOffsets[topic]\n    }\n\n    for (const topic of this.topics) {\n      committedOffsets[topic] = {}\n    }\n\n    this.resolvedOffsets = indexTopics(this.topics)\n  }\n\n  committedOffsets() {\n    if (!this[PRIVATE.COMMITTED_OFFSETS]) {\n      this[PRIVATE.COMMITTED_OFFSETS] = this.groupId\n        ? this.cluster.committedOffsets({ groupId: this.groupId })\n        : {}\n    }\n\n    return this[PRIVATE.COMMITTED_OFFSETS]\n  }\n}\n", "const Long = require('../utils/long')\nconst ABORTED_MESSAGE_KEY = Buffer.from([0, 0, 0, 0])\n\nconst isAbortMarker = ({ key }) => {\n  // Handle null/undefined keys.\n  if (!key) return false\n  // Cast key to buffer defensively\n  return Buffer.from(key).equals(ABORTED_MESSAGE_KEY)\n}\n\n/**\n * Remove messages marked as aborted according to the aborted transactions list.\n *\n * Start of an aborted transaction is determined by message offset.\n * End of an aborted transaction is determined by control messages.\n * @param {Message[]} messages\n * @param {Transaction[]} [abortedTransactions]\n * @returns {Message[]} Messages which did not participate in an aborted transaction\n *\n * @typedef {object} Message\n * @param {Buffer} key\n * @param {lastOffset} key  Int64\n * @param {RecordBatch}  batchContext\n *\n * @typedef {object} Transaction\n * @param {string} firstOffset  Int64\n * @param {string} producerId  Int64\n *\n * @typedef {object} RecordBatch\n * @param {string}  producerId  Int64\n * @param {boolean}  inTransaction\n */\nmodule.exports = ({ messages, abortedTransactions }) => {\n  const currentAbortedTransactions = new Map()\n\n  if (!abortedTransactions || !abortedTransactions.length) {\n    return messages\n  }\n\n  const remainingAbortedTransactions = [...abortedTransactions]\n\n  return messages.filter(message => {\n    // If the message offset is GTE the first offset of the next aborted transaction\n    // then we have stepped into an aborted transaction.\n    if (\n      remainingAbortedTransactions.length &&\n      Long.fromValue(message.offset).gte(remainingAbortedTransactions[0].firstOffset)\n    ) {\n      const { producerId } = remainingAbortedTransactions.shift()\n      currentAbortedTransactions.set(producerId, true)\n    }\n\n    const { producerId, inTransaction } = message.batchContext\n\n    if (isAbortMarker(message)) {\n      // Transaction is over, we no longer need to ignore messages from this producer\n      currentAbortedTransactions.delete(producerId)\n    } else if (currentAbortedTransactions.has(producerId) && inTransaction) {\n      return false\n    }\n\n    return true\n  })\n}\n", "const Long = require('../utils/long')\nconst filterAbortedMessages = require('./filterAbortedMessages')\n\n/**\n * A batch collects messages returned from a single fetch call.\n *\n * A batch could contain _multiple_ Kafka RecordBatches.\n */\nmodule.exports = class Batch {\n  constructor(topic, fetchedOffset, partitionData) {\n    this.fetchedOffset = fetchedOffset\n    const longFetchedOffset = Long.fromValue(this.fetchedOffset)\n    const { abortedTransactions, messages } = partitionData\n\n    this.topic = topic\n    this.partition = partitionData.partition\n    this.highWatermark = partitionData.highWatermark\n\n    this.rawMessages = messages\n    // Apparently fetch can return different offsets than the target offset provided to the fetch API.\n    // Discard messages that are not in the requested offset\n    // https://github.com/apache/kafka/blob/bf237fa7c576bd141d78fdea9f17f65ea269c290/clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java#L912\n    this.messagesWithinOffset = this.rawMessages.filter(message =>\n      Long.fromValue(message.offset).gte(longFetchedOffset)\n    )\n\n    // 1. Don't expose aborted messages\n    // 2. Don't expose control records\n    // @see https://kafka.apache.org/documentation/#controlbatch\n    this.messages = filterAbortedMessages({\n      messages: this.messagesWithinOffset,\n      abortedTransactions,\n    }).filter(message => !message.isControlRecord)\n  }\n\n  isEmpty() {\n    return this.messages.length === 0\n  }\n\n  isEmptyIncludingFiltered() {\n    return this.messagesWithinOffset.length === 0\n  }\n\n  /**\n   * If the batch contained raw messages (i.e was not truly empty) but all messages were filtered out due to\n   * log compaction, control records or other reasons\n   */\n  isEmptyDueToFiltering() {\n    return this.isEmpty() && this.rawMessages.length > 0\n  }\n\n  isEmptyControlRecord() {\n    return (\n      this.isEmpty() && this.messagesWithinOffset.some(({ isControlRecord }) => isControlRecord)\n    )\n  }\n\n  /**\n   * With compressed messages, it's possible for the returned messages to have offsets smaller than the starting offset.\n   * These messages will be filtered out (i.e. they are not even included in this.messagesWithinOffset)\n   * If these are the only messages, the batch will appear as an empty batch.\n   *\n   * isEmpty() and isEmptyIncludingFiltered() will always return true if the batch is empty,\n   * but this method will only return true if the batch is empty due to log compacted messages.\n   *\n   * @returns boolean True if the batch is empty, because of log compacted messages in the partition.\n   */\n  isEmptyDueToLogCompactedMessages() {\n    const hasMessages = this.rawMessages.length > 0\n    return hasMessages && this.isEmptyIncludingFiltered()\n  }\n\n  firstOffset() {\n    return this.isEmptyIncludingFiltered() ? null : this.messagesWithinOffset[0].offset\n  }\n\n  lastOffset() {\n    if (this.isEmptyDueToLogCompactedMessages()) {\n      return this.fetchedOffset\n    }\n\n    if (this.isEmptyIncludingFiltered()) {\n      return Long.fromValue(this.highWatermark)\n        .add(-1)\n        .toString()\n    }\n\n    return this.messagesWithinOffset[this.messagesWithinOffset.length - 1].offset\n  }\n\n  /**\n   * Returns the lag based on the last offset in the batch (also known as \"high\")\n   */\n  offsetLag() {\n    const lastOffsetOfPartition = Long.fromValue(this.highWatermark).add(-1)\n    const lastConsumedOffset = Long.fromValue(this.lastOffset())\n    return lastOffsetOfPartition.add(lastConsumedOffset.multiply(-1)).toString()\n  }\n\n  /**\n   * Returns the lag based on the first offset in the batch\n   */\n  offsetLagLow() {\n    if (this.isEmptyIncludingFiltered()) {\n      return '0'\n    }\n\n    const lastOffsetOfPartition = Long.fromValue(this.highWatermark).add(-1)\n    const firstConsumedOffset = Long.fromValue(this.firstOffset())\n    return lastOffsetOfPartition.add(firstConsumedOffset.multiply(-1)).toString()\n  }\n}\n", "module.exports = class SeekOffsets extends Map {\n  getKey(topic, partition) {\n    return JSON.stringify([topic, partition])\n  }\n\n  set(topic, partition, offset) {\n    const key = this.getKey(topic, partition)\n    super.set(key, offset)\n  }\n\n  has(topic, partition) {\n    const key = this.getKey(topic, partition)\n    return super.has(key)\n  }\n\n  pop(topic, partition) {\n    if (this.size === 0 || !this.has(topic, partition)) {\n      return\n    }\n\n    const key = this.getKey(topic, partition)\n    const offset = this.get(key)\n\n    this.delete(key)\n    return { topic, partition, offset }\n  }\n}\n", "const createState = topic => ({\n  topic,\n  paused: new Set(),\n  pauseAll: false,\n  resumed: new Set(),\n})\n\nmodule.exports = class SubscriptionState {\n  constructor() {\n    this.assignedPartitionsByTopic = {}\n    this.subscriptionStatesByTopic = {}\n  }\n\n  /**\n   * Replace the current assignment with a new set of assignments\n   *\n   * @param {Array<TopicPartitions>} topicPartitions Example: [{ topic: 'topic-name', partitions: [1, 2] }]\n   */\n  assign(topicPartitions = []) {\n    this.assignedPartitionsByTopic = topicPartitions.reduce(\n      (assigned, { topic, partitions = [] }) => {\n        return { ...assigned, [topic]: { topic, partitions } }\n      },\n      {}\n    )\n  }\n\n  /**\n   * @param {Array<TopicPartitions>} topicPartitions Example: [{ topic: 'topic-name', partitions: [1, 2] }]\n   */\n  pause(topicPartitions = []) {\n    topicPartitions.forEach(({ topic, partitions }) => {\n      const state = this.subscriptionStatesByTopic[topic] || createState(topic)\n\n      if (typeof partitions === 'undefined') {\n        state.paused.clear()\n        state.resumed.clear()\n        state.pauseAll = true\n      } else if (Array.isArray(partitions)) {\n        partitions.forEach(partition => {\n          state.paused.add(partition)\n          state.resumed.delete(partition)\n        })\n        state.pauseAll = false\n      }\n\n      this.subscriptionStatesByTopic[topic] = state\n    })\n  }\n\n  /**\n   * @param {Array<TopicPartitions>} topicPartitions Example: [{ topic: 'topic-name', partitions: [1, 2] }]\n   */\n  resume(topicPartitions = []) {\n    topicPartitions.forEach(({ topic, partitions }) => {\n      const state = this.subscriptionStatesByTopic[topic] || createState(topic)\n\n      if (typeof partitions === 'undefined') {\n        state.paused.clear()\n        state.resumed.clear()\n        state.pauseAll = false\n      } else if (Array.isArray(partitions)) {\n        partitions.forEach(partition => {\n          state.paused.delete(partition)\n\n          if (state.pauseAll) {\n            state.resumed.add(partition)\n          }\n        })\n      }\n\n      this.subscriptionStatesByTopic[topic] = state\n    })\n  }\n\n  /**\n   * @returns {Array<import(\"../../types\").TopicPartitions>} topicPartitions\n   * Example: [{ topic: 'topic-name', partitions: [1, 2] }]\n   */\n  assigned() {\n    return Object.values(this.assignedPartitionsByTopic).map(({ topic, partitions }) => ({\n      topic,\n      partitions: partitions.sort(),\n    }))\n  }\n\n  /**\n   * @returns {Array<import(\"../../types\").TopicPartitions>} topicPartitions\n   * Example: [{ topic: 'topic-name', partitions: [1, 2] }]\n   */\n  active() {\n    return Object.values(this.assignedPartitionsByTopic).map(({ topic, partitions }) => ({\n      topic,\n      partitions: partitions.filter(partition => !this.isPaused(topic, partition)).sort(),\n    }))\n  }\n\n  /**\n   * @returns {Array<import(\"../../types\").TopicPartitions>} topicPartitions\n   * Example: [{ topic: 'topic-name', partitions: [1, 2] }]\n   */\n  paused() {\n    return Object.values(this.assignedPartitionsByTopic)\n      .map(({ topic, partitions }) => ({\n        topic,\n        partitions: partitions.filter(partition => this.isPaused(topic, partition)).sort(),\n      }))\n      .filter(({ partitions }) => partitions.length !== 0)\n  }\n\n  isPaused(topic, partition) {\n    const state = this.subscriptionStatesByTopic[topic]\n\n    if (!state) {\n      return false\n    }\n\n    const partitionResumed = state.resumed.has(partition)\n    const partitionPaused = state.paused.has(partition)\n\n    return (state.pauseAll && !partitionResumed) || partitionPaused\n  }\n}\n", "const Encoder = require('../protocol/encoder')\nconst Decoder = require('../protocol/decoder')\n\nconst MemberMetadata = {\n  /**\n   * @param {Object} metadata\n   * @param {number} metadata.version\n   * @param {Array<string>} metadata.topics\n   * @param {Buffer} [metadata.userData=Buffer.alloc(0)]\n   *\n   * @returns Buffer\n   */\n  encode({ version, topics, userData = Buffer.alloc(0) }) {\n    return new Encoder()\n      .writeInt16(version)\n      .writeArray(topics)\n      .writeBytes(userData).buffer\n  },\n\n  /**\n   * @param {Buffer} buffer\n   * @returns {Object}\n   */\n  decode(buffer) {\n    const decoder = new Decoder(buffer)\n    return {\n      version: decoder.readInt16(),\n      topics: decoder.readArray(d => d.readString()),\n      userData: decoder.readBytes(),\n    }\n  },\n}\n\nconst MemberAssignment = {\n  /**\n   * @param {object} options\n   * @param {number} options.version\n   * @param {Object<String,Array>} options.assignment, example:\n   *                               {\n   *                                 'topic-A': [0, 2, 4, 6],\n   *                                 'topic-B': [0, 2],\n   *                               }\n   * @param {Buffer} [options.userData=Buffer.alloc(0)]\n   *\n   * @returns Buffer\n   */\n  encode({ version, assignment, userData = Buffer.alloc(0) }) {\n    return new Encoder()\n      .writeInt16(version)\n      .writeArray(\n        Object.keys(assignment).map(topic =>\n          new Encoder().writeString(topic).writeArray(assignment[topic])\n        )\n      )\n      .writeBytes(userData).buffer\n  },\n\n  /**\n   * @param {Buffer} buffer\n   * @returns {Object|null}\n   */\n  decode(buffer) {\n    const decoder = new Decoder(buffer)\n    const decodePartitions = d => d.readInt32()\n    const decodeAssignment = d => ({\n      topic: d.readString(),\n      partitions: d.readArray(decodePartitions),\n    })\n    const indexAssignment = (obj, { topic, partitions }) =>\n      Object.assign(obj, { [topic]: partitions })\n\n    if (!decoder.canReadInt16()) {\n      return null\n    }\n\n    return {\n      version: decoder.readInt16(),\n      assignment: decoder.readArray(decodeAssignment).reduce(indexAssignment, {}),\n      userData: decoder.readBytes(),\n    }\n  },\n}\n\nmodule.exports = {\n  MemberMetadata,\n  MemberAssignment,\n}\n", "const sleep = require('../utils/sleep')\nconst websiteUrl = require('../utils/websiteUrl')\nconst arrayDiff = require('../utils/arrayDiff')\nconst createRetry = require('../retry')\nconst sharedPromiseTo = require('../utils/sharedPromiseTo')\n\nconst OffsetManager = require('./offsetManager')\nconst Batch = require('./batch')\nconst SeekOffsets = require('./seekOffsets')\nconst SubscriptionState = require('./subscriptionState')\nconst {\n  events: { GROUP_JOIN, HEARTBEAT, CONNECT, RECEIVED_UNSUBSCRIBED_TOPICS },\n} = require('./instrumentationEvents')\nconst { MemberAssignment } = require('./assignerProtocol')\nconst {\n  KafkaJSError,\n  KafkaJSNonRetriableError,\n  KafkaJSStaleTopicMetadataAssignment,\n  isRebalancing,\n} = require('../errors')\n\nconst { keys } = Object\n\nconst STALE_METADATA_ERRORS = [\n  'LEADER_NOT_AVAILABLE',\n  // Fetch before v9 uses NOT_LEADER_FOR_PARTITION\n  'NOT_LEADER_FOR_PARTITION',\n  // Fetch after v9 uses {FENCED,UNKNOWN}_LEADER_EPOCH\n  'FENCED_LEADER_EPOCH',\n  'UNKNOWN_LEADER_EPOCH',\n  'UNKNOWN_TOPIC_OR_PARTITION',\n]\n\nconst PRIVATE = {\n  JOIN: Symbol('private:ConsumerGroup:join'),\n  SYNC: Symbol('private:ConsumerGroup:sync'),\n  SHARED_HEARTBEAT: Symbol('private:ConsumerGroup:sharedHeartbeat'),\n}\n\nmodule.exports = class ConsumerGroup {\n  /**\n   * @param {object} options\n   * @param {import('../../types').RetryOptions} options.retry\n   * @param {import('../../types').Cluster} options.cluster\n   * @param {string} options.groupId\n   * @param {string[]} options.topics\n   * @param {Record<string, { fromBeginning?: boolean }>} options.topicConfigurations\n   * @param {import('../../types').Logger} options.logger\n   * @param {import('../instrumentation/emitter')} options.instrumentationEmitter\n   * @param {import('../../types').Assigner[]} options.assigners\n   * @param {number} options.sessionTimeout\n   * @param {number} options.rebalanceTimeout\n   * @param {number} options.maxBytesPerPartition\n   * @param {number} options.minBytes\n   * @param {number} options.maxBytes\n   * @param {number} options.maxWaitTimeInMs\n   * @param {boolean} options.autoCommit\n   * @param {number} options.autoCommitInterval\n   * @param {number} options.autoCommitThreshold\n   * @param {number} options.isolationLevel\n   * @param {string} options.rackId\n   * @param {number} options.metadataMaxAge\n   */\n  constructor({\n    retry,\n    cluster,\n    groupId,\n    topics,\n    topicConfigurations,\n    logger,\n    instrumentationEmitter,\n    assigners,\n    sessionTimeout,\n    rebalanceTimeout,\n    maxBytesPerPartition,\n    minBytes,\n    maxBytes,\n    maxWaitTimeInMs,\n    autoCommit,\n    autoCommitInterval,\n    autoCommitThreshold,\n    isolationLevel,\n    rackId,\n    metadataMaxAge,\n  }) {\n    /** @type {import(\"../../types\").Cluster} */\n    this.cluster = cluster\n    this.groupId = groupId\n    this.topics = topics\n    this.topicsSubscribed = topics\n    this.topicConfigurations = topicConfigurations\n    this.logger = logger.namespace('ConsumerGroup')\n    this.instrumentationEmitter = instrumentationEmitter\n    this.retrier = createRetry(Object.assign({}, retry))\n    this.assigners = assigners\n    this.sessionTimeout = sessionTimeout\n    this.rebalanceTimeout = rebalanceTimeout\n    this.maxBytesPerPartition = maxBytesPerPartition\n    this.minBytes = minBytes\n    this.maxBytes = maxBytes\n    this.maxWaitTime = maxWaitTimeInMs\n    this.autoCommit = autoCommit\n    this.autoCommitInterval = autoCommitInterval\n    this.autoCommitThreshold = autoCommitThreshold\n    this.isolationLevel = isolationLevel\n    this.rackId = rackId\n    this.metadataMaxAge = metadataMaxAge\n\n    this.seekOffset = new SeekOffsets()\n    this.coordinator = null\n    this.generationId = null\n    this.leaderId = null\n    this.memberId = null\n    this.members = null\n    this.groupProtocol = null\n\n    this.partitionsPerSubscribedTopic = null\n    /**\n     * Preferred read replica per topic and partition\n     *\n     * Each of the partitions tracks the preferred read replica (`nodeId`) and a timestamp\n     * until when that preference is valid.\n     *\n     * @type {{[topicName: string]: {[partition: number]: {nodeId: number, expireAt: number}}}}\n     */\n    this.preferredReadReplicasPerTopicPartition = {}\n    this.offsetManager = null\n    this.subscriptionState = new SubscriptionState()\n\n    this.lastRequest = Date.now()\n\n    this[PRIVATE.SHARED_HEARTBEAT] = sharedPromiseTo(async ({ interval }) => {\n      const { groupId, generationId, memberId } = this\n      const now = Date.now()\n\n      if (memberId && now >= this.lastRequest + interval) {\n        const payload = {\n          groupId,\n          memberId,\n          groupGenerationId: generationId,\n        }\n\n        await this.coordinator.heartbeat(payload)\n        this.instrumentationEmitter.emit(HEARTBEAT, payload)\n        this.lastRequest = Date.now()\n      }\n    })\n  }\n\n  isLeader() {\n    return this.leaderId && this.memberId === this.leaderId\n  }\n\n  getNodeIds() {\n    return this.cluster.getNodeIds()\n  }\n\n  async connect() {\n    await this.cluster.connect()\n    this.instrumentationEmitter.emit(CONNECT)\n    await this.cluster.refreshMetadataIfNecessary()\n  }\n\n  async [PRIVATE.JOIN]() {\n    const { groupId, sessionTimeout, rebalanceTimeout } = this\n\n    this.coordinator = await this.cluster.findGroupCoordinator({ groupId })\n\n    const groupData = await this.coordinator.joinGroup({\n      groupId,\n      sessionTimeout,\n      rebalanceTimeout,\n      memberId: this.memberId || '',\n      groupProtocols: this.assigners.map(assigner =>\n        assigner.protocol({\n          topics: this.topicsSubscribed,\n        })\n      ),\n    })\n\n    this.generationId = groupData.generationId\n    this.leaderId = groupData.leaderId\n    this.memberId = groupData.memberId\n    this.members = groupData.members\n    this.groupProtocol = groupData.groupProtocol\n  }\n\n  async leave() {\n    const { groupId, memberId } = this\n    if (memberId) {\n      await this.coordinator.leaveGroup({ groupId, memberId })\n      this.memberId = null\n    }\n  }\n\n  async [PRIVATE.SYNC]() {\n    let assignment = []\n    const {\n      groupId,\n      generationId,\n      memberId,\n      members,\n      groupProtocol,\n      topics,\n      topicsSubscribed,\n      coordinator,\n    } = this\n\n    if (this.isLeader()) {\n      this.logger.debug('Chosen as group leader', { groupId, generationId, memberId, topics })\n      const assigner = this.assigners.find(({ name }) => name === groupProtocol)\n\n      if (!assigner) {\n        throw new KafkaJSNonRetriableError(\n          `Unsupported partition assigner \"${groupProtocol}\", the assigner wasn't found in the assigners list`\n        )\n      }\n\n      await this.cluster.refreshMetadata()\n      assignment = await assigner.assign({ members, topics: topicsSubscribed })\n\n      this.logger.debug('Group assignment', {\n        groupId,\n        generationId,\n        groupProtocol,\n        assignment,\n        topics: topicsSubscribed,\n      })\n    }\n\n    // Keep track of the partitions for the subscribed topics\n    this.partitionsPerSubscribedTopic = this.generatePartitionsPerSubscribedTopic()\n    const { memberAssignment } = await this.coordinator.syncGroup({\n      groupId,\n      generationId,\n      memberId,\n      groupAssignment: assignment,\n    })\n\n    const decodedMemberAssignment = MemberAssignment.decode(memberAssignment)\n    const decodedAssignment =\n      decodedMemberAssignment != null ? decodedMemberAssignment.assignment : {}\n\n    this.logger.debug('Received assignment', {\n      groupId,\n      generationId,\n      memberId,\n      memberAssignment: decodedAssignment,\n    })\n\n    const assignedTopics = keys(decodedAssignment)\n    const topicsNotSubscribed = arrayDiff(assignedTopics, topicsSubscribed)\n\n    if (topicsNotSubscribed.length > 0) {\n      const payload = {\n        groupId,\n        generationId,\n        memberId,\n        assignedTopics,\n        topicsSubscribed,\n        topicsNotSubscribed,\n      }\n\n      this.instrumentationEmitter.emit(RECEIVED_UNSUBSCRIBED_TOPICS, payload)\n      this.logger.warn('Consumer group received unsubscribed topics', {\n        ...payload,\n        helpUrl: websiteUrl(\n          'docs/faq',\n          'why-am-i-receiving-messages-for-topics-i-m-not-subscribed-to'\n        ),\n      })\n    }\n\n    // Remove unsubscribed topics from the list\n    const safeAssignment = arrayDiff(assignedTopics, topicsNotSubscribed)\n    const currentMemberAssignment = safeAssignment.map(topic => ({\n      topic,\n      partitions: decodedAssignment[topic],\n    }))\n\n    // Check if the consumer is aware of all assigned partitions\n    for (const assignment of currentMemberAssignment) {\n      const { topic, partitions: assignedPartitions } = assignment\n      const knownPartitions = this.partitionsPerSubscribedTopic.get(topic)\n      const isAwareOfAllAssignedPartitions = assignedPartitions.every(partition =>\n        knownPartitions.includes(partition)\n      )\n\n      if (!isAwareOfAllAssignedPartitions) {\n        this.logger.warn('Consumer is not aware of all assigned partitions, refreshing metadata', {\n          groupId,\n          generationId,\n          memberId,\n          topic,\n          knownPartitions,\n          assignedPartitions,\n        })\n\n        // If the consumer is not aware of all assigned partitions, refresh metadata\n        // and update the list of partitions per subscribed topic. It's enough to perform\n        // this operation once since refresh metadata will update metadata for all topics\n        await this.cluster.refreshMetadata()\n        this.partitionsPerSubscribedTopic = this.generatePartitionsPerSubscribedTopic()\n        break\n      }\n    }\n\n    this.topics = currentMemberAssignment.map(({ topic }) => topic)\n    this.subscriptionState.assign(currentMemberAssignment)\n    this.offsetManager = new OffsetManager({\n      cluster: this.cluster,\n      topicConfigurations: this.topicConfigurations,\n      instrumentationEmitter: this.instrumentationEmitter,\n      memberAssignment: currentMemberAssignment.reduce(\n        (partitionsByTopic, { topic, partitions }) => ({\n          ...partitionsByTopic,\n          [topic]: partitions,\n        }),\n        {}\n      ),\n      autoCommit: this.autoCommit,\n      autoCommitInterval: this.autoCommitInterval,\n      autoCommitThreshold: this.autoCommitThreshold,\n      coordinator,\n      groupId,\n      generationId,\n      memberId,\n    })\n  }\n\n  joinAndSync() {\n    const startJoin = Date.now()\n    return this.retrier(async bail => {\n      try {\n        await this[PRIVATE.JOIN]()\n        await this[PRIVATE.SYNC]()\n\n        const memberAssignment = this.assigned().reduce(\n          (result, { topic, partitions }) => ({ ...result, [topic]: partitions }),\n          {}\n        )\n\n        const payload = {\n          groupId: this.groupId,\n          memberId: this.memberId,\n          leaderId: this.leaderId,\n          isLeader: this.isLeader(),\n          memberAssignment,\n          groupProtocol: this.groupProtocol,\n          duration: Date.now() - startJoin,\n        }\n\n        this.instrumentationEmitter.emit(GROUP_JOIN, payload)\n        this.logger.info('Consumer has joined the group', payload)\n      } catch (e) {\n        if (isRebalancing(e)) {\n          // Rebalance in progress isn't a retriable protocol error since the consumer\n          // has to go through find coordinator and join again before it can\n          // actually retry the operation. We wrap the original error in a retriable error\n          // here instead in order to restart the join + sync sequence using the retrier.\n          throw new KafkaJSError(e)\n        }\n\n        if (e.type === 'UNKNOWN_MEMBER_ID') {\n          this.memberId = null\n          throw new KafkaJSError(e)\n        }\n\n        bail(e)\n      }\n    })\n  }\n\n  /**\n   * @param {import(\"../../types\").TopicPartition} topicPartition\n   */\n  resetOffset({ topic, partition }) {\n    this.offsetManager.resetOffset({ topic, partition })\n  }\n\n  /**\n   * @param {import(\"../../types\").TopicPartitionOffset} topicPartitionOffset\n   */\n  resolveOffset({ topic, partition, offset }) {\n    this.offsetManager.resolveOffset({ topic, partition, offset })\n  }\n\n  /**\n   * Update the consumer offset for the given topic/partition. This will be used\n   * on the next fetch. If this API is invoked for the same topic/partition more\n   * than once, the latest offset will be used on the next fetch.\n   *\n   * @param {import(\"../../types\").TopicPartitionOffset} topicPartitionOffset\n   */\n  seek({ topic, partition, offset }) {\n    this.seekOffset.set(topic, partition, offset)\n  }\n\n  pause(topicPartitions) {\n    this.logger.info(`Pausing fetching from ${topicPartitions.length} topics`, {\n      topicPartitions,\n    })\n    this.subscriptionState.pause(topicPartitions)\n  }\n\n  resume(topicPartitions) {\n    this.logger.info(`Resuming fetching from ${topicPartitions.length} topics`, {\n      topicPartitions,\n    })\n    this.subscriptionState.resume(topicPartitions)\n  }\n\n  assigned() {\n    return this.subscriptionState.assigned()\n  }\n\n  paused() {\n    return this.subscriptionState.paused()\n  }\n\n  /**\n   * @param {string} topic\n   * @param {string} partition\n   * @returns {boolean} whether the specified topic-partition are paused or not\n   */\n  isPaused(topic, partition) {\n    return this.subscriptionState.isPaused(topic, partition)\n  }\n\n  async commitOffsetsIfNecessary() {\n    await this.offsetManager.commitOffsetsIfNecessary()\n  }\n\n  async commitOffsets(offsets) {\n    await this.offsetManager.commitOffsets(offsets)\n  }\n\n  uncommittedOffsets() {\n    return this.offsetManager.uncommittedOffsets()\n  }\n\n  async heartbeat({ interval }) {\n    return this[PRIVATE.SHARED_HEARTBEAT]({ interval })\n  }\n\n  async fetch(nodeId) {\n    try {\n      await this.cluster.refreshMetadataIfNecessary()\n      this.checkForStaleAssignment()\n\n      let topicPartitions = this.subscriptionState.assigned()\n      topicPartitions = this.filterPartitionsByNode(nodeId, topicPartitions)\n\n      await this.seekOffsets(topicPartitions)\n\n      const committedOffsets = this.offsetManager.committedOffsets()\n      const activeTopicPartitions = this.getActiveTopicPartitions()\n\n      const requests = topicPartitions\n        .map(({ topic, partitions }) => ({\n          topic,\n          partitions: partitions\n            .filter(\n              partition =>\n                /**\n                 * When recovering from OffsetOutOfRange, each partition can recover\n                 * concurrently, which invalidates resolved and committed offsets as part\n                 * of the recovery mechanism (see OffsetManager.clearOffsets). In concurrent\n                 * scenarios this can initiate a new fetch with invalid offsets.\n                 *\n                 * This was further highlighted by https://github.com/tulios/kafkajs/pull/570,\n                 * which increased concurrency, making this more likely to happen.\n                 *\n                 * This is solved by only making requests for partitions with initialized offsets.\n                 *\n                 * See the following pull request which explains the context of the problem:\n                 * @issue https://github.com/tulios/kafkajs/pull/578\n                 */\n                committedOffsets[topic][partition] != null &&\n                activeTopicPartitions[topic].has(partition)\n            )\n            .map(partition => ({\n              partition,\n              fetchOffset: this.offsetManager.nextOffset(topic, partition).toString(),\n              maxBytes: this.maxBytesPerPartition,\n            })),\n        }))\n        .filter(({ partitions }) => partitions.length)\n\n      if (!requests.length) {\n        await sleep(this.maxWaitTime)\n        return []\n      }\n\n      const broker = await this.cluster.findBroker({ nodeId })\n\n      const { responses } = await broker.fetch({\n        maxWaitTime: this.maxWaitTime,\n        minBytes: this.minBytes,\n        maxBytes: this.maxBytes,\n        isolationLevel: this.isolationLevel,\n        topics: requests,\n        rackId: this.rackId,\n      })\n\n      return responses.flatMap(({ topicName, partitions }) => {\n        const topicRequestData = requests.find(({ topic }) => topic === topicName)\n\n        let preferredReadReplicas = this.preferredReadReplicasPerTopicPartition[topicName]\n        if (!preferredReadReplicas) {\n          this.preferredReadReplicasPerTopicPartition[topicName] = preferredReadReplicas = {}\n        }\n\n        return partitions\n          .filter(\n            ({ partition }) =>\n              !this.seekOffset.has(topicName, partition) &&\n              !this.subscriptionState.isPaused(topicName, partition)\n          )\n          .map(partitionData => {\n            const { partition, preferredReadReplica } = partitionData\n\n            if (preferredReadReplica != null && preferredReadReplica !== -1) {\n              const { nodeId: currentPreferredReadReplica } = preferredReadReplicas[partition] || {}\n              if (currentPreferredReadReplica !== preferredReadReplica) {\n                this.logger.info(`Preferred read replica is now ${preferredReadReplica}`, {\n                  groupId: this.groupId,\n                  memberId: this.memberId,\n                  topic: topicName,\n                  partition,\n                })\n              }\n              preferredReadReplicas[partition] = {\n                nodeId: preferredReadReplica,\n                expireAt: Date.now() + this.metadataMaxAge,\n              }\n            }\n\n            const partitionRequestData = topicRequestData.partitions.find(\n              ({ partition }) => partition === partitionData.partition\n            )\n\n            const fetchedOffset = partitionRequestData.fetchOffset\n            return new Batch(topicName, fetchedOffset, partitionData)\n          })\n      })\n    } catch (e) {\n      await this.recoverFromFetch(e)\n      return []\n    }\n  }\n\n  async recoverFromFetch(e) {\n    if (STALE_METADATA_ERRORS.includes(e.type) || e.name === 'KafkaJSTopicMetadataNotLoaded') {\n      this.logger.debug('Stale cluster metadata, refreshing...', {\n        groupId: this.groupId,\n        memberId: this.memberId,\n        error: e.message,\n      })\n\n      await this.cluster.refreshMetadata()\n      await this.joinAndSync()\n      return\n    }\n\n    if (e.name === 'KafkaJSStaleTopicMetadataAssignment') {\n      this.logger.warn(`${e.message}, resync group`, {\n        groupId: this.groupId,\n        memberId: this.memberId,\n        topic: e.topic,\n        unknownPartitions: e.unknownPartitions,\n      })\n\n      await this.joinAndSync()\n      return\n    }\n\n    if (e.name === 'KafkaJSOffsetOutOfRange') {\n      await this.recoverFromOffsetOutOfRange(e)\n      return\n    }\n\n    if (e.name === 'KafkaJSConnectionClosedError') {\n      this.cluster.removeBroker({ host: e.host, port: e.port })\n      return\n    }\n\n    if (e.name === 'KafkaJSBrokerNotFound' || e.name === 'KafkaJSConnectionClosedError') {\n      this.logger.debug(`${e.message}, refreshing metadata and retrying...`)\n      await this.cluster.refreshMetadata()\n      return\n    }\n\n    throw e\n  }\n\n  async recoverFromOffsetOutOfRange(e) {\n    // If we are fetching from a follower try with the leader before resetting offsets\n    const preferredReadReplicas = this.preferredReadReplicasPerTopicPartition[e.topic]\n    if (preferredReadReplicas && typeof preferredReadReplicas[e.partition] === 'number') {\n      this.logger.info('Offset out of range while fetching from follower, retrying with leader', {\n        topic: e.topic,\n        partition: e.partition,\n        groupId: this.groupId,\n        memberId: this.memberId,\n      })\n      delete preferredReadReplicas[e.partition]\n    } else {\n      this.logger.error('Offset out of range, resetting to default offset', {\n        topic: e.topic,\n        partition: e.partition,\n        groupId: this.groupId,\n        memberId: this.memberId,\n      })\n\n      await this.offsetManager.setDefaultOffset({\n        topic: e.topic,\n        partition: e.partition,\n      })\n    }\n  }\n\n  generatePartitionsPerSubscribedTopic() {\n    const map = new Map()\n\n    for (const topic of this.topicsSubscribed) {\n      const partitions = this.cluster\n        .findTopicPartitionMetadata(topic)\n        .map(m => m.partitionId)\n        .sort()\n\n      map.set(topic, partitions)\n    }\n\n    return map\n  }\n\n  checkForStaleAssignment() {\n    if (!this.partitionsPerSubscribedTopic) {\n      return\n    }\n\n    const newPartitionsPerSubscribedTopic = this.generatePartitionsPerSubscribedTopic()\n\n    for (const [topic, partitions] of newPartitionsPerSubscribedTopic) {\n      const diff = arrayDiff(partitions, this.partitionsPerSubscribedTopic.get(topic))\n\n      if (diff.length > 0) {\n        throw new KafkaJSStaleTopicMetadataAssignment('Topic has been updated', {\n          topic,\n          unknownPartitions: diff,\n        })\n      }\n    }\n  }\n\n  async seekOffsets(topicPartitions) {\n    for (const { topic, partitions } of topicPartitions) {\n      for (const partition of partitions) {\n        const seekEntry = this.seekOffset.pop(topic, partition)\n        if (!seekEntry) {\n          continue\n        }\n\n        this.logger.debug('Seek offset', {\n          groupId: this.groupId,\n          memberId: this.memberId,\n          seek: seekEntry,\n        })\n        await this.offsetManager.seek(seekEntry)\n      }\n    }\n\n    await this.offsetManager.resolveOffsets()\n  }\n\n  hasSeekOffset({ topic, partition }) {\n    return this.seekOffset.has(topic, partition)\n  }\n\n  /**\n   * For each of the partitions find the best nodeId to read it from\n   *\n   * @param {string} topic\n   * @param {number[]} partitions\n   * @returns {{[nodeId: number]: number[]}} per-node assignment of partitions\n   * @see Cluster~findLeaderForPartitions\n   */\n  // Invariant: The resulting object has each partition referenced exactly once\n  findReadReplicaForPartitions(topic, partitions) {\n    const partitionMetadata = this.cluster.findTopicPartitionMetadata(topic)\n    const preferredReadReplicas = this.preferredReadReplicasPerTopicPartition[topic]\n    return partitions.reduce((result, id) => {\n      const partitionId = parseInt(id, 10)\n      const metadata = partitionMetadata.find(p => p.partitionId === partitionId)\n      if (!metadata) {\n        return result\n      }\n\n      if (metadata.leader == null) {\n        throw new KafkaJSError('Invalid partition metadata', { topic, partitionId, metadata })\n      }\n\n      // Pick the preferred replica if there is one, and it isn't known to be offline, otherwise the leader.\n      let nodeId = metadata.leader\n      if (preferredReadReplicas) {\n        const { nodeId: preferredReadReplica, expireAt } = preferredReadReplicas[partitionId] || {}\n        if (Date.now() >= expireAt) {\n          this.logger.debug('Preferred read replica information has expired, using leader', {\n            topic,\n            partitionId,\n            groupId: this.groupId,\n            memberId: this.memberId,\n            preferredReadReplica,\n            leader: metadata.leader,\n          })\n          // Drop the entry\n          delete preferredReadReplicas[partitionId]\n        } else if (preferredReadReplica != null) {\n          // Valid entry, check whether it is not offline\n          // Note that we don't delete the preference here, and rather hope that eventually that replica comes online again\n          const offlineReplicas = metadata.offlineReplicas\n          if (Array.isArray(offlineReplicas) && offlineReplicas.includes(nodeId)) {\n            this.logger.debug('Preferred read replica is offline, using leader', {\n              topic,\n              partitionId,\n              groupId: this.groupId,\n              memberId: this.memberId,\n              preferredReadReplica,\n              leader: metadata.leader,\n            })\n          } else {\n            nodeId = preferredReadReplica\n          }\n        }\n      }\n      const current = result[nodeId] || []\n      return { ...result, [nodeId]: [...current, partitionId] }\n    }, {})\n  }\n\n  filterPartitionsByNode(nodeId, topicPartitions) {\n    return topicPartitions.map(({ topic, partitions }) => ({\n      topic,\n      partitions: this.findReadReplicaForPartitions(topic, partitions)[nodeId] || [],\n    }))\n  }\n\n  getActiveTopicPartitions() {\n    const activeSubscriptionState = this.subscriptionState.active()\n\n    const activeTopicPartitions = {}\n    activeSubscriptionState.forEach(({ topic, partitions }) => {\n      activeTopicPartitions[topic] = new Set(partitions)\n    })\n\n    return activeTopicPartitions\n  }\n}\n", "/**\n * @param {number} count\n * @param {(index: number) => T} [callback]\n * @template T\n */\nconst seq = (count, callback = x => x) =>\n  new Array(count).fill(0).map((_, index) => callback(index))\n\nmodule.exports = seq\n", "const EventEmitter = require('events')\n\n/**\n * Fetches data from all assigned nodes, waits for workerQueue to drain and repeats.\n *\n * @param {object} options\n * @param {number} options.nodeId\n * @param {import('./workerQueue').WorkerQueue} options.workerQueue\n * @param {Map<string, string[]>} options.partitionAssignments\n * @param {(nodeId: number) => Promise<T[]>} options.fetch\n * @param {import('../../types').Logger} options.logger\n * @template T\n */\nconst createFetcher = ({\n  nodeId,\n  workerQueue,\n  partitionAssignments,\n  fetch,\n  logger: rootLogger,\n}) => {\n  const logger = rootLogger.namespace(`Fetcher ${nodeId}`)\n  const emitter = new EventEmitter()\n  let isRunning = false\n\n  const getWorkerQueue = () => workerQueue\n  const assignmentKey = ({ topic, partition }) => `${topic}|${partition}`\n  const getAssignedFetcher = batch => partitionAssignments.get(assignmentKey(batch))\n  const assignTopicPartition = batch => partitionAssignments.set(assignmentKey(batch), nodeId)\n  const unassignTopicPartition = batch => partitionAssignments.delete(assignmentKey(batch))\n  const filterUnassignedBatches = batches =>\n    batches.filter(batch => {\n      const assignedFetcher = getAssignedFetcher(batch)\n      if (assignedFetcher != null && assignedFetcher !== nodeId) {\n        logger.info(\n          'Filtering out batch due to partition already being processed by another fetcher',\n          {\n            topic: batch.topic,\n            partition: batch.partition,\n            assignedFetcher: assignedFetcher,\n            fetcher: nodeId,\n          }\n        )\n        return false\n      }\n\n      return true\n    })\n\n  const start = async () => {\n    if (isRunning) return\n    isRunning = true\n\n    while (isRunning) {\n      try {\n        const batches = await fetch(nodeId)\n        if (isRunning) {\n          const availableBatches = filterUnassignedBatches(batches)\n\n          if (availableBatches.length > 0) {\n            availableBatches.forEach(assignTopicPartition)\n            try {\n              await workerQueue.push(...availableBatches)\n            } finally {\n              availableBatches.forEach(unassignTopicPartition)\n            }\n          }\n        }\n      } catch (error) {\n        isRunning = false\n        emitter.emit('end')\n        throw error\n      }\n    }\n    emitter.emit('end')\n  }\n\n  const stop = async () => {\n    if (!isRunning) return\n    isRunning = false\n    await new Promise(resolve => emitter.once('end', () => resolve()))\n  }\n\n  return { start, stop, getWorkerQueue }\n}\n\nmodule.exports = createFetcher\n", "/**\n * @typedef {(batch: T, metadata: { workerId: number }) => Promise<void>} Handler\n * @template T\n *\n * @typedef {ReturnType<typeof createWorker>} Worker\n */\n\nconst sharedPromiseTo = require('../utils/sharedPromiseTo')\n\n/**\n * @param {{ handler: Handler<T>, workerId: number }} options\n * @template T\n */\nconst createWorker = ({ handler, workerId }) => {\n  /**\n   * Takes batches from next() until it returns undefined.\n   *\n   * @param {{ next: () => { batch: T, resolve: () => void, reject: (e: Error) => void } | undefined }} param0\n   * @returns {Promise<void>}\n   */\n  const run = sharedPromiseTo(async ({ next }) => {\n    while (true) {\n      const item = next()\n      if (!item) break\n\n      const { batch, resolve, reject } = item\n\n      try {\n        await handler(batch, { workerId })\n        resolve()\n      } catch (error) {\n        reject(error)\n      }\n    }\n  })\n\n  return { run }\n}\n\nmodule.exports = createWorker\n", "/**\n * @typedef {ReturnType<typeof createWorkerQueue>} WorkerQueue\n */\n\n/**\n * @param {object} options\n * @param {import('./worker').Worker<T>[]} options.workers\n * @template T\n */\nconst createWorkerQueue = ({ workers }) => {\n  /** @type {{ batch: T, resolve: (value?: any) => void, reject: (e: Error) => void}[]} */\n  const queue = []\n\n  const getWorkers = () => workers\n\n  /**\n   * Waits until workers have processed all batches in the queue.\n   *\n   * @param {...T} batches\n   * @returns {Promise<void>}\n   */\n  const push = async (...batches) => {\n    const promises = batches.map(\n      batch => new Promise((resolve, reject) => queue.push({ batch, resolve, reject }))\n    )\n\n    workers.forEach(worker => worker.run({ next: () => queue.shift() }))\n\n    const results = await Promise.allSettled(promises)\n    const rejected = results.find(result => result.status === 'rejected')\n    if (rejected) {\n      // @ts-ignore\n      throw rejected.reason\n    }\n  }\n\n  return { push, getWorkers }\n}\n\nmodule.exports = createWorkerQueue\n", "const seq = require('../utils/seq')\nconst createFetcher = require('./fetcher')\nconst createWorker = require('./worker')\nconst createWorkerQueue = require('./workerQueue')\nconst { KafkaJSFetcherRebalanceError, KafkaJSNoBrokerAvailableError } = require('../errors')\n\n/** @typedef {ReturnType<typeof createFetchManager>} FetchManager */\n\n/**\n * @param {object} options\n * @param {import('../../types').Logger} options.logger\n * @param {() => number[]} options.getNodeIds\n * @param {(nodeId: number) => Promise<import('../../types').Batch[]>} options.fetch\n * @param {import('./worker').Handler<T>} options.handler\n * @param {number} [options.concurrency]\n * @template T\n */\nconst createFetchManager = ({\n  logger: rootLogger,\n  getNodeIds,\n  fetch,\n  handler,\n  concurrency = 1,\n}) => {\n  const logger = rootLogger.namespace('FetchManager')\n  const workers = seq(concurrency, workerId => createWorker({ handler, workerId }))\n  const workerQueue = createWorkerQueue({ workers })\n\n  let fetchers = []\n\n  const getFetchers = () => fetchers\n\n  const createFetchers = () => {\n    const nodeIds = getNodeIds()\n    const partitionAssignments = new Map()\n\n    if (nodeIds.length === 0) {\n      throw new KafkaJSNoBrokerAvailableError()\n    }\n\n    const validateShouldRebalance = () => {\n      const current = getNodeIds()\n      const hasChanged =\n        nodeIds.length !== current.length || nodeIds.some(nodeId => !current.includes(nodeId))\n      if (hasChanged && current.length !== 0) {\n        throw new KafkaJSFetcherRebalanceError()\n      }\n    }\n\n    const fetchers = nodeIds.map(nodeId =>\n      createFetcher({\n        nodeId,\n        workerQueue,\n        partitionAssignments,\n        fetch: async nodeId => {\n          validateShouldRebalance()\n          return fetch(nodeId)\n        },\n        logger,\n      })\n    )\n\n    logger.debug(`Created ${fetchers.length} fetchers`, { nodeIds, concurrency })\n    return fetchers\n  }\n\n  const start = async () => {\n    logger.debug('Starting...')\n\n    while (true) {\n      fetchers = createFetchers()\n\n      try {\n        await Promise.all(fetchers.map(fetcher => fetcher.start()))\n      } catch (error) {\n        await stop()\n\n        if (error instanceof KafkaJSFetcherRebalanceError) {\n          logger.debug('Rebalancing fetchers...')\n          continue\n        }\n\n        throw error\n      }\n\n      break\n    }\n  }\n\n  const stop = async () => {\n    logger.debug('Stopping fetchers...')\n    await Promise.all(fetchers.map(fetcher => fetcher.stop()))\n    logger.debug('Stopped fetchers')\n  }\n\n  return { start, stop, getFetchers }\n}\n\nmodule.exports = createFetchManager\n", "const { EventEmitter } = require('events')\nconst Long = require('../utils/long')\nconst createRetry = require('../retry')\nconst { isKafkaJSError, isRebalancing } = require('../errors')\n\nconst {\n  events: { FETCH, FETCH_START, START_BATCH_PROCESS, END_BATCH_PROCESS, REBALANCING },\n} = require('./instrumentationEvents')\nconst createFetchManager = require('./fetchManager')\n\nconst isSameOffset = (offsetA, offsetB) => Long.fromValue(offsetA).equals(Long.fromValue(offsetB))\nconst CONSUMING_START = 'consuming-start'\nconst CONSUMING_STOP = 'consuming-stop'\n\nmodule.exports = class Runner extends EventEmitter {\n  /**\n   * @param {object} options\n   * @param {import(\"../../types\").Logger} options.logger\n   * @param {import(\"./consumerGroup\")} options.consumerGroup\n   * @param {import(\"../instrumentation/emitter\")} options.instrumentationEmitter\n   * @param {boolean} [options.eachBatchAutoResolve=true]\n   * @param {number} options.concurrency\n   * @param {(payload: import(\"../../types\").EachBatchPayload) => Promise<void>} [options.eachBatch]\n   * @param {(payload: import(\"../../types\").EachMessagePayload) => Promise<void>} [options.eachMessage]\n   * @param {number} [options.heartbeatInterval]\n   * @param {(reason: Error) => void} options.onCrash\n   * @param {import(\"../../types\").RetryOptions} [options.retry]\n   * @param {boolean} [options.autoCommit=true]\n   */\n  constructor({\n    logger,\n    consumerGroup,\n    instrumentationEmitter,\n    eachBatchAutoResolve = true,\n    concurrency,\n    eachBatch,\n    eachMessage,\n    heartbeatInterval,\n    onCrash,\n    retry,\n    autoCommit = true,\n  }) {\n    super()\n    this.logger = logger.namespace('Runner')\n    this.consumerGroup = consumerGroup\n    this.instrumentationEmitter = instrumentationEmitter\n    this.eachBatchAutoResolve = eachBatchAutoResolve\n    this.eachBatch = eachBatch\n    this.eachMessage = eachMessage\n    this.heartbeatInterval = heartbeatInterval\n    this.retrier = createRetry(Object.assign({}, retry))\n    this.onCrash = onCrash\n    this.autoCommit = autoCommit\n    this.fetchManager = createFetchManager({\n      logger: this.logger,\n      getNodeIds: () => this.consumerGroup.getNodeIds(),\n      fetch: nodeId => this.fetch(nodeId),\n      handler: batch => this.handleBatch(batch),\n      concurrency,\n    })\n\n    this.running = false\n    this.consuming = false\n  }\n\n  get consuming() {\n    return this._consuming\n  }\n\n  set consuming(value) {\n    if (this._consuming !== value) {\n      this._consuming = value\n      this.emit(value ? CONSUMING_START : CONSUMING_STOP)\n    }\n  }\n\n  async start() {\n    if (this.running) {\n      return\n    }\n\n    try {\n      await this.consumerGroup.connect()\n      await this.consumerGroup.joinAndSync()\n    } catch (e) {\n      return this.onCrash(e)\n    }\n\n    this.running = true\n    this.scheduleFetchManager()\n  }\n\n  scheduleFetchManager() {\n    if (!this.running) {\n      this.consuming = false\n\n      this.logger.info('consumer not running, exiting', {\n        groupId: this.consumerGroup.groupId,\n        memberId: this.consumerGroup.memberId,\n      })\n\n      return\n    }\n\n    this.consuming = true\n\n    this.retrier(async (bail, retryCount, retryTime) => {\n      if (!this.running) {\n        return\n      }\n\n      try {\n        await this.fetchManager.start()\n      } catch (e) {\n        if (isRebalancing(e)) {\n          this.logger.warn('The group is rebalancing, re-joining', {\n            groupId: this.consumerGroup.groupId,\n            memberId: this.consumerGroup.memberId,\n            error: e.message,\n          })\n\n          this.instrumentationEmitter.emit(REBALANCING, {\n            groupId: this.consumerGroup.groupId,\n            memberId: this.consumerGroup.memberId,\n          })\n\n          await this.consumerGroup.joinAndSync()\n          return\n        }\n\n        if (e.type === 'UNKNOWN_MEMBER_ID') {\n          this.logger.error('The coordinator is not aware of this member, re-joining the group', {\n            groupId: this.consumerGroup.groupId,\n            memberId: this.consumerGroup.memberId,\n            error: e.message,\n          })\n\n          this.consumerGroup.memberId = null\n          await this.consumerGroup.joinAndSync()\n          return\n        }\n\n        if (e.name === 'KafkaJSNotImplemented') {\n          return bail(e)\n        }\n\n        if (e.name === 'KafkaJSNoBrokerAvailableError') {\n          return bail(e)\n        }\n\n        this.logger.debug('Error while scheduling fetch manager, trying again...', {\n          groupId: this.consumerGroup.groupId,\n          memberId: this.consumerGroup.memberId,\n          error: e.message,\n          stack: e.stack,\n          retryCount,\n          retryTime,\n        })\n\n        throw e\n      }\n    })\n      .then(() => {\n        this.scheduleFetchManager()\n      })\n      .catch(e => {\n        this.onCrash(e)\n        this.consuming = false\n        this.running = false\n      })\n  }\n\n  async stop() {\n    if (!this.running) {\n      return\n    }\n\n    this.logger.debug('stop consumer group', {\n      groupId: this.consumerGroup.groupId,\n      memberId: this.consumerGroup.memberId,\n    })\n\n    this.running = false\n\n    try {\n      await this.fetchManager.stop()\n      await this.waitForConsumer()\n      await this.consumerGroup.leave()\n    } catch (e) {}\n  }\n\n  waitForConsumer() {\n    return new Promise(resolve => {\n      if (!this.consuming) {\n        return resolve()\n      }\n\n      this.logger.debug('waiting for consumer to finish...', {\n        groupId: this.consumerGroup.groupId,\n        memberId: this.consumerGroup.memberId,\n      })\n\n      this.once(CONSUMING_STOP, () => resolve())\n    })\n  }\n\n  async heartbeat() {\n    try {\n      await this.consumerGroup.heartbeat({ interval: this.heartbeatInterval })\n    } catch (e) {\n      if (isRebalancing(e)) {\n        await this.autoCommitOffsets()\n      }\n      throw e\n    }\n  }\n\n  async processEachMessage(batch) {\n    const { topic, partition } = batch\n\n    const pause = () => {\n      this.consumerGroup.pause([{ topic, partitions: [partition] }])\n      return () => this.consumerGroup.resume([{ topic, partitions: [partition] }])\n    }\n    for (const message of batch.messages) {\n      if (!this.running || this.consumerGroup.hasSeekOffset({ topic, partition })) {\n        break\n      }\n\n      try {\n        await this.eachMessage({\n          topic,\n          partition,\n          message,\n          heartbeat: () => this.heartbeat(),\n          pause,\n        })\n      } catch (e) {\n        if (!isKafkaJSError(e)) {\n          this.logger.error(`Error when calling eachMessage`, {\n            topic,\n            partition,\n            offset: message.offset,\n            stack: e.stack,\n            error: e,\n          })\n        }\n\n        // In case of errors, commit the previously consumed offsets unless autoCommit is disabled\n        await this.autoCommitOffsets()\n        throw e\n      }\n\n      this.consumerGroup.resolveOffset({ topic, partition, offset: message.offset })\n      await this.heartbeat()\n      await this.autoCommitOffsetsIfNecessary()\n\n      if (this.consumerGroup.isPaused(topic, partition)) {\n        break\n      }\n    }\n  }\n\n  async processEachBatch(batch) {\n    const { topic, partition } = batch\n    const lastFilteredMessage = batch.messages[batch.messages.length - 1]\n\n    const pause = () => {\n      this.consumerGroup.pause([{ topic, partitions: [partition] }])\n      return () => this.consumerGroup.resume([{ topic, partitions: [partition] }])\n    }\n\n    try {\n      await this.eachBatch({\n        batch,\n        resolveOffset: offset => {\n          /**\n           * The transactional producer generates a control record after committing the transaction.\n           * The control record is the last record on the RecordBatch, and it is filtered before it\n           * reaches the eachBatch callback. When disabling auto-resolve, the user-land code won't\n           * be able to resolve the control record offset, since it never reaches the callback,\n           * causing stuck consumers as the consumer will never move the offset marker.\n           *\n           * When the last offset of the batch is resolved, we should automatically resolve\n           * the control record offset as this entry doesn't have any meaning to the user-land code,\n           * and won't interfere with the stream processing.\n           *\n           * @see https://github.com/apache/kafka/blob/9aa660786e46c1efbf5605a6a69136a1dac6edb9/clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java#L1499-L1505\n           */\n          const offsetToResolve =\n            lastFilteredMessage && isSameOffset(offset, lastFilteredMessage.offset)\n              ? batch.lastOffset()\n              : offset\n\n          this.consumerGroup.resolveOffset({ topic, partition, offset: offsetToResolve })\n        },\n        heartbeat: () => this.heartbeat(),\n        /**\n         * Pause consumption for the current topic-partition being processed\n         */\n        pause,\n        /**\n         * Commit offsets if provided. Otherwise commit most recent resolved offsets\n         * if the autoCommit conditions are met.\n         *\n         * @param {import('../../types').OffsetsByTopicPartition} [offsets] Optional.\n         */\n        commitOffsetsIfNecessary: async offsets => {\n          return offsets\n            ? this.consumerGroup.commitOffsets(offsets)\n            : this.consumerGroup.commitOffsetsIfNecessary()\n        },\n        uncommittedOffsets: () => this.consumerGroup.uncommittedOffsets(),\n        isRunning: () => this.running,\n        isStale: () => this.consumerGroup.hasSeekOffset({ topic, partition }),\n      })\n    } catch (e) {\n      if (!isKafkaJSError(e)) {\n        this.logger.error(`Error when calling eachBatch`, {\n          topic,\n          partition,\n          offset: batch.firstOffset(),\n          stack: e.stack,\n          error: e,\n        })\n      }\n\n      // eachBatch has a special resolveOffset which can be used\n      // to keep track of the messages\n      await this.autoCommitOffsets()\n      throw e\n    }\n\n    // resolveOffset for the last offset can be disabled to allow the users of eachBatch to\n    // stop their consumers without resolving unprocessed offsets (issues/18)\n    if (this.eachBatchAutoResolve) {\n      this.consumerGroup.resolveOffset({ topic, partition, offset: batch.lastOffset() })\n    }\n  }\n\n  async fetch(nodeId) {\n    if (!this.running) {\n      this.logger.debug('consumer not running, exiting', {\n        groupId: this.consumerGroup.groupId,\n        memberId: this.consumerGroup.memberId,\n      })\n\n      return []\n    }\n\n    const startFetch = Date.now()\n\n    this.instrumentationEmitter.emit(FETCH_START, { nodeId })\n\n    const batches = await this.consumerGroup.fetch(nodeId)\n\n    this.instrumentationEmitter.emit(FETCH, {\n      /**\n       * PR #570 removed support for the number of batches in this instrumentation event;\n       * The new implementation uses an async generation to deliver the batches, which makes\n       * this number impossible to get. The number is set to 0 to keep the event backward\n       * compatible until we bump KafkaJS to version 2, following the end of node 8 LTS.\n       *\n       * @since 2019-11-29\n       */\n      numberOfBatches: 0,\n      duration: Date.now() - startFetch,\n      nodeId,\n    })\n\n    if (batches.length === 0) {\n      await this.heartbeat()\n    }\n\n    return batches\n  }\n\n  async handleBatch(batch) {\n    if (!this.running) {\n      this.logger.debug('consumer not running, exiting', {\n        groupId: this.consumerGroup.groupId,\n        memberId: this.consumerGroup.memberId,\n      })\n\n      return\n    }\n\n    /** @param {import('./batch')} batch */\n    const onBatch = async batch => {\n      const startBatchProcess = Date.now()\n      const payload = {\n        topic: batch.topic,\n        partition: batch.partition,\n        highWatermark: batch.highWatermark,\n        offsetLag: batch.offsetLag(),\n        /**\n         * @since 2019-06-24 (>= 1.8.0)\n         *\n         * offsetLag returns the lag based on the latest offset in the batch, to\n         * keep the event backward compatible we just introduced \"offsetLagLow\"\n         * which calculates the lag based on the first offset in the batch\n         */\n        offsetLagLow: batch.offsetLagLow(),\n        batchSize: batch.messages.length,\n        firstOffset: batch.firstOffset(),\n        lastOffset: batch.lastOffset(),\n      }\n\n      /**\n       * If the batch contained only control records or only aborted messages then we still\n       * need to resolve and auto-commit to ensure the consumer can move forward.\n       *\n       * We also need to emit batch instrumentation events to allow any listeners keeping\n       * track of offsets to know about the latest point of consumption.\n       *\n       * Added in #1256\n       *\n       * @see https://github.com/apache/kafka/blob/9aa660786e46c1efbf5605a6a69136a1dac6edb9/clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java#L1499-L1505\n       */\n      if (batch.isEmptyDueToFiltering()) {\n        this.instrumentationEmitter.emit(START_BATCH_PROCESS, payload)\n\n        this.consumerGroup.resolveOffset({\n          topic: batch.topic,\n          partition: batch.partition,\n          offset: batch.lastOffset(),\n        })\n        await this.autoCommitOffsetsIfNecessary()\n\n        this.instrumentationEmitter.emit(END_BATCH_PROCESS, {\n          ...payload,\n          duration: Date.now() - startBatchProcess,\n        })\n\n        await this.heartbeat()\n        return\n      }\n\n      if (batch.isEmpty()) {\n        await this.heartbeat()\n        return\n      }\n\n      this.instrumentationEmitter.emit(START_BATCH_PROCESS, payload)\n\n      if (this.eachMessage) {\n        await this.processEachMessage(batch)\n      } else if (this.eachBatch) {\n        await this.processEachBatch(batch)\n      }\n\n      this.instrumentationEmitter.emit(END_BATCH_PROCESS, {\n        ...payload,\n        duration: Date.now() - startBatchProcess,\n      })\n\n      await this.autoCommitOffsets()\n      await this.heartbeat()\n    }\n\n    await onBatch(batch)\n  }\n\n  autoCommitOffsets() {\n    if (this.autoCommit) {\n      return this.consumerGroup.commitOffsets()\n    }\n  }\n\n  autoCommitOffsetsIfNecessary() {\n    if (this.autoCommit) {\n      return this.consumerGroup.commitOffsetsIfNecessary()\n    }\n  }\n\n  commitOffsets(offsets) {\n    if (!this.running) {\n      this.logger.debug('consumer not running, exiting', {\n        groupId: this.consumerGroup.groupId,\n        memberId: this.consumerGroup.memberId,\n        offsets,\n      })\n      return\n    }\n\n    return this.retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await this.consumerGroup.commitOffsets(offsets)\n      } catch (e) {\n        if (!this.running) {\n          this.logger.debug('consumer not running, exiting', {\n            error: e.message,\n            groupId: this.consumerGroup.groupId,\n            memberId: this.consumerGroup.memberId,\n            offsets,\n          })\n          return\n        }\n\n        if (e.name === 'KafkaJSNotImplemented') {\n          return bail(e)\n        }\n\n        this.logger.debug('Error while committing offsets, trying again...', {\n          groupId: this.consumerGroup.groupId,\n          memberId: this.consumerGroup.memberId,\n          error: e.message,\n          stack: e.stack,\n          retryCount,\n          retryTime,\n          offsets,\n        })\n\n        throw e\n      }\n    })\n  }\n}\n", "const { MemberMetadata, MemberAssignment } = require('../../assignerProtocol')\n\n/**\n * RoundRobinAssigner\n * @type {import('types').PartitionAssigner}\n */\nmodule.exports = ({ cluster }) => ({\n  name: 'RoundRobinAssigner',\n  version: 0,\n\n  /**\n   * Assign the topics to the provided members.\n   *\n   * The members array contains information about each member, `memberMetadata` is the result of the\n   * `protocol` operation.\n   *\n   * @param {object} group\n   * @param {import('types').GroupMember[]} group.members array of members, e.g:\n                              [{ memberId: 'test-5f93f5a3', memberMetadata: Buffer }]\n   * @param {string[]} group.topics\n   * @returns {Promise<import('types').GroupMemberAssignment[]>} object partitions per topic per member, e.g:\n   *                   [\n   *                     {\n   *                       memberId: 'test-5f93f5a3',\n   *                       memberAssignment: {\n   *                         'topic-A': [0, 2, 4, 6],\n   *                         'topic-B': [1],\n   *                       },\n   *                     },\n   *                     {\n   *                       memberId: 'test-3d3d5341',\n   *                       memberAssignment: {\n   *                         'topic-A': [1, 3, 5],\n   *                         'topic-B': [0, 2],\n   *                       },\n   *                     }\n   *                   ]\n   */\n  async assign({ members, topics }) {\n    const membersCount = members.length\n    const sortedMembers = members.map(({ memberId }) => memberId).sort()\n    const assignment = {}\n\n    const topicsPartitions = topics.flatMap(topic => {\n      const partitionMetadata = cluster.findTopicPartitionMetadata(topic)\n      return partitionMetadata.map(m => ({ topic: topic, partitionId: m.partitionId }))\n    })\n\n    topicsPartitions.forEach((topicPartition, i) => {\n      const assignee = sortedMembers[i % membersCount]\n\n      if (!assignment[assignee]) {\n        assignment[assignee] = Object.create(null)\n      }\n\n      if (!assignment[assignee][topicPartition.topic]) {\n        assignment[assignee][topicPartition.topic] = []\n      }\n\n      assignment[assignee][topicPartition.topic].push(topicPartition.partitionId)\n    })\n\n    return Object.keys(assignment).map(memberId => ({\n      memberId,\n      memberAssignment: MemberAssignment.encode({\n        version: this.version,\n        assignment: assignment[memberId],\n      }),\n    }))\n  },\n\n  protocol({ topics }) {\n    return {\n      name: this.name,\n      metadata: MemberMetadata.encode({\n        version: this.version,\n        topics,\n      }),\n    }\n  },\n})\n", "const roundRobin = require('./roundRobinAssigner')\n\nmodule.exports = {\n  roundRobin,\n}\n", "const Long = require('../utils/long')\nconst createRetry = require('../retry')\nconst { initialRetryTime } = require('../retry/defaults')\nconst ConsumerGroup = require('./consumerGroup')\nconst Runner = require('./runner')\nconst { events, wrap: wrapEvent, unwrap: unwrapEvent } = require('./instrumentationEvents')\nconst InstrumentationEventEmitter = require('../instrumentation/emitter')\nconst { KafkaJSNonRetriableError } = require('../errors')\nconst { roundRobin } = require('./assigners')\nconst { EARLIEST_OFFSET, LATEST_OFFSET } = require('../constants')\nconst ISOLATION_LEVEL = require('../protocol/isolationLevel')\nconst sharedPromiseTo = require('../utils/sharedPromiseTo')\n\nconst { keys, values } = Object\nconst { CONNECT, DISCONNECT, STOP, CRASH } = events\n\nconst eventNames = values(events)\nconst eventKeys = keys(events)\n  .map(key => `consumer.events.${key}`)\n  .join(', ')\n\nconst specialOffsets = [\n  Long.fromValue(EARLIEST_OFFSET).toString(),\n  Long.fromValue(LATEST_OFFSET).toString(),\n]\n\n/**\n * @param {Object} params\n * @param {import(\"../../types\").Cluster} params.cluster\n * @param {String} params.groupId\n * @param {import('../../types').RetryOptions} [params.retry]\n * @param {import('../../types').Logger} params.logger\n * @param {import('../../types').PartitionAssigner[]} [params.partitionAssigners]\n * @param {number} [params.sessionTimeout]\n * @param {number} [params.rebalanceTimeout]\n * @param {number} [params.heartbeatInterval]\n * @param {number} [params.maxBytesPerPartition]\n * @param {number} [params.minBytes]\n * @param {number} [params.maxBytes]\n * @param {number} [params.maxWaitTimeInMs]\n * @param {number} [params.isolationLevel]\n * @param {string} [params.rackId]\n * @param {InstrumentationEventEmitter} [params.instrumentationEmitter]\n * @param {number} params.metadataMaxAge\n *\n * @returns {import(\"../../types\").Consumer}\n */\nmodule.exports = ({\n  cluster,\n  groupId,\n  retry,\n  logger: rootLogger,\n  partitionAssigners = [roundRobin],\n  sessionTimeout = 30000,\n  rebalanceTimeout = 60000,\n  heartbeatInterval = 3000,\n  maxBytesPerPartition = 1048576, // 1MB\n  minBytes = 1,\n  maxBytes = 10485760, // 10MB\n  maxWaitTimeInMs = 5000,\n  isolationLevel = ISOLATION_LEVEL.READ_COMMITTED,\n  rackId = '',\n  instrumentationEmitter: rootInstrumentationEmitter,\n  metadataMaxAge,\n}) => {\n  if (!groupId) {\n    throw new KafkaJSNonRetriableError('Consumer groupId must be a non-empty string.')\n  }\n\n  const logger = rootLogger.namespace('Consumer')\n  const instrumentationEmitter = rootInstrumentationEmitter || new InstrumentationEventEmitter()\n  const assigners = partitionAssigners.map(createAssigner =>\n    createAssigner({ groupId, logger, cluster })\n  )\n\n  /** @type {Record<string, { fromBeginning?: boolean }>} */\n  const topics = {}\n  let runner = null\n  /** @type {ConsumerGroup} */\n  let consumerGroup = null\n  let restartTimeout = null\n\n  if (heartbeatInterval >= sessionTimeout) {\n    throw new KafkaJSNonRetriableError(\n      `Consumer heartbeatInterval (${heartbeatInterval}) must be lower than sessionTimeout (${sessionTimeout}). It is recommended to set heartbeatInterval to approximately a third of the sessionTimeout.`\n    )\n  }\n\n  /** @type {import(\"../../types\").Consumer[\"connect\"]} */\n  const connect = async () => {\n    await cluster.connect()\n    instrumentationEmitter.emit(CONNECT)\n  }\n\n  /** @type {import(\"../../types\").Consumer[\"disconnect\"]} */\n  const disconnect = async () => {\n    try {\n      await stop()\n      logger.debug('consumer has stopped, disconnecting', { groupId })\n      await cluster.disconnect()\n      instrumentationEmitter.emit(DISCONNECT)\n    } catch (e) {\n      logger.error(`Caught error when disconnecting the consumer: ${e.message}`, {\n        stack: e.stack,\n        groupId,\n      })\n      throw e\n    }\n  }\n\n  /** @type {import(\"../../types\").Consumer[\"stop\"]} */\n  const stop = sharedPromiseTo(async () => {\n    try {\n      if (runner) {\n        await runner.stop()\n        runner = null\n        consumerGroup = null\n        instrumentationEmitter.emit(STOP)\n      }\n\n      clearTimeout(restartTimeout)\n      logger.info('Stopped', { groupId })\n    } catch (e) {\n      logger.error(`Caught error when stopping the consumer: ${e.message}`, {\n        stack: e.stack,\n        groupId,\n      })\n\n      throw e\n    }\n  })\n\n  /** @type {import(\"../../types\").Consumer[\"subscribe\"]} */\n  const subscribe = async ({ topic, topics: subscriptionTopics, fromBeginning = false }) => {\n    if (consumerGroup) {\n      throw new KafkaJSNonRetriableError('Cannot subscribe to topic while consumer is running')\n    }\n\n    if (!topic && !subscriptionTopics) {\n      throw new KafkaJSNonRetriableError('Missing required argument \"topics\"')\n    }\n\n    if (subscriptionTopics != null && !Array.isArray(subscriptionTopics)) {\n      throw new KafkaJSNonRetriableError('Argument \"topics\" must be an array')\n    }\n\n    const subscriptions = subscriptionTopics || [topic]\n\n    for (const subscription of subscriptions) {\n      if (typeof subscription !== 'string' && !(subscription instanceof RegExp)) {\n        throw new KafkaJSNonRetriableError(\n          `Invalid topic ${subscription} (${typeof subscription}), the topic name has to be a String or a RegExp`\n        )\n      }\n    }\n\n    const hasRegexSubscriptions = subscriptions.some(subscription => subscription instanceof RegExp)\n    const metadata = hasRegexSubscriptions ? await cluster.metadata() : undefined\n\n    const topicsToSubscribe = []\n    for (const subscription of subscriptions) {\n      const isRegExp = subscription instanceof RegExp\n      if (isRegExp) {\n        const topicRegExp = subscription\n        const matchedTopics = metadata.topicMetadata\n          .map(({ topic: topicName }) => topicName)\n          .filter(topicName => topicRegExp.test(topicName))\n\n        logger.debug('Subscription based on RegExp', {\n          groupId,\n          topicRegExp: topicRegExp.toString(),\n          matchedTopics,\n        })\n\n        topicsToSubscribe.push(...matchedTopics)\n      } else {\n        topicsToSubscribe.push(subscription)\n      }\n    }\n\n    for (const t of topicsToSubscribe) {\n      topics[t] = { fromBeginning }\n    }\n\n    await cluster.addMultipleTargetTopics(topicsToSubscribe)\n  }\n\n  /** @type {import(\"../../types\").Consumer[\"run\"]} */\n  const run = async ({\n    autoCommit = true,\n    autoCommitInterval = null,\n    autoCommitThreshold = null,\n    eachBatchAutoResolve = true,\n    partitionsConsumedConcurrently: concurrency = 1,\n    eachBatch = null,\n    eachMessage = null,\n  } = {}) => {\n    if (consumerGroup) {\n      logger.warn('consumer#run was called, but the consumer is already running', { groupId })\n      return\n    }\n\n    const start = async onCrash => {\n      logger.info('Starting', { groupId })\n\n      consumerGroup = new ConsumerGroup({\n        logger: rootLogger,\n        topics: keys(topics),\n        topicConfigurations: topics,\n        retry,\n        cluster,\n        groupId,\n        assigners,\n        sessionTimeout,\n        rebalanceTimeout,\n        maxBytesPerPartition,\n        minBytes,\n        maxBytes,\n        maxWaitTimeInMs,\n        instrumentationEmitter,\n        isolationLevel,\n        rackId,\n        metadataMaxAge,\n        autoCommit,\n        autoCommitInterval,\n        autoCommitThreshold,\n      })\n\n      runner = new Runner({\n        logger: rootLogger,\n        consumerGroup,\n        instrumentationEmitter,\n        heartbeatInterval,\n        retry,\n        autoCommit,\n        eachBatchAutoResolve,\n        eachBatch,\n        eachMessage,\n        onCrash,\n        concurrency,\n      })\n\n      await runner.start()\n    }\n\n    const onCrash = async e => {\n      logger.error(`Crash: ${e.name}: ${e.message}`, {\n        groupId,\n        retryCount: e.retryCount,\n        stack: e.stack,\n      })\n\n      if (e.name === 'KafkaJSConnectionClosedError') {\n        cluster.removeBroker({ host: e.host, port: e.port })\n      }\n\n      await disconnect()\n\n      const getOriginalCause = error => {\n        if (error.cause) {\n          return getOriginalCause(error.cause)\n        }\n\n        return error\n      }\n\n      const isErrorRetriable =\n        e.name === 'KafkaJSNumberOfRetriesExceeded' || getOriginalCause(e).retriable === true\n      const shouldRestart =\n        isErrorRetriable &&\n        (!retry ||\n          !retry.restartOnFailure ||\n          (await retry.restartOnFailure(e).catch(error => {\n            logger.error(\n              'Caught error when invoking user-provided \"restartOnFailure\" callback. Defaulting to restarting.',\n              {\n                error: error.message || error,\n                cause: e.message || e,\n                groupId,\n              }\n            )\n\n            return true\n          })))\n\n      instrumentationEmitter.emit(CRASH, {\n        error: e,\n        groupId,\n        restart: shouldRestart,\n      })\n\n      if (shouldRestart) {\n        const retryTime = e.retryTime || (retry && retry.initialRetryTime) || initialRetryTime\n        logger.error(`Restarting the consumer in ${retryTime}ms`, {\n          retryCount: e.retryCount,\n          retryTime,\n          groupId,\n        })\n\n        restartTimeout = setTimeout(() => start(onCrash), retryTime)\n      }\n    }\n\n    await start(onCrash)\n  }\n\n  /** @type {import(\"../../types\").Consumer[\"on\"]} */\n  const on = (eventName, listener) => {\n    if (!eventNames.includes(eventName)) {\n      throw new KafkaJSNonRetriableError(`Event name should be one of ${eventKeys}`)\n    }\n\n    return instrumentationEmitter.addListener(unwrapEvent(eventName), event => {\n      event.type = wrapEvent(event.type)\n      Promise.resolve(listener(event)).catch(e => {\n        logger.error(`Failed to execute listener: ${e.message}`, {\n          eventName,\n          stack: e.stack,\n        })\n      })\n    })\n  }\n\n  /**\n   * @type {import(\"../../types\").Consumer[\"commitOffsets\"]}\n   * @param topicPartitions\n   *   Example: [{ topic: 'topic-name', partition: 0, offset: '1', metadata: 'event-id-3' }]\n   */\n  const commitOffsets = async (topicPartitions = []) => {\n    const commitsByTopic = topicPartitions.reduce(\n      (payload, { topic, partition, offset, metadata = null }) => {\n        if (!topic) {\n          throw new KafkaJSNonRetriableError(`Invalid topic ${topic}`)\n        }\n\n        if (isNaN(partition)) {\n          throw new KafkaJSNonRetriableError(\n            `Invalid partition, expected a number received ${partition}`\n          )\n        }\n\n        let commitOffset\n        try {\n          commitOffset = Long.fromValue(offset)\n        } catch (_) {\n          throw new KafkaJSNonRetriableError(`Invalid offset, expected a long received ${offset}`)\n        }\n\n        if (commitOffset.lessThan(0)) {\n          throw new KafkaJSNonRetriableError('Offset must not be a negative number')\n        }\n\n        if (metadata !== null && typeof metadata !== 'string') {\n          throw new KafkaJSNonRetriableError(\n            `Invalid offset metadata, expected string or null, received ${metadata}`\n          )\n        }\n\n        const topicCommits = payload[topic] || []\n\n        topicCommits.push({ partition, offset: commitOffset, metadata })\n\n        return { ...payload, [topic]: topicCommits }\n      },\n      {}\n    )\n\n    if (!consumerGroup) {\n      throw new KafkaJSNonRetriableError(\n        'Consumer group was not initialized, consumer#run must be called first'\n      )\n    }\n\n    const topics = Object.keys(commitsByTopic)\n\n    return runner.commitOffsets({\n      topics: topics.map(topic => {\n        return {\n          topic,\n          partitions: commitsByTopic[topic],\n        }\n      }),\n    })\n  }\n\n  /** @type {import(\"../../types\").Consumer[\"seek\"]} */\n  const seek = ({ topic, partition, offset }) => {\n    if (!topic) {\n      throw new KafkaJSNonRetriableError(`Invalid topic ${topic}`)\n    }\n\n    if (isNaN(partition)) {\n      throw new KafkaJSNonRetriableError(\n        `Invalid partition, expected a number received ${partition}`\n      )\n    }\n\n    let seekOffset\n    try {\n      seekOffset = Long.fromValue(offset)\n    } catch (_) {\n      throw new KafkaJSNonRetriableError(`Invalid offset, expected a long received ${offset}`)\n    }\n\n    if (seekOffset.lessThan(0) && !specialOffsets.includes(seekOffset.toString())) {\n      throw new KafkaJSNonRetriableError('Offset must not be a negative number')\n    }\n\n    if (!consumerGroup) {\n      throw new KafkaJSNonRetriableError(\n        'Consumer group was not initialized, consumer#run must be called first'\n      )\n    }\n\n    consumerGroup.seek({ topic, partition, offset: seekOffset.toString() })\n  }\n\n  /** @type {import(\"../../types\").Consumer[\"describeGroup\"]} */\n  const describeGroup = async () => {\n    const coordinator = await cluster.findGroupCoordinator({ groupId })\n    const retrier = createRetry(retry)\n    return retrier(async () => {\n      const { groups } = await coordinator.describeGroups({ groupIds: [groupId] })\n      return groups.find(group => group.groupId === groupId)\n    })\n  }\n\n  /**\n   * @type {import(\"../../types\").Consumer[\"pause\"]}\n   * @param topicPartitions\n   *   Example: [{ topic: 'topic-name', partitions: [1, 2] }]\n   */\n  const pause = (topicPartitions = []) => {\n    for (const topicPartition of topicPartitions) {\n      if (!topicPartition || !topicPartition.topic) {\n        throw new KafkaJSNonRetriableError(\n          `Invalid topic ${(topicPartition && topicPartition.topic) || topicPartition}`\n        )\n      } else if (\n        typeof topicPartition.partitions !== 'undefined' &&\n        (!Array.isArray(topicPartition.partitions) || topicPartition.partitions.some(isNaN))\n      ) {\n        throw new KafkaJSNonRetriableError(\n          `Array of valid partitions required to pause specific partitions instead of ${topicPartition.partitions}`\n        )\n      }\n    }\n\n    if (!consumerGroup) {\n      throw new KafkaJSNonRetriableError(\n        'Consumer group was not initialized, consumer#run must be called first'\n      )\n    }\n\n    consumerGroup.pause(topicPartitions)\n  }\n\n  /**\n   * Returns the list of topic partitions paused on this consumer\n   *\n   * @type {import(\"../../types\").Consumer[\"paused\"]}\n   */\n  const paused = () => {\n    if (!consumerGroup) {\n      return []\n    }\n\n    return consumerGroup.paused()\n  }\n\n  /**\n   * @type {import(\"../../types\").Consumer[\"resume\"]}\n   * @param topicPartitions\n   *  Example: [{ topic: 'topic-name', partitions: [1, 2] }]\n   */\n  const resume = (topicPartitions = []) => {\n    for (const topicPartition of topicPartitions) {\n      if (!topicPartition || !topicPartition.topic) {\n        throw new KafkaJSNonRetriableError(\n          `Invalid topic ${(topicPartition && topicPartition.topic) || topicPartition}`\n        )\n      } else if (\n        typeof topicPartition.partitions !== 'undefined' &&\n        (!Array.isArray(topicPartition.partitions) || topicPartition.partitions.some(isNaN))\n      ) {\n        throw new KafkaJSNonRetriableError(\n          `Array of valid partitions required to resume specific partitions instead of ${topicPartition.partitions}`\n        )\n      }\n    }\n\n    if (!consumerGroup) {\n      throw new KafkaJSNonRetriableError(\n        'Consumer group was not initialized, consumer#run must be called first'\n      )\n    }\n\n    consumerGroup.resume(topicPartitions)\n  }\n\n  /**\n   * @return {Object} logger\n   */\n  const getLogger = () => logger\n\n  return {\n    connect,\n    disconnect,\n    subscribe,\n    stop,\n    run,\n    commitOffsets,\n    seek,\n    describeGroup,\n    pause,\n    paused,\n    resume,\n    on,\n    events,\n    logger: getLogger,\n  }\n}\n", "const sleep = require('./sleep')\nconst { KafkaJSTimeout } = require('../errors')\n\nmodule.exports = (\n  fn,\n  { delay = 50, maxWait = 10000, timeoutMessage = 'Timeout', ignoreTimeout = false } = {}\n) => {\n  let timeoutId\n  let totalWait = 0\n  let fulfilled = false\n\n  const checkCondition = async (resolve, reject) => {\n    totalWait += delay\n    if (fulfilled) {\n      return\n    }\n\n    await sleep(delay)\n\n    try {\n      const result = await fn(totalWait)\n      if (result) {\n        fulfilled = true\n        clearTimeout(timeoutId)\n        return resolve(result)\n      }\n\n      checkCondition(resolve, reject)\n    } catch (e) {\n      fulfilled = true\n      clearTimeout(timeoutId)\n      reject(e)\n    }\n  }\n\n  return new Promise((resolve, reject) => {\n    checkCondition(resolve, reject)\n\n    if (ignoreTimeout) {\n      return\n    }\n\n    timeoutId = setTimeout(() => {\n      if (!fulfilled) {\n        fulfilled = true\n        return reject(new KafkaJSTimeout(timeoutMessage))\n      }\n    }, maxWait)\n  })\n}\n", "module.exports = async (array, groupFn) => {\n  const result = new Map()\n\n  for (const item of array) {\n    const group = await Promise.resolve(groupFn(item))\n    result.set(group, result.has(group) ? [...result.get(group), item] : [item])\n  }\n\n  return result\n}\n", "const swapObject = require('../utils/swapObject')\nconst networkEvents = require('../network/instrumentationEvents')\nconst InstrumentationEventType = require('../instrumentation/eventType')\nconst adminType = InstrumentationEventType('admin')\n\nconst events = {\n  CONNECT: adminType('connect'),\n  DISCONNECT: adminType('disconnect'),\n  REQUEST: adminType(networkEvents.NETWORK_REQUEST),\n  REQUEST_TIMEOUT: adminType(networkEvents.NETWORK_REQUEST_TIMEOUT),\n  REQUEST_QUEUE_SIZE: adminType(networkEvents.NETWORK_REQUEST_QUEUE_SIZE),\n}\n\nconst wrappedEvents = {\n  [events.REQUEST]: networkEvents.NETWORK_REQUEST,\n  [events.REQUEST_TIMEOUT]: networkEvents.NETWORK_REQUEST_TIMEOUT,\n  [events.REQUEST_QUEUE_SIZE]: networkEvents.NETWORK_REQUEST_QUEUE_SIZE,\n}\n\nconst reversedWrappedEvents = swapObject(wrappedEvents)\nconst unwrap = eventName => wrappedEvents[eventName] || eventName\nconst wrap = eventName => reversedWrappedEvents[eventName] || eventName\n\nmodule.exports = {\n  events,\n  wrap,\n  unwrap,\n}\n", "/**\n * @see https://github.com/apache/kafka/blob/a15387f34d142684859c2a57fcbef25edcdce25a/clients/src/main/java/org/apache/kafka/common/resource/ResourceType.java#L25-L31\n * @typedef {number} ACLResourceTypes\n *\n * Enum for ACL Resource Types\n * @readonly\n * @enum {ACLResourceTypes}\n */\n\nmodule.exports = {\n  /**\n   * Represents any ResourceType which this client cannot understand,\n   * perhaps because this client is too old.\n   */\n  UNKNOWN: 0,\n  /**\n   * In a filter, matches any ResourceType.\n   */\n  ANY: 1,\n  /**\n   * A Kafka topic.\n   * @see http://kafka.apache.org/documentation/#topicconfigs\n   */\n  TOPIC: 2,\n  /**\n   * A consumer group.\n   * @see http://kafka.apache.org/documentation/#consumerconfigs\n   */\n  GROUP: 3,\n  /**\n   * The cluster as a whole.\n   */\n  CLUSTER: 4,\n  /**\n   * A transactional ID.\n   */\n  TRANSACTIONAL_ID: 5,\n  /**\n   * A token ID.\n   */\n  DELEGATION_TOKEN: 6,\n}\n", "// From:\n// https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/common/acl/AclOperation.java#L44\n\n/**\n * @typedef {number} ACLOperationTypes\n *\n * Enum for ACL Operations Types\n * @readonly\n * @enum {ACLOperationTypes}\n */\nmodule.exports = {\n  /**\n   * Represents any AclOperation which this client cannot understand, perhaps because this\n   * client is too old.\n   */\n  UNKNOWN: 0,\n  /**\n   * In a filter, matches any AclOperation.\n   */\n  ANY: 1,\n  /**\n   * ALL operation.\n   */\n  ALL: 2,\n  /**\n   * READ operation.\n   */\n  READ: 3,\n  /**\n   * WRITE operation.\n   */\n  WRITE: 4,\n  /**\n   * CREATE operation.\n   */\n  CREATE: 5,\n  /**\n   * DELETE operation.\n   */\n  DELETE: 6,\n  /**\n   * ALTER operation.\n   */\n  ALTER: 7,\n  /**\n   * DESCRIBE operation.\n   */\n  DESCRIBE: 8,\n  /**\n   * CLUSTER_ACTION operation.\n   */\n  CLUSTER_ACTION: 9,\n  /**\n   * DESCRIBE_CONFIGS operation.\n   */\n  DESCRIBE_CONFIGS: 10,\n  /**\n   * ALTER_CONFIGS operation.\n   */\n  ALTER_CONFIGS: 11,\n  /**\n   * IDEMPOTENT_WRITE operation.\n   */\n  IDEMPOTENT_WRITE: 12,\n}\n", "// From:\n// https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/common/acl/AclPermissionType.java/#L31\n\n/**\n * @typedef {number} ACLPermissionTypes\n *\n * Enum for Permission Types\n * @readonly\n * @enum {ACLPermissionTypes}\n */\nmodule.exports = {\n  /**\n   * Represents any AclPermissionType which this client cannot understand,\n   * perhaps because this client is too old.\n   */\n  UNKNOWN: 0,\n  /**\n   * In a filter, matches any AclPermissionType.\n   */\n  ANY: 1,\n  /**\n   * Disallows access.\n   */\n  DENY: 2,\n  /**\n   * Grants access.\n   */\n  ALLOW: 3,\n}\n", "// From:\n// https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/common/resource/PatternType.java#L32\n\n/**\n * @typedef {number} ACLResourcePatternTypes\n *\n * Enum for ACL Resource Pattern Type\n * @readonly\n * @enum {ACLResourcePatternTypes}\n */\nmodule.exports = {\n  /**\n   * Represents any PatternType which this client cannot understand, perhaps because this client is too old.\n   */\n  UNKNOWN: 0,\n  /**\n   * In a filter, matches any resource pattern type.\n   */\n  ANY: 1,\n  /**\n   * In a filter, will perform pattern matching.\n   *\n   * e.g. Given a filter of {@code ResourcePatternFilter(TOPIC, \"payments.received\", MATCH)`}, the filter match\n   * any {@link ResourcePattern} that matches topic 'payments.received'. This might include:\n   * <ul>\n   *     <li>A Literal pattern with the same type and name, e.g. {@code ResourcePattern(TOPIC, \"payments.received\", LITERAL)}</li>\n   *     <li>A Wildcard pattern with the same type, e.g. {@code ResourcePattern(TOPIC, \"*\", LITERAL)}</li>\n   *     <li>A Prefixed pattern with the same type and where the name is a matching prefix, e.g. {@code ResourcePattern(TOPIC, \"payments.\", PREFIXED)}</li>\n   * </ul>\n   */\n  MATCH: 2,\n  /**\n   * A literal resource name.\n   *\n   * A literal name defines the full name of a resource, e.g. topic with name 'foo', or group with name 'bob'.\n   *\n   * The special wildcard character {@code *} can be used to represent a resource with any name.\n   */\n  LITERAL: 3,\n  /**\n   * A prefixed resource name.\n   *\n   * A prefixed name defines a prefix for a resource, e.g. topics with names that start with 'foo'.\n   */\n  PREFIXED: 4,\n}\n", "const createRetry = require('../retry')\nconst waitFor = require('../utils/waitFor')\nconst groupBy = require('../utils/groupBy')\nconst createConsumer = require('../consumer')\nconst InstrumentationEventEmitter = require('../instrumentation/emitter')\nconst { events, wrap: wrapEvent, unwrap: unwrapEvent } = require('./instrumentationEvents')\nconst { LEVELS } = require('../loggers')\nconst {\n  KafkaJSNonRetriableError,\n  KafkaJSDeleteGroupsError,\n  KafkaJSBrokerNotFound,\n  KafkaJSDeleteTopicRecordsError,\n  KafkaJSAggregateError,\n} = require('../errors')\nconst { staleMetadata } = require('../protocol/error')\nconst CONFIG_RESOURCE_TYPES = require('../protocol/configResourceTypes')\nconst ACL_RESOURCE_TYPES = require('../protocol/aclResourceTypes')\nconst ACL_OPERATION_TYPES = require('../protocol/aclOperationTypes')\nconst ACL_PERMISSION_TYPES = require('../protocol/aclPermissionTypes')\nconst RESOURCE_PATTERN_TYPES = require('../protocol/resourcePatternTypes')\nconst { EARLIEST_OFFSET, LATEST_OFFSET } = require('../constants')\n\nconst { CONNECT, DISCONNECT } = events\n\nconst NO_CONTROLLER_ID = -1\n\nconst { values, keys, entries } = Object\nconst eventNames = values(events)\nconst eventKeys = keys(events)\n  .map(key => `admin.events.${key}`)\n  .join(', ')\n\nconst retryOnLeaderNotAvailable = (fn, opts = {}) => {\n  const callback = async () => {\n    try {\n      return await fn()\n    } catch (e) {\n      if (e.type !== 'LEADER_NOT_AVAILABLE') {\n        throw e\n      }\n      return false\n    }\n  }\n\n  return waitFor(callback, opts)\n}\n\nconst isConsumerGroupRunning = description => ['Empty', 'Dead'].includes(description.state)\nconst findTopicPartitions = async (cluster, topic) => {\n  await cluster.addTargetTopic(topic)\n  await cluster.refreshMetadataIfNecessary()\n\n  return cluster\n    .findTopicPartitionMetadata(topic)\n    .map(({ partitionId }) => partitionId)\n    .sort()\n}\nconst indexByPartition = array =>\n  array.reduce(\n    (obj, { partition, ...props }) => Object.assign(obj, { [partition]: { ...props } }),\n    {}\n  )\n\n/**\n *\n * @param {Object} params\n * @param {import(\"../../types\").Logger} params.logger\n * @param {InstrumentationEventEmitter} [params.instrumentationEmitter]\n * @param {import('../../types').RetryOptions} params.retry\n * @param {import(\"../../types\").Cluster} params.cluster\n *\n * @returns {import(\"../../types\").Admin}\n */\nmodule.exports = ({\n  logger: rootLogger,\n  instrumentationEmitter: rootInstrumentationEmitter,\n  retry,\n  cluster,\n}) => {\n  const logger = rootLogger.namespace('Admin')\n  const instrumentationEmitter = rootInstrumentationEmitter || new InstrumentationEventEmitter()\n\n  /**\n   * @returns {Promise}\n   */\n  const connect = async () => {\n    await cluster.connect()\n    instrumentationEmitter.emit(CONNECT)\n  }\n\n  /**\n   * @return {Promise}\n   */\n  const disconnect = async () => {\n    await cluster.disconnect()\n    instrumentationEmitter.emit(DISCONNECT)\n  }\n\n  /**\n   * @return {Promise}\n   */\n  const listTopics = async () => {\n    const { topicMetadata } = await cluster.metadata()\n    const topics = topicMetadata.map(t => t.topic)\n    return topics\n  }\n\n  /**\n   * @param {Object} request\n   * @param {array} request.topics\n   * @param {boolean} [request.validateOnly=false]\n   * @param {number} [request.timeout=5000]\n   * @param {boolean} [request.waitForLeaders=true]\n   * @return {Promise}\n   */\n  const createTopics = async ({ topics, validateOnly, timeout, waitForLeaders = true }) => {\n    if (!topics || !Array.isArray(topics)) {\n      throw new KafkaJSNonRetriableError(`Invalid topics array ${topics}`)\n    }\n\n    if (topics.filter(({ topic }) => typeof topic !== 'string').length > 0) {\n      throw new KafkaJSNonRetriableError(\n        'Invalid topics array, the topic names have to be a valid string'\n      )\n    }\n\n    const topicNames = new Set(topics.map(({ topic }) => topic))\n    if (topicNames.size < topics.length) {\n      throw new KafkaJSNonRetriableError(\n        'Invalid topics array, it cannot have multiple entries for the same topic'\n      )\n    }\n\n    for (const { topic, configEntries } of topics) {\n      if (configEntries == null) {\n        continue\n      }\n\n      if (!Array.isArray(configEntries)) {\n        throw new KafkaJSNonRetriableError(\n          `Invalid configEntries for topic \"${topic}\", must be an array`\n        )\n      }\n\n      configEntries.forEach((entry, index) => {\n        if (typeof entry !== 'object' || entry == null) {\n          throw new KafkaJSNonRetriableError(\n            `Invalid configEntries for topic \"${topic}\". Entry ${index} must be an object`\n          )\n        }\n\n        for (const requiredProperty of ['name', 'value']) {\n          if (\n            !Object.prototype.hasOwnProperty.call(entry, requiredProperty) ||\n            typeof entry[requiredProperty] !== 'string'\n          ) {\n            throw new KafkaJSNonRetriableError(\n              `Invalid configEntries for topic \"${topic}\". Entry ${index} must have a valid \"${requiredProperty}\" property`\n            )\n          }\n        }\n      })\n    }\n\n    const retrier = createRetry(retry)\n\n    return retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await cluster.refreshMetadata()\n        const broker = await cluster.findControllerBroker()\n        await broker.createTopics({ topics, validateOnly, timeout })\n\n        if (waitForLeaders) {\n          const topicNamesArray = Array.from(topicNames.values())\n          await retryOnLeaderNotAvailable(async () => await broker.metadata(topicNamesArray), {\n            delay: 100,\n            maxWait: timeout,\n            timeoutMessage: 'Timed out while waiting for topic leaders',\n          })\n        }\n\n        return true\n      } catch (e) {\n        if (e.type === 'NOT_CONTROLLER') {\n          logger.warn('Could not create topics', { error: e.message, retryCount, retryTime })\n          throw e\n        }\n\n        if (e instanceof KafkaJSAggregateError) {\n          if (e.errors.every(error => error.type === 'TOPIC_ALREADY_EXISTS')) {\n            return false\n          }\n        }\n\n        bail(e)\n      }\n    })\n  }\n  /**\n   * @param {array} topicPartitions\n   * @param {boolean} [validateOnly=false]\n   * @param {number} [timeout=5000]\n   * @return {Promise<void>}\n   */\n  const createPartitions = async ({ topicPartitions, validateOnly, timeout }) => {\n    if (!topicPartitions || !Array.isArray(topicPartitions)) {\n      throw new KafkaJSNonRetriableError(`Invalid topic partitions array ${topicPartitions}`)\n    }\n    if (topicPartitions.length === 0) {\n      throw new KafkaJSNonRetriableError(`Empty topic partitions array`)\n    }\n\n    if (topicPartitions.filter(({ topic }) => typeof topic !== 'string').length > 0) {\n      throw new KafkaJSNonRetriableError(\n        'Invalid topic partitions array, the topic names have to be a valid string'\n      )\n    }\n\n    const topicNames = new Set(topicPartitions.map(({ topic }) => topic))\n    if (topicNames.size < topicPartitions.length) {\n      throw new KafkaJSNonRetriableError(\n        'Invalid topic partitions array, it cannot have multiple entries for the same topic'\n      )\n    }\n\n    const retrier = createRetry(retry)\n\n    return retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await cluster.refreshMetadata()\n        const broker = await cluster.findControllerBroker()\n        await broker.createPartitions({ topicPartitions, validateOnly, timeout })\n      } catch (e) {\n        if (e.type === 'NOT_CONTROLLER') {\n          logger.warn('Could not create topics', { error: e.message, retryCount, retryTime })\n          throw e\n        }\n\n        bail(e)\n      }\n    })\n  }\n\n  /**\n   * @param {string[]} topics\n   * @param {number} [timeout=5000]\n   * @return {Promise}\n   */\n  const deleteTopics = async ({ topics, timeout }) => {\n    if (!topics || !Array.isArray(topics)) {\n      throw new KafkaJSNonRetriableError(`Invalid topics array ${topics}`)\n    }\n\n    if (topics.filter(topic => typeof topic !== 'string').length > 0) {\n      throw new KafkaJSNonRetriableError('Invalid topics array, the names must be a valid string')\n    }\n\n    const retrier = createRetry(retry)\n\n    return retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await cluster.refreshMetadata()\n        const broker = await cluster.findControllerBroker()\n        await broker.deleteTopics({ topics, timeout })\n\n        // Remove deleted topics\n        for (const topic of topics) {\n          cluster.targetTopics.delete(topic)\n        }\n\n        await cluster.refreshMetadata()\n      } catch (e) {\n        if (['NOT_CONTROLLER', 'UNKNOWN_TOPIC_OR_PARTITION'].includes(e.type)) {\n          logger.warn('Could not delete topics', { error: e.message, retryCount, retryTime })\n          throw e\n        }\n\n        if (e.type === 'REQUEST_TIMED_OUT') {\n          logger.error(\n            'Could not delete topics, check if \"delete.topic.enable\" is set to \"true\" (the default value is \"false\") or increase the timeout',\n            {\n              error: e.message,\n              retryCount,\n              retryTime,\n            }\n          )\n        }\n\n        bail(e)\n      }\n    })\n  }\n\n  /**\n   * @param {string} topic\n   */\n\n  const fetchTopicOffsets = async topic => {\n    if (!topic || typeof topic !== 'string') {\n      throw new KafkaJSNonRetriableError(`Invalid topic ${topic}`)\n    }\n\n    const retrier = createRetry(retry)\n\n    return retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await cluster.addTargetTopic(topic)\n        await cluster.refreshMetadataIfNecessary()\n\n        const metadata = cluster.findTopicPartitionMetadata(topic)\n        const high = await cluster.fetchTopicsOffset([\n          {\n            topic,\n            fromBeginning: false,\n            partitions: metadata.map(p => ({ partition: p.partitionId })),\n          },\n        ])\n\n        const low = await cluster.fetchTopicsOffset([\n          {\n            topic,\n            fromBeginning: true,\n            partitions: metadata.map(p => ({ partition: p.partitionId })),\n          },\n        ])\n\n        const { partitions: highPartitions } = high.pop()\n        const { partitions: lowPartitions } = low.pop()\n        return highPartitions.map(({ partition, offset }) => ({\n          partition,\n          offset,\n          high: offset,\n          low: lowPartitions.find(({ partition: lowPartition }) => lowPartition === partition)\n            .offset,\n        }))\n      } catch (e) {\n        if (e.type === 'UNKNOWN_TOPIC_OR_PARTITION') {\n          await cluster.refreshMetadata()\n          throw e\n        }\n\n        bail(e)\n      }\n    })\n  }\n\n  /**\n   * @param {string} topic\n   * @param {number} [timestamp]\n   */\n\n  const fetchTopicOffsetsByTimestamp = async (topic, timestamp) => {\n    if (!topic || typeof topic !== 'string') {\n      throw new KafkaJSNonRetriableError(`Invalid topic ${topic}`)\n    }\n\n    const retrier = createRetry(retry)\n\n    return retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await cluster.addTargetTopic(topic)\n        await cluster.refreshMetadataIfNecessary()\n\n        const metadata = cluster.findTopicPartitionMetadata(topic)\n        const partitions = metadata.map(p => ({ partition: p.partitionId }))\n\n        const high = await cluster.fetchTopicsOffset([\n          {\n            topic,\n            fromBeginning: false,\n            partitions,\n          },\n        ])\n        const { partitions: highPartitions } = high.pop()\n\n        const offsets = await cluster.fetchTopicsOffset([\n          {\n            topic,\n            fromTimestamp: timestamp,\n            partitions,\n          },\n        ])\n        const { partitions: lowPartitions } = offsets.pop()\n\n        return lowPartitions.map(({ partition, offset }) => ({\n          partition,\n          offset:\n            parseInt(offset, 10) >= 0\n              ? offset\n              : highPartitions.find(({ partition: highPartition }) => highPartition === partition)\n                  .offset,\n        }))\n      } catch (e) {\n        if (e.type === 'UNKNOWN_TOPIC_OR_PARTITION') {\n          await cluster.refreshMetadata()\n          throw e\n        }\n\n        bail(e)\n      }\n    })\n  }\n\n  /**\n   * Fetch offsets for a topic or multiple topics\n   *\n   * Note: set either topic or topics but not both.\n   *\n   * @param {string} groupId\n   * @param {string[]} topics - list of topics to fetch offsets for, defaults to `[]` which fetches all topics for `groupId`.\n   * @param {boolean} [resolveOffsets=false]\n   * @return {Promise}\n   */\n  const fetchOffsets = async ({ groupId, topics, resolveOffsets = false }) => {\n    if (!groupId) {\n      throw new KafkaJSNonRetriableError(`Invalid groupId ${groupId}`)\n    }\n\n    if (!topics) {\n      topics = []\n    }\n\n    if (!Array.isArray(topics)) {\n      throw new KafkaJSNonRetriableError('Expected topics array to be set')\n    }\n\n    const coordinator = await cluster.findGroupCoordinator({ groupId })\n    const topicsToFetch = await Promise.all(\n      topics.map(async topic => {\n        const partitions = await findTopicPartitions(cluster, topic)\n        const partitionsToFetch = partitions.map(partition => ({ partition }))\n        return { topic, partitions: partitionsToFetch }\n      })\n    )\n    let { responses: consumerOffsets } = await coordinator.offsetFetch({\n      groupId,\n      topics: topicsToFetch,\n    })\n\n    if (resolveOffsets) {\n      consumerOffsets = await Promise.all(\n        consumerOffsets.map(async ({ topic, partitions }) => {\n          const indexedOffsets = indexByPartition(await fetchTopicOffsets(topic))\n          const recalculatedPartitions = partitions.map(({ offset, partition, ...props }) => {\n            let resolvedOffset = offset\n            if (Number(offset) === EARLIEST_OFFSET) {\n              resolvedOffset = indexedOffsets[partition].low\n            }\n            if (Number(offset) === LATEST_OFFSET) {\n              resolvedOffset = indexedOffsets[partition].high\n            }\n            return {\n              partition,\n              offset: resolvedOffset,\n              ...props,\n            }\n          })\n\n          await setOffsets({ groupId, topic, partitions: recalculatedPartitions })\n\n          return {\n            topic,\n            partitions: recalculatedPartitions,\n          }\n        })\n      )\n    }\n\n    return consumerOffsets.map(({ topic, partitions }) => {\n      const completePartitions = partitions.map(({ partition, offset, metadata }) => ({\n        partition,\n        offset,\n        metadata: metadata || null,\n      }))\n\n      return { topic, partitions: completePartitions }\n    })\n  }\n\n  /**\n   * @param {string} groupId\n   * @param {string} topic\n   * @param {boolean} [earliest=false]\n   * @return {Promise}\n   */\n  const resetOffsets = async ({ groupId, topic, earliest = false }) => {\n    if (!groupId) {\n      throw new KafkaJSNonRetriableError(`Invalid groupId ${groupId}`)\n    }\n\n    if (!topic) {\n      throw new KafkaJSNonRetriableError(`Invalid topic ${topic}`)\n    }\n\n    const partitions = await findTopicPartitions(cluster, topic)\n    const partitionsToSeek = partitions.map(partition => ({\n      partition,\n      offset: cluster.defaultOffset({ fromBeginning: earliest }),\n    }))\n\n    return setOffsets({ groupId, topic, partitions: partitionsToSeek })\n  }\n\n  /**\n   * @param {string} groupId\n   * @param {string} topic\n   * @param {Array<SeekEntry>} partitions\n   * @return {Promise}\n   *\n   * @typedef {Object} SeekEntry\n   * @property {number} partition\n   * @property {string} offset\n   */\n  const setOffsets = async ({ groupId, topic, partitions }) => {\n    if (!groupId) {\n      throw new KafkaJSNonRetriableError(`Invalid groupId ${groupId}`)\n    }\n\n    if (!topic) {\n      throw new KafkaJSNonRetriableError(`Invalid topic ${topic}`)\n    }\n\n    if (!partitions || partitions.length === 0) {\n      throw new KafkaJSNonRetriableError(`Invalid partitions`)\n    }\n\n    const consumer = createConsumer({\n      logger: rootLogger.namespace('Admin', LEVELS.NOTHING),\n      cluster,\n      groupId,\n    })\n\n    await consumer.subscribe({ topic, fromBeginning: true })\n    const description = await consumer.describeGroup()\n\n    if (!isConsumerGroupRunning(description)) {\n      throw new KafkaJSNonRetriableError(\n        `The consumer group must have no running instances, current state: ${description.state}`\n      )\n    }\n\n    return new Promise((resolve, reject) => {\n      consumer.on(consumer.events.FETCH, async () =>\n        consumer\n          .stop()\n          .then(resolve)\n          .catch(reject)\n      )\n\n      consumer\n        .run({\n          eachBatchAutoResolve: false,\n          eachBatch: async () => true,\n        })\n        .catch(reject)\n\n      // This consumer doesn't need to consume any data\n      consumer.pause([{ topic }])\n\n      for (const seekData of partitions) {\n        consumer.seek({ topic, ...seekData })\n      }\n    })\n  }\n\n  const isBrokerConfig = type =>\n    [CONFIG_RESOURCE_TYPES.BROKER, CONFIG_RESOURCE_TYPES.BROKER_LOGGER].includes(type)\n\n  /**\n   * Broker configs can only be returned by the target broker\n   *\n   * @see\n   * https://github.com/apache/kafka/blob/821c1ac6641845aeca96a43bc2b946ecec5cba4f/clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java#L3783\n   * https://github.com/apache/kafka/blob/821c1ac6641845aeca96a43bc2b946ecec5cba4f/clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java#L2027\n   *\n   * @param {Broker} defaultBroker. Broker used in case the configuration is not a broker config\n   */\n  const groupResourcesByBroker = ({ resources, defaultBroker }) =>\n    groupBy(resources, async ({ type, name: nodeId }) => {\n      return isBrokerConfig(type)\n        ? await cluster.findBroker({ nodeId: String(nodeId) })\n        : defaultBroker\n    })\n\n  /**\n   * @param {Array<ResourceConfigQuery>} resources\n   * @param {boolean} [includeSynonyms=false]\n   * @return {Promise}\n   *\n   * @typedef {Object} ResourceConfigQuery\n   * @property {ConfigResourceType} type\n   * @property {string} name\n   * @property {Array<String>} [configNames=[]]\n   */\n  const describeConfigs = async ({ resources, includeSynonyms }) => {\n    if (!resources || !Array.isArray(resources)) {\n      throw new KafkaJSNonRetriableError(`Invalid resources array ${resources}`)\n    }\n\n    if (resources.length === 0) {\n      throw new KafkaJSNonRetriableError('Resources array cannot be empty')\n    }\n\n    const validResourceTypes = Object.values(CONFIG_RESOURCE_TYPES)\n    const invalidType = resources.find(r => !validResourceTypes.includes(r.type))\n\n    if (invalidType) {\n      throw new KafkaJSNonRetriableError(\n        `Invalid resource type ${invalidType.type}: ${JSON.stringify(invalidType)}`\n      )\n    }\n\n    const invalidName = resources.find(r => !r.name || typeof r.name !== 'string')\n\n    if (invalidName) {\n      throw new KafkaJSNonRetriableError(\n        `Invalid resource name ${invalidName.name}: ${JSON.stringify(invalidName)}`\n      )\n    }\n\n    const invalidConfigs = resources.find(\n      r => !Array.isArray(r.configNames) && r.configNames != null\n    )\n\n    if (invalidConfigs) {\n      const { configNames } = invalidConfigs\n      throw new KafkaJSNonRetriableError(\n        `Invalid resource configNames ${configNames}: ${JSON.stringify(invalidConfigs)}`\n      )\n    }\n\n    const retrier = createRetry(retry)\n\n    return retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await cluster.refreshMetadata()\n        const controller = await cluster.findControllerBroker()\n        const resourcerByBroker = await groupResourcesByBroker({\n          resources,\n          defaultBroker: controller,\n        })\n\n        const describeConfigsAction = async broker => {\n          const targetBroker = broker || controller\n          return targetBroker.describeConfigs({\n            resources: resourcerByBroker.get(targetBroker),\n            includeSynonyms,\n          })\n        }\n\n        const brokers = Array.from(resourcerByBroker.keys())\n        const responses = await Promise.all(brokers.map(describeConfigsAction))\n        const responseResources = responses.reduce(\n          (result, { resources }) => [...result, ...resources],\n          []\n        )\n\n        return { resources: responseResources }\n      } catch (e) {\n        if (e.type === 'NOT_CONTROLLER') {\n          logger.warn('Could not describe configs', { error: e.message, retryCount, retryTime })\n          throw e\n        }\n\n        bail(e)\n      }\n    })\n  }\n\n  /**\n   * @param {Array<ResourceConfig>} resources\n   * @param {boolean} [validateOnly=false]\n   * @return {Promise}\n   *\n   * @typedef {Object} ResourceConfig\n   * @property {ConfigResourceType} type\n   * @property {string} name\n   * @property {Array<ResourceConfigEntry>} configEntries\n   *\n   * @typedef {Object} ResourceConfigEntry\n   * @property {string} name\n   * @property {string} value\n   */\n  const alterConfigs = async ({ resources, validateOnly }) => {\n    if (!resources || !Array.isArray(resources)) {\n      throw new KafkaJSNonRetriableError(`Invalid resources array ${resources}`)\n    }\n\n    if (resources.length === 0) {\n      throw new KafkaJSNonRetriableError('Resources array cannot be empty')\n    }\n\n    const validResourceTypes = Object.values(CONFIG_RESOURCE_TYPES)\n    const invalidType = resources.find(r => !validResourceTypes.includes(r.type))\n\n    if (invalidType) {\n      throw new KafkaJSNonRetriableError(\n        `Invalid resource type ${invalidType.type}: ${JSON.stringify(invalidType)}`\n      )\n    }\n\n    const invalidName = resources.find(r => !r.name || typeof r.name !== 'string')\n\n    if (invalidName) {\n      throw new KafkaJSNonRetriableError(\n        `Invalid resource name ${invalidName.name}: ${JSON.stringify(invalidName)}`\n      )\n    }\n\n    const invalidConfigs = resources.find(r => !Array.isArray(r.configEntries))\n\n    if (invalidConfigs) {\n      const { configEntries } = invalidConfigs\n      throw new KafkaJSNonRetriableError(\n        `Invalid resource configEntries ${configEntries}: ${JSON.stringify(invalidConfigs)}`\n      )\n    }\n\n    const invalidConfigValue = resources.find(r =>\n      r.configEntries.some(e => typeof e.name !== 'string' || typeof e.value !== 'string')\n    )\n\n    if (invalidConfigValue) {\n      throw new KafkaJSNonRetriableError(\n        `Invalid resource config value: ${JSON.stringify(invalidConfigValue)}`\n      )\n    }\n\n    const retrier = createRetry(retry)\n\n    return retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await cluster.refreshMetadata()\n        const controller = await cluster.findControllerBroker()\n        const resourcerByBroker = await groupResourcesByBroker({\n          resources,\n          defaultBroker: controller,\n        })\n\n        const alterConfigsAction = async broker => {\n          const targetBroker = broker || controller\n          return targetBroker.alterConfigs({\n            resources: resourcerByBroker.get(targetBroker),\n            validateOnly: !!validateOnly,\n          })\n        }\n\n        const brokers = Array.from(resourcerByBroker.keys())\n        const responses = await Promise.all(brokers.map(alterConfigsAction))\n        const responseResources = responses.reduce(\n          (result, { resources }) => [...result, ...resources],\n          []\n        )\n\n        return { resources: responseResources }\n      } catch (e) {\n        if (e.type === 'NOT_CONTROLLER') {\n          logger.warn('Could not alter configs', { error: e.message, retryCount, retryTime })\n          throw e\n        }\n\n        bail(e)\n      }\n    })\n  }\n\n  /**\n   * Fetch metadata for provided topics.\n   *\n   * If no topics are provided fetch metadata for all topics.\n   * @see https://kafka.apache.org/protocol#The_Messages_Metadata\n   *\n   * @param {Object} [options]\n   * @param {string[]} [options.topics]\n   * @return {Promise<TopicsMetadata>}\n   *\n   * @typedef {Object} TopicsMetadata\n   * @property {Array<TopicMetadata>} topics\n   *\n   * @typedef {Object} TopicMetadata\n   * @property {String} name\n   * @property {Array<PartitionMetadata>} partitions\n   *\n   * @typedef {Object} PartitionMetadata\n   * @property {number} partitionErrorCode Response error code\n   * @property {number} partitionId Topic partition id\n   * @property {number} leader  The id of the broker acting as leader for this partition.\n   * @property {Array<number>} replicas The set of all nodes that host this partition.\n   * @property {Array<number>} isr The set of nodes that are in sync with the leader for this partition.\n   */\n  const fetchTopicMetadata = async ({ topics = [] } = {}) => {\n    if (topics) {\n      topics.forEach(topic => {\n        if (!topic || typeof topic !== 'string') {\n          throw new KafkaJSNonRetriableError(`Invalid topic ${topic}`)\n        }\n      })\n    }\n\n    const metadata = await cluster.metadata({ topics })\n\n    return {\n      topics: metadata.topicMetadata.map(topicMetadata => ({\n        name: topicMetadata.topic,\n        partitions: topicMetadata.partitionMetadata,\n      })),\n    }\n  }\n\n  /**\n   * Describe cluster\n   *\n   * @return {Promise<ClusterMetadata>}\n   *\n   * @typedef {Object} ClusterMetadata\n   * @property {Array<Broker>} brokers\n   * @property {Number} controller Current controller id. Returns null if unknown.\n   * @property {String} clusterId\n   *\n   * @typedef {Object} Broker\n   * @property {Number} nodeId\n   * @property {String} host\n   * @property {Number} port\n   */\n  const describeCluster = async () => {\n    const { brokers: nodes, clusterId, controllerId } = await cluster.metadata({ topics: [] })\n    const brokers = nodes.map(({ nodeId, host, port }) => ({\n      nodeId,\n      host,\n      port,\n    }))\n    const controller =\n      controllerId == null || controllerId === NO_CONTROLLER_ID ? null : controllerId\n\n    return {\n      brokers,\n      controller,\n      clusterId,\n    }\n  }\n\n  /**\n   * List groups in a broker\n   *\n   * @return {Promise<ListGroups>}\n   *\n   * @typedef {Object} ListGroups\n   * @property {Array<ListGroup>} groups\n   *\n   * @typedef {Object} ListGroup\n   * @property {string} groupId\n   * @property {string} protocolType\n   */\n  const listGroups = async () => {\n    await cluster.refreshMetadata()\n    let groups = []\n    for (var nodeId in cluster.brokerPool.brokers) {\n      const broker = await cluster.findBroker({ nodeId })\n      const response = await broker.listGroups()\n      groups = groups.concat(response.groups)\n    }\n\n    return { groups }\n  }\n\n  /**\n   * Describe groups by group ids\n   * @param {Array<string>} groupIds\n   *\n   * @typedef {Object} GroupDescriptions\n   * @property {Array<GroupDescription>} groups\n   *\n   * @return {Promise<GroupDescriptions>}\n   */\n  const describeGroups = async groupIds => {\n    const coordinatorsForGroup = await Promise.all(\n      groupIds.map(async groupId => {\n        const coordinator = await cluster.findGroupCoordinator({ groupId })\n        return {\n          coordinator,\n          groupId,\n        }\n      })\n    )\n\n    const groupsByCoordinator = Object.values(\n      coordinatorsForGroup.reduce((coordinators, { coordinator, groupId }) => {\n        const group = coordinators[coordinator.nodeId]\n\n        if (group) {\n          coordinators[coordinator.nodeId] = {\n            ...group,\n            groupIds: [...group.groupIds, groupId],\n          }\n        } else {\n          coordinators[coordinator.nodeId] = { coordinator, groupIds: [groupId] }\n        }\n        return coordinators\n      }, {})\n    )\n\n    const responses = await Promise.all(\n      groupsByCoordinator.map(async ({ coordinator, groupIds }) => {\n        const retrier = createRetry(retry)\n        const { groups } = await retrier(() => coordinator.describeGroups({ groupIds }))\n        return groups\n      })\n    )\n\n    const groups = [].concat.apply([], responses)\n\n    return { groups }\n  }\n\n  /**\n   * Delete groups in a broker\n   *\n   * @param {string[]} [groupIds]\n   * @return {Promise<DeleteGroups>}\n   *\n   * @typedef {Array} DeleteGroups\n   * @property {string} groupId\n   * @property {number} errorCode\n   */\n  const deleteGroups = async groupIds => {\n    if (!groupIds || !Array.isArray(groupIds)) {\n      throw new KafkaJSNonRetriableError(`Invalid groupIds array ${groupIds}`)\n    }\n\n    const invalidGroupId = groupIds.some(g => typeof g !== 'string')\n\n    if (invalidGroupId) {\n      throw new KafkaJSNonRetriableError(`Invalid groupId name: ${JSON.stringify(invalidGroupId)}`)\n    }\n\n    const retrier = createRetry(retry)\n\n    let results = []\n\n    let clonedGroupIds = groupIds.slice()\n\n    return retrier(async (bail, retryCount, retryTime) => {\n      try {\n        if (clonedGroupIds.length === 0) return []\n\n        await cluster.refreshMetadata()\n\n        const brokersPerGroups = {}\n        const brokersPerNode = {}\n        for (const groupId of clonedGroupIds) {\n          const broker = await cluster.findGroupCoordinator({ groupId })\n          if (brokersPerGroups[broker.nodeId] === undefined) brokersPerGroups[broker.nodeId] = []\n          brokersPerGroups[broker.nodeId].push(groupId)\n          brokersPerNode[broker.nodeId] = broker\n        }\n\n        const res = await Promise.all(\n          Object.keys(brokersPerNode).map(\n            async nodeId => await brokersPerNode[nodeId].deleteGroups(brokersPerGroups[nodeId])\n          )\n        )\n\n        const errors = res\n          .flatMap(({ results }) =>\n            results.map(({ groupId, errorCode, error }) => {\n              return { groupId, errorCode, error }\n            })\n          )\n          .filter(({ errorCode }) => errorCode !== 0)\n\n        clonedGroupIds = errors.map(({ groupId }) => groupId)\n\n        if (errors.length > 0) throw new KafkaJSDeleteGroupsError('Error in DeleteGroups', errors)\n\n        results = res.flatMap(({ results }) => results)\n\n        return results\n      } catch (e) {\n        if (e.type === 'NOT_CONTROLLER' || e.type === 'COORDINATOR_NOT_AVAILABLE') {\n          logger.warn('Could not delete groups', { error: e.message, retryCount, retryTime })\n          throw e\n        }\n\n        bail(e)\n      }\n    })\n  }\n\n  /**\n   * Delete topic records up to the selected partition offsets\n   *\n   * @param {string} topic\n   * @param {Array<SeekEntry>} partitions\n   * @return {Promise}\n   *\n   * @typedef {Object} SeekEntry\n   * @property {number} partition\n   * @property {string} offset\n   */\n  const deleteTopicRecords = async ({ topic, partitions }) => {\n    if (!topic || typeof topic !== 'string') {\n      throw new KafkaJSNonRetriableError(`Invalid topic \"${topic}\"`)\n    }\n\n    if (!partitions || partitions.length === 0) {\n      throw new KafkaJSNonRetriableError(`Invalid partitions`)\n    }\n\n    const partitionsByBroker = cluster.findLeaderForPartitions(\n      topic,\n      partitions.map(p => p.partition)\n    )\n\n    const partitionsFound = values(partitionsByBroker).flat()\n    const topicOffsets = await fetchTopicOffsets(topic)\n\n    const leaderNotFoundErrors = []\n    partitions.forEach(({ partition, offset }) => {\n      // throw if no leader found for partition\n      if (!partitionsFound.includes(partition)) {\n        leaderNotFoundErrors.push({\n          partition,\n          offset,\n          error: new KafkaJSBrokerNotFound('Could not find the leader for the partition', {\n            retriable: false,\n          }),\n        })\n        return\n      }\n      const { low } = topicOffsets.find(p => p.partition === partition) || {\n        high: undefined,\n        low: undefined,\n      }\n      // warn in case of offset below low watermark\n      if (parseInt(offset) < parseInt(low) && parseInt(offset) !== -1) {\n        logger.warn(\n          'The requested offset is before the earliest offset maintained on the partition - no records will be deleted from this partition',\n          {\n            topic,\n            partition,\n            offset,\n          }\n        )\n      }\n    })\n\n    if (leaderNotFoundErrors.length > 0) {\n      throw new KafkaJSDeleteTopicRecordsError({ topic, partitions: leaderNotFoundErrors })\n    }\n\n    const seekEntriesByBroker = entries(partitionsByBroker).reduce(\n      (obj, [nodeId, nodePartitions]) => {\n        obj[nodeId] = {\n          topic,\n          partitions: partitions.filter(p => nodePartitions.includes(p.partition)),\n        }\n        return obj\n      },\n      {}\n    )\n\n    const retrier = createRetry(retry)\n    return retrier(async bail => {\n      try {\n        const partitionErrors = []\n\n        const brokerRequests = entries(seekEntriesByBroker).map(\n          ([nodeId, { topic, partitions }]) => async () => {\n            const broker = await cluster.findBroker({ nodeId })\n            await broker.deleteRecords({ topics: [{ topic, partitions }] })\n            // remove successful entry so it's ignored on retry\n            delete seekEntriesByBroker[nodeId]\n          }\n        )\n\n        await Promise.all(\n          brokerRequests.map(request =>\n            request().catch(e => {\n              if (e.name === 'KafkaJSDeleteTopicRecordsError') {\n                e.partitions.forEach(({ partition, offset, error }) => {\n                  partitionErrors.push({\n                    partition,\n                    offset,\n                    error,\n                  })\n                })\n              } else {\n                // then it's an unknown error, not from the broker response\n                throw e\n              }\n            })\n          )\n        )\n\n        if (partitionErrors.length > 0) {\n          throw new KafkaJSDeleteTopicRecordsError({\n            topic,\n            partitions: partitionErrors,\n          })\n        }\n      } catch (e) {\n        if (\n          e.retriable &&\n          e.partitions.some(\n            ({ error }) => staleMetadata(error) || error.name === 'KafkaJSMetadataNotLoaded'\n          )\n        ) {\n          await cluster.refreshMetadata()\n        }\n        throw e\n      }\n    })\n  }\n\n  /**\n   * @param {Array<ACLEntry>} acl\n   * @return {Promise<void>}\n   *\n   * @typedef {Object} ACLEntry\n   */\n  const createAcls = async ({ acl }) => {\n    if (!acl || !Array.isArray(acl)) {\n      throw new KafkaJSNonRetriableError(`Invalid ACL array ${acl}`)\n    }\n    if (acl.length === 0) {\n      throw new KafkaJSNonRetriableError('Empty ACL array')\n    }\n\n    // Validate principal\n    if (acl.some(({ principal }) => typeof principal !== 'string')) {\n      throw new KafkaJSNonRetriableError(\n        'Invalid ACL array, the principals have to be a valid string'\n      )\n    }\n\n    // Validate host\n    if (acl.some(({ host }) => typeof host !== 'string')) {\n      throw new KafkaJSNonRetriableError('Invalid ACL array, the hosts have to be a valid string')\n    }\n\n    // Validate resourceName\n    if (acl.some(({ resourceName }) => typeof resourceName !== 'string')) {\n      throw new KafkaJSNonRetriableError(\n        'Invalid ACL array, the resourceNames have to be a valid string'\n      )\n    }\n\n    let invalidType\n    // Validate operation\n    const validOperationTypes = Object.values(ACL_OPERATION_TYPES)\n    invalidType = acl.find(i => !validOperationTypes.includes(i.operation))\n\n    if (invalidType) {\n      throw new KafkaJSNonRetriableError(\n        `Invalid operation type ${invalidType.operation}: ${JSON.stringify(invalidType)}`\n      )\n    }\n\n    // Validate resourcePatternTypes\n    const validResourcePatternTypes = Object.values(RESOURCE_PATTERN_TYPES)\n    invalidType = acl.find(i => !validResourcePatternTypes.includes(i.resourcePatternType))\n\n    if (invalidType) {\n      throw new KafkaJSNonRetriableError(\n        `Invalid resource pattern type ${invalidType.resourcePatternType}: ${JSON.stringify(\n          invalidType\n        )}`\n      )\n    }\n\n    // Validate permissionTypes\n    const validPermissionTypes = Object.values(ACL_PERMISSION_TYPES)\n    invalidType = acl.find(i => !validPermissionTypes.includes(i.permissionType))\n\n    if (invalidType) {\n      throw new KafkaJSNonRetriableError(\n        `Invalid permission type ${invalidType.permissionType}: ${JSON.stringify(invalidType)}`\n      )\n    }\n\n    // Validate resourceTypes\n    const validResourceTypes = Object.values(ACL_RESOURCE_TYPES)\n    invalidType = acl.find(i => !validResourceTypes.includes(i.resourceType))\n\n    if (invalidType) {\n      throw new KafkaJSNonRetriableError(\n        `Invalid resource type ${invalidType.resourceType}: ${JSON.stringify(invalidType)}`\n      )\n    }\n\n    const retrier = createRetry(retry)\n\n    return retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await cluster.refreshMetadata()\n        const broker = await cluster.findControllerBroker()\n        await broker.createAcls({ acl })\n\n        return true\n      } catch (e) {\n        if (e.type === 'NOT_CONTROLLER') {\n          logger.warn('Could not create ACL', { error: e.message, retryCount, retryTime })\n          throw e\n        }\n\n        bail(e)\n      }\n    })\n  }\n\n  /**\n   * @param {ACLResourceTypes} resourceType The type of resource\n   * @param {string} resourceName The name of the resource\n   * @param {ACLResourcePatternTypes} resourcePatternType The resource pattern type filter\n   * @param {string} principal The principal name\n   * @param {string} host The hostname\n   * @param {ACLOperationTypes} operation The type of operation\n   * @param {ACLPermissionTypes} permissionType The type of permission\n   * @return {Promise<void>}\n   *\n   * @typedef {number} ACLResourceTypes\n   * @typedef {number} ACLResourcePatternTypes\n   * @typedef {number} ACLOperationTypes\n   * @typedef {number} ACLPermissionTypes\n   */\n  const describeAcls = async ({\n    resourceType,\n    resourceName,\n    resourcePatternType,\n    principal,\n    host,\n    operation,\n    permissionType,\n  }) => {\n    // Validate principal\n    if (typeof principal !== 'string' && typeof principal !== 'undefined') {\n      throw new KafkaJSNonRetriableError(\n        'Invalid principal, the principal have to be a valid string'\n      )\n    }\n\n    // Validate host\n    if (typeof host !== 'string' && typeof host !== 'undefined') {\n      throw new KafkaJSNonRetriableError('Invalid host, the host have to be a valid string')\n    }\n\n    // Validate resourceName\n    if (typeof resourceName !== 'string' && typeof resourceName !== 'undefined') {\n      throw new KafkaJSNonRetriableError(\n        'Invalid resourceName, the resourceName have to be a valid string'\n      )\n    }\n\n    // Validate operation\n    const validOperationTypes = Object.values(ACL_OPERATION_TYPES)\n    if (!validOperationTypes.includes(operation)) {\n      throw new KafkaJSNonRetriableError(`Invalid operation type ${operation}`)\n    }\n\n    // Validate resourcePatternType\n    const validResourcePatternTypes = Object.values(RESOURCE_PATTERN_TYPES)\n    if (!validResourcePatternTypes.includes(resourcePatternType)) {\n      throw new KafkaJSNonRetriableError(\n        `Invalid resource pattern filter type ${resourcePatternType}`\n      )\n    }\n\n    // Validate permissionType\n    const validPermissionTypes = Object.values(ACL_PERMISSION_TYPES)\n    if (!validPermissionTypes.includes(permissionType)) {\n      throw new KafkaJSNonRetriableError(`Invalid permission type ${permissionType}`)\n    }\n\n    // Validate resourceType\n    const validResourceTypes = Object.values(ACL_RESOURCE_TYPES)\n    if (!validResourceTypes.includes(resourceType)) {\n      throw new KafkaJSNonRetriableError(`Invalid resource type ${resourceType}`)\n    }\n\n    const retrier = createRetry(retry)\n\n    return retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await cluster.refreshMetadata()\n        const broker = await cluster.findControllerBroker()\n        const { resources } = await broker.describeAcls({\n          resourceType,\n          resourceName,\n          resourcePatternType,\n          principal,\n          host,\n          operation,\n          permissionType,\n        })\n        return { resources }\n      } catch (e) {\n        if (e.type === 'NOT_CONTROLLER') {\n          logger.warn('Could not describe ACL', { error: e.message, retryCount, retryTime })\n          throw e\n        }\n\n        bail(e)\n      }\n    })\n  }\n\n  /**\n   * @param {Array<ACLFilter>} filters\n   * @return {Promise<void>}\n   *\n   * @typedef {Object} ACLFilter\n   */\n  const deleteAcls = async ({ filters }) => {\n    if (!filters || !Array.isArray(filters)) {\n      throw new KafkaJSNonRetriableError(`Invalid ACL Filter array ${filters}`)\n    }\n\n    if (filters.length === 0) {\n      throw new KafkaJSNonRetriableError('Empty ACL Filter array')\n    }\n\n    // Validate principal\n    if (\n      filters.some(\n        ({ principal }) => typeof principal !== 'string' && typeof principal !== 'undefined'\n      )\n    ) {\n      throw new KafkaJSNonRetriableError(\n        'Invalid ACL Filter array, the principals have to be a valid string'\n      )\n    }\n\n    // Validate host\n    if (filters.some(({ host }) => typeof host !== 'string' && typeof host !== 'undefined')) {\n      throw new KafkaJSNonRetriableError(\n        'Invalid ACL Filter array, the hosts have to be a valid string'\n      )\n    }\n\n    // Validate resourceName\n    if (\n      filters.some(\n        ({ resourceName }) =>\n          typeof resourceName !== 'string' && typeof resourceName !== 'undefined'\n      )\n    ) {\n      throw new KafkaJSNonRetriableError(\n        'Invalid ACL Filter array, the resourceNames have to be a valid string'\n      )\n    }\n\n    let invalidType\n    // Validate operation\n    const validOperationTypes = Object.values(ACL_OPERATION_TYPES)\n    invalidType = filters.find(i => !validOperationTypes.includes(i.operation))\n\n    if (invalidType) {\n      throw new KafkaJSNonRetriableError(\n        `Invalid operation type ${invalidType.operation}: ${JSON.stringify(invalidType)}`\n      )\n    }\n\n    // Validate resourcePatternTypes\n    const validResourcePatternTypes = Object.values(RESOURCE_PATTERN_TYPES)\n    invalidType = filters.find(i => !validResourcePatternTypes.includes(i.resourcePatternType))\n\n    if (invalidType) {\n      throw new KafkaJSNonRetriableError(\n        `Invalid resource pattern type ${invalidType.resourcePatternType}: ${JSON.stringify(\n          invalidType\n        )}`\n      )\n    }\n\n    // Validate permissionTypes\n    const validPermissionTypes = Object.values(ACL_PERMISSION_TYPES)\n    invalidType = filters.find(i => !validPermissionTypes.includes(i.permissionType))\n\n    if (invalidType) {\n      throw new KafkaJSNonRetriableError(\n        `Invalid permission type ${invalidType.permissionType}: ${JSON.stringify(invalidType)}`\n      )\n    }\n\n    // Validate resourceTypes\n    const validResourceTypes = Object.values(ACL_RESOURCE_TYPES)\n    invalidType = filters.find(i => !validResourceTypes.includes(i.resourceType))\n\n    if (invalidType) {\n      throw new KafkaJSNonRetriableError(\n        `Invalid resource type ${invalidType.resourceType}: ${JSON.stringify(invalidType)}`\n      )\n    }\n\n    const retrier = createRetry(retry)\n\n    return retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await cluster.refreshMetadata()\n        const broker = await cluster.findControllerBroker()\n        const { filterResponses } = await broker.deleteAcls({ filters })\n        return { filterResponses }\n      } catch (e) {\n        if (e.type === 'NOT_CONTROLLER') {\n          logger.warn('Could not delete ACL', { error: e.message, retryCount, retryTime })\n          throw e\n        }\n\n        bail(e)\n      }\n    })\n  }\n\n  /**\n   * Alter the replicas partitions are assigned to for a topic\n   * @param {Object} request\n   * @param {import(\"../../types\").IPartitionReassignment[]} request.topics topics and the paritions to be reassigned\n   * @param {number} [request.timeout]\n   * @returns {Promise}\n   */\n  const alterPartitionReassignments = async ({ topics, timeout }) => {\n    if (!topics || !Array.isArray(topics)) {\n      throw new KafkaJSNonRetriableError(`Invalid topics array ${topics}`)\n    }\n\n    if (topics.filter(({ topic }) => typeof topic !== 'string').length > 0) {\n      throw new KafkaJSNonRetriableError(\n        'Invalid topics array, the topic names have to be a valid string'\n      )\n    }\n\n    const topicNames = new Set(topics.map(({ topic }) => topic))\n    if (topicNames.size < topics.length) {\n      throw new KafkaJSNonRetriableError(\n        'Invalid topics array, it cannot have multiple entries for the same topic'\n      )\n    }\n\n    for (const { topic, partitionAssignment } of topics) {\n      if (!partitionAssignment || !Array.isArray(partitionAssignment)) {\n        throw new KafkaJSNonRetriableError(\n          `Invalid partitions array: ${partitionAssignment} for topic: ${topic}`\n        )\n      }\n\n      for (const { partition, replicas } of partitionAssignment) {\n        if (\n          partition === null ||\n          partition === undefined ||\n          typeof partition !== 'number' ||\n          partition < 0\n        ) {\n          throw new KafkaJSNonRetriableError(\n            `Invalid partitions index: ${partition} for topic: ${topic}`\n          )\n        }\n\n        if (!replicas || !Array.isArray(replicas)) {\n          throw new KafkaJSNonRetriableError(\n            `Invalid replica assignment: ${replicas} for topic: ${topic} on partition: ${partition}`\n          )\n        }\n\n        if (replicas.filter(replica => typeof replica !== 'number' || replica < 0).length >= 1) {\n          throw new KafkaJSNonRetriableError(\n            `Invalid replica assignment: ${replicas} for topic: ${topic} on partition: ${partition}. Replicas must be a non negative number`\n          )\n        }\n      }\n    }\n\n    const retrier = createRetry(retry)\n\n    return retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await cluster.refreshMetadata()\n        const broker = await cluster.findControllerBroker()\n        await broker.alterPartitionReassignments({ topics, timeout })\n      } catch (e) {\n        if (e.type === 'NOT_CONTROLLER') {\n          logger.warn('Could not reassign partitions', { error: e.message, retryCount, retryTime })\n          throw e\n        }\n\n        bail(e)\n      }\n    })\n  }\n\n  /**\n   * List the partition reassignments in progress.\n   * If a partition is not going through a reassignment, its AddingReplicas and RemovingReplicas fields will simply be empty.\n   * If a partition doesn't exist, no response will be returned for it.\n   * @param {Object} request\n   * @param {import(\"../../types\").TopicPartitions[]} request.topics topics and the paritions to be returned, if this is null will return all the topics.\n   * @param {number} [request.timeout]\n   * @returns {Promise<import(\"../../types\").ListPartitionReassignmentsResponse>}\n   */\n  const listPartitionReassignments = async ({ topics = null, timeout }) => {\n    if (topics) {\n      if (!Array.isArray(topics)) {\n        throw new KafkaJSNonRetriableError(`Invalid topics array ${topics}`)\n      }\n\n      if (topics.filter(({ topic }) => typeof topic !== 'string').length > 0) {\n        throw new KafkaJSNonRetriableError(\n          'Invalid topics array, the topic names have to be a valid string'\n        )\n      }\n\n      const topicNames = new Set(topics.map(({ topic }) => topic))\n      if (topicNames.size < topics.length) {\n        throw new KafkaJSNonRetriableError(\n          'Invalid topics array, it cannot have multiple entries for the same topic'\n        )\n      }\n\n      for (const { topic, partitions } of topics) {\n        if (!partitions || !Array.isArray(partitions)) {\n          throw new KafkaJSNonRetriableError(\n            `Invalid partition array: ${partitions} for topic: ${topic}`\n          )\n        }\n\n        if (\n          partitions.filter(partition => typeof partition !== 'number' || partition < 0).length >= 1\n        ) {\n          throw new KafkaJSNonRetriableError(\n            `Invalid partition array: ${partitions} for topic: ${topic}. The partition indices have to be a valid number greater than 0.`\n          )\n        }\n      }\n    }\n\n    const retrier = createRetry(retry)\n\n    return retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await cluster.refreshMetadata()\n        const broker = await cluster.findControllerBroker()\n        const response = await broker.listPartitionReassignments({ topics, timeout })\n\n        return { topics: response.topics }\n      } catch (e) {\n        if (e.type === 'NOT_CONTROLLER') {\n          logger.warn('Could not reassign partitions', { error: e.message, retryCount, retryTime })\n          throw e\n        }\n\n        bail(e)\n      }\n    })\n  }\n\n  /** @type {import(\"../../types\").Admin[\"on\"]} */\n  const on = (eventName, listener) => {\n    if (!eventNames.includes(eventName)) {\n      throw new KafkaJSNonRetriableError(`Event name should be one of ${eventKeys}`)\n    }\n\n    return instrumentationEmitter.addListener(unwrapEvent(eventName), event => {\n      event.type = wrapEvent(event.type)\n      Promise.resolve(listener(event)).catch(e => {\n        logger.error(`Failed to execute listener: ${e.message}`, {\n          eventName,\n          stack: e.stack,\n        })\n      })\n    })\n  }\n\n  /**\n   * @return {Object} logger\n   */\n  const getLogger = () => logger\n\n  return {\n    connect,\n    disconnect,\n    listTopics,\n    createTopics,\n    deleteTopics,\n    createPartitions,\n    fetchTopicMetadata,\n    describeCluster,\n    events,\n    fetchOffsets,\n    fetchTopicOffsets,\n    fetchTopicOffsetsByTimestamp,\n    setOffsets,\n    resetOffsets,\n    describeConfigs,\n    alterConfigs,\n    on,\n    logger: getLogger,\n    listGroups,\n    describeGroups,\n    deleteGroups,\n    describeAcls,\n    deleteAcls,\n    createAcls,\n    deleteTopicRecords,\n    alterPartitionReassignments,\n    listPartitionReassignments,\n  }\n}\n", "const KEEP_ALIVE_DELAY = 60000 // in ms\n\n/**\n * @returns {import(\"../../types\").ISocketFactory}\n */\nmodule.exports = () => {\n  const net = require('net')\n  const tls = require('tls')\n\n  return ({ host, port, ssl, onConnect }) => {\n    const socket = ssl\n      ? tls.connect(\n          Object.assign({ host, port }, !net.isIP(host) ? { servername: host } : {}, ssl),\n          onConnect\n        )\n      : net.connect({ host, port }, onConnect)\n\n    socket.setKeepAlive(true, KEEP_ALIVE_DELAY)\n\n    return socket\n  }\n}\n", "module.exports = fn => {\n  let called = false\n\n  return (...args) => {\n    if (!called) {\n      called = true\n      return fn(...args)\n    }\n  }\n}\n", "const {\n  createLogger,\n  LEVELS: { INFO },\n} = require('./loggers')\n\nconst InstrumentationEventEmitter = require('./instrumentation/emitter')\nconst LoggerConsole = require('./loggers/console')\nconst Cluster = require('./cluster')\nconst createProducer = require('./producer')\nconst createConsumer = require('./consumer')\nconst createAdmin = require('./admin')\nconst ISOLATION_LEVEL = require('./protocol/isolationLevel')\nconst defaultSocketFactory = require('./network/socketFactory')\nconst once = require('./utils/once')\nconst websiteUrl = require('./utils/websiteUrl')\n\nconst PRIVATE = {\n  CREATE_CLUSTER: Symbol('private:Kafka:createCluster'),\n  CLUSTER_RETRY: Symbol('private:Kafka:clusterRetry'),\n  LOGGER: Symbol('private:Kafka:logger'),\n  OFFSETS: Symbol('private:Kafka:offsets'),\n}\n\nconst DEFAULT_METADATA_MAX_AGE = 300000\nconst warnOfDefaultPartitioner = once(logger => {\n  if (process.env.KAFKAJS_NO_PARTITIONER_WARNING == null) {\n    logger.warn(\n      `KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at ${websiteUrl(\n        'docs/migration-guide-v2.0.0',\n        'producer-new-default-partitioner'\n      )} for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\"`\n    )\n  }\n})\n\nmodule.exports = class Client {\n  /**\n   * @param {Object} options\n   * @param {Array<string>} options.brokers example: ['127.0.0.1:9092', '127.0.0.1:9094']\n   * @param {Object} options.ssl\n   * @param {Object} options.sasl\n   * @param {string} options.clientId\n   * @param {number} [options.connectionTimeout=1000] - in milliseconds\n   * @param {number} options.authenticationTimeout - in milliseconds\n   * @param {number} options.reauthenticationThreshold - in milliseconds\n   * @param {number} [options.requestTimeout=30000] - in milliseconds\n   * @param {boolean} [options.enforceRequestTimeout]\n   * @param {import(\"../types\").RetryOptions} [options.retry]\n   * @param {import(\"../types\").ISocketFactory} [options.socketFactory]\n   */\n  constructor({\n    brokers,\n    ssl,\n    sasl,\n    clientId,\n    connectionTimeout = 1000,\n    authenticationTimeout,\n    reauthenticationThreshold,\n    requestTimeout,\n    enforceRequestTimeout = true,\n    retry,\n    socketFactory = defaultSocketFactory(),\n    logLevel = INFO,\n    logCreator = LoggerConsole,\n  }) {\n    this[PRIVATE.OFFSETS] = new Map()\n    this[PRIVATE.LOGGER] = createLogger({ level: logLevel, logCreator })\n    this[PRIVATE.CLUSTER_RETRY] = retry\n    this[PRIVATE.CREATE_CLUSTER] = ({\n      metadataMaxAge,\n      allowAutoTopicCreation = true,\n      maxInFlightRequests = null,\n      instrumentationEmitter = null,\n      isolationLevel,\n    }) =>\n      new Cluster({\n        logger: this[PRIVATE.LOGGER],\n        retry: this[PRIVATE.CLUSTER_RETRY],\n        offsets: this[PRIVATE.OFFSETS],\n        socketFactory,\n        brokers,\n        ssl,\n        sasl,\n        clientId,\n        connectionTimeout,\n        authenticationTimeout,\n        reauthenticationThreshold,\n        requestTimeout,\n        enforceRequestTimeout,\n        metadataMaxAge,\n        instrumentationEmitter,\n        allowAutoTopicCreation,\n        maxInFlightRequests,\n        isolationLevel,\n      })\n  }\n\n  /**\n   * @public\n   */\n  producer({\n    createPartitioner,\n    retry,\n    metadataMaxAge = DEFAULT_METADATA_MAX_AGE,\n    allowAutoTopicCreation,\n    idempotent,\n    transactionalId,\n    transactionTimeout,\n    maxInFlightRequests,\n  } = {}) {\n    const instrumentationEmitter = new InstrumentationEventEmitter()\n    const cluster = this[PRIVATE.CREATE_CLUSTER]({\n      metadataMaxAge,\n      allowAutoTopicCreation,\n      maxInFlightRequests,\n      instrumentationEmitter,\n    })\n\n    if (createPartitioner == null) {\n      warnOfDefaultPartitioner(this[PRIVATE.LOGGER])\n    }\n\n    return createProducer({\n      retry: { ...this[PRIVATE.CLUSTER_RETRY], ...retry },\n      logger: this[PRIVATE.LOGGER],\n      cluster,\n      createPartitioner,\n      idempotent,\n      transactionalId,\n      transactionTimeout,\n      instrumentationEmitter,\n    })\n  }\n\n  /**\n   * @public\n   */\n  consumer({\n    groupId,\n    partitionAssigners,\n    metadataMaxAge = DEFAULT_METADATA_MAX_AGE,\n    sessionTimeout,\n    rebalanceTimeout,\n    heartbeatInterval,\n    maxBytesPerPartition,\n    minBytes,\n    maxBytes,\n    maxWaitTimeInMs,\n    retry = { retries: 5 },\n    allowAutoTopicCreation,\n    maxInFlightRequests,\n    readUncommitted = false,\n    rackId = '',\n  } = {}) {\n    const isolationLevel = readUncommitted\n      ? ISOLATION_LEVEL.READ_UNCOMMITTED\n      : ISOLATION_LEVEL.READ_COMMITTED\n\n    const instrumentationEmitter = new InstrumentationEventEmitter()\n    const cluster = this[PRIVATE.CREATE_CLUSTER]({\n      metadataMaxAge,\n      allowAutoTopicCreation,\n      maxInFlightRequests,\n      isolationLevel,\n      instrumentationEmitter,\n    })\n\n    return createConsumer({\n      retry: { ...this[PRIVATE.CLUSTER_RETRY], ...retry },\n      logger: this[PRIVATE.LOGGER],\n      cluster,\n      groupId,\n      partitionAssigners,\n      sessionTimeout,\n      rebalanceTimeout,\n      heartbeatInterval,\n      maxBytesPerPartition,\n      minBytes,\n      maxBytes,\n      maxWaitTimeInMs,\n      isolationLevel,\n      instrumentationEmitter,\n      rackId,\n      metadataMaxAge,\n    })\n  }\n\n  /**\n   * @public\n   */\n  admin({ retry } = {}) {\n    const instrumentationEmitter = new InstrumentationEventEmitter()\n    const cluster = this[PRIVATE.CREATE_CLUSTER]({\n      allowAutoTopicCreation: false,\n      instrumentationEmitter,\n    })\n\n    return createAdmin({\n      retry: { ...this[PRIVATE.CLUSTER_RETRY], ...retry },\n      logger: this[PRIVATE.LOGGER],\n      instrumentationEmitter,\n      cluster,\n    })\n  }\n\n  /**\n   * @public\n   */\n  logger() {\n    return this[PRIVATE.LOGGER]\n  }\n}\n", "const Kafka = require('./src')\nconst PartitionAssigners = require('./src/consumer/assigners')\nconst AssignerProtocol = require('./src/consumer/assignerProtocol')\nconst Partitioners = require('./src/producer/partitioners')\nconst Compression = require('./src/protocol/message/compression')\nconst ConfigResourceTypes = require('./src/protocol/configResourceTypes')\nconst ConfigSource = require('./src/protocol/configSource')\nconst AclResourceTypes = require('./src/protocol/aclResourceTypes')\nconst AclOperationTypes = require('./src/protocol/aclOperationTypes')\nconst AclPermissionTypes = require('./src/protocol/aclPermissionTypes')\nconst ResourcePatternTypes = require('./src/protocol/resourcePatternTypes')\nconst { isRebalancing, isKafkaJSError, ...errors } = require('./src/errors')\nconst { LEVELS } = require('./src/loggers')\n\nmodule.exports = {\n  Kafka,\n  PartitionAssigners,\n  AssignerProtocol,\n  Partitioners,\n  logLevel: LEVELS,\n  CompressionTypes: Compression.Types,\n  CompressionCodecs: Compression.Codecs,\n  ConfigResourceTypes,\n  AclResourceTypes,\n  AclOperationTypes,\n  AclPermissionTypes,\n  ResourcePatternTypes,\n  ConfigSource,\n  ...errors,\n}\n", "/* eslint-disable no-console */\nconst { ConfigResourceTypes, Kafka } = require(\"kafkajs\");\n\n/**\n * Removes topics in BigMac given the following\n * @param {*} brokerString - Comma delimited list of brokers\n * @param {*} namespace - String in the format of `--${event.project}--`, only used for temp branches for easy identification and cleanup\n */\nexports.listTopics = async function (brokerString, namespace) {\n  const brokers = brokerString.split(\",\");\n\n  const kafka = new Kafka({\n    clientId: \"admin\",\n    brokers: brokers,\n    ssl: true,\n  });\n  var admin = kafka.admin();\n\n  await admin.connect();\n\n  const currentTopics = await admin.listTopics();\n  var lingeringTopics = currentTopics.filter(\n    (topic) =>\n      topic.startsWith(namespace) ||\n      topic.startsWith(`_confluent-ksql-${namespace}`)\n  );\n\n  await admin.disconnect();\n  return lingeringTopics;\n};\n\n/**\n * Generates topics in BigMac given the following\n * @param { string[] } brokers - List of brokers\n * @param {{ topic: string, numPartitions: number, replicationFactor: number }[]}\n *   desiredTopicConfigs - array of topics to create or update.\n *   The `topic` property should include any namespace.\n */\nexports.createTopics = async function (brokers, desiredTopicConfigs) {\n  const kafka = new Kafka({\n    clientId: \"admin\",\n    brokers,\n    ssl: true,\n  });\n  var admin = kafka.admin();\n  await admin.connect();\n\n  // Fetch topic names from MSK, filtering out __ internal management topic\n  const listTopicResponse = await admin.listTopics();\n  const existingTopicNames = listTopicResponse.filter(\n    (name) => !name.startsWith(\"_\")\n  );\n\n  console.log(\"Existing topics:\", JSON.stringify(existingTopicNames, null, 2));\n\n  // Fetch the metadata for those topics from MSK\n  const fetchTopicResponse = await admin.fetchTopicMetadata({\n    topics: existingTopicNames,\n  });\n  const existingTopicConfigs = fetchTopicResponse.topics;\n  console.log(\n    \"Topics Metadata:\",\n    JSON.stringify(existingTopicConfigs, null, 2)\n  );\n\n  // Any desired topics whose names don't exist in MSK need to be created\n  const topicsToCreate = desiredTopicConfigs.filter(\n    (desired) => !existingTopicNames.includes(desired.topic)\n  );\n\n  /*\n   * Any topics which do exist, but with fewer partitions than desired,\n   * need to be updated. Partitions can't be removed, only added.\n   */\n  const topicsToUpdate = desiredTopicConfigs.filter((desired) =>\n    existingTopicConfigs.some(\n      (existing) =>\n        desired.topic === existing.name &&\n        desired.numPartitions > existing.partitions.length\n    )\n  );\n\n  // Format the request to update those topics (by creating partitions)\n  const partitionsToCreate = topicsToUpdate.map((topic) => ({\n    topic: topic.topic,\n    count: topic.numPartitions,\n  }));\n\n  // Describe existing topics for informational logs\n  let existingTopicDescriptions = [];\n  if (existingTopicConfigs.length > 0) {\n    const resourcesToDescribe = existingTopicConfigs.map((topic) => ({\n      name: topic.name,\n      type: ConfigResourceTypes.TOPIC,\n    }));\n    existingTopicDescriptions = await admin.describeConfigs({\n      resources: resourcesToDescribe,\n    });\n  }\n\n  console.log(\"Topics to Create:\", JSON.stringify(topicsToCreate, null, 2));\n  console.log(\"Topics to Update:\", JSON.stringify(topicsToUpdate, null, 2));\n  console.log(\n    \"Partitions to Create:\",\n    JSON.stringify(partitionsToCreate, null, 2)\n  );\n  console.log(\n    \"Topic configuration options:\",\n    JSON.stringify(existingTopicDescriptions, null, 2)\n  );\n\n  // Create all the new topics\n  await admin.createTopics({ topics: topicsToCreate });\n\n  // Create all the new partitions\n  if (partitionsToCreate.length > 0) {\n    await admin.createPartitions({ topicPartitions: partitionsToCreate });\n  }\n\n  await admin.disconnect();\n};\n\n/**\n * Deletes all topics for an ephemeral (`--` prefixed) namespace\n * @param { string[] } brokers - List of brokers\n * @param {string} topicNamespace\n */\nexports.deleteTopics = async function (brokers, topicNamespace) {\n  if (!topicNamespace.startsWith(\"--\")) {\n    throw \"ERROR:  The deleteTopics function only operates against topics that begin with --.\";\n  }\n\n  const kafka = new Kafka({\n    clientId: \"admin\",\n    brokers,\n    ssl: true,\n    requestTimeout: 295000, // 5s short of the lambda function's timeout\n  });\n  var admin = kafka.admin();\n\n  await admin.connect();\n\n  const existingTopicNames = await admin.listTopics();\n  console.log(`All existing topics: ${existingTopicNames}`);\n  var topicsToDelete = existingTopicNames.filter(\n    (name) =>\n      name.startsWith(topicNamespace) ||\n      name.startsWith(`_confluent-ksql-${topicNamespace}`)\n  );\n  console.log(`Deleting topics:  ${topicsToDelete}`);\n\n  await admin.deleteTopics({\n    topics: topicsToDelete,\n  });\n\n  await admin.disconnect();\n  return topicsToDelete;\n};\n", "/* eslint-disable no-console */\nconst topics = require(\"../libs/topics-lib.js\");\n\n/**\n * Handler to be triggered in temporary branches by the destroy workflow, cleans up topics with the known namespace format\n * `--${event.project}--${event.stage}--`\n * @param {*} event\n * @param {*} _context\n * @param {*} _callback\n */\nexports.handler = async function (event, _context, _callback) {\n  console.log(\"Received event:\", JSON.stringify(event, null, 2));\n  let namespace = event.stage\n    ? `--${process.env.project}--${event.stage}--`\n    : `--${process.env.project}--`;\n  return await topics.listTopics(process.env.brokerString, namespace);\n};\n"],
./.cdk/cdk.out/asset.2dc0ceae92daa689bcc4ed59c25909f2daf31d8b57e54e23f1b692d63904a91c/index.js.map:4: TODO: validate topics not null/empty\n    const fetch = this.lookupRequest(apiKeys.Fetch, requests.Fetch)\n\n    // Shuffle topic-partitions to ensure fair response allocation across partitions (KIP-74)\n    const flattenedTopicPartitions = topics.reduce((topicPartitions, { topic, partitions }) => {\n      partitions.forEach(partition => {\n        topicPartitions.push({ topic, partition })\n      })\n      return topicPartitions\n    }, [])\n\n    const shuffledTopicPartitions = shuffle(flattenedTopicPartitions)\n\n    // Consecutive partitions for the same topic can be combined into a single `topic` entry\n    const consolidatedTopicPartitions = shuffledTopicPartitions.reduce(\n      (topicPartitions, { topic, partition }) => {\n        const last = topicPartitions[topicPartitions.length - 1]\n\n        if (last != null && last.topic === topic) {\n          topicPartitions[topicPartitions.length - 1].partitions.push(partition)\n        } else {\n          topicPartitions.push({ topic, partitions: [partition] })\n        }\n\n        return topicPartitions\n      },\n      []\n    )\n\n    return await this[PRIVATE.SEND_REQUEST](\n      fetch({\n        replicaId,\n        isolationLevel,\n        maxWaitTime,\n        minBytes,\n        maxBytes,\n        topics: consolidatedTopicPartitions,\n        rackId,\n      })\n    )\n  }\n\n  /**\n   * @public\n   * @param {object} request\n   * @param {string} request.groupId The group id\n   * @param {number} request.groupGenerationId The generation of the group\n   * @param {string} request.memberId The member id assigned by the group coordinator\n   * @returns {Promise}\n   */\n  async heartbeat({ groupId, groupGenerationId, memberId }) {\n    const heartbeat = this.lookupRequest(apiKeys.Heartbeat, requests.Heartbeat)\n    return await this[PRIVATE.SEND_REQUEST](heartbeat({ groupId, groupGenerationId, memberId }))\n  }\n\n  /**\n   * @public\n   * @param {object} request\n   * @param {string} request.groupId The unique group id\n   * @param {import(\"../protocol/coordinatorTypes\").CoordinatorType} request.coordinatorType The type of coordinator to find\n   * @returns {Promise}\n   */\n  async findGroupCoordinator({ groupId, coordinatorType }) {\n    // TODO: validate groupId, mandatory\n    const findCoordinator = this.lookupRequest(apiKeys.GroupCoordinator, requests.GroupCoordinator)\n    return await this[PRIVATE.SEND_REQUEST](findCoordinator({ groupId, coordinatorType }))\n  }\n\n  /**\n   * @public\n   * @param {object} request\n   * @param {string} request.groupId The unique group id\n   * @param {number} request.sessionTimeout The coordinator considers the consumer dead if it receives\n   *                                no heartbeat after this timeout in ms\n   * @param {number} request.rebalanceTimeout The maximum time that the coordinator will wait for each member\n   *                                  to rejoin when rebalancing the group\n   * @param {string} [request.memberId=\"\"] The assigned consumer id or an empty string for a new consumer\n   * @param {string} [request.protocolType=\"consumer\"] Unique name for class of protocols implemented by group\n   * @param {Array} request.groupProtocols List of protocols that the member supports (assignment strategy)\n   *                                [{ name: 'AssignerName', metadata: '{\"version\": 1, \"topics\": []}' }]\n   * @returns {Promise}\n   */\n  async joinGroup({\n    groupId,\n    sessionTimeout,\n    rebalanceTimeout,\n    memberId = '',\n    protocolType = 'consumer',\n    groupProtocols,\n  }) {\n    const joinGroup = this.lookupRequest(apiKeys.JoinGroup, requests.JoinGroup)\n    const makeRequest = (assignedMemberId = memberId) =>\n      this[PRIVATE.SEND_REQUEST](\n        joinGroup({\n          groupId,\n          sessionTimeout,\n          rebalanceTimeout,\n          memberId: assignedMemberId,\n          protocolType,\n          groupProtocols,\n        })\n      )\n\n    try {\n      return await makeRequest()\n    } catch (error) {\n      if (error.name === 'KafkaJSMemberIdRequired') {\n        return makeRequest(error.memberId)\n      }\n\n      throw error\n    }\n  }\n\n  /**\n   * @public\n   * @param {object} request\n   * @param {string} request.groupId\n   * @param {string} request.memberId\n   * @returns {Promise}\n   */\n  async leaveGroup({ groupId, memberId }) {\n    const leaveGroup = this.lookupRequest(apiKeys.LeaveGroup, requests.LeaveGroup)\n    return await this[PRIVATE.SEND_REQUEST](leaveGroup({ groupId, memberId }))\n  }\n\n  /**\n   * @public\n   * @param {object} request\n   * @param {string} request.groupId\n   * @param {number} request.generationId\n   * @param {string} request.memberId\n   * @param {object} request.groupAssignment\n   * @returns {Promise}\n   */\n  async syncGroup({ groupId, generationId, memberId, groupAssignment }) {\n    const syncGroup = this.lookupRequest(apiKeys.SyncGroup, requests.SyncGroup)\n    return await this[PRIVATE.SEND_REQUEST](\n      syncGroup({\n        groupId,\n        generationId,\n        memberId,\n        groupAssignment,\n      })\n    )\n  }\n\n  /**\n   * @public\n   * @param {object} request\n   * @param {number} request.replicaId=-1 Broker id of the follower. For normal consumers, use -1\n   * @param {number} request.isolationLevel=1 This setting controls the visibility of transactional records (default READ_COMMITTED, Kafka >0.11 only)\n   * @param {TopicPartitionOffset[]} request.topics e.g:\n   *\n   * @typedef {Object} TopicPartitionOffset\n   * @property {string} topic\n   * @property {PartitionOffset[]} partitions\n   *\n   * @typedef {Object} PartitionOffset\n   * @property {number} partition\n   * @property {number} [timestamp=-1]\n   *\n   *\n   * @returns {Promise}\n   */\n  async listOffsets({ replicaId, isolationLevel, topics }) {\n    const listOffsets = this.lookupRequest(apiKeys.ListOffsets, requests.ListOffsets)\n    const result = await this[PRIVATE.SEND_REQUEST](\n      listOffsets({ replicaId, isolationLevel, topics })\n    )\n\n    // ListOffsets >= v1 will return a single `offset` rather than an array of `offsets` (ListOffsets V0).\n    // Normalize to just return `offset`.\n    for (const response of result.responses) {\n      response.partitions = response.partitions.map(({ offsets, ...partitionData }) => {\n        return offsets ? { ...partitionData, offset: offsets.pop() } : partitionData\n      })\n    }\n\n    return result\n  }\n\n  /**\n   * @public\n   * @param {object} request\n   * @param {string} request.groupId\n   * @param {number} request.groupGenerationId\n   * @param {string} request.memberId\n   * @param {number} [request.retentionTime=-1] -1 signals to the broker that its default configuration\n   *                                    should be used.\n   * @param {object} request.topics Topics to commit offsets, e.g:\n   *                  [\n   *                    {\n   *                      topic: 'topic-name',\n   *                      partitions: [\n   *                        { partition: 0, offset: '11' }\n   *                      ]\n   *                    }\n   *                  ]\n   * @returns {Promise}\n   */\n  async offsetCommit({ groupId, groupGenerationId, memberId, retentionTime, topics }) {\n    const offsetCommit = this.lookupRequest(apiKeys.OffsetCommit, requests.OffsetCommit)\n    return await this[PRIVATE.SEND_REQUEST](\n      offsetCommit({\n        groupId,\n        groupGenerationId,\n        memberId,\n        retentionTime,\n        topics,\n      })\n    )\n  }\n\n  /**\n   * @public\n   * @param {object} request\n   * @param {string} request.groupId\n   * @param {object} request.topics - If the topic array is null fetch offsets for all topics. e.g:\n   *                  [\n   *                    {\n   *                      topic: 'topic-name',\n   *                      partitions: [\n   *                        { partition: 0 }\n   *                      ]\n   *                    }\n   *                  ]\n   * @returns {Promise}\n   */\n  async offsetFetch({ groupId, topics }) {\n    const offsetFetch = this.lookupRequest(apiKeys.OffsetFetch, requests.OffsetFetch)\n    return await this[PRIVATE.SEND_REQUEST](offsetFetch({ groupId, topics }))\n  }\n\n  /**\n   * @public\n   * @param {object} request\n   * @param {Array} request.groupIds\n   * @returns {Promise}\n   */\n  async describeGroups({ groupIds }) {\n    const describeGroups = this.lookupRequest(apiKeys.DescribeGroups, requests.DescribeGroups)\n    return await this[PRIVATE.SEND_REQUEST](describeGroups({ groupIds }))\n  }\n\n  /**\n   * @public\n   * @param {object} request\n   * @param {Array} request.topics e.g:\n   *                 [\n   *                   {\n   *                     topic: 'topic-name',\n   *                     numPartitions: 1,\n   *                     replicationFactor: 1\n   *                   }\n   *                 ]\n   * @param {boolean} [request.validateOnly=false] If this is true, the request will be validated, but the topic\n   *                                       won't be created\n   * @param {number} [request.timeout=5000] The time in ms to wait for a topic to be completely created\n   *                                on the controller node\n   * @returns {Promise}\n   */\n  async createTopics({ topics, validateOnly = false, timeout = 5000 }) {\n    const createTopics = this.lookupRequest(apiKeys.CreateTopics, requests.CreateTopics)\n    return await this[PRIVATE.SEND_REQUEST](createTopics({ topics, validateOnly, timeout }))\n  }\n\n  /**\n   * @public\n   * @param {object} request\n   * @param {Array} request.topicPartitions e.g:\n   *                 [\n   *                   {\n   *                     topic: 'topic-name',\n   *                     count: 3,\n   *                     assignments: []\n   *                   }\n   *                 ]\n   * @param {boolean} [request.validateOnly=false] If this is true, the request will be validated, but the topic\n   *                                       won't be created\n   * @param {number} [request.timeout=5000] The time in ms to wait for a topic to be completely created\n   *                                on the controller node\n   * @returns {Promise<void>}\n   */\n  async createPartitions({ topicPartitions, validateOnly = false, timeout = 5000 }) {\n    const createPartitions = this.lookupRequest(apiKeys.CreatePartitions, requests.CreatePartitions)\n    return await this[PRIVATE.SEND_REQUEST](\n      createPartitions({ topicPartitions, validateOnly, timeout })\n    )\n  }\n\n  /**\n   * @public\n   * @param {object} request\n   * @param {string[]} request.topics An array of topics to be deleted\n   * @param {number} [request.timeout=5000] The time in ms to wait for a topic to be completely deleted on the\n   *                                controller node.\n   * @returns {Promise}\n   */\n  async deleteTopics({ topics, timeout = 5000 }) {\n    const deleteTopics = this.lookupRequest(apiKeys.DeleteTopics, requests.DeleteTopics)\n    return await this[PRIVATE.SEND_REQUEST](deleteTopics({ topics, timeout }))\n  }\n\n  /**\n   * @public\n   * @param {object} request\n   * @param {import(\"../../types\").ResourceConfigQuery[]} request.resources\n   *                                 [{\n   *                                   type: RESOURCE_TYPES.TOPIC,\n   *                                   name: 'topic-name',\n   *                                   configNames: ['compression.type', 'retention.ms']\n   *                                 }]\n   * @param {boolean} [request.includeSynonyms=false]\n   * @returns {Promise}\n   */\n  async describeConfigs({ resources, includeSynonyms = false }) {\n    const describeConfigs = this.lookupRequest(apiKeys.DescribeConfigs, requests.DescribeConfigs)\n    return await this[PRIVATE.SEND_REQUEST](describeConfigs({ resources, includeSynonyms }))\n  }\n\n  /**\n   * @public\n   * @param {object} request\n   * @param {import(\"../../types\").IResourceConfig[]} request.resources\n   *                                 [{\n   *                                  type: RESOURCE_TYPES.TOPIC,\n   *                                  name: 'topic-name',\n   *                                  configEntries: [\n   *                                    {\n   *                                      name: 'cleanup.policy',\n   *                                      value: 'compact'\n   *                                    }\n   *                                  ]\n   *                                 }]\n   * @param {boolean} [request.validateOnly=false]\n   * @returns {Promise}\n   */\n  async alterConfigs({ resources, validateOnly = false }) {\n    const alterConfigs = this.lookupRequest(apiKeys.AlterConfigs, requests.AlterConfigs)\n    return await this[PRIVATE.SEND_REQUEST](alterConfigs({ resources, validateOnly }))\n  }\n\n  /**\n   * Send an `InitProducerId` request to fetch a PID and bump the producer epoch.\n   *\n   * Request should be made to the transaction coordinator.\n   * @public\n   * @param {object} request\n   * @param {number} request.transactionTimeout The time in ms to wait for before aborting idle transactions\n   * @param {number} [request.transactionalId] The transactional id or null if the producer is not transactional\n   * @returns {Promise}\n   */\n  async initProducerId({ transactionalId, transactionTimeout }) {\n    const initProducerId = this.lookupRequest(apiKeys.InitProducerId, requests.InitProducerId)\n    return await this[PRIVATE.SEND_REQUEST](initProducerId({ transactionalId, transactionTimeout }))\n  }\n\n  /**\n   * Send an `AddPartitionsToTxn` request to mark a TopicPartition as participating in the transaction.\n   *\n   * Request should be made to the transaction coordinator.\n   * @public\n   * @param {object} request\n   * @param {string} request.transactionalId The transactional id corresponding to the transaction.\n   * @param {number} request.producerId Current producer id in use by the transactional id.\n   * @param {number} request.producerEpoch Current epoch associated with the producer id.\n   * @param {object[]} request.topics e.g:\n   *                  [\n   *                    {\n   *                      topic: 'topic-name',\n   *                      partitions: [ 0, 1]\n   *                    }\n   *                  ]\n   * @returns {Promise}\n   */\n  async addPartitionsToTxn({ transactionalId, producerId, producerEpoch, topics }) {\n    const addPartitionsToTxn = this.lookupRequest(\n      apiKeys.AddPartitionsToTxn,\n      requests.AddPartitionsToTxn\n    )\n    return await this[PRIVATE.SEND_REQUEST](\n      addPartitionsToTxn({ transactionalId, producerId, producerEpoch, topics })\n    )\n  }\n\n  /**\n   * Send an `AddOffsetsToTxn` request.\n   *\n   * Request should be made to the transaction coordinator.\n   * @public\n   * @param {object} request\n   * @param {string} request.transactionalId The transactional id corresponding to the transaction.\n   * @param {number} request.producerId Current producer id in use by the transactional id.\n   * @param {number} request.producerEpoch Current epoch associated with the producer id.\n   * @param {string} request.groupId The unique group identifier (for the consumer group)\n   * @returns {Promise}\n   */\n  async addOffsetsToTxn({ transactionalId, producerId, producerEpoch, groupId }) {\n    const addOffsetsToTxn = this.lookupRequest(apiKeys.AddOffsetsToTxn, requests.AddOffsetsToTxn)\n    return await this[PRIVATE.SEND_REQUEST](\n      addOffsetsToTxn({ transactionalId, producerId, producerEpoch, groupId })\n    )\n  }\n\n  /**\n   * Send a `TxnOffsetCommit` request to persist the offsets in the `__consumer_offsets` topics.\n   *\n   * Request should be made to the consumer coordinator.\n   * @public\n   * @param {object} request\n   * @param {OffsetCommitTopic[]} request.topics\n   * @param {string} request.transactionalId The transactional id corresponding to the transaction.\n   * @param {string} request.groupId The unique group identifier (for the consumer group)\n   * @param {number} request.producerId Current producer id in use by the transactional id.\n   * @param {number} request.producerEpoch Current epoch associated with the producer id.\n   * @param {OffsetCommitTopic[]} request.topics\n   *\n   * @typedef {Object} OffsetCommitTopic\n   * @property {string} topic\n   * @property {OffsetCommitTopicPartition[]} partitions\n   *\n   * @typedef {Object} OffsetCommitTopicPartition\n   * @property {number} partition\n   * @property {number} offset\n   * @property {string} [metadata]\n   *\n   * @returns {Promise}\n   */\n  async txnOffsetCommit({ transactionalId, groupId, producerId, producerEpoch, topics }) {\n    const txnOffsetCommit = this.lookupRequest(apiKeys.TxnOffsetCommit, requests.TxnOffsetCommit)\n    return await this[PRIVATE.SEND_REQUEST](\n      txnOffsetCommit({ transactionalId, groupId, producerId, producerEpoch, topics })\n    )\n  }\n\n  /**\n   * Send an `EndTxn` request to indicate transaction should be committed or aborted.\n   *\n   * Request should be made to the transaction coordinator.\n   * @public\n   * @param {object} request\n   * @param {string} request.transactionalId The transactional id corresponding to the transaction.\n   * @param {number} request.producerId Current producer id in use by the transactional id.\n   * @param {number} request.producerEpoch Current epoch associated with the producer id.\n   * @param {boolean} request.transactionResult The result of the transaction (false = ABORT, true = COMMIT)\n   * @returns {Promise}\n   */\n  async endTxn({ transactionalId, producerId, producerEpoch, transactionResult }) {\n    const endTxn = this.lookupRequest(apiKeys.EndTxn, requests.EndTxn)\n    return await this[PRIVATE.SEND_REQUEST](\n      endTxn({ transactionalId, producerId, producerEpoch, transactionResult })\n    )\n  }\n\n  /**\n   * Send request for list of groups\n   * @public\n   * @returns {Promise}\n   */\n  async listGroups() {\n    const listGroups = this.lookupRequest(apiKeys.ListGroups, requests.ListGroups)\n    return await this[PRIVATE.SEND_REQUEST](listGroups())\n  }\n\n  /**\n   * Send request to delete groups\n   * @param {string[]} groupIds\n   * @public\n   * @returns {Promise}\n   */\n  async deleteGroups(groupIds) {\n    const deleteGroups = this.lookupRequest(apiKeys.DeleteGroups, requests.DeleteGroups)\n    return await this[PRIVATE.SEND_REQUEST](deleteGroups(groupIds))\n  }\n\n  /**\n   * Send request to delete records\n   * @public\n   * @param {object} request\n   * @param {TopicPartitionRecords[]} request.topics\n   *                          [\n   *                            {\n   *                              topic: 'my-topic-name',\n   *                              partitions: [\n   *                                { partition: 0, offset 2 },\n   *                                { partition: 1, offset 4 },\n   *                              ],\n   *                            }\n   *                          ]\n   * @returns {Promise<Array>} example:\n   *                          {\n   *                            throttleTime: 0\n   *                           [\n   *                              {\n   *                                topic: 'my-topic-name',\n   *                                partitions: [\n   *                                 { partition: 0, lowWatermark: '2n', errorCode: 0 },\n   *                                 { partition: 1, lowWatermark: '4n', errorCode: 0 },\n   *                               ],\n   *                             },\n   *                           ]\n   *                          }\n   *\n   * @typedef {object} TopicPartitionRecords\n   * @property {string} topic\n   * @property {PartitionRecord[]} partitions\n   *\n   * @typedef {object} PartitionRecord\n   * @property {number} partition\n   * @property {number} offset\n   */\n  async deleteRecords({ topics }) {\n    const deleteRecords = this.lookupRequest(apiKeys.DeleteRecords, requests.DeleteRecords)\n    return await this[PRIVATE.SEND_REQUEST](deleteRecords({ topics }))\n  }\n\n  /**\n   * @public\n   * @param {object} request\n   * @param {import(\"../../types\").AclEntry[]} request.acl e.g:\n   *                 [\n   *                   {\n   *                     resourceType: AclResourceTypes.TOPIC,\n   *                     resourceName: 'topic-name',\n   *                     resourcePatternType: ResourcePatternTypes.LITERAL,\n   *                     principal: 'User:bob',\n   *                     host: '*',\n   *                     operation: AclOperationTypes.ALL,\n   *                     permissionType: AclPermissionTypes.DENY,\n   *                   }\n   *                 ]\n   * @returns {Promise<void>}\n   */\n  async createAcls({ acl }) {\n    const createAcls = this.lookupRequest(apiKeys.CreateAcls, requests.CreateAcls)\n    return await this[PRIVATE.SEND_REQUEST](createAcls({ creations: acl }))\n  }\n\n  /**\n   * @public\n   * @param {import(\"../../types\").AclEntry} aclEntry\n   * @returns {Promise<void>}\n   */\n  async describeAcls({\n    resourceType,\n    resourceName,\n    resourcePatternType,\n    principal,\n    host,\n    operation,\n    permissionType,\n  }) {\n    const describeAcls = this.lookupRequest(apiKeys.DescribeAcls, requests.DescribeAcls)\n    return await this[PRIVATE.SEND_REQUEST](\n      describeAcls({\n        resourceType,\n        resourceName,\n        resourcePatternType,\n        principal,\n        host,\n        operation,\n        permissionType,\n      })\n    )\n  }\n\n  /**\n   * @public\n   * @param {Object} request\n   * @param {import(\"../../types\").AclEntry[]} request.filters\n   * @returns {Promise<void>}\n   */\n  async deleteAcls({ filters }) {\n    const deleteAcls = this.lookupRequest(apiKeys.DeleteAcls, requests.DeleteAcls)\n    return await this[PRIVATE.SEND_REQUEST](deleteAcls({ filters }))\n  }\n\n  /**\n   * @public\n   * @param {Object} request\n   * @param {import(\"../../types\").PartitionReassignment[]} request.topics\n   * @param {number} [request.timeout]\n   * @returns {Promise}\n   */\n  async alterPartitionReassignments({ topics, timeout }) {\n    const alterPartitionReassignments = this.lookupRequest(\n      apiKeys.AlterPartitionReassignments,\n      requests.AlterPartitionReassignments\n    )\n    return await this[PRIVATE.SEND_REQUEST](alterPartitionReassignments({ topics, timeout }))\n  }\n\n  /**\n   * @public\n   * @param {Object} request\n   * @param {import(\"../../types\").TopicPartitions[]} request.topics can be null\n   * @param {number} [request.timeout]\n   * @returns {Promise}\n   */\n  async listPartitionReassignments({ topics = null, timeout }) {\n    const listPartitionReassignments = this.lookupRequest(\n      apiKeys.ListPartitionReassignments,\n      requests.ListPartitionReassignments\n    )\n    return await this[PRIVATE.SEND_REQUEST](listPartitionReassignments({ topics, timeout }))\n  }\n\n  /**\n   * @private\n   */\n  async [PRIVATE.SEND_REQUEST](protocolRequest) {\n    try {\n      return await this.connectionPool.send(protocolRequest)\n    } catch (e) {\n      if (e.name === 'KafkaJSConnectionClosedError') {\n        await this.disconnect()\n      }\n\n      throw e\n    }\n  }\n}\n", "module.exports = {\n  maxRetryTime: 1000,\n  initialRetryTime: 50,\n  factor: 0.02, // randomization factor\n  multiplier: 1.5, // exponential factor\n  retries: 15, // max retries\n}\n", "module.exports = {\n  maxRetryTime: 30 * 1000,\n  initialRetryTime: 300,\n  factor: 0.2, // randomization factor\n  multiplier: 2, // exponential factor\n  retries: 5, // max retries\n}\n", "const { KafkaJSNumberOfRetriesExceeded, KafkaJSNonRetriableError } = require('../errors')\n\nconst isTestMode = process.env.NODE_ENV === 'test'\nconst RETRY_DEFAULT = isTestMode ? require('./defaults.test') : require('./defaults')\n\nconst random = (min, max) => {\n  return Math.random() * (max - min) + min\n}\n\nconst randomFromRetryTime = (factor, retryTime) => {\n  const delta = factor * retryTime\n  return Math.ceil(random(retryTime - delta, retryTime + delta))\n}\n\nconst UNRECOVERABLE_ERRORS = ['RangeError', 'ReferenceError', 'SyntaxError', 'TypeError']\nconst isErrorUnrecoverable = e => UNRECOVERABLE_ERRORS.includes(e.name)\nconst isErrorRetriable = error =>\n  (error.retriable || error.retriable !== false) && !isErrorUnrecoverable(error)\n\nconst createRetriable = (configs, resolve, reject, fn) => {\n  let aborted = false\n  const { factor, multiplier, maxRetryTime, retries } = configs\n\n  const bail = error => {\n    aborted = true\n    reject(error || new Error('Aborted'))\n  }\n\n  const calculateExponentialRetryTime = retryTime => {\n    return Math.min(randomFromRetryTime(factor, retryTime) * multiplier, maxRetryTime)\n  }\n\n  const retry = (retryTime, retryCount = 0) => {\n    if (aborted) return\n\n    const nextRetryTime = calculateExponentialRetryTime(retryTime)\n    const shouldRetry = retryCount < retries\n\n    const scheduleRetry = () => {\n      setTimeout(() => retry(nextRetryTime, retryCount + 1), retryTime)\n    }\n\n    fn(bail, retryCount, retryTime)\n      .then(resolve)\n      .catch(e => {\n        if (isErrorRetriable(e)) {\n          if (shouldRetry) {\n            scheduleRetry()\n          } else {\n            reject(\n              new KafkaJSNumberOfRetriesExceeded(e, { retryCount, retryTime, cause: e.cause || e })\n            )\n          }\n        } else {\n          reject(new KafkaJSNonRetriableError(e, { cause: e.cause || e }))\n        }\n      })\n  }\n\n  return retry\n}\n\n/**\n * @typedef {(fn: (bail: (err: Error) => void, retryCount: number, retryTime: number) => any) => Promise<ReturnType<fn>>} Retrier\n */\n\n/**\n * @param {import(\"../../types\").RetryOptions} [opts]\n * @returns {Retrier}\n */\nmodule.exports = (opts = {}) => fn => {\n  return new Promise((resolve, reject) => {\n    const configs = Object.assign({}, RETRY_DEFAULT, opts)\n    const start = createRetriable(configs, resolve, reject, fn)\n    start(randomFromRetryTime(configs.factor, configs.initialRetryTime))\n  })\n}\n", "module.exports = (a, b) => {\n  const result = []\n  const length = a.length\n  let i = 0\n\n  while (i < length) {\n    if (b.indexOf(a[i]) === -1) {\n      result.push(a[i])\n    }\n    i += 1\n  }\n\n  return result\n}\n", "const Broker = require('../broker')\nconst createRetry = require('../retry')\nconst shuffle = require('../utils/shuffle')\nconst arrayDiff = require('../utils/arrayDiff')\nconst { KafkaJSBrokerNotFound, KafkaJSProtocolError } = require('../errors')\n\nconst { keys, assign, values } = Object\nconst hasBrokerBeenReplaced = (broker, { host, port, rack }) =>\n  broker.connectionPool.host !== host ||\n  broker.connectionPool.port !== port ||\n  broker.connectionPool.rack !== rack\n\nmodule.exports = class BrokerPool {\n  /**\n   * @param {object} options\n   * @param {import(\"./connectionPoolBuilder\").ConnectionPoolBuilder} options.connectionPoolBuilder\n   * @param {import(\"../../types\").Logger} options.logger\n   * @param {import(\"../../types\").RetryOptions} [options.retry]\n   * @param {boolean} [options.allowAutoTopicCreation]\n   * @param {number} [options.authenticationTimeout]\n   * @param {number} [options.metadataMaxAge]\n   */\n  constructor({\n    connectionPoolBuilder,\n    logger,\n    retry,\n    allowAutoTopicCreation,\n    authenticationTimeout,\n    metadataMaxAge,\n  }) {\n    this.rootLogger = logger\n    this.connectionPoolBuilder = connectionPoolBuilder\n    this.metadataMaxAge = metadataMaxAge || 0\n    this.logger = logger.namespace('BrokerPool')\n    this.retrier = createRetry(assign({}, retry))\n\n    this.createBroker = options =>\n      new Broker({\n        allowAutoTopicCreation,\n        authenticationTimeout,\n        ...options,\n      })\n\n    this.brokers = {}\n    /** @type {Broker | undefined} */\n    this.seedBroker = undefined\n    /** @type {import(\"../../types\").BrokerMetadata | null} */\n    this.metadata = null\n    this.metadataExpireAt = null\n    this.versions = null\n  }\n\n  /**\n   * @public\n   * @returns {Boolean}\n   */\n  hasConnectedBrokers() {\n    const brokers = values(this.brokers)\n    return (\n      !!brokers.find(broker => broker.isConnected()) ||\n      (this.seedBroker ? this.seedBroker.isConnected() : false)\n    )\n  }\n\n  async createSeedBroker() {\n    if (this.seedBroker) {\n      await this.seedBroker.disconnect()\n    }\n\n    const connectionPool = await this.connectionPoolBuilder.build()\n\n    this.seedBroker = this.createBroker({\n      connectionPool,\n      logger: this.rootLogger,\n    })\n  }\n\n  /**\n   * @public\n   * @returns {Promise<void>}\n   */\n  async connect() {\n    if (this.hasConnectedBrokers()) {\n      return\n    }\n\n    if (!this.seedBroker) {\n      await this.createSeedBroker()\n    }\n\n    return this.retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await this.seedBroker.connect()\n        this.versions = this.seedBroker.versions\n      } catch (e) {\n        if (e.name === 'KafkaJSConnectionError' || e.type === 'ILLEGAL_SASL_STATE') {\n          // Connection builder will always rotate the seed broker\n          await this.createSeedBroker()\n          this.logger.error(\n            `Failed to connect to seed broker, trying another broker from the list: ${e.message}`,\n            { retryCount, retryTime }\n          )\n        } else {\n          this.logger.error(e.message, { retryCount, retryTime })\n        }\n\n        if (e.retriable) throw e\n        bail(e)\n      }\n    })\n  }\n\n  /**\n   * @public\n   * @returns {Promise}\n   */\n  async disconnect() {\n    this.seedBroker && (await this.seedBroker.disconnect())\n    await Promise.all(values(this.brokers).map(broker => broker.disconnect()))\n\n    this.brokers = {}\n    this.metadata = null\n    this.versions = null\n  }\n\n  /**\n   * @public\n   * @param {Object} destination\n   * @param {string} destination.host\n   * @param {number} destination.port\n   */\n  removeBroker({ host, port }) {\n    const removedBroker = values(this.brokers).find(\n      broker => broker.connectionPool.host === host && broker.connectionPool.port === port\n    )\n\n    if (removedBroker) {\n      delete this.brokers[removedBroker.nodeId]\n      this.metadataExpireAt = null\n\n      if (this.seedBroker.nodeId === removedBroker.nodeId) {\n        this.seedBroker = shuffle(values(this.brokers))[0]\n      }\n    }\n  }\n\n  /**\n   * @public\n   * @param {Array<String>} topics\n   * @returns {Promise<null>}\n   */\n  async refreshMetadata(topics) {\n    const broker = await this.findConnectedBroker()\n    const { host: seedHost, port: seedPort } = this.seedBroker.connectionPool\n\n    return this.retrier(async (bail, retryCount, retryTime) => {\n      try {\n        this.metadata = await broker.metadata(topics)\n        this.metadataExpireAt = Date.now() + this.metadataMaxAge\n\n        const replacedBrokers = []\n\n        this.brokers = await this.metadata.brokers.reduce(\n          async (resultPromise, { nodeId, host, port, rack }) => {\n            const result = await resultPromise\n\n            if (result[nodeId]) {\n              if (!hasBrokerBeenReplaced(result[nodeId], { host, port, rack })) {\n                return result\n              }\n\n              replacedBrokers.push(result[nodeId])\n            }\n\n            if (host === seedHost && port === seedPort) {\n              this.seedBroker.nodeId = nodeId\n              this.seedBroker.connectionPool.rack = rack\n              return assign(result, {\n                [nodeId]: this.seedBroker,\n              })\n            }\n\n            return assign(result, {\n              [nodeId]: this.createBroker({\n                logger: this.rootLogger,\n                versions: this.versions,\n                connectionPool: await this.connectionPoolBuilder.build({ host, port, rack }),\n                nodeId,\n              }),\n            })\n          },\n          this.brokers\n        )\n\n        const freshBrokerIds = this.metadata.brokers.map(({ nodeId }) => `${nodeId}`).sort()\n        const currentBrokerIds = keys(this.brokers).sort()\n        const unusedBrokerIds = arrayDiff(currentBrokerIds, freshBrokerIds)\n\n        const brokerDisconnects = unusedBrokerIds.map(nodeId => {\n          const broker = this.brokers[nodeId]\n          return broker.disconnect().then(() => {\n            delete this.brokers[nodeId]\n          })\n        })\n\n        const replacedBrokersDisconnects = replacedBrokers.map(broker => broker.disconnect())\n        await Promise.all([...brokerDisconnects, ...replacedBrokersDisconnects])\n      } catch (e) {\n        if (e.type === 'LEADER_NOT_AVAILABLE') {\n          throw e\n        }\n\n        bail(e)\n      }\n    })\n  }\n\n  /**\n   * Only refreshes metadata if the data is stale according to the `metadataMaxAge` param or does not contain information about the provided topics\n   *\n   * @public\n   * @param {Array<String>} topics\n   * @returns {Promise<null>}\n   */\n  async refreshMetadataIfNecessary(topics) {\n    const shouldRefresh =\n      this.metadata == null ||\n      this.metadataExpireAt == null ||\n      Date.now() > this.metadataExpireAt ||\n      !topics.every(topic =>\n        this.metadata.topicMetadata.some(topicMetadata => topicMetadata.topic === topic)\n      )\n\n    if (shouldRefresh) {\n      return this.refreshMetadata(topics)\n    }\n  }\n\n  /** @type {() => string[]} */\n  getNodeIds() {\n    return keys(this.brokers)\n  }\n\n  /**\n   * @public\n   * @param {object} options\n   * @param {string} options.nodeId\n   * @returns {Promise<Broker>}\n   */\n  async findBroker({ nodeId }) {\n    const broker = this.brokers[nodeId]\n\n    if (!broker) {\n      throw new KafkaJSBrokerNotFound(`Broker ${nodeId} not found in the cached metadata`)\n    }\n\n    await this.connectBroker(broker)\n    return broker\n  }\n\n  /**\n   * @public\n   * @param {(params: { nodeId: string, broker: Broker }) => Promise<T>} callback\n   * @returns {Promise<T>}\n   * @template T\n   */\n  async withBroker(callback) {\n    const brokers = shuffle(keys(this.brokers))\n    if (brokers.length === 0) {\n      throw new KafkaJSBrokerNotFound('No brokers in the broker pool')\n    }\n\n    for (const nodeId of brokers) {\n      const broker = await this.findBroker({ nodeId })\n      try {\n        return await callback({ nodeId, broker })\n      } catch (e) {}\n    }\n\n    return null\n  }\n\n  /**\n   * @public\n   * @returns {Promise<Broker>}\n   */\n  async findConnectedBroker() {\n    const nodeIds = shuffle(keys(this.brokers))\n    const connectedBrokerId = nodeIds.find(nodeId => this.brokers[nodeId].isConnected())\n\n    if (connectedBrokerId) {\n      return await this.findBroker({ nodeId: connectedBrokerId })\n    }\n\n    // Cycle through the nodes until one connects\n    for (const nodeId of nodeIds) {\n      try {\n        return await this.findBroker({ nodeId })\n      } catch (e) {}\n    }\n\n    // Failed to connect to all known brokers, metadata might be old\n    await this.connect()\n    return this.seedBroker\n  }\n\n  /**\n   * @private\n   * @param {Broker} broker\n   * @returns {Promise<null>}\n   */\n  async connectBroker(broker) {\n    if (broker.isConnected()) {\n      return\n    }\n\n    return this.retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await broker.connect()\n      } catch (e) {\n        if (e.name === 'KafkaJSConnectionError' || e.type === 'ILLEGAL_SASL_STATE') {\n          await broker.disconnect()\n        }\n\n        // To avoid reconnecting to an unavailable host, we bail on connection errors\n        // and refresh metadata on a higher level before reconnecting\n        if (e.name === 'KafkaJSConnectionError') {\n          return bail(e)\n        }\n\n        if (e.type === 'ILLEGAL_SASL_STATE') {\n          // Rebuild the connection pool since it can't recover from illegal SASL state\n          broker.connectionPool = await this.connectionPoolBuilder.build({\n            host: broker.connectionPool.host,\n            port: broker.connectionPool.port,\n            rack: broker.connectionPool.rack,\n          })\n\n          this.logger.error(`Failed to connect to broker, reconnecting`, { retryCount, retryTime })\n          throw new KafkaJSProtocolError(e, { retriable: true })\n        }\n\n        if (e.retriable) throw e\n        this.logger.error(e, { retryCount, retryTime, stack: e.stack })\n        bail(e)\n      }\n    })\n  }\n}\n", "/**\n * @template T\n * @param { (...args: any) => Promise<T> } [asyncFunction]\n * Promise returning function that will only ever be invoked sequentially.\n * @returns { (...args: any) => Promise<T> }\n * Function that may invoke asyncFunction if there is not a currently executing invocation.\n * Returns promise from the currently executing invocation.\n */\nmodule.exports = asyncFunction => {\n  let promise = null\n\n  return (...args) => {\n    if (promise == null) {\n      promise = asyncFunction(...args).finally(() => (promise = null))\n    }\n    return promise\n  }\n}\n", "/**\n * @param {Object} options\n * @param {import(\"../../types\").ISocketFactory} options.socketFactory\n * @param {string} options.host\n * @param {number} options.port\n * @param {Object} options.ssl\n * @param {() => void} options.onConnect\n * @param {(data: Buffer) => void} options.onData\n * @param {() => void} options.onEnd\n * @param {(err: Error) => void} options.onError\n * @param {() => void} options.onTimeout\n */\nmodule.exports = ({\n  socketFactory,\n  host,\n  port,\n  ssl,\n  onConnect,\n  onData,\n  onEnd,\n  onError,\n  onTimeout,\n}) => {\n  const socket = socketFactory({ host, port, ssl, onConnect })\n\n  socket.on('data', onData)\n  socket.on('end', onEnd)\n  socket.on('error', onError)\n  socket.on('timeout', onTimeout)\n\n  return socket\n}\n", "const Encoder = require('./encoder')\n\nmodule.exports = async ({ correlationId, clientId, request: { apiKey, apiVersion, encode } }) => {\n  const payload = await encode()\n  const requestPayload = new Encoder()\n    .writeInt16(apiKey)\n    .writeInt16(apiVersion)\n    .writeInt32(correlationId)\n    .writeString(clientId)\n    .writeEncoder(payload)\n\n  return new Encoder().writeInt32(requestPayload.size()).writeEncoder(requestPayload)\n}\n", "const EARLIEST_OFFSET = -2\nconst LATEST_OFFSET = -1\nconst INT_32_MAX_VALUE = Math.pow(2, 31) - 1\n\nmodule.exports = {\n  EARLIEST_OFFSET,\n  LATEST_OFFSET,\n  INT_32_MAX_VALUE,\n}\n", "module.exports = () => ({\n  KAFKAJS_DEBUG_PROTOCOL_BUFFERS: process.env.KAFKAJS_DEBUG_PROTOCOL_BUFFERS,\n  KAFKAJS_DEBUG_EXTENDED_PROTOCOL_BUFFERS: process.env.KAFKAJS_DEBUG_EXTENDED_PROTOCOL_BUFFERS,\n})\n", "/** @type {<T1 extends string>(namespace: T1) => <T2 extends string>(type: T2) => `${T1}.${T2}`} */\nmodule.exports = namespace => type => `${namespace}.${type}`\n", "const InstrumentationEventType = require('../instrumentation/eventType')\nconst eventType = InstrumentationEventType('network')\n\nmodule.exports = {\n  NETWORK_REQUEST: eventType('request'),\n  NETWORK_REQUEST_TIMEOUT: eventType('request_timeout'),\n  NETWORK_REQUEST_QUEUE_SIZE: eventType('request_queue_size'),\n}\n", "const { KafkaJSRequestTimeoutError, KafkaJSNonRetriableError } = require('../../errors')\nconst events = require('../instrumentationEvents')\n\nconst PRIVATE = {\n  STATE: Symbol('private:SocketRequest:state'),\n  EMIT_EVENT: Symbol('private:SocketRequest:emitEvent'),\n}\n\nconst REQUEST_STATE = {\n  PENDING: Symbol('PENDING'),\n  SENT: Symbol('SENT'),\n  COMPLETED: Symbol('COMPLETED'),\n  REJECTED: Symbol('REJECTED'),\n}\n\n/**\n * SocketRequest abstracts the life cycle of a socket request, making it easier to track\n * request durations and to have individual timeouts per request.\n *\n * @typedef {Object} SocketRequest\n * @property {number} createdAt\n * @property {number} sentAt\n * @property {number} pendingDuration\n * @property {number} duration\n * @property {number} requestTimeout\n * @property {string} broker\n * @property {string} clientId\n * @property {RequestEntry} entry\n * @property {boolean} expectResponse\n * @property {Function} send\n * @property {Function} timeout\n *\n * @typedef {Object} RequestEntry\n * @property {string} apiKey\n * @property {string} apiName\n * @property {number} apiVersion\n * @property {number} correlationId\n * @property {Function} resolve\n * @property {Function} reject\n */\nmodule.exports = class SocketRequest {\n  /**\n   * @param {Object} options\n   * @param {number} options.requestTimeout\n   * @param {string} options.broker - e.g: 127.0.0.1:9092\n   * @param {string} options.clientId\n   * @param {RequestEntry} options.entry\n   * @param {boolean} options.expectResponse\n   * @param {Function} options.send\n   * @param {() => void} options.timeout\n   * @param {import(\"../../instrumentation/emitter\")} [options.instrumentationEmitter=null]\n   */\n  constructor({\n    requestTimeout,\n    broker,\n    clientId,\n    entry,\n    expectResponse,\n    send,\n    timeout,\n    instrumentationEmitter = null,\n  }) {\n    this.createdAt = Date.now()\n    this.requestTimeout = requestTimeout\n    this.broker = broker\n    this.clientId = clientId\n    this.entry = entry\n    this.correlationId = entry.correlationId\n    this.expectResponse = expectResponse\n    this.sendRequest = send\n    this.timeoutHandler = timeout\n\n    this.sentAt = null\n    this.duration = null\n    this.pendingDuration = null\n\n    this[PRIVATE.STATE] = REQUEST_STATE.PENDING\n    this[PRIVATE.EMIT_EVENT] = (eventName, payload) =>\n      instrumentationEmitter && instrumentationEmitter.emit(eventName, payload)\n  }\n\n  send() {\n    this.throwIfInvalidState({\n      accepted: [REQUEST_STATE.PENDING],\n      next: REQUEST_STATE.SENT,\n    })\n\n    this.sendRequest()\n    this.sentAt = Date.now()\n    this.pendingDuration = this.sentAt - this.createdAt\n    this[PRIVATE.STATE] = REQUEST_STATE.SENT\n  }\n\n  timeoutRequest() {\n    const { apiName, apiKey, apiVersion } = this.entry\n    const requestInfo = `${apiName}(key: ${apiKey}, version: ${apiVersion})`\n    const eventData = {\n      broker: this.broker,\n      clientId: this.clientId,\n      correlationId: this.correlationId,\n      createdAt: this.createdAt,\n      sentAt: this.sentAt,\n      pendingDuration: this.pendingDuration,\n    }\n\n    this.timeoutHandler()\n    this.rejected(new KafkaJSRequestTimeoutError(`Request ${requestInfo} timed out`, eventData))\n    this[PRIVATE.EMIT_EVENT](events.NETWORK_REQUEST_TIMEOUT, {\n      ...eventData,\n      apiName,\n      apiKey,\n      apiVersion,\n    })\n  }\n\n  completed({ size, payload }) {\n    this.throwIfInvalidState({\n      accepted: [REQUEST_STATE.SENT],\n      next: REQUEST_STATE.COMPLETED,\n    })\n\n    const { entry, correlationId, broker, clientId, createdAt, sentAt, pendingDuration } = this\n\n    this[PRIVATE.STATE] = REQUEST_STATE.COMPLETED\n    this.duration = Date.now() - this.sentAt\n    entry.resolve({ correlationId, entry, size, payload })\n\n    this[PRIVATE.EMIT_EVENT](events.NETWORK_REQUEST, {\n      broker,\n      clientId,\n      correlationId,\n      size,\n      createdAt,\n      sentAt,\n      pendingDuration,\n      duration: this.duration,\n      apiName: entry.apiName,\n      apiKey: entry.apiKey,\n      apiVersion: entry.apiVersion,\n    })\n  }\n\n  rejected(error) {\n    this.throwIfInvalidState({\n      accepted: [REQUEST_STATE.PENDING, REQUEST_STATE.SENT],\n      next: REQUEST_STATE.REJECTED,\n    })\n\n    this[PRIVATE.STATE] = REQUEST_STATE.REJECTED\n    this.duration = Date.now() - this.sentAt\n    this.entry.reject(error)\n  }\n\n  /**\n   * @private\n   */\n  throwIfInvalidState({ accepted, next }) {\n    if (accepted.includes(this[PRIVATE.STATE])) {\n      return\n    }\n\n    const current = this[PRIVATE.STATE].toString()\n\n    throw new KafkaJSNonRetriableError(\n      `Invalid state, can't transition from ${current} to ${next.toString()}`\n    )\n  }\n}\n", "const { EventEmitter } = require('events')\nconst SocketRequest = require('./socketRequest')\nconst events = require('../instrumentationEvents')\nconst { KafkaJSInvariantViolation } = require('../../errors')\n\nconst PRIVATE = {\n  EMIT_QUEUE_SIZE_EVENT: Symbol('private:RequestQueue:emitQueueSizeEvent'),\n  EMIT_REQUEST_QUEUE_EMPTY: Symbol('private:RequestQueue:emitQueueEmpty'),\n}\n\nconst REQUEST_QUEUE_EMPTY = 'requestQueueEmpty'\nconst CHECK_PENDING_REQUESTS_INTERVAL = 10\n\nmodule.exports = class RequestQueue extends EventEmitter {\n  /**\n   * @param {Object} options\n   * @param {number} options.maxInFlightRequests\n   * @param {number} options.requestTimeout\n   * @param {boolean} options.enforceRequestTimeout\n   * @param {string} options.clientId\n   * @param {string} options.broker\n   * @param {import(\"../../../types\").Logger} options.logger\n   * @param {import(\"../../instrumentation/emitter\")} [options.instrumentationEmitter=null]\n   * @param {() => boolean} [options.isConnected]\n   */\n  constructor({\n    instrumentationEmitter = null,\n    maxInFlightRequests,\n    requestTimeout,\n    enforceRequestTimeout,\n    clientId,\n    broker,\n    logger,\n    isConnected = () => true,\n  }) {\n    super()\n    this.instrumentationEmitter = instrumentationEmitter\n    this.maxInFlightRequests = maxInFlightRequests\n    this.requestTimeout = requestTimeout\n    this.enforceRequestTimeout = enforceRequestTimeout\n    this.clientId = clientId\n    this.broker = broker\n    this.logger = logger\n    this.isConnected = isConnected\n\n    this.inflight = new Map()\n    this.pending = []\n\n    /**\n     * Until when this request queue is throttled and shouldn't send requests\n     *\n     * The value represents the timestamp of the end of the throttling in ms-since-epoch. If the value\n     * is smaller than the current timestamp no throttling is active.\n     *\n     * @type {number}\n     */\n    this.throttledUntil = -1\n\n    /**\n     * Timeout id if we have scheduled a check for pending requests due to client-side throttling\n     *\n     * @type {null|NodeJS.Timeout}\n     */\n    this.throttleCheckTimeoutId = null\n\n    this[PRIVATE.EMIT_REQUEST_QUEUE_EMPTY] = () => {\n      if (this.pending.length === 0 && this.inflight.size === 0) {\n        this.emit(REQUEST_QUEUE_EMPTY)\n      }\n    }\n\n    this[PRIVATE.EMIT_QUEUE_SIZE_EVENT] = () => {\n      instrumentationEmitter &&\n        instrumentationEmitter.emit(events.NETWORK_REQUEST_QUEUE_SIZE, {\n          broker: this.broker,\n          clientId: this.clientId,\n          queueSize: this.pending.length,\n        })\n\n      this[PRIVATE.EMIT_REQUEST_QUEUE_EMPTY]()\n    }\n  }\n\n  /**\n   * @public\n   */\n  scheduleRequestTimeoutCheck() {\n    if (this.enforceRequestTimeout) {\n      this.destroy()\n\n      this.requestTimeoutIntervalId = setInterval(() => {\n        this.inflight.forEach(request => {\n          if (Date.now() - request.sentAt > request.requestTimeout) {\n            request.timeoutRequest()\n          }\n        })\n\n        if (!this.isConnected()) {\n          this.destroy()\n        }\n      }, Math.min(this.requestTimeout, 100))\n    }\n  }\n\n  maybeThrottle(clientSideThrottleTime) {\n    if (clientSideThrottleTime !== null && clientSideThrottleTime > 0) {\n      this.logger.debug(`Client side throttling in effect for ${clientSideThrottleTime}ms`)\n      const minimumThrottledUntil = Date.now() + clientSideThrottleTime\n      this.throttledUntil = Math.max(minimumThrottledUntil, this.throttledUntil)\n    }\n  }\n\n  createSocketRequest(pushedRequest) {\n    const { correlationId } = pushedRequest.entry\n    const defaultRequestTimeout = this.requestTimeout\n    const customRequestTimeout = pushedRequest.requestTimeout\n\n    // Some protocol requests have custom request timeouts (e.g JoinGroup, Fetch, etc). The custom\n    // timeouts are influenced by user configurations, which can be lower than the default requestTimeout\n    const requestTimeout = Math.max(defaultRequestTimeout, customRequestTimeout || 0)\n\n    const socketRequest = new SocketRequest({\n      entry: pushedRequest.entry,\n      expectResponse: pushedRequest.expectResponse,\n      broker: this.broker,\n      clientId: this.clientId,\n      instrumentationEmitter: this.instrumentationEmitter,\n      requestTimeout,\n      send: () => {\n        if (this.inflight.has(correlationId)) {\n          throw new KafkaJSInvariantViolation('Correlation id already exists')\n        }\n        this.inflight.set(correlationId, socketRequest)\n        pushedRequest.sendRequest()\n      },\n      timeout: () => {\n        this.inflight.delete(correlationId)\n        this.checkPendingRequests()\n        // Try to emit REQUEST_QUEUE_EMPTY. Otherwise, waitForPendingRequests may stuck forever\n        this[PRIVATE.EMIT_REQUEST_QUEUE_EMPTY]()\n      },\n    })\n\n    return socketRequest\n  }\n\n  /**\n   * @typedef {Object} PushedRequest\n   * @property {import(\"./socketRequest\").RequestEntry} entry\n   * @property {boolean} expectResponse\n   * @property {Function} sendRequest\n   * @property {number} [requestTimeout]\n   *\n   * @public\n   * @param {PushedRequest} pushedRequest\n   */\n  push(pushedRequest) {\n    const { correlationId } = pushedRequest.entry\n    const socketRequest = this.createSocketRequest(pushedRequest)\n\n    if (this.canSendSocketRequestImmediately()) {\n      this.sendSocketRequest(socketRequest)\n      return\n    }\n\n    this.pending.push(socketRequest)\n    this.scheduleCheckPendingRequests()\n\n    this.logger.debug(`Request enqueued`, {\n      clientId: this.clientId,\n      broker: this.broker,\n      correlationId,\n    })\n\n    this[PRIVATE.EMIT_QUEUE_SIZE_EVENT]()\n  }\n\n  /**\n   * @param {SocketRequest} socketRequest\n   */\n  sendSocketRequest(socketRequest) {\n    socketRequest.send()\n\n    if (!socketRequest.expectResponse) {\n      this.logger.debug(`Request does not expect a response, resolving immediately`, {\n        clientId: this.clientId,\n        broker: this.broker,\n        correlationId: socketRequest.correlationId,\n      })\n\n      this.inflight.delete(socketRequest.correlationId)\n      socketRequest.completed({ size: 0, payload: null })\n    }\n  }\n\n  /**\n   * @public\n   * @param {object} response\n   * @param {number} response.correlationId\n   * @param {Buffer} response.payload\n   * @param {number} response.size\n   */\n  fulfillRequest({ correlationId, payload, size }) {\n    const socketRequest = this.inflight.get(correlationId)\n    this.inflight.delete(correlationId)\n    this.checkPendingRequests()\n\n    if (socketRequest) {\n      socketRequest.completed({ size, payload })\n    } else {\n      this.logger.warn(`Response without match`, {\n        clientId: this.clientId,\n        broker: this.broker,\n        correlationId,\n      })\n    }\n\n    this[PRIVATE.EMIT_REQUEST_QUEUE_EMPTY]()\n  }\n\n  /**\n   * @public\n   * @param {Error} error\n   */\n  rejectAll(error) {\n    const requests = [...this.inflight.values(), ...this.pending]\n\n    for (const socketRequest of requests) {\n      socketRequest.rejected(error)\n      this.inflight.delete(socketRequest.correlationId)\n    }\n\n    this.pending = []\n    this.inflight.clear()\n    this[PRIVATE.EMIT_QUEUE_SIZE_EVENT]()\n  }\n\n  /**\n   * @public\n   */\n  waitForPendingRequests() {\n    return new Promise(resolve => {\n      if (this.pending.length === 0 && this.inflight.size === 0) {\n        return resolve()\n      }\n\n      this.logger.debug('Waiting for pending requests', {\n        clientId: this.clientId,\n        broker: this.broker,\n        currentInflightRequests: this.inflight.size,\n        currentPendingQueueSize: this.pending.length,\n      })\n\n      this.once(REQUEST_QUEUE_EMPTY, () => resolve())\n    })\n  }\n\n  /**\n   * @public\n   */\n  destroy() {\n    clearInterval(this.requestTimeoutIntervalId)\n    clearTimeout(this.throttleCheckTimeoutId)\n    this.throttleCheckTimeoutId = null\n  }\n\n  canSendSocketRequestImmediately() {\n    const shouldEnqueue =\n      (this.maxInFlightRequests != null && this.inflight.size >= this.maxInFlightRequests) ||\n      this.throttledUntil > Date.now()\n\n    return !shouldEnqueue\n  }\n\n  /**\n   * Check and process pending requests either now or in the future\n   *\n   * This function will send out as many pending requests as possible taking throttling and\n   * in-flight limits into account.\n   */\n  checkPendingRequests() {\n    while (this.pending.length > 0 && this.canSendSocketRequestImmediately()) {\n      const pendingRequest = this.pending.shift() // first in first out\n      this.sendSocketRequest(pendingRequest)\n\n      this.logger.debug(`Consumed pending request`, {\n        clientId: this.clientId,\n        broker: this.broker,\n        correlationId: pendingRequest.correlationId,\n        pendingDuration: pendingRequest.pendingDuration,\n        currentPendingQueueSize: this.pending.length,\n      })\n\n      this[PRIVATE.EMIT_QUEUE_SIZE_EVENT]()\n    }\n\n    this.scheduleCheckPendingRequests()\n  }\n\n  /**\n   * Ensure that pending requests will be checked in the future\n   *\n   * If there is a client-side throttling in place this will ensure that we will check\n   * the pending request queue eventually.\n   */\n  scheduleCheckPendingRequests() {\n    // If we're throttled: Schedule checkPendingRequests when the throttle\n    // should be resolved. If there is already something scheduled we assume that that\n    // will be fine, and potentially fix up a new timeout if needed at that time.\n    // Note that if we're merely \"overloaded\" by having too many inflight requests\n    // we will anyways check the queue when one of them gets fulfilled.\n    let scheduleAt = this.throttledUntil - Date.now()\n    if (!this.throttleCheckTimeoutId) {\n      if (this.pending.length > 0) {\n        scheduleAt = scheduleAt > 0 ? scheduleAt : CHECK_PENDING_REQUESTS_INTERVAL\n      }\n      this.throttleCheckTimeoutId = setTimeout(() => {\n        this.throttleCheckTimeoutId = null\n        this.checkPendingRequests()\n      }, scheduleAt)\n    }\n  }\n}\n", "const CONNECTION_STATUS = {\n  CONNECTED: 'connected',\n  DISCONNECTING: 'disconnecting',\n  DISCONNECTED: 'disconnected',\n}\n\nconst CONNECTED_STATUS = [CONNECTION_STATUS.CONNECTED, CONNECTION_STATUS.DISCONNECTING]\n\nmodule.exports = {\n  CONNECTION_STATUS,\n  CONNECTED_STATUS,\n}\n", "/**\n * http://www.ietf.org/rfc/rfc2595.txt\n *\n * The mechanism consists of a single message from the client to the\n * server.  The client sends the authorization identity (identity to\n * login as), followed by a US-ASCII NUL character, followed by the\n * authentication identity (identity whose password will be used),\n * followed by a US-ASCII NUL character, followed by the clear-text\n * password.  The client may leave the authorization identity empty to\n * indicate that it is the same as the authentication identity.\n *\n * The server will verify the authentication identity and password with\n * the system authentication database and verify that the authentication\n * credentials permit the client to login as the authorization identity.\n * If both steps succeed, the user is logged in.\n */\n\nconst Encoder = require('../../encoder')\n\nconst US_ASCII_NULL_CHAR = '\\u0000'\n\nmodule.exports = ({ authorizationIdentity = null, username, password }) => ({\n  encode: async () => {\n    return new Encoder().writeBytes(\n      [authorizationIdentity, username, password].join(US_ASCII_NULL_CHAR)\n    ).buffer\n  },\n})\n", "module.exports = {\n  decode: async () => true,\n  parse: async () => true,\n}\n", "module.exports = {\n  request: require('./request'),\n  response: require('./response'),\n}\n", "const { request, response } = require('../../protocol/sasl/plain')\nconst { KafkaJSSASLAuthenticationError } = require('../../errors')\n\nconst plainAuthenticatorProvider = sasl => ({ host, port, logger, saslAuthenticate }) => {\n  return {\n    authenticate: async () => {\n      if (sasl.username == null || sasl.password == null) {\n        throw new KafkaJSSASLAuthenticationError('SASL Plain: Invalid username or password')\n      }\n\n      const broker = `${host}:${port}`\n\n      try {\n        logger.debug('Authenticate with SASL PLAIN', { broker })\n        await saslAuthenticate({ request: request(sasl), response })\n        logger.debug('SASL PLAIN authentication successful', { broker })\n      } catch (e) {\n        const error = new KafkaJSSASLAuthenticationError(\n          `SASL PLAIN authentication failed: ${e.message}`\n        )\n        logger.error(error.message, { broker })\n        throw error\n      }\n    },\n  }\n}\n\nmodule.exports = plainAuthenticatorProvider\n", "/**\n * https://tools.ietf.org/html/rfc5802\n *\n * First, the client sends the \"client-first-message\" containing:\n *\n *  -> a GS2 header consisting of a flag indicating whether channel\n * binding is supported-but-not-used, not supported, or used, and an\n * optional SASL authorization identity;\n *\n *  -> SCRAM username and a random, unique nonce attributes.\n *\n * Note that the client's first message will always start with \"n\", \"y\",\n * or \"p\"; otherwise, the message is invalid and authentication MUST\n * fail.  This is important, as it allows for GS2 extensibility (e.g.,\n * to add support for security layers).\n */\n\nconst Encoder = require('../../../encoder')\n\nmodule.exports = ({ clientFirstMessage }) => ({\n  encode: async () => new Encoder().writeBytes(clientFirstMessage).buffer,\n})\n", "/* eslint no-unused-vars: [\"error\", { \"varsIgnorePattern\": \"_\" }] */\n\nconst Decoder = require('../../../decoder')\n\nconst ENTRY_REGEX = /^([rsiev])=(.*)$/\n\nmodule.exports = {\n  decode: async rawData => {\n    return new Decoder(rawData).readBytes()\n  },\n  parse: async data => {\n    const processed = data\n      .toString()\n      .split(',')\n      .map(str => {\n        const [_, key, value] = str.match(ENTRY_REGEX)\n        return [key, value]\n      })\n      .reduce((obj, entry) => ({ ...obj, [entry[0]]: entry[1] }), {})\n\n    return { original: data.toString(), ...processed }\n  },\n}\n", "const Encoder = require('../../../encoder')\n\nmodule.exports = ({ finalMessage }) => ({\n  encode: async () => new Encoder().writeBytes(finalMessage).buffer,\n})\n", "module.exports = require('../firstMessage/response')\n", "module.exports = {\n  firstMessage: {\n    request: require('./firstMessage/request'),\n    response: require('./firstMessage/response'),\n  },\n  finalMessage: {\n    request: require('./finalMessage/request'),\n    response: require('./finalMessage/response'),\n  },\n}\n", "const crypto = require('crypto')\nconst scram = require('../../protocol/sasl/scram')\nconst { KafkaJSSASLAuthenticationError, KafkaJSNonRetriableError } = require('../../errors')\n\nconst GS2_HEADER = 'n,,'\n\nconst EQUAL_SIGN_REGEX = /=/g\nconst COMMA_SIGN_REGEX = /,/g\n\nconst URLSAFE_BASE64_PLUS_REGEX = /\\+/g\nconst URLSAFE_BASE64_SLASH_REGEX = /\\//g\nconst URLSAFE_BASE64_TRAILING_EQUAL_REGEX = /=+$/\n\nconst HMAC_CLIENT_KEY = 'Client Key'\nconst HMAC_SERVER_KEY = 'Server Key'\n\nconst DIGESTS = {\n  SHA256: {\n    length: 32,\n    type: 'sha256',\n    minIterations: 4096,\n  },\n  SHA512: {\n    length: 64,\n    type: 'sha512',\n    minIterations: 4096,\n  },\n}\n\nconst encode64 = str => Buffer.from(str).toString('base64')\n\nclass SCRAM {\n  /**\n   * From https://tools.ietf.org/html/rfc5802#section-5.1\n   *\n   * The characters ',' or '=' in usernames are sent as '=2C' and\n   * '=3D' respectively.  If the server receives a username that\n   * contains '=' not followed by either '2C' or '3D', then the\n   * server MUST fail the authentication.\n   *\n   * @returns {String}\n   */\n  static sanitizeString(str) {\n    return str.replace(EQUAL_SIGN_REGEX, '=3D').replace(COMMA_SIGN_REGEX, '=2C')\n  }\n\n  /**\n   * In cryptography, a nonce is an arbitrary number that can be used just once.\n   * It is similar in spirit to a nonce * word, hence the name. It is often a random or pseudo-random\n   * number issued in an authentication protocol to * ensure that old communications cannot be reused\n   * in replay attacks.\n   *\n   * @returns {String}\n   */\n  static nonce() {\n    return crypto\n      .randomBytes(16)\n      .toString('base64')\n      .replace(URLSAFE_BASE64_PLUS_REGEX, '-') // make it url safe\n      .replace(URLSAFE_BASE64_SLASH_REGEX, '_')\n      .replace(URLSAFE_BASE64_TRAILING_EQUAL_REGEX, '')\n      .toString('ascii')\n  }\n\n  /**\n   * Hi() is, essentially, PBKDF2 [RFC2898] with HMAC() as the\n   * pseudorandom function (PRF) and with dkLen == output length of\n   * HMAC() == output length of H()\n   *\n   * @returns {Promise<Buffer>}\n   */\n  static hi(password, salt, iterations, digestDefinition) {\n    return new Promise((resolve, reject) => {\n      crypto.pbkdf2(\n        password,\n        salt,\n        iterations,\n        digestDefinition.length,\n        digestDefinition.type,\n        (err, derivedKey) => (err ? reject(err) : resolve(derivedKey))\n      )\n    })\n  }\n\n  /**\n   * Apply the exclusive-or operation to combine the octet string\n   * on the left of this operator with the octet string on the right of\n   * this operator.  The length of the output and each of the two\n   * inputs will be the same for this use\n   *\n   * @returns {Buffer}\n   */\n  static xor(left, right) {\n    const bufferA = Buffer.from(left)\n    const bufferB = Buffer.from(right)\n    const length = Buffer.byteLength(bufferA)\n\n    if (length !== Buffer.byteLength(bufferB)) {\n      throw new KafkaJSNonRetriableError('Buffers must be of the same length')\n    }\n\n    const result = []\n    for (let i = 0; i < length; i++) {\n      result.push(bufferA[i] ^ bufferB[i])\n    }\n\n    return Buffer.from(result)\n  }\n\n  /**\n   * @param {SASLOptions} sasl\n   * @param {Logger} logger\n   * @param {Function} saslAuthenticate\n   * @param {DigestDefinition} digestDefinition\n   */\n  constructor(sasl, host, port, logger, saslAuthenticate, digestDefinition) {\n    this.sasl = sasl\n    this.host = host\n    this.port = port\n    this.logger = logger\n    this.saslAuthenticate = saslAuthenticate\n    this.digestDefinition = digestDefinition\n\n    const digestType = digestDefinition.type.toUpperCase()\n    this.PREFIX = `SASL SCRAM ${digestType} authentication`\n\n    this.currentNonce = SCRAM.nonce()\n  }\n\n  async authenticate() {\n    const { PREFIX } = this\n    const broker = `${this.host}:${this.port}`\n\n    if (this.sasl.username == null || this.sasl.password == null) {\n      throw new KafkaJSSASLAuthenticationError(`${this.PREFIX}: Invalid username or password`)\n    }\n\n    try {\n      this.logger.debug('Exchanging first client message', { broker })\n      const clientMessageResponse = await this.sendClientFirstMessage()\n\n      this.logger.debug('Sending final message', { broker })\n      const finalResponse = await this.sendClientFinalMessage(clientMessageResponse)\n\n      if (finalResponse.e) {\n        throw new Error(finalResponse.e)\n      }\n\n      const serverKey = await this.serverKey(clientMessageResponse)\n      const serverSignature = this.serverSignature(serverKey, clientMessageResponse)\n\n      if (finalResponse.v !== serverSignature) {\n        throw new Error('Invalid server signature in server final message')\n      }\n\n      this.logger.debug(`${PREFIX} successful`, { broker })\n    } catch (e) {\n      const error = new KafkaJSSASLAuthenticationError(`${PREFIX} failed: ${e.message}`)\n      this.logger.error(error.message, { broker })\n      throw error\n    }\n  }\n\n  /**\n   * @private\n   */\n  async sendClientFirstMessage() {\n    const clientFirstMessage = `${GS2_HEADER}${this.firstMessageBare()}`\n    const request = scram.firstMessage.request({ clientFirstMessage })\n    const response = scram.firstMessage.response\n\n    return this.saslAuthenticate({\n      request,\n      response,\n    })\n  }\n\n  /**\n   * @private\n   */\n  async sendClientFinalMessage(clientMessageResponse) {\n    const { PREFIX } = this\n    const iterations = parseInt(clientMessageResponse.i, 10)\n    const { minIterations } = this.digestDefinition\n\n    if (!clientMessageResponse.r.startsWith(this.currentNonce)) {\n      throw new KafkaJSSASLAuthenticationError(\n        `${PREFIX} failed: Invalid server nonce, it does not start with the client nonce`\n      )\n    }\n\n    if (iterations < minIterations) {\n      throw new KafkaJSSASLAuthenticationError(\n        `${PREFIX} failed: Requested iterations ${iterations} is less than the minimum ${minIterations}`\n      )\n    }\n\n    const finalMessageWithoutProof = this.finalMessageWithoutProof(clientMessageResponse)\n    const clientProof = await this.clientProof(clientMessageResponse)\n    const finalMessage = `${finalMessageWithoutProof},p=${clientProof}`\n    const request = scram.finalMessage.request({ finalMessage })\n    const response = scram.finalMessage.response\n\n    return this.saslAuthenticate({\n      request,\n      response,\n    })\n  }\n\n  /**\n   * @private\n   */\n  async clientProof(clientMessageResponse) {\n    const clientKey = await this.clientKey(clientMessageResponse)\n    const storedKey = this.H(clientKey)\n    const clientSignature = this.clientSignature(storedKey, clientMessageResponse)\n    return encode64(SCRAM.xor(clientKey, clientSignature))\n  }\n\n  /**\n   * @private\n   */\n  async clientKey(clientMessageResponse) {\n    const saltedPassword = await this.saltPassword(clientMessageResponse)\n    return this.HMAC(saltedPassword, HMAC_CLIENT_KEY)\n  }\n\n  /**\n   * @private\n   */\n  async serverKey(clientMessageResponse) {\n    const saltedPassword = await this.saltPassword(clientMessageResponse)\n    return this.HMAC(saltedPassword, HMAC_SERVER_KEY)\n  }\n\n  /**\n   * @private\n   */\n  clientSignature(storedKey, clientMessageResponse) {\n    return this.HMAC(storedKey, this.authMessage(clientMessageResponse))\n  }\n\n  /**\n   * @private\n   */\n  serverSignature(serverKey, clientMessageResponse) {\n    return encode64(this.HMAC(serverKey, this.authMessage(clientMessageResponse)))\n  }\n\n  /**\n   * @private\n   */\n  authMessage(clientMessageResponse) {\n    return [\n      this.firstMessageBare(),\n      clientMessageResponse.original,\n      this.finalMessageWithoutProof(clientMessageResponse),\n    ].join(',')\n  }\n\n  /**\n   * @private\n   */\n  async saltPassword(clientMessageResponse) {\n    const salt = Buffer.from(clientMessageResponse.s, 'base64')\n    const iterations = parseInt(clientMessageResponse.i, 10)\n    return SCRAM.hi(this.encodedPassword(), salt, iterations, this.digestDefinition)\n  }\n\n  /**\n   * @private\n   */\n  firstMessageBare() {\n    return `n=${this.encodedUsername()},r=${this.currentNonce}`\n  }\n\n  /**\n   * @private\n   */\n  finalMessageWithoutProof(clientMessageResponse) {\n    const rnonce = clientMessageResponse.r\n    return `c=${encode64(GS2_HEADER)},r=${rnonce}`\n  }\n\n  /**\n   * @private\n   */\n  encodedUsername() {\n    const { username } = this.sasl\n    return SCRAM.sanitizeString(username).toString('utf-8')\n  }\n\n  /**\n   * @private\n   */\n  encodedPassword() {\n    const { password } = this.sasl\n    return password.toString('utf-8')\n  }\n\n  /**\n   * @private\n   */\n  H(data) {\n    return crypto\n      .createHash(this.digestDefinition.type)\n      .update(data)\n      .digest()\n  }\n\n  /**\n   * @private\n   */\n  HMAC(key, data) {\n    return crypto\n      .createHmac(this.digestDefinition.type, key)\n      .update(data)\n      .digest()\n  }\n}\n\nmodule.exports = {\n  DIGESTS,\n  SCRAM,\n}\n", "const { SCRAM, DIGESTS } = require('./scram')\n\nconst scram256AuthenticatorProvider = sasl => ({ host, port, logger, saslAuthenticate }) => {\n  const scram = new SCRAM(sasl, host, port, logger, saslAuthenticate, DIGESTS.SHA256)\n  return {\n    authenticate: async () => await scram.authenticate(),\n  }\n}\n\nmodule.exports = scram256AuthenticatorProvider\n", "const { SCRAM, DIGESTS } = require('./scram')\n\nconst scram512AuthenticatorProvider = sasl => ({ host, port, logger, saslAuthenticate }) => {\n  const scram = new SCRAM(sasl, host, port, logger, saslAuthenticate, DIGESTS.SHA512)\n  return {\n    authenticate: async () => await scram.authenticate(),\n  }\n}\n\nmodule.exports = scram512AuthenticatorProvider\n", "const Encoder = require('../../encoder')\n\nconst US_ASCII_NULL_CHAR = '\\u0000'\n\nmodule.exports = ({ authorizationIdentity, accessKeyId, secretAccessKey, sessionToken = '' }) => ({\n  encode: async () => {\n    return new Encoder().writeBytes(\n      [authorizationIdentity, accessKeyId, secretAccessKey, sessionToken].join(US_ASCII_NULL_CHAR)\n    ).buffer\n  },\n})\n", "module.exports = {\n  decode: async () => true,\n  parse: async () => true,\n}\n", "module.exports = {\n  request: require('./request'),\n  response: require('./response'),\n}\n", "const { request, response } = require('../../protocol/sasl/awsIam')\nconst { KafkaJSSASLAuthenticationError } = require('../../errors')\n\nconst awsIAMAuthenticatorProvider = sasl => ({ host, port, logger, saslAuthenticate }) => {\n  return {\n    authenticate: async () => {\n      if (!sasl.authorizationIdentity) {\n        throw new KafkaJSSASLAuthenticationError('SASL AWS-IAM: Missing authorizationIdentity')\n      }\n      if (!sasl.accessKeyId) {\n        throw new KafkaJSSASLAuthenticationError('SASL AWS-IAM: Missing accessKeyId')\n      }\n      if (!sasl.secretAccessKey) {\n        throw new KafkaJSSASLAuthenticationError('SASL AWS-IAM: Missing secretAccessKey')\n      }\n      if (!sasl.sessionToken) {\n        sasl.sessionToken = ''\n      }\n\n      const broker = `${host}:${port}`\n\n      try {\n        logger.debug('Authenticate with SASL AWS-IAM', { broker })\n        await saslAuthenticate({ request: request(sasl), response })\n        logger.debug('SASL AWS-IAM authentication successful', { broker })\n      } catch (e) {\n        const error = new KafkaJSSASLAuthenticationError(\n          `SASL AWS-IAM authentication failed: ${e.message}`\n        )\n        logger.error(error.message, { broker })\n        throw error\n      }\n    },\n  }\n}\n\nmodule.exports = awsIAMAuthenticatorProvider\n", "/**\n * http://www.ietf.org/rfc/rfc5801.txt\n *\n * See org.apache.kafka.common.security.oauthbearer.internals.OAuthBearerClientInitialResponse\n * for official Java client implementation.\n *\n * The mechanism consists of a message from the client to the server.\n * The client sends the \"n,\"\" GS header, followed by the authorizationIdentitty\n * prefixed by \"a=\" (if present), followed by \",\", followed by a US-ASCII SOH\n * character, followed by \"auth=Bearer \", followed by the token value, followed\n * by US-ASCII SOH character, followed by SASL extensions in OAuth \"friendly\"\n * format and then closed by two additionals US-ASCII SOH characters.\n *\n * SASL extensions are optional an must be expressed as key-value pairs in an\n * object. Each expression is converted as, the extension entry key, followed\n * by \"=\", followed by extension entry value. Each extension is separated by a\n * US-ASCII SOH character. If extensions are not present, their relative part\n * in the message, including the US-ASCII SOH character, is omitted.\n *\n * The client may leave the authorization identity empty to\n * indicate that it is the same as the authentication identity.\n *\n * The server will verify the authentication token and verify that the\n * authentication credentials permit the client to login as the authorization\n * identity. If both steps succeed, the user is logged in.\n */\n\nconst Encoder = require('../../encoder')\n\nconst SEPARATOR = '\\u0001' // SOH - Start Of Header ASCII\n\nfunction formatExtensions(extensions) {\n  let msg = ''\n\n  if (extensions == null) {\n    return msg\n  }\n\n  let prefix = ''\n  for (const k in extensions) {\n    msg += `${prefix}${k}=${extensions[k]}`\n    prefix = SEPARATOR\n  }\n\n  return msg\n}\n\nmodule.exports = async ({ authorizationIdentity = null }, oauthBearerToken) => {\n  const authzid = authorizationIdentity == null ? '' : `\"a=${authorizationIdentity}`\n  let ext = formatExtensions(oauthBearerToken.extensions)\n  if (ext.length > 0) {\n    ext = `${SEPARATOR}${ext}`\n  }\n\n  const oauthMsg = `n,${authzid},${SEPARATOR}auth=Bearer ${oauthBearerToken.value}${ext}${SEPARATOR}${SEPARATOR}`\n\n  return {\n    encode: async () => {\n      return new Encoder().writeBytes(Buffer.from(oauthMsg)).buffer\n    },\n  }\n}\n", "module.exports = {\n  decode: async () => true,\n  parse: async () => true,\n}\n", "module.exports = {\n  request: require('./request'),\n  response: require('./response'),\n}\n", "/**\n * The sasl object must include a property named oauthBearerProvider, an\n * async function that is used to return the OAuth bearer token.\n *\n * The OAuth bearer token must be an object with properties value and\n * (optionally) extensions, that will be sent during the SASL/OAUTHBEARER\n * request.\n *\n * The implementation of the oauthBearerProvider must take care that tokens are\n * reused and refreshed when appropriate.\n */\n\nconst { request } = require('../../protocol/sasl/oauthBearer')\nconst { KafkaJSSASLAuthenticationError } = require('../../errors')\n\nconst oauthBearerAuthenticatorProvider = sasl => ({ host, port, logger, saslAuthenticate }) => {\n  return {\n    authenticate: async () => {\n      const { oauthBearerProvider } = sasl\n\n      if (oauthBearerProvider == null) {\n        throw new KafkaJSSASLAuthenticationError(\n          'SASL OAUTHBEARER: Missing OAuth bearer token provider'\n        )\n      }\n\n      const oauthBearerToken = await oauthBearerProvider()\n\n      if (oauthBearerToken.value == null) {\n        throw new KafkaJSSASLAuthenticationError('SASL OAUTHBEARER: Invalid OAuth bearer token')\n      }\n\n      const broker = `${host}:${port}`\n\n      try {\n        logger.debug('Authenticate with SASL OAUTHBEARER', { broker })\n        await saslAuthenticate({ request: await request(sasl, oauthBearerToken) })\n        logger.debug('SASL OAUTHBEARER authentication successful', { broker })\n      } catch (e) {\n        const error = new KafkaJSSASLAuthenticationError(\n          `SASL OAUTHBEARER authentication failed: ${e.message}`\n        )\n        logger.error(error.message, { broker })\n        throw error\n      }\n    },\n  }\n}\n\nmodule.exports = oauthBearerAuthenticatorProvider\n", "const { requests, lookup } = require('../../protocol/requests')\nconst apiKeys = require('../../protocol/requests/apiKeys')\nconst plainAuthenticatorProvider = require('./plain')\nconst scram256AuthenticatorProvider = require('./scram256')\nconst scram512AuthenticatorProvider = require('./scram512')\nconst awsIAMAuthenticatorProvider = require('./awsIam')\nconst oauthBearerAuthenticatorProvider = require('./oauthBearer')\nconst { KafkaJSSASLAuthenticationError } = require('../../errors')\n\nconst BUILT_IN_AUTHENTICATION_PROVIDERS = {\n  AWS: awsIAMAuthenticatorProvider,\n  PLAIN: plainAuthenticatorProvider,\n  OAUTHBEARER: oauthBearerAuthenticatorProvider,\n  'SCRAM-SHA-256': scram256AuthenticatorProvider,\n  'SCRAM-SHA-512': scram512AuthenticatorProvider,\n}\n\nconst UNLIMITED_SESSION_LIFETIME = '0'\n\nmodule.exports = class SASLAuthenticator {\n  constructor(connection, logger, versions, supportAuthenticationProtocol) {\n    this.connection = connection\n    this.logger = logger\n    this.sessionLifetime = UNLIMITED_SESSION_LIFETIME\n\n    const lookupRequest = lookup(versions)\n    this.saslHandshake = lookupRequest(apiKeys.SaslHandshake, requests.SaslHandshake)\n    this.protocolAuthentication = supportAuthenticationProtocol\n      ? lookupRequest(apiKeys.SaslAuthenticate, requests.SaslAuthenticate)\n      : null\n  }\n\n  async authenticate() {\n    const mechanism = this.connection.sasl.mechanism.toUpperCase()\n    const handshake = await this.connection.send(this.saslHandshake({ mechanism }))\n    if (!handshake.enabledMechanisms.includes(mechanism)) {\n      throw new KafkaJSSASLAuthenticationError(\n        `SASL ${mechanism} mechanism is not supported by the server`\n      )\n    }\n\n    const saslAuthenticate = async ({ request, response }) => {\n      if (this.protocolAuthentication) {\n        const requestAuthBytes = await request.encode()\n        const authResponse = await this.connection.send(\n          this.protocolAuthentication({ authBytes: requestAuthBytes })\n        )\n\n        // `0` is a string because `sessionLifetimeMs` is an int64 encoded as string.\n        // This is not present in SaslAuthenticateV0, so we default to `\"0\"`\n        this.sessionLifetime = authResponse.sessionLifetimeMs || UNLIMITED_SESSION_LIFETIME\n\n        if (!response) {\n          return\n        }\n\n        const { authBytes: responseAuthBytes } = authResponse\n        const payloadDecoded = await response.decode(responseAuthBytes)\n        return response.parse(payloadDecoded)\n      }\n\n      return this.connection.sendAuthRequest({ request, response })\n    }\n\n    if (\n      !this.connection.sasl.authenticationProvider &&\n      Object.keys(BUILT_IN_AUTHENTICATION_PROVIDERS).includes(mechanism)\n    ) {\n      this.connection.sasl.authenticationProvider = BUILT_IN_AUTHENTICATION_PROVIDERS[mechanism](\n        this.connection.sasl\n      )\n    }\n    await this.connection.sasl\n      .authenticationProvider({\n        host: this.connection.host,\n        port: this.connection.port,\n        logger: this.logger.namespace(`SaslAuthenticator-${mechanism}`),\n        saslAuthenticate,\n      })\n      .authenticate()\n  }\n}\n", "const createSocket = require('./socket')\nconst createRequest = require('../protocol/request')\nconst Decoder = require('../protocol/decoder')\nconst { KafkaJSConnectionError, KafkaJSConnectionClosedError } = require('../errors')\nconst { INT_32_MAX_VALUE } = require('../constants')\nconst getEnv = require('../env')\nconst RequestQueue = require('./requestQueue')\nconst { CONNECTION_STATUS, CONNECTED_STATUS } = require('./connectionStatus')\nconst sharedPromiseTo = require('../utils/sharedPromiseTo')\nconst Long = require('../utils/long')\nconst SASLAuthenticator = require('../broker/saslAuthenticator')\nconst apiKeys = require('../protocol/requests/apiKeys')\n\nconst requestInfo = ({ apiName, apiKey, apiVersion }) =>\n  `${apiName}(key: ${apiKey}, version: ${apiVersion})`\n\n/**\n * @param request - request from protocol\n * @returns {boolean}\n */\nconst isAuthenticatedRequest = request => {\n  return ![apiKeys.ApiVersions, apiKeys.SaslHandshake, apiKeys.SaslAuthenticate].includes(\n    request.apiKey\n  )\n}\n\nconst PRIVATE = {\n  SHOULD_REAUTHENTICATE: Symbol('private:Connection:shouldReauthenticate'),\n  AUTHENTICATE: Symbol('private:Connection:authenticate'),\n}\n\nmodule.exports = class Connection {\n  /**\n   * @param {Object} options\n   * @param {string} options.host\n   * @param {number} options.port\n   * @param {import(\"../../types\").Logger} options.logger\n   * @param {import(\"../../types\").ISocketFactory} options.socketFactory\n   * @param {string} [options.clientId='kafkajs']\n   * @param {number} options.requestTimeout The maximum amount of time the client will wait for the response of a request,\n   *                                in milliseconds\n   * @param {string} [options.rack=null]\n   * @param {Object} [options.ssl=null] Options for the TLS Secure Context. It accepts all options,\n   *                            usually \"cert\", \"key\" and \"ca\". More information at\n   *                            https://nodejs.org/api/tls.html#tls_tls_createsecurecontext_options\n   * @param {Object} [options.sasl=null] Attributes used for SASL authentication. Options based on the\n   *                             key \"mechanism\". Connection is not actively using the SASL attributes\n   *                             but acting as a data object for this information\n   * @param {number} [options.reauthenticationThreshold=10000]\n   * @param {number} options.connectionTimeout The connection timeout, in milliseconds\n   * @param {boolean} [options.enforceRequestTimeout]\n   * @param {number} [options.maxInFlightRequests=null] The maximum number of unacknowledged requests on a connection before\n   *                                            enqueuing\n   * @param {import(\"../instrumentation/emitter\")} [options.instrumentationEmitter=null]\n   */\n  constructor({\n    host,\n    port,\n    logger,\n    socketFactory,\n    requestTimeout,\n    reauthenticationThreshold = 10000,\n    rack = null,\n    ssl = null,\n    sasl = null,\n    clientId = 'kafkajs',\n    connectionTimeout,\n    enforceRequestTimeout = true,\n    maxInFlightRequests = null,\n    instrumentationEmitter = null,\n  }) {\n    this.host = host\n    this.port = port\n    this.rack = rack\n    this.clientId = clientId\n    this.broker = `${this.host}:${this.port}`\n    this.logger = logger.namespace('Connection')\n\n    this.socketFactory = socketFactory\n    this.ssl = ssl\n    this.sasl = sasl\n\n    this.requestTimeout = requestTimeout\n    this.connectionTimeout = connectionTimeout\n    this.reauthenticationThreshold = reauthenticationThreshold\n\n    this.bytesBuffered = 0\n    this.bytesNeeded = Decoder.int32Size()\n    this.chunks = []\n\n    this.connectionStatus = CONNECTION_STATUS.DISCONNECTED\n    this.correlationId = 0\n    this.requestQueue = new RequestQueue({\n      instrumentationEmitter,\n      maxInFlightRequests,\n      requestTimeout,\n      enforceRequestTimeout,\n      clientId,\n      broker: this.broker,\n      logger: logger.namespace('RequestQueue'),\n      isConnected: () => this.isConnected(),\n    })\n\n    this.versions = null\n\n    this.authHandlers = null\n    this.authExpectResponse = false\n\n    const log = level => (message, extra = {}) => {\n      const logFn = this.logger[level]\n      logFn(message, { broker: this.broker, clientId, ...extra })\n    }\n\n    this.logDebug = log('debug')\n    this.logError = log('error')\n\n    const env = getEnv()\n    this.shouldLogBuffers = env.KAFKAJS_DEBUG_PROTOCOL_BUFFERS === '1'\n    this.shouldLogFetchBuffer =\n      this.shouldLogBuffers && env.KAFKAJS_DEBUG_EXTENDED_PROTOCOL_BUFFERS === '1'\n\n    this.authenticatedAt = null\n    this.sessionLifetime = Long.ZERO\n    this.supportAuthenticationProtocol = null\n\n    /**\n     * @private\n     * @returns {Promise}\n     */\n    this[PRIVATE.AUTHENTICATE] = sharedPromiseTo(async () => {\n      if (this.sasl && !this.isAuthenticated()) {\n        const authenticator = new SASLAuthenticator(\n          this,\n          this.logger,\n          this.versions,\n          this.supportAuthenticationProtocol\n        )\n\n        await authenticator.authenticate()\n        this.authenticatedAt = process.hrtime()\n        this.sessionLifetime = Long.fromValue(authenticator.sessionLifetime)\n      }\n    })\n  }\n\n  getSupportAuthenticationProtocol() {\n    return this.supportAuthenticationProtocol\n  }\n\n  setSupportAuthenticationProtocol(isSupported) {\n    this.supportAuthenticationProtocol = isSupported\n  }\n\n  setVersions(versions) {\n    this.versions = versions\n  }\n\n  isConnected() {\n    return CONNECTED_STATUS.includes(this.connectionStatus)\n  }\n\n  /**\n   * @public\n   * @returns {Promise}\n   */\n  connect() {\n    return new Promise((resolve, reject) => {\n      if (this.isConnected()) {\n        return resolve(true)\n      }\n\n      this.authenticatedAt = null\n\n      let timeoutId\n\n      const onConnect = () => {\n        clearTimeout(timeoutId)\n        this.connectionStatus = CONNECTION_STATUS.CONNECTED\n        this.requestQueue.scheduleRequestTimeoutCheck()\n        resolve(true)\n      }\n\n      const onData = data => {\n        this.processData(data)\n      }\n\n      const onEnd = async () => {\n        clearTimeout(timeoutId)\n\n        const wasConnected = this.isConnected()\n\n        if (this.authHandlers) {\n          this.authHandlers.onError()\n        } else if (wasConnected) {\n          this.logDebug('Kafka server has closed connection')\n          this.rejectRequests(\n            new KafkaJSConnectionClosedError('Closed connection', {\n              host: this.host,\n              port: this.port,\n            })\n          )\n        }\n\n        await this.disconnect()\n      }\n\n      const onError = async e => {\n        clearTimeout(timeoutId)\n\n        const error = new KafkaJSConnectionError(`Connection error: ${e.message}`, {\n          broker: `${this.host}:${this.port}`,\n          code: e.code,\n        })\n\n        this.logError(error.message, { stack: e.stack })\n        this.rejectRequests(error)\n        await this.disconnect()\n\n        reject(error)\n      }\n\n      const onTimeout = async () => {\n        const error = new KafkaJSConnectionError('Connection timeout', {\n          broker: `${this.host}:${this.port}`,\n        })\n\n        this.logError(error.message)\n        this.rejectRequests(error)\n        await this.disconnect()\n        reject(error)\n      }\n\n      this.logDebug(`Connecting`, {\n        ssl: !!this.ssl,\n        sasl: !!this.sasl,\n      })\n\n      try {\n        timeoutId = setTimeout(onTimeout, this.connectionTimeout)\n        this.socket = createSocket({\n          socketFactory: this.socketFactory,\n          host: this.host,\n          port: this.port,\n          ssl: this.ssl,\n          onConnect,\n          onData,\n          onEnd,\n          onError,\n          onTimeout,\n        })\n      } catch (e) {\n        clearTimeout(timeoutId)\n        reject(\n          new KafkaJSConnectionError(`Failed to connect: ${e.message}`, {\n            broker: `${this.host}:${this.port}`,\n          })\n        )\n      }\n    })\n  }\n\n  /**\n   * @public\n   * @returns {Promise}\n   */\n  async disconnect() {\n    this.authenticatedAt = null\n    this.connectionStatus = CONNECTION_STATUS.DISCONNECTING\n    this.logDebug('disconnecting...')\n\n    await this.requestQueue.waitForPendingRequests()\n    this.requestQueue.destroy()\n\n    if (this.socket) {\n      this.socket.end()\n      this.socket.unref()\n    }\n\n    this.connectionStatus = CONNECTION_STATUS.DISCONNECTED\n    this.logDebug('disconnected')\n    return true\n  }\n\n  /**\n   * @public\n   * @returns {boolean}\n   */\n  isAuthenticated() {\n    return this.authenticatedAt != null && !this[PRIVATE.SHOULD_REAUTHENTICATE]()\n  }\n\n  /***\n   * @private\n   */\n  [PRIVATE.SHOULD_REAUTHENTICATE]() {\n    if (this.sessionLifetime.equals(Long.ZERO)) {\n      return false\n    }\n\n    if (this.authenticatedAt == null) {\n      return true\n    }\n\n    const [secondsSince, remainingNanosSince] = process.hrtime(this.authenticatedAt)\n    const millisSince = Long.fromValue(secondsSince)\n      .multiply(1000)\n      .add(Long.fromValue(remainingNanosSince).divide(1000000))\n\n    const reauthenticateAt = millisSince.add(this.reauthenticationThreshold)\n    return reauthenticateAt.greaterThanOrEqual(this.sessionLifetime)\n  }\n\n  /** @public */\n  async authenticate() {\n    await this[PRIVATE.AUTHENTICATE]()\n  }\n\n  /**\n   * @public\n   * @returns {Promise}\n   */\n  sendAuthRequest({ request, response }) {\n    this.authExpectResponse = !!response\n\n    /**\n     * TODO: rewrite removing the async promise executor\n     */\n\n    /* eslint-disable no-async-promise-executor */\n    return new Promise(async (resolve, reject) => {\n      this.authHandlers = {\n        onSuccess: rawData => {\n          this.authHandlers = null\n          this.authExpectResponse = false\n\n          response\n            .decode(rawData)\n            .then(data => response.parse(data))\n            .then(resolve)\n            .catch(reject)\n        },\n        onError: () => {\n          this.authHandlers = null\n          this.authExpectResponse = false\n\n          reject(\n            new KafkaJSConnectionError('Connection closed by the server', {\n              broker: `${this.host}:${this.port}`,\n            })\n          )\n        },\n      }\n\n      try {\n        const requestPayload = await request.encode()\n\n        this.failIfNotConnected()\n        this.socket.write(requestPayload, 'binary')\n      } catch (e) {\n        reject(e)\n      }\n    })\n  }\n\n  /**\n   * @public\n   * @param {object} protocol\n   * @param {object} protocol.request It is defined by the protocol and consists of an object with \"apiKey\",\n   *                         \"apiVersion\", \"apiName\" and an \"encode\" function. The encode function\n   *                         must return an instance of Encoder\n   *\n   * @param {object} protocol.response It is defined by the protocol and consists of an object with two functions:\n   *                          \"decode\" and \"parse\"\n   *\n   * @param {number} [protocol.requestTimeout=null] Override for the default requestTimeout\n   * @param {boolean} [protocol.logResponseError=true] Whether to log errors\n   * @returns {Promise<data>} where data is the return of \"response#parse\"\n   */\n  async send({ request, response, requestTimeout = null, logResponseError = true }) {\n    if (!this.isAuthenticated() && isAuthenticatedRequest(request)) {\n      await this[PRIVATE.AUTHENTICATE]()\n    }\n\n    this.failIfNotConnected()\n\n    const expectResponse = !request.expectResponse || request.expectResponse()\n    const sendRequest = async () => {\n      const { clientId } = this\n      const correlationId = this.nextCorrelationId()\n\n      const requestPayload = await createRequest({ request, correlationId, clientId })\n      const { apiKey, apiName, apiVersion } = request\n      this.logDebug(`Request ${requestInfo(request)}`, {\n        correlationId,\n        expectResponse,\n        size: Buffer.byteLength(requestPayload.buffer),\n      })\n\n      return new Promise((resolve, reject) => {\n        try {\n          this.failIfNotConnected()\n          const entry = { apiKey, apiName, apiVersion, correlationId, resolve, reject }\n\n          this.requestQueue.push({\n            entry,\n            expectResponse,\n            requestTimeout,\n            sendRequest: () => {\n              this.socket.write(requestPayload.buffer, 'binary')\n            },\n          })\n        } catch (e) {\n          reject(e)\n        }\n      })\n    }\n\n    const { correlationId, size, entry, payload } = await sendRequest()\n\n    if (!expectResponse) {\n      return\n    }\n\n    try {\n      const payloadDecoded = await response.decode(payload)\n\n      /**\n       * @see KIP-219\n       * If the response indicates that the client-side needs to throttle, do that.\n       */\n      this.requestQueue.maybeThrottle(payloadDecoded.clientSideThrottleTime)\n\n      const data = await response.parse(payloadDecoded)\n      const isFetchApi = entry.apiName === 'Fetch'\n      this.logDebug(`Response ${requestInfo(entry)}`, {\n        correlationId,\n        size,\n        data: isFetchApi && !this.shouldLogFetchBuffer ? '[filtered]' : data,\n      })\n\n      return data\n    } catch (e) {\n      if (logResponseError) {\n        this.logError(`Response ${requestInfo(entry)}`, {\n          error: e.message,\n          correlationId,\n          size,\n        })\n      }\n\n      const isBuffer = Buffer.isBuffer(payload)\n      this.logDebug(`Response ${requestInfo(entry)}`, {\n        error: e.message,\n        correlationId,\n        payload:\n          isBuffer && !this.shouldLogBuffers ? { type: 'Buffer', data: '[filtered]' } : payload,\n      })\n\n      throw e\n    }\n  }\n\n  /**\n   * @private\n   */\n  failIfNotConnected() {\n    if (!this.isConnected()) {\n      throw new KafkaJSConnectionError('Not connected', {\n        broker: `${this.host}:${this.port}`,\n      })\n    }\n  }\n\n  /**\n   * @private\n   */\n  nextCorrelationId() {\n    if (this.correlationId >= INT_32_MAX_VALUE) {\n      this.correlationId = 0\n    }\n\n    return this.correlationId++\n  }\n\n  /**\n   * @private\n   */\n  processData(rawData) {\n    if (this.authHandlers && !this.authExpectResponse) {\n      return this.authHandlers.onSuccess(rawData)\n    }\n\n    // Accumulate the new chunk\n    this.chunks.push(rawData)\n    this.bytesBuffered += Buffer.byteLength(rawData)\n\n    // Process data if there are enough bytes to read the expected response size,\n    // otherwise keep buffering\n    while (this.bytesNeeded <= this.bytesBuffered) {\n      const buffer = this.chunks.length > 1 ? Buffer.concat(this.chunks) : this.chunks[0]\n      const decoder = new Decoder(buffer)\n      const expectedResponseSize = decoder.readInt32()\n\n      // Return early if not enough bytes to read the full response\n      if (!decoder.canReadBytes(expectedResponseSize)) {\n        this.chunks = [buffer]\n        this.bytesBuffered = Buffer.byteLength(buffer)\n        this.bytesNeeded = Decoder.int32Size() + expectedResponseSize\n        return\n      }\n\n      const response = new Decoder(decoder.readBytes(expectedResponseSize))\n\n      // Reset the buffered chunks as the rest of the bytes\n      const remainderBuffer = decoder.readAll()\n      this.chunks = [remainderBuffer]\n      this.bytesBuffered = Buffer.byteLength(remainderBuffer)\n      this.bytesNeeded = Decoder.int32Size()\n\n      if (this.authHandlers) {\n        const rawResponseSize = Decoder.int32Size() + expectedResponseSize\n        const rawResponseBuffer = buffer.slice(0, rawResponseSize)\n        return this.authHandlers.onSuccess(rawResponseBuffer)\n      }\n\n      const correlationId = response.readInt32()\n      const payload = response.readAll()\n\n      this.requestQueue.fulfillRequest({\n        size: expectedResponseSize,\n        correlationId,\n        payload,\n      })\n    }\n  }\n\n  /**\n   * @private\n   */\n  rejectRequests(error) {\n    this.requestQueue.rejectAll(error)\n  }\n}\n", "const apiKeys = require('../protocol/requests/apiKeys')\nconst Connection = require('./connection')\n\nmodule.exports = class ConnectionPool {\n  /**\n   * @param {ConstructorParameters<typeof Connection>[0]} options\n   */\n  constructor(options) {\n    this.logger = options.logger.namespace('ConnectionPool')\n    this.connectionTimeout = options.connectionTimeout\n    this.host = options.host\n    this.port = options.port\n    this.rack = options.rack\n    this.ssl = options.ssl\n    this.sasl = options.sasl\n    this.clientId = options.clientId\n    this.socketFactory = options.socketFactory\n\n    this.pool = new Array(2).fill().map(() => new Connection(options))\n  }\n\n  isConnected() {\n    return this.pool.some(c => c.isConnected())\n  }\n\n  isAuthenticated() {\n    return this.pool.some(c => c.isAuthenticated())\n  }\n\n  setSupportAuthenticationProtocol(isSupported) {\n    this.map(c => c.setSupportAuthenticationProtocol(isSupported))\n  }\n\n  setVersions(versions) {\n    this.map(c => c.setVersions(versions))\n  }\n\n  map(callback) {\n    return this.pool.map(c => callback(c))\n  }\n\n  async send(protocolRequest) {\n    const connection = await this.getConnectionByRequest(protocolRequest)\n    return connection.send(protocolRequest)\n  }\n\n  getConnectionByRequest({ request: { apiKey } }) {\n    const index = { [apiKeys.Fetch]: 1 }[apiKey] || 0\n    return this.getConnection(index)\n  }\n\n  async getConnection(index = 0) {\n    const connection = this.pool[index]\n\n    if (!connection.isConnected()) {\n      await connection.connect()\n    }\n\n    return connection\n  }\n\n  async destroy() {\n    await Promise.all(this.map(c => c.disconnect()))\n  }\n}\n", "const { KafkaJSConnectionError, KafkaJSNonRetriableError } = require('../errors')\nconst ConnectionPool = require('../network/connectionPool')\n\n/**\n * @typedef {Object} ConnectionPoolBuilder\n * @property {(destination?: { host?: string, port?: number, rack?: string }) => Promise<ConnectionPool>} build\n */\n\n/**\n * @param {Object} options\n * @param {import(\"../../types\").ISocketFactory} [options.socketFactory]\n * @param {string[]|(() => string[])} options.brokers\n * @param {Object} [options.ssl]\n * @param {Object} [options.sasl]\n * @param {string} options.clientId\n * @param {number} options.requestTimeout\n * @param {boolean} [options.enforceRequestTimeout]\n * @param {number} [options.connectionTimeout]\n * @param {number} [options.maxInFlightRequests]\n * @param {import(\"../../types\").RetryOptions} [options.retry]\n * @param {import(\"../../types\").Logger} options.logger\n * @param {import(\"../instrumentation/emitter\")} [options.instrumentationEmitter]\n * @param {number} [options.reauthenticationThreshold]\n * @returns {ConnectionPoolBuilder}\n */\nmodule.exports = ({\n  socketFactory,\n  brokers,\n  ssl,\n  sasl,\n  clientId,\n  requestTimeout,\n  enforceRequestTimeout,\n  connectionTimeout,\n  maxInFlightRequests,\n  logger,\n  instrumentationEmitter = null,\n  reauthenticationThreshold,\n}) => {\n  let index = 0\n\n  const isValidBroker = broker => {\n    return broker && typeof broker === 'string' && broker.length > 0\n  }\n\n  const validateBrokers = brokers => {\n    if (!brokers) {\n      throw new KafkaJSNonRetriableError(`Failed to connect: brokers should not be null`)\n    }\n\n    if (Array.isArray(brokers)) {\n      if (!brokers.length) {\n        throw new KafkaJSNonRetriableError(`Failed to connect: brokers array is empty`)\n      }\n\n      brokers.forEach((broker, index) => {\n        if (!isValidBroker(broker)) {\n          throw new KafkaJSNonRetriableError(\n            `Failed to connect: broker at index ${index} is invalid \"${typeof broker}\"`\n          )\n        }\n      })\n    }\n  }\n\n  const getBrokers = async () => {\n    let list\n\n    if (typeof brokers === 'function') {\n      try {\n        list = await brokers()\n      } catch (e) {\n        const wrappedError = new KafkaJSConnectionError(\n          `Failed to connect: \"config.brokers\" threw: ${e.message}`\n        )\n        wrappedError.stack = `${wrappedError.name}\\n  Caused by: ${e.stack}`\n        throw wrappedError\n      }\n    } else {\n      list = brokers\n    }\n\n    validateBrokers(list)\n\n    return list\n  }\n\n  return {\n    build: async ({ host, port, rack } = {}) => {\n      if (!host) {\n        const list = await getBrokers()\n\n        const randomBroker = list[index++ % list.length]\n\n        host = randomBroker.split(':')[0]\n        port = Number(randomBroker.split(':')[1])\n      }\n\n      return new ConnectionPool({\n        host,\n        port,\n        rack,\n        sasl,\n        ssl,\n        clientId,\n        socketFactory,\n        connectionTimeout,\n        requestTimeout,\n        enforceRequestTimeout,\n        maxInFlightRequests,\n        instrumentationEmitter,\n        logger,\n        reauthenticationThreshold,\n      })\n    },\n  }\n}\n", "const BrokerPool = require('./brokerPool')\nconst Lock = require('../utils/lock')\nconst sharedPromiseTo = require('../utils/sharedPromiseTo')\nconst createRetry = require('../retry')\nconst connectionPoolBuilder = require('./connectionPoolBuilder')\nconst { EARLIEST_OFFSET, LATEST_OFFSET } = require('../constants')\nconst {\n  KafkaJSError,\n  KafkaJSBrokerNotFound,\n  KafkaJSMetadataNotLoaded,\n  KafkaJSTopicMetadataNotLoaded,\n  KafkaJSGroupCoordinatorNotFound,\n} = require('../errors')\nconst COORDINATOR_TYPES = require('../protocol/coordinatorTypes')\n\nconst { keys } = Object\n\nconst mergeTopics = (obj, { topic, partitions }) => ({\n  ...obj,\n  [topic]: [...(obj[topic] || []), ...partitions],\n})\n\nconst PRIVATE = {\n  CONNECT: Symbol('private:Cluster:connect'),\n  REFRESH_METADATA: Symbol('private:Cluster:refreshMetadata'),\n  REFRESH_METADATA_IF_NECESSARY: Symbol('private:Cluster:refreshMetadataIfNecessary'),\n  FIND_CONTROLLER_BROKER: Symbol('private:Cluster:findControllerBroker'),\n}\n\nmodule.exports = class Cluster {\n  /**\n   * @param {Object} options\n   * @param {Array<string>} options.brokers example: ['127.0.0.1:9092', '127.0.0.1:9094']\n   * @param {Object} options.ssl\n   * @param {Object} options.sasl\n   * @param {string} options.clientId\n   * @param {number} options.connectionTimeout - in milliseconds\n   * @param {number} options.authenticationTimeout - in milliseconds\n   * @param {number} options.reauthenticationThreshold - in milliseconds\n   * @param {number} [options.requestTimeout=30000] - in milliseconds\n   * @param {boolean} [options.enforceRequestTimeout]\n   * @param {number} options.metadataMaxAge - in milliseconds\n   * @param {boolean} options.allowAutoTopicCreation\n   * @param {number} options.maxInFlightRequests\n   * @param {number} options.isolationLevel\n   * @param {import(\"../../types\").RetryOptions} options.retry\n   * @param {import(\"../../types\").Logger} options.logger\n   * @param {import(\"../../types\").ISocketFactory} options.socketFactory\n   * @param {Map} [options.offsets]\n   * @param {import(\"../instrumentation/emitter\")} [options.instrumentationEmitter=null]\n   */\n  constructor({\n    logger: rootLogger,\n    socketFactory,\n    brokers,\n    ssl,\n    sasl,\n    clientId,\n    connectionTimeout,\n    authenticationTimeout,\n    reauthenticationThreshold,\n    requestTimeout = 30000,\n    enforceRequestTimeout,\n    metadataMaxAge,\n    retry,\n    allowAutoTopicCreation,\n    maxInFlightRequests,\n    isolationLevel,\n    instrumentationEmitter = null,\n    offsets = new Map(),\n  }) {\n    this.rootLogger = rootLogger\n    this.logger = rootLogger.namespace('Cluster')\n    this.retrier = createRetry(retry)\n    this.connectionPoolBuilder = connectionPoolBuilder({\n      logger: rootLogger,\n      instrumentationEmitter,\n      socketFactory,\n      brokers,\n      ssl,\n      sasl,\n      clientId,\n      connectionTimeout,\n      requestTimeout,\n      enforceRequestTimeout,\n      maxInFlightRequests,\n      reauthenticationThreshold,\n    })\n\n    this.targetTopics = new Set()\n    this.mutatingTargetTopics = new Lock({\n      description: `updating target topics`,\n      timeout: requestTimeout,\n    })\n    this.isolationLevel = isolationLevel\n    this.brokerPool = new BrokerPool({\n      connectionPoolBuilder: this.connectionPoolBuilder,\n      logger: this.rootLogger,\n      retry,\n      allowAutoTopicCreation,\n      authenticationTimeout,\n      metadataMaxAge,\n    })\n    this.committedOffsetsByGroup = offsets\n\n    this[PRIVATE.CONNECT] = sharedPromiseTo(async () => {\n      return await this.brokerPool.connect()\n    })\n\n    this[PRIVATE.REFRESH_METADATA] = sharedPromiseTo(async () => {\n      return await this.brokerPool.refreshMetadata(Array.from(this.targetTopics))\n    })\n\n    this[PRIVATE.REFRESH_METADATA_IF_NECESSARY] = sharedPromiseTo(async () => {\n      return await this.brokerPool.refreshMetadataIfNecessary(Array.from(this.targetTopics))\n    })\n\n    this[PRIVATE.FIND_CONTROLLER_BROKER] = sharedPromiseTo(async () => {\n      const { metadata } = this.brokerPool\n\n      if (!metadata || metadata.controllerId == null) {\n        throw new KafkaJSMetadataNotLoaded('Topic metadata not loaded')\n      }\n\n      const broker = await this.findBroker({ nodeId: metadata.controllerId })\n\n      if (!broker) {\n        throw new KafkaJSBrokerNotFound(\n          `Controller broker with id ${metadata.controllerId} not found in the cached metadata`\n        )\n      }\n\n      return broker\n    })\n  }\n\n  isConnected() {\n    return this.brokerPool.hasConnectedBrokers()\n  }\n\n  /**\n   * @public\n   * @returns {Promise<void>}\n   */\n  async connect() {\n    await this[PRIVATE.CONNECT]()\n  }\n\n  /**\n   * @public\n   * @returns {Promise<void>}\n   */\n  async disconnect() {\n    await this.brokerPool.disconnect()\n  }\n\n  /**\n   * @public\n   * @param {object} destination\n   * @param {String} destination.host\n   * @param {Number} destination.port\n   */\n  removeBroker({ host, port }) {\n    this.brokerPool.removeBroker({ host, port })\n  }\n\n  /**\n   * @public\n   * @returns {Promise<void>}\n   */\n  async refreshMetadata() {\n    await this[PRIVATE.REFRESH_METADATA]()\n  }\n\n  /**\n   * @public\n   * @returns {Promise<void>}\n   */\n  async refreshMetadataIfNecessary() {\n    await this[PRIVATE.REFRESH_METADATA_IF_NECESSARY]()\n  }\n\n  /**\n   * @public\n   * @returns {Promise<import(\"../../types\").BrokerMetadata>}\n   */\n  async metadata({ topics = [] } = {}) {\n    return this.retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await this.brokerPool.refreshMetadataIfNecessary(topics)\n        return this.brokerPool.withBroker(async ({ broker }) => broker.metadata(topics))\n      } catch (e) {\n        if (e.type === 'LEADER_NOT_AVAILABLE') {\n          throw e\n        }\n\n        bail(e)\n      }\n    })\n  }\n\n  /**\n   * @public\n   * @param {string} topic\n   * @return {Promise}\n   */\n  async addTargetTopic(topic) {\n    return this.addMultipleTargetTopics([topic])\n  }\n\n  /**\n   * @public\n   * @param {string[]} topics\n   * @return {Promise}\n   */\n  async addMultipleTargetTopics(topics) {\n    await this.mutatingTargetTopics.acquire()\n\n    try {\n      const previousSize = this.targetTopics.size\n      const previousTopics = new Set(this.targetTopics)\n      for (const topic of topics) {\n        this.targetTopics.add(topic)\n      }\n\n      const hasChanged = previousSize !== this.targetTopics.size || !this.brokerPool.metadata\n\n      if (hasChanged) {\n        try {\n          await this.refreshMetadata()\n        } catch (e) {\n          if (\n            e.type === 'INVALID_TOPIC_EXCEPTION' ||\n            e.type === 'UNKNOWN_TOPIC_OR_PARTITION' ||\n            e.type === 'TOPIC_AUTHORIZATION_FAILED'\n          ) {\n            this.targetTopics = previousTopics\n          }\n\n          throw e\n        }\n      }\n    } finally {\n      await this.mutatingTargetTopics.release()\n    }\n  }\n\n  /** @type {() => string[]} */\n  getNodeIds() {\n    return this.brokerPool.getNodeIds()\n  }\n\n  /**\n   * @public\n   * @param {object} options\n   * @param {string} options.nodeId\n   * @returns {Promise<import(\"../../types\").Broker>}\n   */\n  async findBroker({ nodeId }) {\n    try {\n      return await this.brokerPool.findBroker({ nodeId })\n    } catch (e) {\n      // The client probably has stale metadata\n      if (\n        e.name === 'KafkaJSBrokerNotFound' ||\n        e.name === 'KafkaJSLockTimeout' ||\n        e.name === 'KafkaJSConnectionError'\n      ) {\n        await this.refreshMetadata()\n      }\n\n      throw e\n    }\n  }\n\n  /**\n   * @public\n   * @returns {Promise<import(\"../../types\").Broker>}\n   */\n  async findControllerBroker() {\n    return await this[PRIVATE.FIND_CONTROLLER_BROKER]()\n  }\n\n  /**\n   * @public\n   * @param {string} topic\n   * @returns {import(\"../../types\").PartitionMetadata[]} Example:\n   *                   [{\n   *                     isr: [2],\n   *                     leader: 2,\n   *                     partitionErrorCode: 0,\n   *                     partitionId: 0,\n   *                     replicas: [2],\n   *                   }]\n   */\n  findTopicPartitionMetadata(topic) {\n    const { metadata } = this.brokerPool\n    if (!metadata || !metadata.topicMetadata) {\n      throw new KafkaJSTopicMetadataNotLoaded('Topic metadata not loaded', { topic })\n    }\n\n    const topicMetadata = metadata.topicMetadata.find(t => t.topic === topic)\n    return topicMetadata ? topicMetadata.partitionMetadata : []\n  }\n\n  /**\n   * @public\n   * @param {string} topic\n   * @param {(number|string)[]} partitions\n   * @returns {Object} Object with leader and partitions. For partitions 0 and 5\n   *                   the result could be:\n   *                     { '0': [0], '2': [5] }\n   *\n   *                   where the key is the nodeId.\n   */\n  findLeaderForPartitions(topic, partitions) {\n    const partitionMetadata = this.findTopicPartitionMetadata(topic)\n    return partitions.reduce((result, id) => {\n      const partitionId = parseInt(id, 10)\n      const metadata = partitionMetadata.find(p => p.partitionId === partitionId)\n\n      if (!metadata) {\n        return result\n      }\n\n      if (metadata.leader === null || metadata.leader === undefined) {\n        throw new KafkaJSError('Invalid partition metadata', { topic, partitionId, metadata })\n      }\n\n      const { leader } = metadata\n      const current = result[leader] || []\n      return { ...result, [leader]: [...current, partitionId] }\n    }, {})\n  }\n\n  /**\n   * @public\n   * @param {object} params\n   * @param {string} params.groupId\n   * @param {import(\"../protocol/coordinatorTypes\").CoordinatorType} [params.coordinatorType=0]\n   * @returns {Promise<import(\"../../types\").Broker>}\n   */\n  async findGroupCoordinator({ groupId, coordinatorType = COORDINATOR_TYPES.GROUP }) {\n    return this.retrier(async (bail, retryCount, retryTime) => {\n      try {\n        const { coordinator } = await this.findGroupCoordinatorMetadata({\n          groupId,\n          coordinatorType,\n        })\n        return await this.findBroker({ nodeId: coordinator.nodeId })\n      } catch (e) {\n        // A new broker can join the cluster before we have the chance\n        // to refresh metadata\n        if (e.name === 'KafkaJSBrokerNotFound' || e.type === 'GROUP_COORDINATOR_NOT_AVAILABLE') {\n          this.logger.debug(`${e.message}, refreshing metadata and trying again...`, {\n            groupId,\n            retryCount,\n            retryTime,\n          })\n\n          await this.refreshMetadata()\n          throw e\n        }\n\n        if (e.code === 'ECONNREFUSED') {\n          // During maintenance the current coordinator can go down; findBroker will\n          // refresh metadata and re-throw the error. findGroupCoordinator has to re-throw\n          // the error to go through the retry cycle.\n          throw e\n        }\n\n        bail(e)\n      }\n    })\n  }\n\n  /**\n   * @public\n   * @param {object} params\n   * @param {string} params.groupId\n   * @param {import(\"../protocol/coordinatorTypes\").CoordinatorType} [params.coordinatorType=0]\n   * @returns {Promise<Object>}\n   */\n  async findGroupCoordinatorMetadata({ groupId, coordinatorType }) {\n    const brokerMetadata = await this.brokerPool.withBroker(async ({ nodeId, broker }) => {\n      return await this.retrier(async (bail, retryCount, retryTime) => {\n        try {\n          const brokerMetadata = await broker.findGroupCoordinator({ groupId, coordinatorType })\n          this.logger.debug('Found group coordinator', {\n            broker: brokerMetadata.host,\n            nodeId: brokerMetadata.coordinator.nodeId,\n          })\n          return brokerMetadata\n        } catch (e) {\n          this.logger.debug('Tried to find group coordinator', {\n            nodeId,\n            groupId,\n            error: e,\n          })\n\n          if (e.type === 'GROUP_COORDINATOR_NOT_AVAILABLE') {\n            this.logger.debug('Group coordinator not available, retrying...', {\n              nodeId,\n              retryCount,\n              retryTime,\n            })\n\n            throw e\n          }\n\n          bail(e)\n        }\n      })\n    })\n\n    if (brokerMetadata) {\n      return brokerMetadata\n    }\n\n    throw new KafkaJSGroupCoordinatorNotFound('Failed to find group coordinator')\n  }\n\n  /**\n   * @param {object} topicConfiguration\n   * @returns {number}\n   */\n  defaultOffset({ fromBeginning }) {\n    return fromBeginning ? EARLIEST_OFFSET : LATEST_OFFSET\n  }\n\n  /**\n   * @public\n   * @param {Array<Object>} topics\n   *                          [\n   *                            {\n   *                              topic: 'my-topic-name',\n   *                              partitions: [{ partition: 0 }],\n   *                              fromBeginning: false\n   *                            }\n   *                          ]\n   * @returns {Promise<import(\"../../types\").TopicOffsets[]>} example:\n   *                          [\n   *                            {\n   *                              topic: 'my-topic-name',\n   *                              partitions: [\n   *                                { partition: 0, offset: '1' },\n   *                                { partition: 1, offset: '2' },\n   *                                { partition: 2, offset: '1' },\n   *                              ],\n   *                            },\n   *                          ]\n   */\n  async fetchTopicsOffset(topics) {\n    const partitionsPerBroker = {}\n    const topicConfigurations = {}\n\n    const addDefaultOffset = topic => partition => {\n      const { timestamp } = topicConfigurations[topic]\n      return { ...partition, timestamp }\n    }\n\n    // Index all topics and partitions per leader (nodeId)\n    for (const topicData of topics) {\n      const { topic, partitions, fromBeginning, fromTimestamp } = topicData\n      const partitionsPerLeader = this.findLeaderForPartitions(\n        topic,\n        partitions.map(p => p.partition)\n      )\n      const timestamp =\n        fromTimestamp != null ? fromTimestamp : this.defaultOffset({ fromBeginning })\n\n      topicConfigurations[topic] = { timestamp }\n\n      keys(partitionsPerLeader).forEach(nodeId => {\n        partitionsPerBroker[nodeId] = partitionsPerBroker[nodeId] || {}\n        partitionsPerBroker[nodeId][topic] = partitions.filter(p =>\n          partitionsPerLeader[nodeId].includes(p.partition)\n        )\n      })\n    }\n\n    // Create a list of requests to fetch the offset of all partitions\n    const requests = keys(partitionsPerBroker).map(async nodeId => {\n      const broker = await this.findBroker({ nodeId })\n      const partitions = partitionsPerBroker[nodeId]\n\n      const { responses: topicOffsets } = await broker.listOffsets({\n        isolationLevel: this.isolationLevel,\n        topics: keys(partitions).map(topic => ({\n          topic,\n          partitions: partitions[topic].map(addDefaultOffset(topic)),\n        })),\n      })\n\n      return topicOffsets\n    })\n\n    // Execute all requests, merge and normalize the responses\n    const responses = await Promise.all(requests)\n    const partitionsPerTopic = responses.flat().reduce(mergeTopics, {})\n\n    return keys(partitionsPerTopic).map(topic => ({\n      topic,\n      partitions: partitionsPerTopic[topic].map(({ partition, offset }) => ({\n        partition,\n        offset,\n      })),\n    }))\n  }\n\n  /**\n   * Retrieve the object mapping for committed offsets for a single consumer group\n   * @param {object} options\n   * @param {string} options.groupId\n   * @returns {Object}\n   */\n  committedOffsets({ groupId }) {\n    if (!this.committedOffsetsByGroup.has(groupId)) {\n      this.committedOffsetsByGroup.set(groupId, {})\n    }\n\n    return this.committedOffsetsByGroup.get(groupId)\n  }\n\n  /**\n   * Mark offset as committed for a single consumer group's topic-partition\n   * @param {object} options\n   * @param {string} options.groupId\n   * @param {string} options.topic\n   * @param {string|number} options.partition\n   * @param {string} options.offset\n   */\n  markOffsetAsCommitted({ groupId, topic, partition, offset }) {\n    const committedOffsets = this.committedOffsets({ groupId })\n\n    committedOffsets[topic] = committedOffsets[topic] || {}\n    committedOffsets[topic][partition] = offset\n  }\n}\n", "/* eslint-disable */\nconst Long = require('../../../utils/long')\n\n// Based on the kafka client 0.10.2 murmur2 implementation\n// https://github.com/apache/kafka/blob/0.10.2/clients/src/main/java/org/apache/kafka/common/utils/Utils.java#L364\n\nconst SEED = Long.fromValue(0x9747b28c)\n\n// 'm' and 'r' are mixing constants generated offline.\n// They're not really 'magic', they just happen to work well.\nconst M = Long.fromValue(0x5bd1e995)\nconst R = Long.fromValue(24)\n\nmodule.exports = key => {\n  const data = Buffer.isBuffer(key) ? key : Buffer.from(String(key))\n  const length = data.length\n\n  // Initialize the hash to a random value\n  let h = Long.fromValue(SEED.xor(length))\n  let length4 = Math.floor(length / 4)\n\n  for (let i = 0; i < length4; i++) {\n    const i4 = i * 4\n    let k =\n      (data[i4 + 0] & 0xff) +\n      ((data[i4 + 1] & 0xff) << 8) +\n      ((data[i4 + 2] & 0xff) << 16) +\n      ((data[i4 + 3] & 0xff) << 24)\n    k = Long.fromValue(k)\n    k = k.multiply(M)\n    k = k.xor(k.toInt() >>> R)\n    k = Long.fromValue(k).multiply(M)\n    h = h.multiply(M)\n    h = h.xor(k)\n  }\n\n  // Handle the last few bytes of the input array\n  switch (length % 4) {\n    case 3:\n      h = h.xor((data[(length & ~3) + 2] & 0xff) << 16)\n    case 2:\n      h = h.xor((data[(length & ~3) + 1] & 0xff) << 8)\n    case 1:\n      h = h.xor(data[length & ~3] & 0xff)\n      h = h.multiply(M)\n  }\n\n  h = h.xor(h.toInt() >>> 13)\n  h = h.multiply(M)\n  h = h.xor(h.toInt() >>> 15)\n\n  return h.toInt()\n}\n", "const { KafkaJSNonRetriableError } = require('../../../errors')\n\nconst toNodeCompatible = crypto => ({\n  randomBytes: size => crypto.getRandomValues(Buffer.allocUnsafe(size)),\n})\n\nlet cryptoImplementation = null\nif (global && global.crypto) {\n  cryptoImplementation =\n    global.crypto.randomBytes === undefined ? toNodeCompatible(global.crypto) : global.crypto\n} else if (global && global.msCrypto) {\n  cryptoImplementation = toNodeCompatible(global.msCrypto)\n} else if (global && !global.crypto) {\n  cryptoImplementation = require('crypto')\n}\n\nconst MAX_BYTES = 65536\n\nmodule.exports = size => {\n  if (size > MAX_BYTES) {\n    throw new KafkaJSNonRetriableError(\n      `Byte length (${size}) exceeds the max number of bytes of entropy available (${MAX_BYTES})`\n    )\n  }\n\n  if (!cryptoImplementation) {\n    throw new KafkaJSNonRetriableError('No available crypto implementation')\n  }\n\n  return cryptoImplementation.randomBytes(size)\n}\n", "const randomBytes = require('./randomBytes')\n\n// Based on the java client 0.10.2\n// https://github.com/apache/kafka/blob/0.10.2/clients/src/main/java/org/apache/kafka/clients/producer/internals/DefaultPartitioner.java\n\n/**\n * A cheap way to deterministically convert a number to a positive value. When the input is\n * positive, the original value is returned. When the input number is negative, the returned\n * positive value is the original value bit AND against 0x7fffffff which is not its absolutely\n * value.\n */\nconst toPositive = x => x & 0x7fffffff\n\n/**\n * The default partitioning strategy:\n *  - If a partition is specified in the message, use it\n *  - If no partition is specified but a key is present choose a partition based on a hash of the key\n *  - If no partition or key is present choose a partition in a round-robin fashion\n */\nmodule.exports = murmur2 => () => {\n  const counters = {}\n\n  return ({ topic, partitionMetadata, message }) => {\n    if (!(topic in counters)) {\n      counters[topic] = randomBytes(32).readUInt32BE(0)\n    }\n    const numPartitions = partitionMetadata.length\n    const availablePartitions = partitionMetadata.filter(p => p.leader >= 0)\n    const numAvailablePartitions = availablePartitions.length\n\n    if (message.partition !== null && message.partition !== undefined) {\n      return message.partition\n    }\n\n    if (message.key !== null && message.key !== undefined) {\n      return toPositive(murmur2(message.key)) % numPartitions\n    }\n\n    if (numAvailablePartitions > 0) {\n      const i = toPositive(++counters[topic]) % numAvailablePartitions\n      return availablePartitions[i].partitionId\n    }\n\n    // no partitions are available, give a non-available partition\n    return toPositive(++counters[topic]) % numPartitions\n  }\n}\n", "const murmur2 = require('./murmur2')\nconst createDefaultPartitioner = require('../legacy/partitioner')\n\nmodule.exports = createDefaultPartitioner(murmur2)\n", "/* eslint-disable */\n\n// Based on the kafka client 0.10.2 murmur2 implementation\n// https://github.com/apache/kafka/blob/0.10.2/clients/src/main/java/org/apache/kafka/common/utils/Utils.java#L364\n\nconst SEED = 0x9747b28c\n\n// 'm' and 'r' are mixing constants generated offline.\n// They're not really 'magic', they just happen to work well.\nconst M = 0x5bd1e995\nconst R = 24\n\nmodule.exports = key => {\n  const data = Buffer.isBuffer(key) ? key : Buffer.from(String(key))\n  const length = data.length\n\n  // Initialize the hash to a random value\n  let h = SEED ^ length\n  let length4 = length / 4\n\n  for (let i = 0; i < length4; i++) {\n    const i4 = i * 4\n    let k =\n      (data[i4 + 0] & 0xff) +\n      ((data[i4 + 1] & 0xff) << 8) +\n      ((data[i4 + 2] & 0xff) << 16) +\n      ((data[i4 + 3] & 0xff) << 24)\n    k *= M\n    k ^= k >>> R\n    k *= M\n    h *= M\n    h ^= k\n  }\n\n  // Handle the last few bytes of the input array\n  switch (length % 4) {\n    case 3:\n      h ^= (data[(length & ~3) + 2] & 0xff) << 16\n    case 2:\n      h ^= (data[(length & ~3) + 1] & 0xff) << 8\n    case 1:\n      h ^= data[length & ~3] & 0xff\n      h *= M\n  }\n\n  h ^= h >>> 13\n  h *= M\n  h ^= h >>> 15\n\n  return h\n}\n", "const murmur2 = require('./murmur2')\nconst createLegacyPartitioner = require('./partitioner')\n\nmodule.exports = createLegacyPartitioner(murmur2)\n", "const DefaultPartitioner = require('./default')\nconst LegacyPartitioner = require('./legacy')\n\nmodule.exports = {\n  DefaultPartitioner,\n  LegacyPartitioner,\n  /**\n   * @deprecated Use DefaultPartitioner instead\n   *\n   * The JavaCompatiblePartitioner was renamed DefaultPartitioner\n   * and made to be the default in 2.0.0.\n   */\n  JavaCompatiblePartitioner: DefaultPartitioner,\n}\n", "module.exports = {\n  UNINITIALIZED: 'UNINITIALIZED',\n  READY: 'READY',\n  TRANSACTING: 'TRANSACTING',\n  COMMITTING: 'COMMITTING',\n  ABORTING: 'ABORTING',\n}\n", "const { EventEmitter } = require('events')\nconst { KafkaJSNonRetriableError } = require('../../errors')\nconst STATES = require('./transactionStates')\n\nconst VALID_STATE_TRANSITIONS = {\n  [STATES.UNINITIALIZED]: [STATES.READY],\n  [STATES.READY]: [STATES.READY, STATES.TRANSACTING],\n  [STATES.TRANSACTING]: [STATES.COMMITTING, STATES.ABORTING],\n  [STATES.COMMITTING]: [STATES.READY],\n  [STATES.ABORTING]: [STATES.READY],\n}\n\nmodule.exports = ({ logger, initialState = STATES.UNINITIALIZED }) => {\n  let currentState = initialState\n\n  const guard = (object, method, { legalStates, async: isAsync = true }) => {\n    if (!object[method]) {\n      throw new KafkaJSNonRetriableError(`Cannot add guard on missing method \"${method}\"`)\n    }\n\n    return (...args) => {\n      const fn = object[method]\n\n      if (!legalStates.includes(currentState)) {\n        const error = new KafkaJSNonRetriableError(\n          `Transaction state exception: Cannot call \"${method}\" in state \"${currentState}\"`\n        )\n\n        if (isAsync) {\n          return Promise.reject(error)\n        } else {\n          throw error\n        }\n      }\n\n      return fn.apply(object, args)\n    }\n  }\n\n  const stateMachine = Object.assign(new EventEmitter(), {\n    /**\n     * Create a clone of \"object\" where we ensure state machine is in correct state\n     * prior to calling any of the configured methods\n     * @param {Object} object The object whose methods we will guard\n     * @param {Object} methodStateMapping Keys are method names on \"object\"\n     * @param {string[]} methodStateMapping.legalStates Legal states for this method\n     * @param {boolean=true} methodStateMapping.async Whether this method is async (throw vs reject)\n     */\n    createGuarded(object, methodStateMapping) {\n      const guardedMethods = Object.keys(methodStateMapping).reduce((guards, method) => {\n        guards[method] = guard(object, method, methodStateMapping[method])\n        return guards\n      }, {})\n\n      return { ...object, ...guardedMethods }\n    },\n    /**\n     * Transition safely to a new state\n     */\n    transitionTo(state) {\n      logger.debug(`Transaction state transition ${currentState} --> ${state}`)\n\n      if (!VALID_STATE_TRANSITIONS[currentState].includes(state)) {\n        throw new KafkaJSNonRetriableError(\n          `Transaction state exception: Invalid transition ${currentState} --> ${state}`\n        )\n      }\n\n      stateMachine.emit('transition', { to: state, from: currentState })\n      currentState = state\n    },\n\n    state() {\n      return currentState\n    },\n  })\n\n  return stateMachine\n}\n", "const createRetry = require('../../retry')\nconst Lock = require('../../utils/lock')\nconst { KafkaJSNonRetriableError } = require('../../errors')\nconst COORDINATOR_TYPES = require('../../protocol/coordinatorTypes')\nconst createStateMachine = require('./transactionStateMachine')\nconst { INT_32_MAX_VALUE } = require('../../constants')\nconst assert = require('assert')\n\nconst STATES = require('./transactionStates')\nconst NO_PRODUCER_ID = -1\nconst SEQUENCE_START = 0\nconst INIT_PRODUCER_RETRIABLE_PROTOCOL_ERRORS = [\n  'NOT_COORDINATOR_FOR_GROUP',\n  'GROUP_COORDINATOR_NOT_AVAILABLE',\n  'GROUP_LOAD_IN_PROGRESS',\n  /**\n   * The producer might have crashed and never committed the transaction; retry the\n   * request so Kafka can abort the current transaction\n   * @see https://github.com/apache/kafka/blob/201da0542726472d954080d54bc585b111aaf86f/clients/src/main/java/org/apache/kafka/clients/producer/internals/TransactionManager.java#L1001-L1002\n   */\n  'CONCURRENT_TRANSACTIONS',\n]\nconst COMMIT_RETRIABLE_PROTOCOL_ERRORS = [\n  'UNKNOWN_TOPIC_OR_PARTITION',\n  'COORDINATOR_LOAD_IN_PROGRESS',\n]\nconst COMMIT_STALE_COORDINATOR_PROTOCOL_ERRORS = ['COORDINATOR_NOT_AVAILABLE', 'NOT_COORDINATOR']\n\n/**\n * @typedef {Object} EosManager\n */\n\n/**\n * Manage behavior for an idempotent producer and transactions.\n *\n * @returns {EosManager}\n */\nmodule.exports = ({\n  logger,\n  cluster,\n  transactionTimeout = 60000,\n  transactional,\n  transactionalId,\n}) => {\n  if (transactional && !transactionalId) {\n    throw new KafkaJSNonRetriableError('Cannot manage transactions without a transactionalId')\n  }\n\n  const retrier = createRetry(cluster.retry)\n\n  /**\n   * Current producer ID\n   */\n  let producerId = NO_PRODUCER_ID\n\n  /**\n   * Current producer epoch\n   */\n  let producerEpoch = 0\n\n  /**\n   * Idempotent production requires that the producer track the sequence number of messages.\n   *\n   * Sequences are sent with every Record Batch and tracked per Topic-Partition\n   */\n  let producerSequence = {}\n\n  /**\n   * Idempotent production requires a mutex lock per broker to serialize requests with sequence number handling\n   */\n  let brokerMutexLocks = {}\n\n  /**\n   * Topic partitions already participating in the transaction\n   */\n  let transactionTopicPartitions = {}\n\n  /**\n   * Offsets have been added to the transaction\n   */\n  let hasOffsetsAddedToTransaction = false\n\n  const stateMachine = createStateMachine({ logger })\n  stateMachine.on('transition', ({ to }) => {\n    if (to === STATES.READY) {\n      transactionTopicPartitions = {}\n      hasOffsetsAddedToTransaction = false\n    }\n  })\n\n  const findTransactionCoordinator = () => {\n    return cluster.findGroupCoordinator({\n      groupId: transactionalId,\n      coordinatorType: COORDINATOR_TYPES.TRANSACTION,\n    })\n  }\n\n  const transactionalGuard = () => {\n    if (!transactional) {\n      throw new KafkaJSNonRetriableError('Method unavailable if non-transactional')\n    }\n  }\n\n  /**\n   * A transaction is ongoing when offsets or partitions added to it\n   *\n   * @returns {boolean}\n   */\n  const isOngoing = () => {\n    return (\n      hasOffsetsAddedToTransaction ||\n      Object.entries(transactionTopicPartitions).some(([, partitions]) => {\n        return Object.entries(partitions).some(\n          ([, isPartitionAddedToTransaction]) => isPartitionAddedToTransaction\n        )\n      })\n    )\n  }\n\n  const eosManager = stateMachine.createGuarded(\n    {\n      /**\n       * Get the current producer id\n       * @returns {number}\n       */\n      getProducerId() {\n        return producerId\n      },\n\n      /**\n       * Get the current producer epoch\n       * @returns {number}\n       */\n      getProducerEpoch() {\n        return producerEpoch\n      },\n\n      getTransactionalId() {\n        return transactionalId\n      },\n\n      /**\n       * Initialize the idempotent producer by making an `InitProducerId` request.\n       * Overwrites any existing state in this transaction manager\n       */\n      async initProducerId() {\n        return retrier(async (bail, retryCount, retryTime) => {\n          try {\n            await cluster.refreshMetadataIfNecessary()\n\n            // If non-transactional we can request the PID from any broker\n            const broker = await (transactional\n              ? findTransactionCoordinator()\n              : cluster.findControllerBroker())\n\n            const result = await broker.initProducerId({\n              transactionalId: transactional ? transactionalId : undefined,\n              transactionTimeout,\n            })\n\n            stateMachine.transitionTo(STATES.READY)\n            producerId = result.producerId\n            producerEpoch = result.producerEpoch\n            producerSequence = {}\n            brokerMutexLocks = {}\n\n            logger.debug('Initialized producer id & epoch', { producerId, producerEpoch })\n          } catch (e) {\n            if (INIT_PRODUCER_RETRIABLE_PROTOCOL_ERRORS.includes(e.type)) {\n              if (e.type === 'CONCURRENT_TRANSACTIONS') {\n                logger.debug('There is an ongoing transaction on this transactionId, retrying', {\n                  error: e.message,\n                  stack: e.stack,\n                  transactionalId,\n                  retryCount,\n                  retryTime,\n                })\n              }\n\n              throw e\n            }\n\n            bail(e)\n          }\n        })\n      },\n\n      /**\n       * Get the current sequence for a given Topic-Partition. Defaults to 0.\n       *\n       * @param {string} topic\n       * @param {string} partition\n       * @returns {number}\n       */\n      getSequence(topic, partition) {\n        if (!eosManager.isInitialized()) {\n          return SEQUENCE_START\n        }\n\n        producerSequence[topic] = producerSequence[topic] || {}\n        producerSequence[topic][partition] = producerSequence[topic][partition] || SEQUENCE_START\n\n        return producerSequence[topic][partition]\n      },\n\n      /**\n       * Update the sequence for a given Topic-Partition.\n       *\n       * Do nothing if not yet initialized (not idempotent)\n       * @param {string} topic\n       * @param {string} partition\n       * @param {number} increment\n       */\n      updateSequence(topic, partition, increment) {\n        if (!eosManager.isInitialized()) {\n          return\n        }\n\n        const previous = eosManager.getSequence(topic, partition)\n        let sequence = previous + increment\n\n        // Sequence is defined as Int32 in the Record Batch,\n        // so theoretically should need to rotate here\n        if (sequence >= INT_32_MAX_VALUE) {\n          logger.debug(\n            `Sequence for ${topic} ${partition} exceeds max value (${sequence}). Rotating to 0.`\n          )\n          sequence = 0\n        }\n\n        producerSequence[topic][partition] = sequence\n      },\n\n      /**\n       * Begin a transaction\n       */\n      beginTransaction() {\n        transactionalGuard()\n        stateMachine.transitionTo(STATES.TRANSACTING)\n      },\n\n      /**\n       * Add partitions to a transaction if they are not already marked as participating.\n       *\n       * Should be called prior to sending any messages during a transaction\n       * @param {TopicData[]} topicData\n       *\n       * @typedef {Object} TopicData\n       * @property {string} topic\n       * @property {object[]} partitions\n       * @property {number} partitions[].partition\n       */\n      async addPartitionsToTransaction(topicData) {\n        transactionalGuard()\n        const newTopicPartitions = {}\n\n        topicData.forEach(({ topic, partitions }) => {\n          transactionTopicPartitions[topic] = transactionTopicPartitions[topic] || {}\n\n          partitions.forEach(({ partition }) => {\n            if (!transactionTopicPartitions[topic][partition]) {\n              newTopicPartitions[topic] = newTopicPartitions[topic] || []\n              newTopicPartitions[topic].push(partition)\n            }\n          })\n        })\n\n        const topics = Object.keys(newTopicPartitions).map(topic => ({\n          topic,\n          partitions: newTopicPartitions[topic],\n        }))\n\n        if (topics.length) {\n          const broker = await findTransactionCoordinator()\n          await broker.addPartitionsToTxn({ transactionalId, producerId, producerEpoch, topics })\n        }\n\n        topics.forEach(({ topic, partitions }) => {\n          partitions.forEach(partition => {\n            transactionTopicPartitions[topic][partition] = true\n          })\n        })\n      },\n\n      /**\n       * Commit the ongoing transaction\n       */\n      async commit() {\n        transactionalGuard()\n        stateMachine.transitionTo(STATES.COMMITTING)\n\n        if (!isOngoing()) {\n          logger.debug('No partitions or offsets registered, not sending EndTxn')\n\n          stateMachine.transitionTo(STATES.READY)\n          return\n        }\n\n        const broker = await findTransactionCoordinator()\n        await broker.endTxn({\n          producerId,\n          producerEpoch,\n          transactionalId,\n          transactionResult: true,\n        })\n\n        stateMachine.transitionTo(STATES.READY)\n      },\n\n      /**\n       * Abort the ongoing transaction\n       */\n      async abort() {\n        transactionalGuard()\n        stateMachine.transitionTo(STATES.ABORTING)\n\n        if (!isOngoing()) {\n          logger.debug('No partitions or offsets registered, not sending EndTxn')\n\n          stateMachine.transitionTo(STATES.READY)\n          return\n        }\n\n        const broker = await findTransactionCoordinator()\n        await broker.endTxn({\n          producerId,\n          producerEpoch,\n          transactionalId,\n          transactionResult: false,\n        })\n\n        stateMachine.transitionTo(STATES.READY)\n      },\n\n      /**\n       * Whether the producer id has already been initialized\n       */\n      isInitialized() {\n        return producerId !== NO_PRODUCER_ID\n      },\n\n      isTransactional() {\n        return transactional\n      },\n\n      isInTransaction() {\n        return stateMachine.state() === STATES.TRANSACTING\n      },\n\n      async acquireBrokerLock(broker) {\n        if (this.isInitialized()) {\n          brokerMutexLocks[broker.nodeId] =\n            brokerMutexLocks[broker.nodeId] || new Lock({ timeout: 0xffff })\n          await brokerMutexLocks[broker.nodeId].acquire()\n        }\n      },\n\n      releaseBrokerLock(broker) {\n        if (this.isInitialized()) brokerMutexLocks[broker.nodeId].release()\n      },\n\n      /**\n       * Mark the provided offsets as participating in the transaction for the given consumer group.\n       *\n       * This allows us to commit an offset as consumed only if the transaction passes.\n       * @param {string} consumerGroupId The unique group identifier\n       * @param {OffsetCommitTopic[]} topics The unique group identifier\n       * @returns {Promise}\n       *\n       * @typedef {Object} OffsetCommitTopic\n       * @property {string} topic\n       * @property {OffsetCommitTopicPartition[]} partitions\n       *\n       * @typedef {Object} OffsetCommitTopicPartition\n       * @property {number} partition\n       * @property {number} offset\n       */\n      async sendOffsets({ consumerGroupId, topics }) {\n        assert(consumerGroupId, 'Missing consumerGroupId')\n        assert(topics, 'Missing offset topics')\n\n        const transactionCoordinator = await findTransactionCoordinator()\n\n        // Do we need to add offsets if we've already done so for this consumer group?\n        await transactionCoordinator.addOffsetsToTxn({\n          transactionalId,\n          producerId,\n          producerEpoch,\n          groupId: consumerGroupId,\n        })\n\n        hasOffsetsAddedToTransaction = true\n\n        let groupCoordinator = await cluster.findGroupCoordinator({\n          groupId: consumerGroupId,\n          coordinatorType: COORDINATOR_TYPES.GROUP,\n        })\n\n        return retrier(async (bail, retryCount, retryTime) => {\n          try {\n            await groupCoordinator.txnOffsetCommit({\n              transactionalId,\n              producerId,\n              producerEpoch,\n              groupId: consumerGroupId,\n              topics,\n            })\n          } catch (e) {\n            if (COMMIT_RETRIABLE_PROTOCOL_ERRORS.includes(e.type)) {\n              logger.debug('Group coordinator is not ready yet, retrying', {\n                error: e.message,\n                stack: e.stack,\n                transactionalId,\n                retryCount,\n                retryTime,\n              })\n\n              throw e\n            }\n\n            if (\n              COMMIT_STALE_COORDINATOR_PROTOCOL_ERRORS.includes(e.type) ||\n              e.code === 'ECONNREFUSED'\n            ) {\n              logger.debug(\n                'Invalid group coordinator, finding new group coordinator and retrying',\n                {\n                  error: e.message,\n                  stack: e.stack,\n                  transactionalId,\n                  retryCount,\n                  retryTime,\n                }\n              )\n\n              groupCoordinator = await cluster.findGroupCoordinator({\n                groupId: consumerGroupId,\n                coordinatorType: COORDINATOR_TYPES.GROUP,\n              })\n\n              throw e\n            }\n\n            bail(e)\n          }\n        })\n      },\n    },\n\n    /**\n     * Transaction state guards\n     */\n    {\n      initProducerId: { legalStates: [STATES.UNINITIALIZED, STATES.READY] },\n      beginTransaction: { legalStates: [STATES.READY], async: false },\n      addPartitionsToTransaction: { legalStates: [STATES.TRANSACTING] },\n      sendOffsets: { legalStates: [STATES.TRANSACTING] },\n      commit: { legalStates: [STATES.TRANSACTING] },\n      abort: { legalStates: [STATES.TRANSACTING] },\n    }\n  )\n\n  return eosManager\n}\n", "module.exports = ({ topic, partitionMetadata, messages, partitioner }) => {\n  if (partitionMetadata.length === 0) {\n    return {}\n  }\n\n  return messages.reduce((result, message) => {\n    const partition = partitioner({ topic, partitionMetadata, message })\n    const current = result[partition] || []\n    return Object.assign(result, { [partition]: [...current, message] })\n  }, {})\n}\n", "module.exports = topicDataForBroker => {\n  return topicDataForBroker.map(\n    ({ topic, partitions, messagesPerPartition, sequencePerPartition }) => ({\n      topic,\n      partitions: partitions.map(partition => ({\n        partition,\n        messages: messagesPerPartition[partition],\n      })),\n    })\n  )\n}\n", "module.exports = ({ topics }) =>\n  topics.flatMap(({ topicName, partitions }) =>\n    partitions.map(partition => ({ topicName, ...partition }))\n  )\n", "const { KafkaJSMetadataNotLoaded } = require('../errors')\nconst { staleMetadata } = require('../protocol/error')\nconst groupMessagesPerPartition = require('./groupMessagesPerPartition')\nconst createTopicData = require('./createTopicData')\nconst responseSerializer = require('./responseSerializer')\n\nconst { keys } = Object\n\n/**\n * @param {Object} options\n * @param {import(\"../../types\").Logger} options.logger\n * @param {import(\"../../types\").Cluster} options.cluster\n * @param {ReturnType<import(\"../../types\").ICustomPartitioner>} options.partitioner\n * @param {import(\"./eosManager\").EosManager} options.eosManager\n * @param {import(\"../retry\").Retrier} options.retrier\n */\nmodule.exports = ({ logger, cluster, partitioner, eosManager, retrier }) => {\n  return async ({ acks, timeout, compression, topicMessages }) => {\n    /** @type {Map<import(\"../../types\").Broker, any[]>} */\n    const responsePerBroker = new Map()\n\n    /** @param {Map<import(\"../../types\").Broker, any[]>} responsePerBroker */\n    const createProducerRequests = async responsePerBroker => {\n      const topicMetadata = new Map()\n\n      await cluster.refreshMetadataIfNecessary()\n\n      for (const { topic, messages } of topicMessages) {\n        const partitionMetadata = cluster.findTopicPartitionMetadata(topic)\n\n        if (partitionMetadata.length === 0) {\n          logger.debug('Producing to topic without metadata', {\n            topic,\n            targetTopics: Array.from(cluster.targetTopics),\n          })\n\n          throw new KafkaJSMetadataNotLoaded('Producing to topic without metadata')\n        }\n\n        const messagesPerPartition = groupMessagesPerPartition({\n          topic,\n          partitionMetadata,\n          messages,\n          partitioner,\n        })\n\n        const partitions = keys(messagesPerPartition)\n        const partitionsPerLeader = cluster.findLeaderForPartitions(topic, partitions)\n        const leaders = keys(partitionsPerLeader)\n\n        topicMetadata.set(topic, {\n          partitionsPerLeader,\n          messagesPerPartition,\n        })\n\n        for (const nodeId of leaders) {\n          const broker = await cluster.findBroker({ nodeId })\n          if (!responsePerBroker.has(broker)) {\n            responsePerBroker.set(broker, null)\n          }\n        }\n      }\n\n      const brokers = Array.from(responsePerBroker.keys())\n      const brokersWithoutResponse = brokers.filter(broker => !responsePerBroker.get(broker))\n\n      return brokersWithoutResponse.map(async broker => {\n        const entries = Array.from(topicMetadata.entries())\n        const topicDataForBroker = entries\n          .filter(([_, { partitionsPerLeader }]) => !!partitionsPerLeader[broker.nodeId])\n          .map(([topic, { partitionsPerLeader, messagesPerPartition, sequencePerPartition }]) => ({\n            topic,\n            partitions: partitionsPerLeader[broker.nodeId],\n            messagesPerPartition,\n          }))\n\n        const topicData = createTopicData(topicDataForBroker)\n\n        await eosManager.acquireBrokerLock(broker)\n        try {\n          if (eosManager.isTransactional()) {\n            await eosManager.addPartitionsToTransaction(topicData)\n          }\n\n          topicData.forEach(({ topic, partitions }) => {\n            partitions.forEach(entry => {\n              entry['firstSequence'] = eosManager.getSequence(topic, entry.partition)\n              eosManager.updateSequence(topic, entry.partition, entry.messages.length)\n            })\n          })\n\n          let response\n          try {\n            response = await broker.produce({\n              transactionalId: eosManager.isTransactional()\n                ? eosManager.getTransactionalId()\n                : undefined,\n              producerId: eosManager.getProducerId(),\n              producerEpoch: eosManager.getProducerEpoch(),\n              acks,\n              timeout,\n              compression,\n              topicData,\n            })\n          } catch (e) {\n            topicData.forEach(({ topic, partitions }) => {\n              partitions.forEach(entry => {\n                eosManager.updateSequence(topic, entry.partition, -entry.messages.length)\n              })\n            })\n            throw e\n          }\n\n          const expectResponse = acks !== 0\n          const formattedResponse = expectResponse ? responseSerializer(response) : []\n\n          responsePerBroker.set(broker, formattedResponse)\n        } catch (e) {\n          responsePerBroker.delete(broker)\n          throw e\n        } finally {\n          await eosManager.releaseBrokerLock(broker)\n        }\n      })\n    }\n\n    return retrier(async (bail, retryCount, retryTime) => {\n      const topics = topicMessages.map(({ topic }) => topic)\n      await cluster.addMultipleTargetTopics(topics)\n\n      try {\n        const requests = await createProducerRequests(responsePerBroker)\n        await Promise.all(requests)\n        return Array.from(responsePerBroker.values()).flat()\n      } catch (e) {\n        if (e.name === 'KafkaJSConnectionClosedError') {\n          cluster.removeBroker({ host: e.host, port: e.port })\n        }\n\n        if (!cluster.isConnected()) {\n          logger.debug(`Cluster has disconnected, reconnecting: ${e.message}`, {\n            retryCount,\n            retryTime,\n          })\n          await cluster.connect()\n          await cluster.refreshMetadata()\n          throw e\n        }\n\n        // This is necessary in case the metadata is stale and the number of partitions\n        // for this topic has increased in the meantime\n        if (\n          staleMetadata(e) ||\n          e.name === 'KafkaJSMetadataNotLoaded' ||\n          e.name === 'KafkaJSConnectionError' ||\n          e.name === 'KafkaJSConnectionClosedError' ||\n          (e.name === 'KafkaJSProtocolError' && e.retriable)\n        ) {\n          logger.error(`Failed to send messages: ${e.message}`, { retryCount, retryTime })\n          await cluster.refreshMetadata()\n          throw e\n        }\n\n        logger.error(`${e.message}`, { retryCount, retryTime })\n        if (e.retriable) throw e\n        bail(e)\n      }\n    })\n  }\n}\n", "const createSendMessages = require('./sendMessages')\nconst { KafkaJSError, KafkaJSNonRetriableError } = require('../errors')\nconst { CONNECTION_STATUS } = require('../network/connectionStatus')\n\nmodule.exports = ({\n  logger,\n  cluster,\n  partitioner,\n  eosManager,\n  idempotent,\n  retrier,\n  getConnectionStatus,\n}) => {\n  const sendMessages = createSendMessages({\n    logger,\n    cluster,\n    retrier,\n    partitioner,\n    eosManager,\n  })\n\n  const validateConnectionStatus = () => {\n    const connectionStatus = getConnectionStatus()\n\n    switch (connectionStatus) {\n      case CONNECTION_STATUS.DISCONNECTING:\n        throw new KafkaJSNonRetriableError(\n          `The producer is disconnecting; therefore, it can't safely accept messages anymore`\n        )\n      case CONNECTION_STATUS.DISCONNECTED:\n        throw new KafkaJSError('The producer is disconnected')\n    }\n  }\n\n  /**\n   * @typedef {Object} TopicMessages\n   * @property {string} topic\n   * @property {Array} messages An array of objects with \"key\" and \"value\", example:\n   *                         [{ key: 'my-key', value: 'my-value'}]\n   *\n   * @typedef {Object} SendBatchRequest\n   * @property {Array<TopicMessages>} topicMessages\n   * @property {number} [acks=-1] Control the number of required acks.\n   *                           -1 = all replicas must acknowledge\n   *                            0 = no acknowledgments\n   *                            1 = only waits for the leader to acknowledge\n   *\n   * @property {number} [timeout=30000] The time to await a response in ms\n   * @property {Compression.Types} [compression=Compression.Types.None] Compression codec\n   *\n   * @param {SendBatchRequest}\n   * @returns {Promise}\n   */\n  const sendBatch = async ({ acks = -1, timeout, compression, topicMessages = [] }) => {\n    if (topicMessages.some(({ topic }) => !topic)) {\n      throw new KafkaJSNonRetriableError(`Invalid topic`)\n    }\n\n    if (idempotent && acks !== -1) {\n      throw new KafkaJSNonRetriableError(\n        `Not requiring ack for all messages invalidates the idempotent producer's EoS guarantees`\n      )\n    }\n\n    for (const { topic, messages } of topicMessages) {\n      if (!messages) {\n        throw new KafkaJSNonRetriableError(\n          `Invalid messages array [${messages}] for topic \"${topic}\"`\n        )\n      }\n\n      const messageWithoutValue = messages.find(message => message.value === undefined)\n      if (messageWithoutValue) {\n        throw new KafkaJSNonRetriableError(\n          `Invalid message without value for topic \"${topic}\": ${JSON.stringify(\n            messageWithoutValue\n          )}`\n        )\n      }\n    }\n\n    validateConnectionStatus()\n    const mergedTopicMessages = topicMessages.reduce((merged, { topic, messages }) => {\n      const index = merged.findIndex(({ topic: mergedTopic }) => topic === mergedTopic)\n\n      if (index === -1) {\n        merged.push({ topic, messages })\n      } else {\n        merged[index].messages = [...merged[index].messages, ...messages]\n      }\n\n      return merged\n    }, [])\n\n    return await sendMessages({\n      acks,\n      timeout,\n      compression,\n      topicMessages: mergedTopicMessages,\n    })\n  }\n\n  /**\n   * @param {ProduceRequest} ProduceRequest\n   * @returns {Promise}\n   *\n   * @typedef {Object} ProduceRequest\n   * @property {string} topic\n   * @property {Array} messages An array of objects with \"key\" and \"value\", example:\n   *                         [{ key: 'my-key', value: 'my-value'}]\n   * @property {number} [acks=-1] Control the number of required acks.\n   *                           -1 = all replicas must acknowledge\n   *                            0 = no acknowledgments\n   *                            1 = only waits for the leader to acknowledge\n   * @property {number} [timeout=30000] The time to await a response in ms\n   * @property {Compression.Types} [compression=Compression.Types.None] Compression codec\n   */\n  const send = async ({ acks, timeout, compression, topic, messages }) => {\n    const topicMessage = { topic, messages }\n    return sendBatch({\n      acks,\n      timeout,\n      compression,\n      topicMessages: [topicMessage],\n    })\n  }\n\n  return {\n    send,\n    sendBatch,\n  }\n}\n", "const { keys } = Object\nmodule.exports = object =>\n  keys(object).reduce((result, key) => ({ ...result, [object[key]]: key }), {})\n", "const swapObject = require('../utils/swapObject')\nconst networkEvents = require('../network/instrumentationEvents')\nconst InstrumentationEventType = require('../instrumentation/eventType')\nconst producerType = InstrumentationEventType('producer')\n\nconst events = {\n  CONNECT: producerType('connect'),\n  DISCONNECT: producerType('disconnect'),\n  REQUEST: producerType(networkEvents.NETWORK_REQUEST),\n  REQUEST_TIMEOUT: producerType(networkEvents.NETWORK_REQUEST_TIMEOUT),\n  REQUEST_QUEUE_SIZE: producerType(networkEvents.NETWORK_REQUEST_QUEUE_SIZE),\n}\n\nconst wrappedEvents = {\n  [events.REQUEST]: networkEvents.NETWORK_REQUEST,\n  [events.REQUEST_TIMEOUT]: networkEvents.NETWORK_REQUEST_TIMEOUT,\n  [events.REQUEST_QUEUE_SIZE]: networkEvents.NETWORK_REQUEST_QUEUE_SIZE,\n}\n\nconst reversedWrappedEvents = swapObject(wrappedEvents)\nconst unwrap = eventName => wrappedEvents[eventName] || eventName\nconst wrap = eventName => reversedWrappedEvents[eventName] || eventName\n\nmodule.exports = {\n  events,\n  wrap,\n  unwrap,\n}\n", "const createRetry = require('../retry')\nconst { CONNECTION_STATUS } = require('../network/connectionStatus')\nconst { DefaultPartitioner } = require('./partitioners/')\nconst InstrumentationEventEmitter = require('../instrumentation/emitter')\nconst createEosManager = require('./eosManager')\nconst createMessageProducer = require('./messageProducer')\nconst { events, wrap: wrapEvent, unwrap: unwrapEvent } = require('./instrumentationEvents')\nconst { KafkaJSNonRetriableError } = require('../errors')\n\nconst { values, keys } = Object\nconst eventNames = values(events)\nconst eventKeys = keys(events)\n  .map(key => `producer.events.${key}`)\n  .join(', ')\n\nconst { CONNECT, DISCONNECT } = events\n\n/**\n *\n * @param {Object} params\n * @param {import('../../types').Cluster} params.cluster\n * @param {import('../../types').Logger} params.logger\n * @param {import('../../types').ICustomPartitioner} [params.createPartitioner]\n * @param {import('../../types').RetryOptions} [params.retry]\n * @param {boolean} [params.idempotent]\n * @param {string} [params.transactionalId]\n * @param {number} [params.transactionTimeout]\n * @param {InstrumentationEventEmitter} [params.instrumentationEmitter]\n *\n * @returns {import('../../types').Producer}\n */\nmodule.exports = ({\n  cluster,\n  logger: rootLogger,\n  createPartitioner = DefaultPartitioner,\n  retry,\n  idempotent = false,\n  transactionalId,\n  transactionTimeout,\n  instrumentationEmitter: rootInstrumentationEmitter,\n}) => {\n  let connectionStatus = CONNECTION_STATUS.DISCONNECTED\n  retry = retry || { retries: idempotent ? Number.MAX_SAFE_INTEGER : 5 }\n\n  if (idempotent && retry.retries < 1) {\n    throw new KafkaJSNonRetriableError(\n      'Idempotent producer must allow retries to protect against transient errors'\n    )\n  }\n\n  const logger = rootLogger.namespace('Producer')\n\n  if (idempotent && retry.retries < Number.MAX_SAFE_INTEGER) {\n    logger.warn('Limiting retries for the idempotent producer may invalidate EoS guarantees')\n  }\n\n  const partitioner = createPartitioner()\n  const retrier = createRetry(Object.assign({}, cluster.retry, retry))\n  const instrumentationEmitter = rootInstrumentationEmitter || new InstrumentationEventEmitter()\n  const idempotentEosManager = createEosManager({\n    logger,\n    cluster,\n    transactionTimeout,\n    transactional: false,\n    transactionalId,\n  })\n\n  const { send, sendBatch } = createMessageProducer({\n    logger,\n    cluster,\n    partitioner,\n    eosManager: idempotentEosManager,\n    idempotent,\n    retrier,\n    getConnectionStatus: () => connectionStatus,\n  })\n\n  let transactionalEosManager\n\n  /** @type {import(\"../../types\").Producer[\"on\"]} */\n  const on = (eventName, listener) => {\n    if (!eventNames.includes(eventName)) {\n      throw new KafkaJSNonRetriableError(`Event name should be one of ${eventKeys}`)\n    }\n\n    return instrumentationEmitter.addListener(unwrapEvent(eventName), event => {\n      event.type = wrapEvent(event.type)\n      Promise.resolve(listener(event)).catch(e => {\n        logger.error(`Failed to execute listener: ${e.message}`, {\n          eventName,\n          stack: e.stack,\n        })\n      })\n    })\n  }\n\n  /**\n   * Begin a transaction. The returned object contains methods to send messages\n   * to the transaction and end the transaction by committing or aborting.\n   *\n   * Only messages sent on the transaction object will participate in the transaction.\n   *\n   * Calling any of the transactional methods after the transaction has ended\n   * will raise an exception (use `isActive` to ascertain if ended).\n   * @returns {Promise<Transaction>}\n   *\n   * @typedef {Object} Transaction\n   * @property {Function} send  Identical to the producer \"send\" method\n   * @property {Function} sendBatch Identical to the producer \"sendBatch\" method\n   * @property {Function} abort Abort the transaction\n   * @property {Function} commit  Commit the transaction\n   * @property {Function} isActive  Whether the transaction is active\n   */\n  const transaction = async () => {\n    if (!transactionalId) {\n      throw new KafkaJSNonRetriableError('Must provide transactional id for transactional producer')\n    }\n\n    let transactionDidEnd = false\n    transactionalEosManager =\n      transactionalEosManager ||\n      createEosManager({\n        logger,\n        cluster,\n        transactionTimeout,\n        transactional: true,\n        transactionalId,\n      })\n\n    if (transactionalEosManager.isInTransaction()) {\n      throw new KafkaJSNonRetriableError(\n        'There is already an ongoing transaction for this producer. Please end the transaction before beginning another.'\n      )\n    }\n\n    // We only initialize the producer id once\n    if (!transactionalEosManager.isInitialized()) {\n      await transactionalEosManager.initProducerId()\n    }\n    transactionalEosManager.beginTransaction()\n\n    const { send: sendTxn, sendBatch: sendBatchTxn } = createMessageProducer({\n      logger,\n      cluster,\n      partitioner,\n      retrier,\n      eosManager: transactionalEosManager,\n      idempotent: true,\n      getConnectionStatus: () => connectionStatus,\n    })\n\n    const isActive = () => transactionalEosManager.isInTransaction() && !transactionDidEnd\n\n    const transactionGuard = fn => (...args) => {\n      if (!isActive()) {\n        return Promise.reject(\n          new KafkaJSNonRetriableError('Cannot continue to use transaction once ended')\n        )\n      }\n\n      return fn(...args)\n    }\n\n    return {\n      sendBatch: transactionGuard(sendBatchTxn),\n      send: transactionGuard(sendTxn),\n      /**\n       * Abort the ongoing transaction.\n       *\n       * @throws {KafkaJSNonRetriableError} If transaction has ended\n       */\n      abort: transactionGuard(async () => {\n        await transactionalEosManager.abort()\n        transactionDidEnd = true\n      }),\n      /**\n       * Commit the ongoing transaction.\n       *\n       * @throws {KafkaJSNonRetriableError} If transaction has ended\n       */\n      commit: transactionGuard(async () => {\n        await transactionalEosManager.commit()\n        transactionDidEnd = true\n      }),\n      /**\n       * Sends a list of specified offsets to the consumer group coordinator, and also marks those offsets as part of the current transaction.\n       *\n       * @throws {KafkaJSNonRetriableError} If transaction has ended\n       */\n      sendOffsets: transactionGuard(async ({ consumerGroupId, topics }) => {\n        await transactionalEosManager.sendOffsets({ consumerGroupId, topics })\n\n        for (const topicOffsets of topics) {\n          const { topic, partitions } = topicOffsets\n          for (const { partition, offset } of partitions) {\n            cluster.markOffsetAsCommitted({\n              groupId: consumerGroupId,\n              topic,\n              partition,\n              offset,\n            })\n          }\n        }\n      }),\n      isActive,\n    }\n  }\n\n  /**\n   * @returns {Object} logger\n   */\n  const getLogger = () => logger\n\n  return {\n    /**\n     * @returns {Promise}\n     */\n    connect: async () => {\n      await cluster.connect()\n      connectionStatus = CONNECTION_STATUS.CONNECTED\n      instrumentationEmitter.emit(CONNECT)\n\n      if (idempotent && !idempotentEosManager.isInitialized()) {\n        await idempotentEosManager.initProducerId()\n      }\n    },\n    /**\n     * @return {Promise}\n     */\n    disconnect: async () => {\n      connectionStatus = CONNECTION_STATUS.DISCONNECTING\n      await cluster.disconnect()\n      connectionStatus = CONNECTION_STATUS.DISCONNECTED\n      instrumentationEmitter.emit(DISCONNECT)\n    },\n    isIdempotent: () => {\n      return idempotent\n    },\n    events,\n    on,\n    send,\n    sendBatch,\n    transaction,\n    logger: getLogger,\n  }\n}\n", "module.exports = timeInMs =>\n  new Promise(resolve => {\n    setTimeout(resolve, timeInMs)\n  })\n", "const Long = require('../../utils/long')\n\nmodule.exports = offset => (!offset && offset !== 0) || Long.fromValue(offset).isNegative()\n", "const isInvalidOffset = require('./isInvalidOffset')\nconst { keys, assign } = Object\n\nconst indexPartitions = (obj, { partition, offset }) => assign(obj, { [partition]: offset })\nconst indexTopics = (obj, { topic, partitions }) =>\n  assign(obj, { [topic]: partitions.reduce(indexPartitions, {}) })\n\nmodule.exports = (consumerOffsets, topicOffsets) => {\n  const indexedConsumerOffsets = consumerOffsets.reduce(indexTopics, {})\n  const indexedTopicOffsets = topicOffsets.reduce(indexTopics, {})\n\n  return keys(indexedConsumerOffsets).map(topic => {\n    const partitions = indexedConsumerOffsets[topic]\n    return {\n      topic,\n      partitions: keys(partitions).map(partition => {\n        const offset = partitions[partition]\n        const resolvedOffset = isInvalidOffset(offset)\n          ? indexedTopicOffsets[topic][partition]\n          : offset\n\n        return { partition: Number(partition), offset: resolvedOffset }\n      }),\n    }\n  })\n}\n", "const swapObject = require('../utils/swapObject')\nconst InstrumentationEventType = require('../instrumentation/eventType')\nconst networkEvents = require('../network/instrumentationEvents')\nconst consumerType = InstrumentationEventType('consumer')\n\n/** @type {import('types').ConsumerEvents} */\nconst events = {\n  HEARTBEAT: consumerType('heartbeat'),\n  COMMIT_OFFSETS: consumerType('commit_offsets'),\n  GROUP_JOIN: consumerType('group_join'),\n  FETCH: consumerType('fetch'),\n  FETCH_START: consumerType('fetch_start'),\n  START_BATCH_PROCESS: consumerType('start_batch_process'),\n  END_BATCH_PROCESS: consumerType('end_batch_process'),\n  CONNECT: consumerType('connect'),\n  DISCONNECT: consumerType('disconnect'),\n  STOP: consumerType('stop'),\n  CRASH: consumerType('crash'),\n  REBALANCING: consumerType('rebalancing'),\n  RECEIVED_UNSUBSCRIBED_TOPICS: consumerType('received_unsubscribed_topics'),\n  REQUEST: consumerType(networkEvents.NETWORK_REQUEST),\n  REQUEST_TIMEOUT: consumerType(networkEvents.NETWORK_REQUEST_TIMEOUT),\n  REQUEST_QUEUE_SIZE: consumerType(networkEvents.NETWORK_REQUEST_QUEUE_SIZE),\n}\n\nconst wrappedEvents = {\n  [events.REQUEST]: networkEvents.NETWORK_REQUEST,\n  [events.REQUEST_TIMEOUT]: networkEvents.NETWORK_REQUEST_TIMEOUT,\n  [events.REQUEST_QUEUE_SIZE]: networkEvents.NETWORK_REQUEST_QUEUE_SIZE,\n}\n\nconst reversedWrappedEvents = swapObject(wrappedEvents)\nconst unwrap = eventName => wrappedEvents[eventName] || eventName\nconst wrap = eventName => reversedWrappedEvents[eventName] || eventName\n\nmodule.exports = {\n  events,\n  wrap,\n  unwrap,\n}\n", "const Long = require('../../utils/long')\nconst isInvalidOffset = require('./isInvalidOffset')\nconst initializeConsumerOffsets = require('./initializeConsumerOffsets')\nconst {\n  events: { COMMIT_OFFSETS },\n} = require('../instrumentationEvents')\n\nconst { keys, assign } = Object\nconst indexTopics = topics => topics.reduce((obj, topic) => assign(obj, { [topic]: {} }), {})\n\nconst PRIVATE = {\n  COMMITTED_OFFSETS: Symbol('private:OffsetManager:committedOffsets'),\n}\nmodule.exports = class OffsetManager {\n  /**\n   * @param {Object} options\n   * @param {import(\"../../../types\").Cluster} options.cluster\n   * @param {import(\"../../../types\").Broker} options.coordinator\n   * @param {import(\"../../../types\").IMemberAssignment} options.memberAssignment\n   * @param {boolean} options.autoCommit\n   * @param {number | null} options.autoCommitInterval\n   * @param {number | null} options.autoCommitThreshold\n   * @param {{[topic: string]: { fromBeginning: boolean }}} options.topicConfigurations\n   * @param {import(\"../../instrumentation/emitter\")} options.instrumentationEmitter\n   * @param {string} options.groupId\n   * @param {number} options.generationId\n   * @param {string} options.memberId\n   */\n  constructor({\n    cluster,\n    coordinator,\n    memberAssignment,\n    autoCommit,\n    autoCommitInterval,\n    autoCommitThreshold,\n    topicConfigurations,\n    instrumentationEmitter,\n    groupId,\n    generationId,\n    memberId,\n  }) {\n    this.cluster = cluster\n    this.coordinator = coordinator\n\n    // memberAssignment format:\n    // {\n    //   'topic1': [0, 1, 2, 3],\n    //   'topic2': [0, 1, 2, 3, 4, 5],\n    // }\n    this.memberAssignment = memberAssignment\n\n    this.topicConfigurations = topicConfigurations\n    this.instrumentationEmitter = instrumentationEmitter\n    this.groupId = groupId\n    this.generationId = generationId\n    this.memberId = memberId\n\n    this.autoCommit = autoCommit\n    this.autoCommitInterval = autoCommitInterval\n    this.autoCommitThreshold = autoCommitThreshold\n    this.lastCommit = Date.now()\n\n    this.topics = keys(memberAssignment)\n    this.clearAllOffsets()\n  }\n\n  /**\n   * @param {string} topic\n   * @param {number} partition\n   * @returns {Long}\n   */\n  nextOffset(topic, partition) {\n    if (!this.resolvedOffsets[topic][partition]) {\n      this.resolvedOffsets[topic][partition] = this.committedOffsets()[topic][partition]\n    }\n\n    let offset = this.resolvedOffsets[topic][partition]\n    if (isInvalidOffset(offset)) {\n      offset = '0'\n    }\n\n    return Long.fromValue(offset)\n  }\n\n  /**\n   * @returns {Promise<import(\"../../../types\").Broker>}\n   */\n  async getCoordinator() {\n    if (!this.coordinator.isConnected()) {\n      this.coordinator = await this.cluster.findBroker(this.coordinator)\n    }\n\n    return this.coordinator\n  }\n\n  /**\n   * @param {import(\"../../../types\").TopicPartition} topicPartition\n   */\n  resetOffset({ topic, partition }) {\n    this.resolvedOffsets[topic][partition] = this.committedOffsets()[topic][partition]\n  }\n\n  /**\n   * @param {import(\"../../../types\").TopicPartitionOffset} topicPartitionOffset\n   */\n  resolveOffset({ topic, partition, offset }) {\n    this.resolvedOffsets[topic][partition] = Long.fromValue(offset)\n      .add(1)\n      .toString()\n  }\n\n  /**\n   * @returns {Long}\n   */\n  countResolvedOffsets() {\n    const committedOffsets = this.committedOffsets()\n\n    const subtractOffsets = (resolvedOffset, committedOffset) => {\n      const resolvedOffsetLong = Long.fromValue(resolvedOffset)\n      return isInvalidOffset(committedOffset)\n        ? resolvedOffsetLong\n        : resolvedOffsetLong.subtract(Long.fromValue(committedOffset))\n    }\n\n    const subtractPartitionOffsets = (resolvedTopicOffsets, committedTopicOffsets) =>\n      keys(resolvedTopicOffsets).map(partition =>\n        subtractOffsets(resolvedTopicOffsets[partition], committedTopicOffsets[partition])\n      )\n\n    const subtractTopicOffsets = topic =>\n      subtractPartitionOffsets(this.resolvedOffsets[topic], committedOffsets[topic])\n\n    const offsetsDiff = this.topics.flatMap(subtractTopicOffsets)\n    return offsetsDiff.reduce((sum, offset) => sum.add(offset), Long.fromValue(0))\n  }\n\n  /**\n   * @param {import(\"../../../types\").TopicPartition} topicPartition\n   */\n  async setDefaultOffset({ topic, partition }) {\n    const { groupId, generationId, memberId } = this\n    const defaultOffset = this.cluster.defaultOffset(this.topicConfigurations[topic])\n    const coordinator = await this.getCoordinator()\n\n    await coordinator.offsetCommit({\n      groupId,\n      memberId,\n      groupGenerationId: generationId,\n      topics: [\n        {\n          topic,\n          partitions: [{ partition, offset: defaultOffset }],\n        },\n      ],\n    })\n\n    this.clearOffsets({ topic, partition })\n  }\n\n  /**\n   * Commit the given offset to the topic/partition. If the consumer isn't assigned to the given\n   * topic/partition this method will be a NO-OP.\n   *\n   * @param {import(\"../../../types\").TopicPartitionOffset} topicPartitionOffset\n   */\n  async seek({ topic, partition, offset }) {\n    if (!this.memberAssignment[topic] || !this.memberAssignment[topic].includes(partition)) {\n      return\n    }\n\n    if (!this.autoCommit) {\n      this.resolveOffset({\n        topic,\n        partition,\n        offset: Long.fromValue(offset)\n          .subtract(1)\n          .toString(),\n      })\n      return\n    }\n\n    const { groupId, generationId, memberId } = this\n    const coordinator = await this.getCoordinator()\n\n    await coordinator.offsetCommit({\n      groupId,\n      memberId,\n      groupGenerationId: generationId,\n      topics: [\n        {\n          topic,\n          partitions: [{ partition, offset }],\n        },\n      ],\n    })\n\n    this.clearOffsets({ topic, partition })\n  }\n\n  async commitOffsetsIfNecessary() {\n    const now = Date.now()\n\n    const timeoutReached =\n      this.autoCommitInterval != null && now >= this.lastCommit + this.autoCommitInterval\n\n    const thresholdReached =\n      this.autoCommitThreshold != null &&\n      this.countResolvedOffsets().gte(Long.fromValue(this.autoCommitThreshold))\n\n    if (timeoutReached || thresholdReached) {\n      return this.commitOffsets()\n    }\n  }\n\n  /**\n   * Return all locally resolved offsets which are not marked as committed, by topic-partition.\n   * @returns {import('../../../types').OffsetsByTopicPartition}\n   */\n  uncommittedOffsets() {\n    const offsets = topic => keys(this.resolvedOffsets[topic])\n    const emptyPartitions = ({ partitions }) => partitions.length > 0\n    const toPartitions = topic => partition => ({\n      partition,\n      offset: this.resolvedOffsets[topic][partition],\n    })\n    const changedOffsets = topic => ({ partition, offset }) => {\n      return (\n        offset !== this.committedOffsets()[topic][partition] &&\n        Long.fromValue(offset).greaterThanOrEqual(0)\n      )\n    }\n\n    // Select and format updated partitions\n    const topicsWithPartitionsToCommit = this.topics\n      .map(topic => ({\n        topic,\n        partitions: offsets(topic)\n          .map(toPartitions(topic))\n          .filter(changedOffsets(topic)),\n      }))\n      .filter(emptyPartitions)\n\n    return { topics: topicsWithPartitionsToCommit }\n  }\n\n  async commitOffsets(offsets = {}) {\n    const { groupId, generationId, memberId } = this\n    const { topics = this.uncommittedOffsets().topics } = offsets\n\n    if (topics.length === 0) {\n      this.lastCommit = Date.now()\n      return\n    }\n\n    const payload = {\n      groupId,\n      memberId,\n      groupGenerationId: generationId,\n      topics,\n    }\n\n    try {\n      const coordinator = await this.getCoordinator()\n      await coordinator.offsetCommit(payload)\n      this.instrumentationEmitter.emit(COMMIT_OFFSETS, payload)\n\n      // Update local reference of committed offsets\n      topics.forEach(({ topic, partitions }) => {\n        const updatedOffsets = partitions.reduce(\n          (obj, { partition, offset }) => assign(obj, { [partition]: offset }),\n          {}\n        )\n\n        this[PRIVATE.COMMITTED_OFFSETS][topic] = assign(\n          {},\n          this.committedOffsets()[topic],\n          updatedOffsets\n        )\n      })\n\n      this.lastCommit = Date.now()\n    } catch (e) {\n      // metadata is stale, the coordinator has changed due to a restart or\n      // broker reassignment\n      if (e.type === 'NOT_COORDINATOR_FOR_GROUP') {\n        await this.cluster.refreshMetadata()\n      }\n\n      throw e\n    }\n  }\n\n  async resolveOffsets() {\n    const { groupId } = this\n    const invalidOffset = topic => partition => {\n      return isInvalidOffset(this.committedOffsets()[topic][partition])\n    }\n\n    const pendingPartitions = this.topics\n      .map(topic => ({\n        topic,\n        partitions: this.memberAssignment[topic]\n          .filter(invalidOffset(topic))\n          .map(partition => ({ partition })),\n      }))\n      .filter(t => t.partitions.length > 0)\n\n    if (pendingPartitions.length === 0) {\n      return\n    }\n\n    const coordinator = await this.getCoordinator()\n    const { responses: consumerOffsets } = await coordinator.offsetFetch({\n      groupId,\n      topics: pendingPartitions,\n    })\n\n    const unresolvedPartitions = consumerOffsets.map(({ topic, partitions }) =>\n      assign(\n        {\n          topic,\n          partitions: partitions\n            .filter(({ offset }) => isInvalidOffset(offset))\n            .map(({ partition }) => assign({ partition })),\n        },\n        this.topicConfigurations[topic]\n      )\n    )\n\n    const indexPartitions = (obj, { partition, offset }) => {\n      return assign(obj, { [partition]: offset })\n    }\n\n    const hasUnresolvedPartitions = () => unresolvedPartitions.some(t => t.partitions.length > 0)\n\n    let offsets = consumerOffsets\n    if (hasUnresolvedPartitions()) {\n      const topicOffsets = await this.cluster.fetchTopicsOffset(unresolvedPartitions)\n      offsets = initializeConsumerOffsets(consumerOffsets, topicOffsets)\n    }\n\n    offsets.forEach(({ topic, partitions }) => {\n      this.committedOffsets()[topic] = partitions.reduce(indexPartitions, {\n        ...this.committedOffsets()[topic],\n      })\n    })\n  }\n\n  /**\n   * @private\n   * @param {import(\"../../../types\").TopicPartition} topicPartition\n   */\n  clearOffsets({ topic, partition }) {\n    delete this.committedOffsets()[topic][partition]\n    delete this.resolvedOffsets[topic][partition]\n  }\n\n  /**\n   * @private\n   */\n  clearAllOffsets() {\n    const committedOffsets = this.committedOffsets()\n\n    for (const topic in committedOffsets) {\n      delete committedOffsets[topic]\n    }\n\n    for (const topic of this.topics) {\n      committedOffsets[topic] = {}\n    }\n\n    this.resolvedOffsets = indexTopics(this.topics)\n  }\n\n  committedOffsets() {\n    if (!this[PRIVATE.COMMITTED_OFFSETS]) {\n      this[PRIVATE.COMMITTED_OFFSETS] = this.groupId\n        ? this.cluster.committedOffsets({ groupId: this.groupId })\n        : {}\n    }\n\n    return this[PRIVATE.COMMITTED_OFFSETS]\n  }\n}\n", "const Long = require('../utils/long')\nconst ABORTED_MESSAGE_KEY = Buffer.from([0, 0, 0, 0])\n\nconst isAbortMarker = ({ key }) => {\n  // Handle null/undefined keys.\n  if (!key) return false\n  // Cast key to buffer defensively\n  return Buffer.from(key).equals(ABORTED_MESSAGE_KEY)\n}\n\n/**\n * Remove messages marked as aborted according to the aborted transactions list.\n *\n * Start of an aborted transaction is determined by message offset.\n * End of an aborted transaction is determined by control messages.\n * @param {Message[]} messages\n * @param {Transaction[]} [abortedTransactions]\n * @returns {Message[]} Messages which did not participate in an aborted transaction\n *\n * @typedef {object} Message\n * @param {Buffer} key\n * @param {lastOffset} key  Int64\n * @param {RecordBatch}  batchContext\n *\n * @typedef {object} Transaction\n * @param {string} firstOffset  Int64\n * @param {string} producerId  Int64\n *\n * @typedef {object} RecordBatch\n * @param {string}  producerId  Int64\n * @param {boolean}  inTransaction\n */\nmodule.exports = ({ messages, abortedTransactions }) => {\n  const currentAbortedTransactions = new Map()\n\n  if (!abortedTransactions || !abortedTransactions.length) {\n    return messages\n  }\n\n  const remainingAbortedTransactions = [...abortedTransactions]\n\n  return messages.filter(message => {\n    // If the message offset is GTE the first offset of the next aborted transaction\n    // then we have stepped into an aborted transaction.\n    if (\n      remainingAbortedTransactions.length &&\n      Long.fromValue(message.offset).gte(remainingAbortedTransactions[0].firstOffset)\n    ) {\n      const { producerId } = remainingAbortedTransactions.shift()\n      currentAbortedTransactions.set(producerId, true)\n    }\n\n    const { producerId, inTransaction } = message.batchContext\n\n    if (isAbortMarker(message)) {\n      // Transaction is over, we no longer need to ignore messages from this producer\n      currentAbortedTransactions.delete(producerId)\n    } else if (currentAbortedTransactions.has(producerId) && inTransaction) {\n      return false\n    }\n\n    return true\n  })\n}\n", "const Long = require('../utils/long')\nconst filterAbortedMessages = require('./filterAbortedMessages')\n\n/**\n * A batch collects messages returned from a single fetch call.\n *\n * A batch could contain _multiple_ Kafka RecordBatches.\n */\nmodule.exports = class Batch {\n  constructor(topic, fetchedOffset, partitionData) {\n    this.fetchedOffset = fetchedOffset\n    const longFetchedOffset = Long.fromValue(this.fetchedOffset)\n    const { abortedTransactions, messages } = partitionData\n\n    this.topic = topic\n    this.partition = partitionData.partition\n    this.highWatermark = partitionData.highWatermark\n\n    this.rawMessages = messages\n    // Apparently fetch can return different offsets than the target offset provided to the fetch API.\n    // Discard messages that are not in the requested offset\n    // https://github.com/apache/kafka/blob/bf237fa7c576bd141d78fdea9f17f65ea269c290/clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java#L912\n    this.messagesWithinOffset = this.rawMessages.filter(message =>\n      Long.fromValue(message.offset).gte(longFetchedOffset)\n    )\n\n    // 1. Don't expose aborted messages\n    // 2. Don't expose control records\n    // @see https://kafka.apache.org/documentation/#controlbatch\n    this.messages = filterAbortedMessages({\n      messages: this.messagesWithinOffset,\n      abortedTransactions,\n    }).filter(message => !message.isControlRecord)\n  }\n\n  isEmpty() {\n    return this.messages.length === 0\n  }\n\n  isEmptyIncludingFiltered() {\n    return this.messagesWithinOffset.length === 0\n  }\n\n  /**\n   * If the batch contained raw messages (i.e was not truly empty) but all messages were filtered out due to\n   * log compaction, control records or other reasons\n   */\n  isEmptyDueToFiltering() {\n    return this.isEmpty() && this.rawMessages.length > 0\n  }\n\n  isEmptyControlRecord() {\n    return (\n      this.isEmpty() && this.messagesWithinOffset.some(({ isControlRecord }) => isControlRecord)\n    )\n  }\n\n  /**\n   * With compressed messages, it's possible for the returned messages to have offsets smaller than the starting offset.\n   * These messages will be filtered out (i.e. they are not even included in this.messagesWithinOffset)\n   * If these are the only messages, the batch will appear as an empty batch.\n   *\n   * isEmpty() and isEmptyIncludingFiltered() will always return true if the batch is empty,\n   * but this method will only return true if the batch is empty due to log compacted messages.\n   *\n   * @returns boolean True if the batch is empty, because of log compacted messages in the partition.\n   */\n  isEmptyDueToLogCompactedMessages() {\n    const hasMessages = this.rawMessages.length > 0\n    return hasMessages && this.isEmptyIncludingFiltered()\n  }\n\n  firstOffset() {\n    return this.isEmptyIncludingFiltered() ? null : this.messagesWithinOffset[0].offset\n  }\n\n  lastOffset() {\n    if (this.isEmptyDueToLogCompactedMessages()) {\n      return this.fetchedOffset\n    }\n\n    if (this.isEmptyIncludingFiltered()) {\n      return Long.fromValue(this.highWatermark)\n        .add(-1)\n        .toString()\n    }\n\n    return this.messagesWithinOffset[this.messagesWithinOffset.length - 1].offset\n  }\n\n  /**\n   * Returns the lag based on the last offset in the batch (also known as \"high\")\n   */\n  offsetLag() {\n    const lastOffsetOfPartition = Long.fromValue(this.highWatermark).add(-1)\n    const lastConsumedOffset = Long.fromValue(this.lastOffset())\n    return lastOffsetOfPartition.add(lastConsumedOffset.multiply(-1)).toString()\n  }\n\n  /**\n   * Returns the lag based on the first offset in the batch\n   */\n  offsetLagLow() {\n    if (this.isEmptyIncludingFiltered()) {\n      return '0'\n    }\n\n    const lastOffsetOfPartition = Long.fromValue(this.highWatermark).add(-1)\n    const firstConsumedOffset = Long.fromValue(this.firstOffset())\n    return lastOffsetOfPartition.add(firstConsumedOffset.multiply(-1)).toString()\n  }\n}\n", "module.exports = class SeekOffsets extends Map {\n  getKey(topic, partition) {\n    return JSON.stringify([topic, partition])\n  }\n\n  set(topic, partition, offset) {\n    const key = this.getKey(topic, partition)\n    super.set(key, offset)\n  }\n\n  has(topic, partition) {\n    const key = this.getKey(topic, partition)\n    return super.has(key)\n  }\n\n  pop(topic, partition) {\n    if (this.size === 0 || !this.has(topic, partition)) {\n      return\n    }\n\n    const key = this.getKey(topic, partition)\n    const offset = this.get(key)\n\n    this.delete(key)\n    return { topic, partition, offset }\n  }\n}\n", "const createState = topic => ({\n  topic,\n  paused: new Set(),\n  pauseAll: false,\n  resumed: new Set(),\n})\n\nmodule.exports = class SubscriptionState {\n  constructor() {\n    this.assignedPartitionsByTopic = {}\n    this.subscriptionStatesByTopic = {}\n  }\n\n  /**\n   * Replace the current assignment with a new set of assignments\n   *\n   * @param {Array<TopicPartitions>} topicPartitions Example: [{ topic: 'topic-name', partitions: [1, 2] }]\n   */\n  assign(topicPartitions = []) {\n    this.assignedPartitionsByTopic = topicPartitions.reduce(\n      (assigned, { topic, partitions = [] }) => {\n        return { ...assigned, [topic]: { topic, partitions } }\n      },\n      {}\n    )\n  }\n\n  /**\n   * @param {Array<TopicPartitions>} topicPartitions Example: [{ topic: 'topic-name', partitions: [1, 2] }]\n   */\n  pause(topicPartitions = []) {\n    topicPartitions.forEach(({ topic, partitions }) => {\n      const state = this.subscriptionStatesByTopic[topic] || createState(topic)\n\n      if (typeof partitions === 'undefined') {\n        state.paused.clear()\n        state.resumed.clear()\n        state.pauseAll = true\n      } else if (Array.isArray(partitions)) {\n        partitions.forEach(partition => {\n          state.paused.add(partition)\n          state.resumed.delete(partition)\n        })\n        state.pauseAll = false\n      }\n\n      this.subscriptionStatesByTopic[topic] = state\n    })\n  }\n\n  /**\n   * @param {Array<TopicPartitions>} topicPartitions Example: [{ topic: 'topic-name', partitions: [1, 2] }]\n   */\n  resume(topicPartitions = []) {\n    topicPartitions.forEach(({ topic, partitions }) => {\n      const state = this.subscriptionStatesByTopic[topic] || createState(topic)\n\n      if (typeof partitions === 'undefined') {\n        state.paused.clear()\n        state.resumed.clear()\n        state.pauseAll = false\n      } else if (Array.isArray(partitions)) {\n        partitions.forEach(partition => {\n          state.paused.delete(partition)\n\n          if (state.pauseAll) {\n            state.resumed.add(partition)\n          }\n        })\n      }\n\n      this.subscriptionStatesByTopic[topic] = state\n    })\n  }\n\n  /**\n   * @returns {Array<import(\"../../types\").TopicPartitions>} topicPartitions\n   * Example: [{ topic: 'topic-name', partitions: [1, 2] }]\n   */\n  assigned() {\n    return Object.values(this.assignedPartitionsByTopic).map(({ topic, partitions }) => ({\n      topic,\n      partitions: partitions.sort(),\n    }))\n  }\n\n  /**\n   * @returns {Array<import(\"../../types\").TopicPartitions>} topicPartitions\n   * Example: [{ topic: 'topic-name', partitions: [1, 2] }]\n   */\n  active() {\n    return Object.values(this.assignedPartitionsByTopic).map(({ topic, partitions }) => ({\n      topic,\n      partitions: partitions.filter(partition => !this.isPaused(topic, partition)).sort(),\n    }))\n  }\n\n  /**\n   * @returns {Array<import(\"../../types\").TopicPartitions>} topicPartitions\n   * Example: [{ topic: 'topic-name', partitions: [1, 2] }]\n   */\n  paused() {\n    return Object.values(this.assignedPartitionsByTopic)\n      .map(({ topic, partitions }) => ({\n        topic,\n        partitions: partitions.filter(partition => this.isPaused(topic, partition)).sort(),\n      }))\n      .filter(({ partitions }) => partitions.length !== 0)\n  }\n\n  isPaused(topic, partition) {\n    const state = this.subscriptionStatesByTopic[topic]\n\n    if (!state) {\n      return false\n    }\n\n    const partitionResumed = state.resumed.has(partition)\n    const partitionPaused = state.paused.has(partition)\n\n    return (state.pauseAll && !partitionResumed) || partitionPaused\n  }\n}\n", "const Encoder = require('../protocol/encoder')\nconst Decoder = require('../protocol/decoder')\n\nconst MemberMetadata = {\n  /**\n   * @param {Object} metadata\n   * @param {number} metadata.version\n   * @param {Array<string>} metadata.topics\n   * @param {Buffer} [metadata.userData=Buffer.alloc(0)]\n   *\n   * @returns Buffer\n   */\n  encode({ version, topics, userData = Buffer.alloc(0) }) {\n    return new Encoder()\n      .writeInt16(version)\n      .writeArray(topics)\n      .writeBytes(userData).buffer\n  },\n\n  /**\n   * @param {Buffer} buffer\n   * @returns {Object}\n   */\n  decode(buffer) {\n    const decoder = new Decoder(buffer)\n    return {\n      version: decoder.readInt16(),\n      topics: decoder.readArray(d => d.readString()),\n      userData: decoder.readBytes(),\n    }\n  },\n}\n\nconst MemberAssignment = {\n  /**\n   * @param {object} options\n   * @param {number} options.version\n   * @param {Object<String,Array>} options.assignment, example:\n   *                               {\n   *                                 'topic-A': [0, 2, 4, 6],\n   *                                 'topic-B': [0, 2],\n   *                               }\n   * @param {Buffer} [options.userData=Buffer.alloc(0)]\n   *\n   * @returns Buffer\n   */\n  encode({ version, assignment, userData = Buffer.alloc(0) }) {\n    return new Encoder()\n      .writeInt16(version)\n      .writeArray(\n        Object.keys(assignment).map(topic =>\n          new Encoder().writeString(topic).writeArray(assignment[topic])\n        )\n      )\n      .writeBytes(userData).buffer\n  },\n\n  /**\n   * @param {Buffer} buffer\n   * @returns {Object|null}\n   */\n  decode(buffer) {\n    const decoder = new Decoder(buffer)\n    const decodePartitions = d => d.readInt32()\n    const decodeAssignment = d => ({\n      topic: d.readString(),\n      partitions: d.readArray(decodePartitions),\n    })\n    const indexAssignment = (obj, { topic, partitions }) =>\n      Object.assign(obj, { [topic]: partitions })\n\n    if (!decoder.canReadInt16()) {\n      return null\n    }\n\n    return {\n      version: decoder.readInt16(),\n      assignment: decoder.readArray(decodeAssignment).reduce(indexAssignment, {}),\n      userData: decoder.readBytes(),\n    }\n  },\n}\n\nmodule.exports = {\n  MemberMetadata,\n  MemberAssignment,\n}\n", "const sleep = require('../utils/sleep')\nconst websiteUrl = require('../utils/websiteUrl')\nconst arrayDiff = require('../utils/arrayDiff')\nconst createRetry = require('../retry')\nconst sharedPromiseTo = require('../utils/sharedPromiseTo')\n\nconst OffsetManager = require('./offsetManager')\nconst Batch = require('./batch')\nconst SeekOffsets = require('./seekOffsets')\nconst SubscriptionState = require('./subscriptionState')\nconst {\n  events: { GROUP_JOIN, HEARTBEAT, CONNECT, RECEIVED_UNSUBSCRIBED_TOPICS },\n} = require('./instrumentationEvents')\nconst { MemberAssignment } = require('./assignerProtocol')\nconst {\n  KafkaJSError,\n  KafkaJSNonRetriableError,\n  KafkaJSStaleTopicMetadataAssignment,\n  isRebalancing,\n} = require('../errors')\n\nconst { keys } = Object\n\nconst STALE_METADATA_ERRORS = [\n  'LEADER_NOT_AVAILABLE',\n  // Fetch before v9 uses NOT_LEADER_FOR_PARTITION\n  'NOT_LEADER_FOR_PARTITION',\n  // Fetch after v9 uses {FENCED,UNKNOWN}_LEADER_EPOCH\n  'FENCED_LEADER_EPOCH',\n  'UNKNOWN_LEADER_EPOCH',\n  'UNKNOWN_TOPIC_OR_PARTITION',\n]\n\nconst PRIVATE = {\n  JOIN: Symbol('private:ConsumerGroup:join'),\n  SYNC: Symbol('private:ConsumerGroup:sync'),\n  SHARED_HEARTBEAT: Symbol('private:ConsumerGroup:sharedHeartbeat'),\n}\n\nmodule.exports = class ConsumerGroup {\n  /**\n   * @param {object} options\n   * @param {import('../../types').RetryOptions} options.retry\n   * @param {import('../../types').Cluster} options.cluster\n   * @param {string} options.groupId\n   * @param {string[]} options.topics\n   * @param {Record<string, { fromBeginning?: boolean }>} options.topicConfigurations\n   * @param {import('../../types').Logger} options.logger\n   * @param {import('../instrumentation/emitter')} options.instrumentationEmitter\n   * @param {import('../../types').Assigner[]} options.assigners\n   * @param {number} options.sessionTimeout\n   * @param {number} options.rebalanceTimeout\n   * @param {number} options.maxBytesPerPartition\n   * @param {number} options.minBytes\n   * @param {number} options.maxBytes\n   * @param {number} options.maxWaitTimeInMs\n   * @param {boolean} options.autoCommit\n   * @param {number} options.autoCommitInterval\n   * @param {number} options.autoCommitThreshold\n   * @param {number} options.isolationLevel\n   * @param {string} options.rackId\n   * @param {number} options.metadataMaxAge\n   */\n  constructor({\n    retry,\n    cluster,\n    groupId,\n    topics,\n    topicConfigurations,\n    logger,\n    instrumentationEmitter,\n    assigners,\n    sessionTimeout,\n    rebalanceTimeout,\n    maxBytesPerPartition,\n    minBytes,\n    maxBytes,\n    maxWaitTimeInMs,\n    autoCommit,\n    autoCommitInterval,\n    autoCommitThreshold,\n    isolationLevel,\n    rackId,\n    metadataMaxAge,\n  }) {\n    /** @type {import(\"../../types\").Cluster} */\n    this.cluster = cluster\n    this.groupId = groupId\n    this.topics = topics\n    this.topicsSubscribed = topics\n    this.topicConfigurations = topicConfigurations\n    this.logger = logger.namespace('ConsumerGroup')\n    this.instrumentationEmitter = instrumentationEmitter\n    this.retrier = createRetry(Object.assign({}, retry))\n    this.assigners = assigners\n    this.sessionTimeout = sessionTimeout\n    this.rebalanceTimeout = rebalanceTimeout\n    this.maxBytesPerPartition = maxBytesPerPartition\n    this.minBytes = minBytes\n    this.maxBytes = maxBytes\n    this.maxWaitTime = maxWaitTimeInMs\n    this.autoCommit = autoCommit\n    this.autoCommitInterval = autoCommitInterval\n    this.autoCommitThreshold = autoCommitThreshold\n    this.isolationLevel = isolationLevel\n    this.rackId = rackId\n    this.metadataMaxAge = metadataMaxAge\n\n    this.seekOffset = new SeekOffsets()\n    this.coordinator = null\n    this.generationId = null\n    this.leaderId = null\n    this.memberId = null\n    this.members = null\n    this.groupProtocol = null\n\n    this.partitionsPerSubscribedTopic = null\n    /**\n     * Preferred read replica per topic and partition\n     *\n     * Each of the partitions tracks the preferred read replica (`nodeId`) and a timestamp\n     * until when that preference is valid.\n     *\n     * @type {{[topicName: string]: {[partition: number]: {nodeId: number, expireAt: number}}}}\n     */\n    this.preferredReadReplicasPerTopicPartition = {}\n    this.offsetManager = null\n    this.subscriptionState = new SubscriptionState()\n\n    this.lastRequest = Date.now()\n\n    this[PRIVATE.SHARED_HEARTBEAT] = sharedPromiseTo(async ({ interval }) => {\n      const { groupId, generationId, memberId } = this\n      const now = Date.now()\n\n      if (memberId && now >= this.lastRequest + interval) {\n        const payload = {\n          groupId,\n          memberId,\n          groupGenerationId: generationId,\n        }\n\n        await this.coordinator.heartbeat(payload)\n        this.instrumentationEmitter.emit(HEARTBEAT, payload)\n        this.lastRequest = Date.now()\n      }\n    })\n  }\n\n  isLeader() {\n    return this.leaderId && this.memberId === this.leaderId\n  }\n\n  getNodeIds() {\n    return this.cluster.getNodeIds()\n  }\n\n  async connect() {\n    await this.cluster.connect()\n    this.instrumentationEmitter.emit(CONNECT)\n    await this.cluster.refreshMetadataIfNecessary()\n  }\n\n  async [PRIVATE.JOIN]() {\n    const { groupId, sessionTimeout, rebalanceTimeout } = this\n\n    this.coordinator = await this.cluster.findGroupCoordinator({ groupId })\n\n    const groupData = await this.coordinator.joinGroup({\n      groupId,\n      sessionTimeout,\n      rebalanceTimeout,\n      memberId: this.memberId || '',\n      groupProtocols: this.assigners.map(assigner =>\n        assigner.protocol({\n          topics: this.topicsSubscribed,\n        })\n      ),\n    })\n\n    this.generationId = groupData.generationId\n    this.leaderId = groupData.leaderId\n    this.memberId = groupData.memberId\n    this.members = groupData.members\n    this.groupProtocol = groupData.groupProtocol\n  }\n\n  async leave() {\n    const { groupId, memberId } = this\n    if (memberId) {\n      await this.coordinator.leaveGroup({ groupId, memberId })\n      this.memberId = null\n    }\n  }\n\n  async [PRIVATE.SYNC]() {\n    let assignment = []\n    const {\n      groupId,\n      generationId,\n      memberId,\n      members,\n      groupProtocol,\n      topics,\n      topicsSubscribed,\n      coordinator,\n    } = this\n\n    if (this.isLeader()) {\n      this.logger.debug('Chosen as group leader', { groupId, generationId, memberId, topics })\n      const assigner = this.assigners.find(({ name }) => name === groupProtocol)\n\n      if (!assigner) {\n        throw new KafkaJSNonRetriableError(\n          `Unsupported partition assigner \"${groupProtocol}\", the assigner wasn't found in the assigners list`\n        )\n      }\n\n      await this.cluster.refreshMetadata()\n      assignment = await assigner.assign({ members, topics: topicsSubscribed })\n\n      this.logger.debug('Group assignment', {\n        groupId,\n        generationId,\n        groupProtocol,\n        assignment,\n        topics: topicsSubscribed,\n      })\n    }\n\n    // Keep track of the partitions for the subscribed topics\n    this.partitionsPerSubscribedTopic = this.generatePartitionsPerSubscribedTopic()\n    const { memberAssignment } = await this.coordinator.syncGroup({\n      groupId,\n      generationId,\n      memberId,\n      groupAssignment: assignment,\n    })\n\n    const decodedMemberAssignment = MemberAssignment.decode(memberAssignment)\n    const decodedAssignment =\n      decodedMemberAssignment != null ? decodedMemberAssignment.assignment : {}\n\n    this.logger.debug('Received assignment', {\n      groupId,\n      generationId,\n      memberId,\n      memberAssignment: decodedAssignment,\n    })\n\n    const assignedTopics = keys(decodedAssignment)\n    const topicsNotSubscribed = arrayDiff(assignedTopics, topicsSubscribed)\n\n    if (topicsNotSubscribed.length > 0) {\n      const payload = {\n        groupId,\n        generationId,\n        memberId,\n        assignedTopics,\n        topicsSubscribed,\n        topicsNotSubscribed,\n      }\n\n      this.instrumentationEmitter.emit(RECEIVED_UNSUBSCRIBED_TOPICS, payload)\n      this.logger.warn('Consumer group received unsubscribed topics', {\n        ...payload,\n        helpUrl: websiteUrl(\n          'docs/faq',\n          'why-am-i-receiving-messages-for-topics-i-m-not-subscribed-to'\n        ),\n      })\n    }\n\n    // Remove unsubscribed topics from the list\n    const safeAssignment = arrayDiff(assignedTopics, topicsNotSubscribed)\n    const currentMemberAssignment = safeAssignment.map(topic => ({\n      topic,\n      partitions: decodedAssignment[topic],\n    }))\n\n    // Check if the consumer is aware of all assigned partitions\n    for (const assignment of currentMemberAssignment) {\n      const { topic, partitions: assignedPartitions } = assignment\n      const knownPartitions = this.partitionsPerSubscribedTopic.get(topic)\n      const isAwareOfAllAssignedPartitions = assignedPartitions.every(partition =>\n        knownPartitions.includes(partition)\n      )\n\n      if (!isAwareOfAllAssignedPartitions) {\n        this.logger.warn('Consumer is not aware of all assigned partitions, refreshing metadata', {\n          groupId,\n          generationId,\n          memberId,\n          topic,\n          knownPartitions,\n          assignedPartitions,\n        })\n\n        // If the consumer is not aware of all assigned partitions, refresh metadata\n        // and update the list of partitions per subscribed topic. It's enough to perform\n        // this operation once since refresh metadata will update metadata for all topics\n        await this.cluster.refreshMetadata()\n        this.partitionsPerSubscribedTopic = this.generatePartitionsPerSubscribedTopic()\n        break\n      }\n    }\n\n    this.topics = currentMemberAssignment.map(({ topic }) => topic)\n    this.subscriptionState.assign(currentMemberAssignment)\n    this.offsetManager = new OffsetManager({\n      cluster: this.cluster,\n      topicConfigurations: this.topicConfigurations,\n      instrumentationEmitter: this.instrumentationEmitter,\n      memberAssignment: currentMemberAssignment.reduce(\n        (partitionsByTopic, { topic, partitions }) => ({\n          ...partitionsByTopic,\n          [topic]: partitions,\n        }),\n        {}\n      ),\n      autoCommit: this.autoCommit,\n      autoCommitInterval: this.autoCommitInterval,\n      autoCommitThreshold: this.autoCommitThreshold,\n      coordinator,\n      groupId,\n      generationId,\n      memberId,\n    })\n  }\n\n  joinAndSync() {\n    const startJoin = Date.now()\n    return this.retrier(async bail => {\n      try {\n        await this[PRIVATE.JOIN]()\n        await this[PRIVATE.SYNC]()\n\n        const memberAssignment = this.assigned().reduce(\n          (result, { topic, partitions }) => ({ ...result, [topic]: partitions }),\n          {}\n        )\n\n        const payload = {\n          groupId: this.groupId,\n          memberId: this.memberId,\n          leaderId: this.leaderId,\n          isLeader: this.isLeader(),\n          memberAssignment,\n          groupProtocol: this.groupProtocol,\n          duration: Date.now() - startJoin,\n        }\n\n        this.instrumentationEmitter.emit(GROUP_JOIN, payload)\n        this.logger.info('Consumer has joined the group', payload)\n      } catch (e) {\n        if (isRebalancing(e)) {\n          // Rebalance in progress isn't a retriable protocol error since the consumer\n          // has to go through find coordinator and join again before it can\n          // actually retry the operation. We wrap the original error in a retriable error\n          // here instead in order to restart the join + sync sequence using the retrier.\n          throw new KafkaJSError(e)\n        }\n\n        if (e.type === 'UNKNOWN_MEMBER_ID') {\n          this.memberId = null\n          throw new KafkaJSError(e)\n        }\n\n        bail(e)\n      }\n    })\n  }\n\n  /**\n   * @param {import(\"../../types\").TopicPartition} topicPartition\n   */\n  resetOffset({ topic, partition }) {\n    this.offsetManager.resetOffset({ topic, partition })\n  }\n\n  /**\n   * @param {import(\"../../types\").TopicPartitionOffset} topicPartitionOffset\n   */\n  resolveOffset({ topic, partition, offset }) {\n    this.offsetManager.resolveOffset({ topic, partition, offset })\n  }\n\n  /**\n   * Update the consumer offset for the given topic/partition. This will be used\n   * on the next fetch. If this API is invoked for the same topic/partition more\n   * than once, the latest offset will be used on the next fetch.\n   *\n   * @param {import(\"../../types\").TopicPartitionOffset} topicPartitionOffset\n   */\n  seek({ topic, partition, offset }) {\n    this.seekOffset.set(topic, partition, offset)\n  }\n\n  pause(topicPartitions) {\n    this.logger.info(`Pausing fetching from ${topicPartitions.length} topics`, {\n      topicPartitions,\n    })\n    this.subscriptionState.pause(topicPartitions)\n  }\n\n  resume(topicPartitions) {\n    this.logger.info(`Resuming fetching from ${topicPartitions.length} topics`, {\n      topicPartitions,\n    })\n    this.subscriptionState.resume(topicPartitions)\n  }\n\n  assigned() {\n    return this.subscriptionState.assigned()\n  }\n\n  paused() {\n    return this.subscriptionState.paused()\n  }\n\n  /**\n   * @param {string} topic\n   * @param {string} partition\n   * @returns {boolean} whether the specified topic-partition are paused or not\n   */\n  isPaused(topic, partition) {\n    return this.subscriptionState.isPaused(topic, partition)\n  }\n\n  async commitOffsetsIfNecessary() {\n    await this.offsetManager.commitOffsetsIfNecessary()\n  }\n\n  async commitOffsets(offsets) {\n    await this.offsetManager.commitOffsets(offsets)\n  }\n\n  uncommittedOffsets() {\n    return this.offsetManager.uncommittedOffsets()\n  }\n\n  async heartbeat({ interval }) {\n    return this[PRIVATE.SHARED_HEARTBEAT]({ interval })\n  }\n\n  async fetch(nodeId) {\n    try {\n      await this.cluster.refreshMetadataIfNecessary()\n      this.checkForStaleAssignment()\n\n      let topicPartitions = this.subscriptionState.assigned()\n      topicPartitions = this.filterPartitionsByNode(nodeId, topicPartitions)\n\n      await this.seekOffsets(topicPartitions)\n\n      const committedOffsets = this.offsetManager.committedOffsets()\n      const activeTopicPartitions = this.getActiveTopicPartitions()\n\n      const requests = topicPartitions\n        .map(({ topic, partitions }) => ({\n          topic,\n          partitions: partitions\n            .filter(\n              partition =>\n                /**\n                 * When recovering from OffsetOutOfRange, each partition can recover\n                 * concurrently, which invalidates resolved and committed offsets as part\n                 * of the recovery mechanism (see OffsetManager.clearOffsets). In concurrent\n                 * scenarios this can initiate a new fetch with invalid offsets.\n                 *\n                 * This was further highlighted by https://github.com/tulios/kafkajs/pull/570,\n                 * which increased concurrency, making this more likely to happen.\n                 *\n                 * This is solved by only making requests for partitions with initialized offsets.\n                 *\n                 * See the following pull request which explains the context of the problem:\n                 * @issue https://github.com/tulios/kafkajs/pull/578\n                 */\n                committedOffsets[topic][partition] != null &&\n                activeTopicPartitions[topic].has(partition)\n            )\n            .map(partition => ({\n              partition,\n              fetchOffset: this.offsetManager.nextOffset(topic, partition).toString(),\n              maxBytes: this.maxBytesPerPartition,\n            })),\n        }))\n        .filter(({ partitions }) => partitions.length)\n\n      if (!requests.length) {\n        await sleep(this.maxWaitTime)\n        return []\n      }\n\n      const broker = await this.cluster.findBroker({ nodeId })\n\n      const { responses } = await broker.fetch({\n        maxWaitTime: this.maxWaitTime,\n        minBytes: this.minBytes,\n        maxBytes: this.maxBytes,\n        isolationLevel: this.isolationLevel,\n        topics: requests,\n        rackId: this.rackId,\n      })\n\n      return responses.flatMap(({ topicName, partitions }) => {\n        const topicRequestData = requests.find(({ topic }) => topic === topicName)\n\n        let preferredReadReplicas = this.preferredReadReplicasPerTopicPartition[topicName]\n        if (!preferredReadReplicas) {\n          this.preferredReadReplicasPerTopicPartition[topicName] = preferredReadReplicas = {}\n        }\n\n        return partitions\n          .filter(\n            ({ partition }) =>\n              !this.seekOffset.has(topicName, partition) &&\n              !this.subscriptionState.isPaused(topicName, partition)\n          )\n          .map(partitionData => {\n            const { partition, preferredReadReplica } = partitionData\n\n            if (preferredReadReplica != null && preferredReadReplica !== -1) {\n              const { nodeId: currentPreferredReadReplica } = preferredReadReplicas[partition] || {}\n              if (currentPreferredReadReplica !== preferredReadReplica) {\n                this.logger.info(`Preferred read replica is now ${preferredReadReplica}`, {\n                  groupId: this.groupId,\n                  memberId: this.memberId,\n                  topic: topicName,\n                  partition,\n                })\n              }\n              preferredReadReplicas[partition] = {\n                nodeId: preferredReadReplica,\n                expireAt: Date.now() + this.metadataMaxAge,\n              }\n            }\n\n            const partitionRequestData = topicRequestData.partitions.find(\n              ({ partition }) => partition === partitionData.partition\n            )\n\n            const fetchedOffset = partitionRequestData.fetchOffset\n            return new Batch(topicName, fetchedOffset, partitionData)\n          })\n      })\n    } catch (e) {\n      await this.recoverFromFetch(e)\n      return []\n    }\n  }\n\n  async recoverFromFetch(e) {\n    if (STALE_METADATA_ERRORS.includes(e.type) || e.name === 'KafkaJSTopicMetadataNotLoaded') {\n      this.logger.debug('Stale cluster metadata, refreshing...', {\n        groupId: this.groupId,\n        memberId: this.memberId,\n        error: e.message,\n      })\n\n      await this.cluster.refreshMetadata()\n      await this.joinAndSync()\n      return\n    }\n\n    if (e.name === 'KafkaJSStaleTopicMetadataAssignment') {\n      this.logger.warn(`${e.message}, resync group`, {\n        groupId: this.groupId,\n        memberId: this.memberId,\n        topic: e.topic,\n        unknownPartitions: e.unknownPartitions,\n      })\n\n      await this.joinAndSync()\n      return\n    }\n\n    if (e.name === 'KafkaJSOffsetOutOfRange') {\n      await this.recoverFromOffsetOutOfRange(e)\n      return\n    }\n\n    if (e.name === 'KafkaJSConnectionClosedError') {\n      this.cluster.removeBroker({ host: e.host, port: e.port })\n      return\n    }\n\n    if (e.name === 'KafkaJSBrokerNotFound' || e.name === 'KafkaJSConnectionClosedError') {\n      this.logger.debug(`${e.message}, refreshing metadata and retrying...`)\n      await this.cluster.refreshMetadata()\n      return\n    }\n\n    throw e\n  }\n\n  async recoverFromOffsetOutOfRange(e) {\n    // If we are fetching from a follower try with the leader before resetting offsets\n    const preferredReadReplicas = this.preferredReadReplicasPerTopicPartition[e.topic]\n    if (preferredReadReplicas && typeof preferredReadReplicas[e.partition] === 'number') {\n      this.logger.info('Offset out of range while fetching from follower, retrying with leader', {\n        topic: e.topic,\n        partition: e.partition,\n        groupId: this.groupId,\n        memberId: this.memberId,\n      })\n      delete preferredReadReplicas[e.partition]\n    } else {\n      this.logger.error('Offset out of range, resetting to default offset', {\n        topic: e.topic,\n        partition: e.partition,\n        groupId: this.groupId,\n        memberId: this.memberId,\n      })\n\n      await this.offsetManager.setDefaultOffset({\n        topic: e.topic,\n        partition: e.partition,\n      })\n    }\n  }\n\n  generatePartitionsPerSubscribedTopic() {\n    const map = new Map()\n\n    for (const topic of this.topicsSubscribed) {\n      const partitions = this.cluster\n        .findTopicPartitionMetadata(topic)\n        .map(m => m.partitionId)\n        .sort()\n\n      map.set(topic, partitions)\n    }\n\n    return map\n  }\n\n  checkForStaleAssignment() {\n    if (!this.partitionsPerSubscribedTopic) {\n      return\n    }\n\n    const newPartitionsPerSubscribedTopic = this.generatePartitionsPerSubscribedTopic()\n\n    for (const [topic, partitions] of newPartitionsPerSubscribedTopic) {\n      const diff = arrayDiff(partitions, this.partitionsPerSubscribedTopic.get(topic))\n\n      if (diff.length > 0) {\n        throw new KafkaJSStaleTopicMetadataAssignment('Topic has been updated', {\n          topic,\n          unknownPartitions: diff,\n        })\n      }\n    }\n  }\n\n  async seekOffsets(topicPartitions) {\n    for (const { topic, partitions } of topicPartitions) {\n      for (const partition of partitions) {\n        const seekEntry = this.seekOffset.pop(topic, partition)\n        if (!seekEntry) {\n          continue\n        }\n\n        this.logger.debug('Seek offset', {\n          groupId: this.groupId,\n          memberId: this.memberId,\n          seek: seekEntry,\n        })\n        await this.offsetManager.seek(seekEntry)\n      }\n    }\n\n    await this.offsetManager.resolveOffsets()\n  }\n\n  hasSeekOffset({ topic, partition }) {\n    return this.seekOffset.has(topic, partition)\n  }\n\n  /**\n   * For each of the partitions find the best nodeId to read it from\n   *\n   * @param {string} topic\n   * @param {number[]} partitions\n   * @returns {{[nodeId: number]: number[]}} per-node assignment of partitions\n   * @see Cluster~findLeaderForPartitions\n   */\n  // Invariant: The resulting object has each partition referenced exactly once\n  findReadReplicaForPartitions(topic, partitions) {\n    const partitionMetadata = this.cluster.findTopicPartitionMetadata(topic)\n    const preferredReadReplicas = this.preferredReadReplicasPerTopicPartition[topic]\n    return partitions.reduce((result, id) => {\n      const partitionId = parseInt(id, 10)\n      const metadata = partitionMetadata.find(p => p.partitionId === partitionId)\n      if (!metadata) {\n        return result\n      }\n\n      if (metadata.leader == null) {\n        throw new KafkaJSError('Invalid partition metadata', { topic, partitionId, metadata })\n      }\n\n      // Pick the preferred replica if there is one, and it isn't known to be offline, otherwise the leader.\n      let nodeId = metadata.leader\n      if (preferredReadReplicas) {\n        const { nodeId: preferredReadReplica, expireAt } = preferredReadReplicas[partitionId] || {}\n        if (Date.now() >= expireAt) {\n          this.logger.debug('Preferred read replica information has expired, using leader', {\n            topic,\n            partitionId,\n            groupId: this.groupId,\n            memberId: this.memberId,\n            preferredReadReplica,\n            leader: metadata.leader,\n          })\n          // Drop the entry\n          delete preferredReadReplicas[partitionId]\n        } else if (preferredReadReplica != null) {\n          // Valid entry, check whether it is not offline\n          // Note that we don't delete the preference here, and rather hope that eventually that replica comes online again\n          const offlineReplicas = metadata.offlineReplicas\n          if (Array.isArray(offlineReplicas) && offlineReplicas.includes(nodeId)) {\n            this.logger.debug('Preferred read replica is offline, using leader', {\n              topic,\n              partitionId,\n              groupId: this.groupId,\n              memberId: this.memberId,\n              preferredReadReplica,\n              leader: metadata.leader,\n            })\n          } else {\n            nodeId = preferredReadReplica\n          }\n        }\n      }\n      const current = result[nodeId] || []\n      return { ...result, [nodeId]: [...current, partitionId] }\n    }, {})\n  }\n\n  filterPartitionsByNode(nodeId, topicPartitions) {\n    return topicPartitions.map(({ topic, partitions }) => ({\n      topic,\n      partitions: this.findReadReplicaForPartitions(topic, partitions)[nodeId] || [],\n    }))\n  }\n\n  getActiveTopicPartitions() {\n    const activeSubscriptionState = this.subscriptionState.active()\n\n    const activeTopicPartitions = {}\n    activeSubscriptionState.forEach(({ topic, partitions }) => {\n      activeTopicPartitions[topic] = new Set(partitions)\n    })\n\n    return activeTopicPartitions\n  }\n}\n", "/**\n * @param {number} count\n * @param {(index: number) => T} [callback]\n * @template T\n */\nconst seq = (count, callback = x => x) =>\n  new Array(count).fill(0).map((_, index) => callback(index))\n\nmodule.exports = seq\n", "const EventEmitter = require('events')\n\n/**\n * Fetches data from all assigned nodes, waits for workerQueue to drain and repeats.\n *\n * @param {object} options\n * @param {number} options.nodeId\n * @param {import('./workerQueue').WorkerQueue} options.workerQueue\n * @param {Map<string, string[]>} options.partitionAssignments\n * @param {(nodeId: number) => Promise<T[]>} options.fetch\n * @param {import('../../types').Logger} options.logger\n * @template T\n */\nconst createFetcher = ({\n  nodeId,\n  workerQueue,\n  partitionAssignments,\n  fetch,\n  logger: rootLogger,\n}) => {\n  const logger = rootLogger.namespace(`Fetcher ${nodeId}`)\n  const emitter = new EventEmitter()\n  let isRunning = false\n\n  const getWorkerQueue = () => workerQueue\n  const assignmentKey = ({ topic, partition }) => `${topic}|${partition}`\n  const getAssignedFetcher = batch => partitionAssignments.get(assignmentKey(batch))\n  const assignTopicPartition = batch => partitionAssignments.set(assignmentKey(batch), nodeId)\n  const unassignTopicPartition = batch => partitionAssignments.delete(assignmentKey(batch))\n  const filterUnassignedBatches = batches =>\n    batches.filter(batch => {\n      const assignedFetcher = getAssignedFetcher(batch)\n      if (assignedFetcher != null && assignedFetcher !== nodeId) {\n        logger.info(\n          'Filtering out batch due to partition already being processed by another fetcher',\n          {\n            topic: batch.topic,\n            partition: batch.partition,\n            assignedFetcher: assignedFetcher,\n            fetcher: nodeId,\n          }\n        )\n        return false\n      }\n\n      return true\n    })\n\n  const start = async () => {\n    if (isRunning) return\n    isRunning = true\n\n    while (isRunning) {\n      try {\n        const batches = await fetch(nodeId)\n        if (isRunning) {\n          const availableBatches = filterUnassignedBatches(batches)\n\n          if (availableBatches.length > 0) {\n            availableBatches.forEach(assignTopicPartition)\n            try {\n              await workerQueue.push(...availableBatches)\n            } finally {\n              availableBatches.forEach(unassignTopicPartition)\n            }\n          }\n        }\n      } catch (error) {\n        isRunning = false\n        emitter.emit('end')\n        throw error\n      }\n    }\n    emitter.emit('end')\n  }\n\n  const stop = async () => {\n    if (!isRunning) return\n    isRunning = false\n    await new Promise(resolve => emitter.once('end', () => resolve()))\n  }\n\n  return { start, stop, getWorkerQueue }\n}\n\nmodule.exports = createFetcher\n", "/**\n * @typedef {(batch: T, metadata: { workerId: number }) => Promise<void>} Handler\n * @template T\n *\n * @typedef {ReturnType<typeof createWorker>} Worker\n */\n\nconst sharedPromiseTo = require('../utils/sharedPromiseTo')\n\n/**\n * @param {{ handler: Handler<T>, workerId: number }} options\n * @template T\n */\nconst createWorker = ({ handler, workerId }) => {\n  /**\n   * Takes batches from next() until it returns undefined.\n   *\n   * @param {{ next: () => { batch: T, resolve: () => void, reject: (e: Error) => void } | undefined }} param0\n   * @returns {Promise<void>}\n   */\n  const run = sharedPromiseTo(async ({ next }) => {\n    while (true) {\n      const item = next()\n      if (!item) break\n\n      const { batch, resolve, reject } = item\n\n      try {\n        await handler(batch, { workerId })\n        resolve()\n      } catch (error) {\n        reject(error)\n      }\n    }\n  })\n\n  return { run }\n}\n\nmodule.exports = createWorker\n", "/**\n * @typedef {ReturnType<typeof createWorkerQueue>} WorkerQueue\n */\n\n/**\n * @param {object} options\n * @param {import('./worker').Worker<T>[]} options.workers\n * @template T\n */\nconst createWorkerQueue = ({ workers }) => {\n  /** @type {{ batch: T, resolve: (value?: any) => void, reject: (e: Error) => void}[]} */\n  const queue = []\n\n  const getWorkers = () => workers\n\n  /**\n   * Waits until workers have processed all batches in the queue.\n   *\n   * @param {...T} batches\n   * @returns {Promise<void>}\n   */\n  const push = async (...batches) => {\n    const promises = batches.map(\n      batch => new Promise((resolve, reject) => queue.push({ batch, resolve, reject }))\n    )\n\n    workers.forEach(worker => worker.run({ next: () => queue.shift() }))\n\n    const results = await Promise.allSettled(promises)\n    const rejected = results.find(result => result.status === 'rejected')\n    if (rejected) {\n      // @ts-ignore\n      throw rejected.reason\n    }\n  }\n\n  return { push, getWorkers }\n}\n\nmodule.exports = createWorkerQueue\n", "const seq = require('../utils/seq')\nconst createFetcher = require('./fetcher')\nconst createWorker = require('./worker')\nconst createWorkerQueue = require('./workerQueue')\nconst { KafkaJSFetcherRebalanceError, KafkaJSNoBrokerAvailableError } = require('../errors')\n\n/** @typedef {ReturnType<typeof createFetchManager>} FetchManager */\n\n/**\n * @param {object} options\n * @param {import('../../types').Logger} options.logger\n * @param {() => number[]} options.getNodeIds\n * @param {(nodeId: number) => Promise<import('../../types').Batch[]>} options.fetch\n * @param {import('./worker').Handler<T>} options.handler\n * @param {number} [options.concurrency]\n * @template T\n */\nconst createFetchManager = ({\n  logger: rootLogger,\n  getNodeIds,\n  fetch,\n  handler,\n  concurrency = 1,\n}) => {\n  const logger = rootLogger.namespace('FetchManager')\n  const workers = seq(concurrency, workerId => createWorker({ handler, workerId }))\n  const workerQueue = createWorkerQueue({ workers })\n\n  let fetchers = []\n\n  const getFetchers = () => fetchers\n\n  const createFetchers = () => {\n    const nodeIds = getNodeIds()\n    const partitionAssignments = new Map()\n\n    if (nodeIds.length === 0) {\n      throw new KafkaJSNoBrokerAvailableError()\n    }\n\n    const validateShouldRebalance = () => {\n      const current = getNodeIds()\n      const hasChanged =\n        nodeIds.length !== current.length || nodeIds.some(nodeId => !current.includes(nodeId))\n      if (hasChanged && current.length !== 0) {\n        throw new KafkaJSFetcherRebalanceError()\n      }\n    }\n\n    const fetchers = nodeIds.map(nodeId =>\n      createFetcher({\n        nodeId,\n        workerQueue,\n        partitionAssignments,\n        fetch: async nodeId => {\n          validateShouldRebalance()\n          return fetch(nodeId)\n        },\n        logger,\n      })\n    )\n\n    logger.debug(`Created ${fetchers.length} fetchers`, { nodeIds, concurrency })\n    return fetchers\n  }\n\n  const start = async () => {\n    logger.debug('Starting...')\n\n    while (true) {\n      fetchers = createFetchers()\n\n      try {\n        await Promise.all(fetchers.map(fetcher => fetcher.start()))\n      } catch (error) {\n        await stop()\n\n        if (error instanceof KafkaJSFetcherRebalanceError) {\n          logger.debug('Rebalancing fetchers...')\n          continue\n        }\n\n        throw error\n      }\n\n      break\n    }\n  }\n\n  const stop = async () => {\n    logger.debug('Stopping fetchers...')\n    await Promise.all(fetchers.map(fetcher => fetcher.stop()))\n    logger.debug('Stopped fetchers')\n  }\n\n  return { start, stop, getFetchers }\n}\n\nmodule.exports = createFetchManager\n", "const { EventEmitter } = require('events')\nconst Long = require('../utils/long')\nconst createRetry = require('../retry')\nconst { isKafkaJSError, isRebalancing } = require('../errors')\n\nconst {\n  events: { FETCH, FETCH_START, START_BATCH_PROCESS, END_BATCH_PROCESS, REBALANCING },\n} = require('./instrumentationEvents')\nconst createFetchManager = require('./fetchManager')\n\nconst isSameOffset = (offsetA, offsetB) => Long.fromValue(offsetA).equals(Long.fromValue(offsetB))\nconst CONSUMING_START = 'consuming-start'\nconst CONSUMING_STOP = 'consuming-stop'\n\nmodule.exports = class Runner extends EventEmitter {\n  /**\n   * @param {object} options\n   * @param {import(\"../../types\").Logger} options.logger\n   * @param {import(\"./consumerGroup\")} options.consumerGroup\n   * @param {import(\"../instrumentation/emitter\")} options.instrumentationEmitter\n   * @param {boolean} [options.eachBatchAutoResolve=true]\n   * @param {number} options.concurrency\n   * @param {(payload: import(\"../../types\").EachBatchPayload) => Promise<void>} [options.eachBatch]\n   * @param {(payload: import(\"../../types\").EachMessagePayload) => Promise<void>} [options.eachMessage]\n   * @param {number} [options.heartbeatInterval]\n   * @param {(reason: Error) => void} options.onCrash\n   * @param {import(\"../../types\").RetryOptions} [options.retry]\n   * @param {boolean} [options.autoCommit=true]\n   */\n  constructor({\n    logger,\n    consumerGroup,\n    instrumentationEmitter,\n    eachBatchAutoResolve = true,\n    concurrency,\n    eachBatch,\n    eachMessage,\n    heartbeatInterval,\n    onCrash,\n    retry,\n    autoCommit = true,\n  }) {\n    super()\n    this.logger = logger.namespace('Runner')\n    this.consumerGroup = consumerGroup\n    this.instrumentationEmitter = instrumentationEmitter\n    this.eachBatchAutoResolve = eachBatchAutoResolve\n    this.eachBatch = eachBatch\n    this.eachMessage = eachMessage\n    this.heartbeatInterval = heartbeatInterval\n    this.retrier = createRetry(Object.assign({}, retry))\n    this.onCrash = onCrash\n    this.autoCommit = autoCommit\n    this.fetchManager = createFetchManager({\n      logger: this.logger,\n      getNodeIds: () => this.consumerGroup.getNodeIds(),\n      fetch: nodeId => this.fetch(nodeId),\n      handler: batch => this.handleBatch(batch),\n      concurrency,\n    })\n\n    this.running = false\n    this.consuming = false\n  }\n\n  get consuming() {\n    return this._consuming\n  }\n\n  set consuming(value) {\n    if (this._consuming !== value) {\n      this._consuming = value\n      this.emit(value ? CONSUMING_START : CONSUMING_STOP)\n    }\n  }\n\n  async start() {\n    if (this.running) {\n      return\n    }\n\n    try {\n      await this.consumerGroup.connect()\n      await this.consumerGroup.joinAndSync()\n    } catch (e) {\n      return this.onCrash(e)\n    }\n\n    this.running = true\n    this.scheduleFetchManager()\n  }\n\n  scheduleFetchManager() {\n    if (!this.running) {\n      this.consuming = false\n\n      this.logger.info('consumer not running, exiting', {\n        groupId: this.consumerGroup.groupId,\n        memberId: this.consumerGroup.memberId,\n      })\n\n      return\n    }\n\n    this.consuming = true\n\n    this.retrier(async (bail, retryCount, retryTime) => {\n      if (!this.running) {\n        return\n      }\n\n      try {\n        await this.fetchManager.start()\n      } catch (e) {\n        if (isRebalancing(e)) {\n          this.logger.warn('The group is rebalancing, re-joining', {\n            groupId: this.consumerGroup.groupId,\n            memberId: this.consumerGroup.memberId,\n            error: e.message,\n          })\n\n          this.instrumentationEmitter.emit(REBALANCING, {\n            groupId: this.consumerGroup.groupId,\n            memberId: this.consumerGroup.memberId,\n          })\n\n          await this.consumerGroup.joinAndSync()\n          return\n        }\n\n        if (e.type === 'UNKNOWN_MEMBER_ID') {\n          this.logger.error('The coordinator is not aware of this member, re-joining the group', {\n            groupId: this.consumerGroup.groupId,\n            memberId: this.consumerGroup.memberId,\n            error: e.message,\n          })\n\n          this.consumerGroup.memberId = null\n          await this.consumerGroup.joinAndSync()\n          return\n        }\n\n        if (e.name === 'KafkaJSNotImplemented') {\n          return bail(e)\n        }\n\n        if (e.name === 'KafkaJSNoBrokerAvailableError') {\n          return bail(e)\n        }\n\n        this.logger.debug('Error while scheduling fetch manager, trying again...', {\n          groupId: this.consumerGroup.groupId,\n          memberId: this.consumerGroup.memberId,\n          error: e.message,\n          stack: e.stack,\n          retryCount,\n          retryTime,\n        })\n\n        throw e\n      }\n    })\n      .then(() => {\n        this.scheduleFetchManager()\n      })\n      .catch(e => {\n        this.onCrash(e)\n        this.consuming = false\n        this.running = false\n      })\n  }\n\n  async stop() {\n    if (!this.running) {\n      return\n    }\n\n    this.logger.debug('stop consumer group', {\n      groupId: this.consumerGroup.groupId,\n      memberId: this.consumerGroup.memberId,\n    })\n\n    this.running = false\n\n    try {\n      await this.fetchManager.stop()\n      await this.waitForConsumer()\n      await this.consumerGroup.leave()\n    } catch (e) {}\n  }\n\n  waitForConsumer() {\n    return new Promise(resolve => {\n      if (!this.consuming) {\n        return resolve()\n      }\n\n      this.logger.debug('waiting for consumer to finish...', {\n        groupId: this.consumerGroup.groupId,\n        memberId: this.consumerGroup.memberId,\n      })\n\n      this.once(CONSUMING_STOP, () => resolve())\n    })\n  }\n\n  async heartbeat() {\n    try {\n      await this.consumerGroup.heartbeat({ interval: this.heartbeatInterval })\n    } catch (e) {\n      if (isRebalancing(e)) {\n        await this.autoCommitOffsets()\n      }\n      throw e\n    }\n  }\n\n  async processEachMessage(batch) {\n    const { topic, partition } = batch\n\n    const pause = () => {\n      this.consumerGroup.pause([{ topic, partitions: [partition] }])\n      return () => this.consumerGroup.resume([{ topic, partitions: [partition] }])\n    }\n    for (const message of batch.messages) {\n      if (!this.running || this.consumerGroup.hasSeekOffset({ topic, partition })) {\n        break\n      }\n\n      try {\n        await this.eachMessage({\n          topic,\n          partition,\n          message,\n          heartbeat: () => this.heartbeat(),\n          pause,\n        })\n      } catch (e) {\n        if (!isKafkaJSError(e)) {\n          this.logger.error(`Error when calling eachMessage`, {\n            topic,\n            partition,\n            offset: message.offset,\n            stack: e.stack,\n            error: e,\n          })\n        }\n\n        // In case of errors, commit the previously consumed offsets unless autoCommit is disabled\n        await this.autoCommitOffsets()\n        throw e\n      }\n\n      this.consumerGroup.resolveOffset({ topic, partition, offset: message.offset })\n      await this.heartbeat()\n      await this.autoCommitOffsetsIfNecessary()\n\n      if (this.consumerGroup.isPaused(topic, partition)) {\n        break\n      }\n    }\n  }\n\n  async processEachBatch(batch) {\n    const { topic, partition } = batch\n    const lastFilteredMessage = batch.messages[batch.messages.length - 1]\n\n    const pause = () => {\n      this.consumerGroup.pause([{ topic, partitions: [partition] }])\n      return () => this.consumerGroup.resume([{ topic, partitions: [partition] }])\n    }\n\n    try {\n      await this.eachBatch({\n        batch,\n        resolveOffset: offset => {\n          /**\n           * The transactional producer generates a control record after committing the transaction.\n           * The control record is the last record on the RecordBatch, and it is filtered before it\n           * reaches the eachBatch callback. When disabling auto-resolve, the user-land code won't\n           * be able to resolve the control record offset, since it never reaches the callback,\n           * causing stuck consumers as the consumer will never move the offset marker.\n           *\n           * When the last offset of the batch is resolved, we should automatically resolve\n           * the control record offset as this entry doesn't have any meaning to the user-land code,\n           * and won't interfere with the stream processing.\n           *\n           * @see https://github.com/apache/kafka/blob/9aa660786e46c1efbf5605a6a69136a1dac6edb9/clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java#L1499-L1505\n           */\n          const offsetToResolve =\n            lastFilteredMessage && isSameOffset(offset, lastFilteredMessage.offset)\n              ? batch.lastOffset()\n              : offset\n\n          this.consumerGroup.resolveOffset({ topic, partition, offset: offsetToResolve })\n        },\n        heartbeat: () => this.heartbeat(),\n        /**\n         * Pause consumption for the current topic-partition being processed\n         */\n        pause,\n        /**\n         * Commit offsets if provided. Otherwise commit most recent resolved offsets\n         * if the autoCommit conditions are met.\n         *\n         * @param {import('../../types').OffsetsByTopicPartition} [offsets] Optional.\n         */\n        commitOffsetsIfNecessary: async offsets => {\n          return offsets\n            ? this.consumerGroup.commitOffsets(offsets)\n            : this.consumerGroup.commitOffsetsIfNecessary()\n        },\n        uncommittedOffsets: () => this.consumerGroup.uncommittedOffsets(),\n        isRunning: () => this.running,\n        isStale: () => this.consumerGroup.hasSeekOffset({ topic, partition }),\n      })\n    } catch (e) {\n      if (!isKafkaJSError(e)) {\n        this.logger.error(`Error when calling eachBatch`, {\n          topic,\n          partition,\n          offset: batch.firstOffset(),\n          stack: e.stack,\n          error: e,\n        })\n      }\n\n      // eachBatch has a special resolveOffset which can be used\n      // to keep track of the messages\n      await this.autoCommitOffsets()\n      throw e\n    }\n\n    // resolveOffset for the last offset can be disabled to allow the users of eachBatch to\n    // stop their consumers without resolving unprocessed offsets (issues/18)\n    if (this.eachBatchAutoResolve) {\n      this.consumerGroup.resolveOffset({ topic, partition, offset: batch.lastOffset() })\n    }\n  }\n\n  async fetch(nodeId) {\n    if (!this.running) {\n      this.logger.debug('consumer not running, exiting', {\n        groupId: this.consumerGroup.groupId,\n        memberId: this.consumerGroup.memberId,\n      })\n\n      return []\n    }\n\n    const startFetch = Date.now()\n\n    this.instrumentationEmitter.emit(FETCH_START, { nodeId })\n\n    const batches = await this.consumerGroup.fetch(nodeId)\n\n    this.instrumentationEmitter.emit(FETCH, {\n      /**\n       * PR #570 removed support for the number of batches in this instrumentation event;\n       * The new implementation uses an async generation to deliver the batches, which makes\n       * this number impossible to get. The number is set to 0 to keep the event backward\n       * compatible until we bump KafkaJS to version 2, following the end of node 8 LTS.\n       *\n       * @since 2019-11-29\n       */\n      numberOfBatches: 0,\n      duration: Date.now() - startFetch,\n      nodeId,\n    })\n\n    if (batches.length === 0) {\n      await this.heartbeat()\n    }\n\n    return batches\n  }\n\n  async handleBatch(batch) {\n    if (!this.running) {\n      this.logger.debug('consumer not running, exiting', {\n        groupId: this.consumerGroup.groupId,\n        memberId: this.consumerGroup.memberId,\n      })\n\n      return\n    }\n\n    /** @param {import('./batch')} batch */\n    const onBatch = async batch => {\n      const startBatchProcess = Date.now()\n      const payload = {\n        topic: batch.topic,\n        partition: batch.partition,\n        highWatermark: batch.highWatermark,\n        offsetLag: batch.offsetLag(),\n        /**\n         * @since 2019-06-24 (>= 1.8.0)\n         *\n         * offsetLag returns the lag based on the latest offset in the batch, to\n         * keep the event backward compatible we just introduced \"offsetLagLow\"\n         * which calculates the lag based on the first offset in the batch\n         */\n        offsetLagLow: batch.offsetLagLow(),\n        batchSize: batch.messages.length,\n        firstOffset: batch.firstOffset(),\n        lastOffset: batch.lastOffset(),\n      }\n\n      /**\n       * If the batch contained only control records or only aborted messages then we still\n       * need to resolve and auto-commit to ensure the consumer can move forward.\n       *\n       * We also need to emit batch instrumentation events to allow any listeners keeping\n       * track of offsets to know about the latest point of consumption.\n       *\n       * Added in #1256\n       *\n       * @see https://github.com/apache/kafka/blob/9aa660786e46c1efbf5605a6a69136a1dac6edb9/clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java#L1499-L1505\n       */\n      if (batch.isEmptyDueToFiltering()) {\n        this.instrumentationEmitter.emit(START_BATCH_PROCESS, payload)\n\n        this.consumerGroup.resolveOffset({\n          topic: batch.topic,\n          partition: batch.partition,\n          offset: batch.lastOffset(),\n        })\n        await this.autoCommitOffsetsIfNecessary()\n\n        this.instrumentationEmitter.emit(END_BATCH_PROCESS, {\n          ...payload,\n          duration: Date.now() - startBatchProcess,\n        })\n\n        await this.heartbeat()\n        return\n      }\n\n      if (batch.isEmpty()) {\n        await this.heartbeat()\n        return\n      }\n\n      this.instrumentationEmitter.emit(START_BATCH_PROCESS, payload)\n\n      if (this.eachMessage) {\n        await this.processEachMessage(batch)\n      } else if (this.eachBatch) {\n        await this.processEachBatch(batch)\n      }\n\n      this.instrumentationEmitter.emit(END_BATCH_PROCESS, {\n        ...payload,\n        duration: Date.now() - startBatchProcess,\n      })\n\n      await this.autoCommitOffsets()\n      await this.heartbeat()\n    }\n\n    await onBatch(batch)\n  }\n\n  autoCommitOffsets() {\n    if (this.autoCommit) {\n      return this.consumerGroup.commitOffsets()\n    }\n  }\n\n  autoCommitOffsetsIfNecessary() {\n    if (this.autoCommit) {\n      return this.consumerGroup.commitOffsetsIfNecessary()\n    }\n  }\n\n  commitOffsets(offsets) {\n    if (!this.running) {\n      this.logger.debug('consumer not running, exiting', {\n        groupId: this.consumerGroup.groupId,\n        memberId: this.consumerGroup.memberId,\n        offsets,\n      })\n      return\n    }\n\n    return this.retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await this.consumerGroup.commitOffsets(offsets)\n      } catch (e) {\n        if (!this.running) {\n          this.logger.debug('consumer not running, exiting', {\n            error: e.message,\n            groupId: this.consumerGroup.groupId,\n            memberId: this.consumerGroup.memberId,\n            offsets,\n          })\n          return\n        }\n\n        if (e.name === 'KafkaJSNotImplemented') {\n          return bail(e)\n        }\n\n        this.logger.debug('Error while committing offsets, trying again...', {\n          groupId: this.consumerGroup.groupId,\n          memberId: this.consumerGroup.memberId,\n          error: e.message,\n          stack: e.stack,\n          retryCount,\n          retryTime,\n          offsets,\n        })\n\n        throw e\n      }\n    })\n  }\n}\n", "const { MemberMetadata, MemberAssignment } = require('../../assignerProtocol')\n\n/**\n * RoundRobinAssigner\n * @type {import('types').PartitionAssigner}\n */\nmodule.exports = ({ cluster }) => ({\n  name: 'RoundRobinAssigner',\n  version: 0,\n\n  /**\n   * Assign the topics to the provided members.\n   *\n   * The members array contains information about each member, `memberMetadata` is the result of the\n   * `protocol` operation.\n   *\n   * @param {object} group\n   * @param {import('types').GroupMember[]} group.members array of members, e.g:\n                              [{ memberId: 'test-5f93f5a3', memberMetadata: Buffer }]\n   * @param {string[]} group.topics\n   * @returns {Promise<import('types').GroupMemberAssignment[]>} object partitions per topic per member, e.g:\n   *                   [\n   *                     {\n   *                       memberId: 'test-5f93f5a3',\n   *                       memberAssignment: {\n   *                         'topic-A': [0, 2, 4, 6],\n   *                         'topic-B': [1],\n   *                       },\n   *                     },\n   *                     {\n   *                       memberId: 'test-3d3d5341',\n   *                       memberAssignment: {\n   *                         'topic-A': [1, 3, 5],\n   *                         'topic-B': [0, 2],\n   *                       },\n   *                     }\n   *                   ]\n   */\n  async assign({ members, topics }) {\n    const membersCount = members.length\n    const sortedMembers = members.map(({ memberId }) => memberId).sort()\n    const assignment = {}\n\n    const topicsPartitions = topics.flatMap(topic => {\n      const partitionMetadata = cluster.findTopicPartitionMetadata(topic)\n      return partitionMetadata.map(m => ({ topic: topic, partitionId: m.partitionId }))\n    })\n\n    topicsPartitions.forEach((topicPartition, i) => {\n      const assignee = sortedMembers[i % membersCount]\n\n      if (!assignment[assignee]) {\n        assignment[assignee] = Object.create(null)\n      }\n\n      if (!assignment[assignee][topicPartition.topic]) {\n        assignment[assignee][topicPartition.topic] = []\n      }\n\n      assignment[assignee][topicPartition.topic].push(topicPartition.partitionId)\n    })\n\n    return Object.keys(assignment).map(memberId => ({\n      memberId,\n      memberAssignment: MemberAssignment.encode({\n        version: this.version,\n        assignment: assignment[memberId],\n      }),\n    }))\n  },\n\n  protocol({ topics }) {\n    return {\n      name: this.name,\n      metadata: MemberMetadata.encode({\n        version: this.version,\n        topics,\n      }),\n    }\n  },\n})\n", "const roundRobin = require('./roundRobinAssigner')\n\nmodule.exports = {\n  roundRobin,\n}\n", "const Long = require('../utils/long')\nconst createRetry = require('../retry')\nconst { initialRetryTime } = require('../retry/defaults')\nconst ConsumerGroup = require('./consumerGroup')\nconst Runner = require('./runner')\nconst { events, wrap: wrapEvent, unwrap: unwrapEvent } = require('./instrumentationEvents')\nconst InstrumentationEventEmitter = require('../instrumentation/emitter')\nconst { KafkaJSNonRetriableError } = require('../errors')\nconst { roundRobin } = require('./assigners')\nconst { EARLIEST_OFFSET, LATEST_OFFSET } = require('../constants')\nconst ISOLATION_LEVEL = require('../protocol/isolationLevel')\nconst sharedPromiseTo = require('../utils/sharedPromiseTo')\n\nconst { keys, values } = Object\nconst { CONNECT, DISCONNECT, STOP, CRASH } = events\n\nconst eventNames = values(events)\nconst eventKeys = keys(events)\n  .map(key => `consumer.events.${key}`)\n  .join(', ')\n\nconst specialOffsets = [\n  Long.fromValue(EARLIEST_OFFSET).toString(),\n  Long.fromValue(LATEST_OFFSET).toString(),\n]\n\n/**\n * @param {Object} params\n * @param {import(\"../../types\").Cluster} params.cluster\n * @param {String} params.groupId\n * @param {import('../../types').RetryOptions} [params.retry]\n * @param {import('../../types').Logger} params.logger\n * @param {import('../../types').PartitionAssigner[]} [params.partitionAssigners]\n * @param {number} [params.sessionTimeout]\n * @param {number} [params.rebalanceTimeout]\n * @param {number} [params.heartbeatInterval]\n * @param {number} [params.maxBytesPerPartition]\n * @param {number} [params.minBytes]\n * @param {number} [params.maxBytes]\n * @param {number} [params.maxWaitTimeInMs]\n * @param {number} [params.isolationLevel]\n * @param {string} [params.rackId]\n * @param {InstrumentationEventEmitter} [params.instrumentationEmitter]\n * @param {number} params.metadataMaxAge\n *\n * @returns {import(\"../../types\").Consumer}\n */\nmodule.exports = ({\n  cluster,\n  groupId,\n  retry,\n  logger: rootLogger,\n  partitionAssigners = [roundRobin],\n  sessionTimeout = 30000,\n  rebalanceTimeout = 60000,\n  heartbeatInterval = 3000,\n  maxBytesPerPartition = 1048576, // 1MB\n  minBytes = 1,\n  maxBytes = 10485760, // 10MB\n  maxWaitTimeInMs = 5000,\n  isolationLevel = ISOLATION_LEVEL.READ_COMMITTED,\n  rackId = '',\n  instrumentationEmitter: rootInstrumentationEmitter,\n  metadataMaxAge,\n}) => {\n  if (!groupId) {\n    throw new KafkaJSNonRetriableError('Consumer groupId must be a non-empty string.')\n  }\n\n  const logger = rootLogger.namespace('Consumer')\n  const instrumentationEmitter = rootInstrumentationEmitter || new InstrumentationEventEmitter()\n  const assigners = partitionAssigners.map(createAssigner =>\n    createAssigner({ groupId, logger, cluster })\n  )\n\n  /** @type {Record<string, { fromBeginning?: boolean }>} */\n  const topics = {}\n  let runner = null\n  /** @type {ConsumerGroup} */\n  let consumerGroup = null\n  let restartTimeout = null\n\n  if (heartbeatInterval >= sessionTimeout) {\n    throw new KafkaJSNonRetriableError(\n      `Consumer heartbeatInterval (${heartbeatInterval}) must be lower than sessionTimeout (${sessionTimeout}). It is recommended to set heartbeatInterval to approximately a third of the sessionTimeout.`\n    )\n  }\n\n  /** @type {import(\"../../types\").Consumer[\"connect\"]} */\n  const connect = async () => {\n    await cluster.connect()\n    instrumentationEmitter.emit(CONNECT)\n  }\n\n  /** @type {import(\"../../types\").Consumer[\"disconnect\"]} */\n  const disconnect = async () => {\n    try {\n      await stop()\n      logger.debug('consumer has stopped, disconnecting', { groupId })\n      await cluster.disconnect()\n      instrumentationEmitter.emit(DISCONNECT)\n    } catch (e) {\n      logger.error(`Caught error when disconnecting the consumer: ${e.message}`, {\n        stack: e.stack,\n        groupId,\n      })\n      throw e\n    }\n  }\n\n  /** @type {import(\"../../types\").Consumer[\"stop\"]} */\n  const stop = sharedPromiseTo(async () => {\n    try {\n      if (runner) {\n        await runner.stop()\n        runner = null\n        consumerGroup = null\n        instrumentationEmitter.emit(STOP)\n      }\n\n      clearTimeout(restartTimeout)\n      logger.info('Stopped', { groupId })\n    } catch (e) {\n      logger.error(`Caught error when stopping the consumer: ${e.message}`, {\n        stack: e.stack,\n        groupId,\n      })\n\n      throw e\n    }\n  })\n\n  /** @type {import(\"../../types\").Consumer[\"subscribe\"]} */\n  const subscribe = async ({ topic, topics: subscriptionTopics, fromBeginning = false }) => {\n    if (consumerGroup) {\n      throw new KafkaJSNonRetriableError('Cannot subscribe to topic while consumer is running')\n    }\n\n    if (!topic && !subscriptionTopics) {\n      throw new KafkaJSNonRetriableError('Missing required argument \"topics\"')\n    }\n\n    if (subscriptionTopics != null && !Array.isArray(subscriptionTopics)) {\n      throw new KafkaJSNonRetriableError('Argument \"topics\" must be an array')\n    }\n\n    const subscriptions = subscriptionTopics || [topic]\n\n    for (const subscription of subscriptions) {\n      if (typeof subscription !== 'string' && !(subscription instanceof RegExp)) {\n        throw new KafkaJSNonRetriableError(\n          `Invalid topic ${subscription} (${typeof subscription}), the topic name has to be a String or a RegExp`\n        )\n      }\n    }\n\n    const hasRegexSubscriptions = subscriptions.some(subscription => subscription instanceof RegExp)\n    const metadata = hasRegexSubscriptions ? await cluster.metadata() : undefined\n\n    const topicsToSubscribe = []\n    for (const subscription of subscriptions) {\n      const isRegExp = subscription instanceof RegExp\n      if (isRegExp) {\n        const topicRegExp = subscription\n        const matchedTopics = metadata.topicMetadata\n          .map(({ topic: topicName }) => topicName)\n          .filter(topicName => topicRegExp.test(topicName))\n\n        logger.debug('Subscription based on RegExp', {\n          groupId,\n          topicRegExp: topicRegExp.toString(),\n          matchedTopics,\n        })\n\n        topicsToSubscribe.push(...matchedTopics)\n      } else {\n        topicsToSubscribe.push(subscription)\n      }\n    }\n\n    for (const t of topicsToSubscribe) {\n      topics[t] = { fromBeginning }\n    }\n\n    await cluster.addMultipleTargetTopics(topicsToSubscribe)\n  }\n\n  /** @type {import(\"../../types\").Consumer[\"run\"]} */\n  const run = async ({\n    autoCommit = true,\n    autoCommitInterval = null,\n    autoCommitThreshold = null,\n    eachBatchAutoResolve = true,\n    partitionsConsumedConcurrently: concurrency = 1,\n    eachBatch = null,\n    eachMessage = null,\n  } = {}) => {\n    if (consumerGroup) {\n      logger.warn('consumer#run was called, but the consumer is already running', { groupId })\n      return\n    }\n\n    const start = async onCrash => {\n      logger.info('Starting', { groupId })\n\n      consumerGroup = new ConsumerGroup({\n        logger: rootLogger,\n        topics: keys(topics),\n        topicConfigurations: topics,\n        retry,\n        cluster,\n        groupId,\n        assigners,\n        sessionTimeout,\n        rebalanceTimeout,\n        maxBytesPerPartition,\n        minBytes,\n        maxBytes,\n        maxWaitTimeInMs,\n        instrumentationEmitter,\n        isolationLevel,\n        rackId,\n        metadataMaxAge,\n        autoCommit,\n        autoCommitInterval,\n        autoCommitThreshold,\n      })\n\n      runner = new Runner({\n        logger: rootLogger,\n        consumerGroup,\n        instrumentationEmitter,\n        heartbeatInterval,\n        retry,\n        autoCommit,\n        eachBatchAutoResolve,\n        eachBatch,\n        eachMessage,\n        onCrash,\n        concurrency,\n      })\n\n      await runner.start()\n    }\n\n    const onCrash = async e => {\n      logger.error(`Crash: ${e.name}: ${e.message}`, {\n        groupId,\n        retryCount: e.retryCount,\n        stack: e.stack,\n      })\n\n      if (e.name === 'KafkaJSConnectionClosedError') {\n        cluster.removeBroker({ host: e.host, port: e.port })\n      }\n\n      await disconnect()\n\n      const getOriginalCause = error => {\n        if (error.cause) {\n          return getOriginalCause(error.cause)\n        }\n\n        return error\n      }\n\n      const isErrorRetriable =\n        e.name === 'KafkaJSNumberOfRetriesExceeded' || getOriginalCause(e).retriable === true\n      const shouldRestart =\n        isErrorRetriable &&\n        (!retry ||\n          !retry.restartOnFailure ||\n          (await retry.restartOnFailure(e).catch(error => {\n            logger.error(\n              'Caught error when invoking user-provided \"restartOnFailure\" callback. Defaulting to restarting.',\n              {\n                error: error.message || error,\n                cause: e.message || e,\n                groupId,\n              }\n            )\n\n            return true\n          })))\n\n      instrumentationEmitter.emit(CRASH, {\n        error: e,\n        groupId,\n        restart: shouldRestart,\n      })\n\n      if (shouldRestart) {\n        const retryTime = e.retryTime || (retry && retry.initialRetryTime) || initialRetryTime\n        logger.error(`Restarting the consumer in ${retryTime}ms`, {\n          retryCount: e.retryCount,\n          retryTime,\n          groupId,\n        })\n\n        restartTimeout = setTimeout(() => start(onCrash), retryTime)\n      }\n    }\n\n    await start(onCrash)\n  }\n\n  /** @type {import(\"../../types\").Consumer[\"on\"]} */\n  const on = (eventName, listener) => {\n    if (!eventNames.includes(eventName)) {\n      throw new KafkaJSNonRetriableError(`Event name should be one of ${eventKeys}`)\n    }\n\n    return instrumentationEmitter.addListener(unwrapEvent(eventName), event => {\n      event.type = wrapEvent(event.type)\n      Promise.resolve(listener(event)).catch(e => {\n        logger.error(`Failed to execute listener: ${e.message}`, {\n          eventName,\n          stack: e.stack,\n        })\n      })\n    })\n  }\n\n  /**\n   * @type {import(\"../../types\").Consumer[\"commitOffsets\"]}\n   * @param topicPartitions\n   *   Example: [{ topic: 'topic-name', partition: 0, offset: '1', metadata: 'event-id-3' }]\n   */\n  const commitOffsets = async (topicPartitions = []) => {\n    const commitsByTopic = topicPartitions.reduce(\n      (payload, { topic, partition, offset, metadata = null }) => {\n        if (!topic) {\n          throw new KafkaJSNonRetriableError(`Invalid topic ${topic}`)\n        }\n\n        if (isNaN(partition)) {\n          throw new KafkaJSNonRetriableError(\n            `Invalid partition, expected a number received ${partition}`\n          )\n        }\n\n        let commitOffset\n        try {\n          commitOffset = Long.fromValue(offset)\n        } catch (_) {\n          throw new KafkaJSNonRetriableError(`Invalid offset, expected a long received ${offset}`)\n        }\n\n        if (commitOffset.lessThan(0)) {\n          throw new KafkaJSNonRetriableError('Offset must not be a negative number')\n        }\n\n        if (metadata !== null && typeof metadata !== 'string') {\n          throw new KafkaJSNonRetriableError(\n            `Invalid offset metadata, expected string or null, received ${metadata}`\n          )\n        }\n\n        const topicCommits = payload[topic] || []\n\n        topicCommits.push({ partition, offset: commitOffset, metadata })\n\n        return { ...payload, [topic]: topicCommits }\n      },\n      {}\n    )\n\n    if (!consumerGroup) {\n      throw new KafkaJSNonRetriableError(\n        'Consumer group was not initialized, consumer#run must be called first'\n      )\n    }\n\n    const topics = Object.keys(commitsByTopic)\n\n    return runner.commitOffsets({\n      topics: topics.map(topic => {\n        return {\n          topic,\n          partitions: commitsByTopic[topic],\n        }\n      }),\n    })\n  }\n\n  /** @type {import(\"../../types\").Consumer[\"seek\"]} */\n  const seek = ({ topic, partition, offset }) => {\n    if (!topic) {\n      throw new KafkaJSNonRetriableError(`Invalid topic ${topic}`)\n    }\n\n    if (isNaN(partition)) {\n      throw new KafkaJSNonRetriableError(\n        `Invalid partition, expected a number received ${partition}`\n      )\n    }\n\n    let seekOffset\n    try {\n      seekOffset = Long.fromValue(offset)\n    } catch (_) {\n      throw new KafkaJSNonRetriableError(`Invalid offset, expected a long received ${offset}`)\n    }\n\n    if (seekOffset.lessThan(0) && !specialOffsets.includes(seekOffset.toString())) {\n      throw new KafkaJSNonRetriableError('Offset must not be a negative number')\n    }\n\n    if (!consumerGroup) {\n      throw new KafkaJSNonRetriableError(\n        'Consumer group was not initialized, consumer#run must be called first'\n      )\n    }\n\n    consumerGroup.seek({ topic, partition, offset: seekOffset.toString() })\n  }\n\n  /** @type {import(\"../../types\").Consumer[\"describeGroup\"]} */\n  const describeGroup = async () => {\n    const coordinator = await cluster.findGroupCoordinator({ groupId })\n    const retrier = createRetry(retry)\n    return retrier(async () => {\n      const { groups } = await coordinator.describeGroups({ groupIds: [groupId] })\n      return groups.find(group => group.groupId === groupId)\n    })\n  }\n\n  /**\n   * @type {import(\"../../types\").Consumer[\"pause\"]}\n   * @param topicPartitions\n   *   Example: [{ topic: 'topic-name', partitions: [1, 2] }]\n   */\n  const pause = (topicPartitions = []) => {\n    for (const topicPartition of topicPartitions) {\n      if (!topicPartition || !topicPartition.topic) {\n        throw new KafkaJSNonRetriableError(\n          `Invalid topic ${(topicPartition && topicPartition.topic) || topicPartition}`\n        )\n      } else if (\n        typeof topicPartition.partitions !== 'undefined' &&\n        (!Array.isArray(topicPartition.partitions) || topicPartition.partitions.some(isNaN))\n      ) {\n        throw new KafkaJSNonRetriableError(\n          `Array of valid partitions required to pause specific partitions instead of ${topicPartition.partitions}`\n        )\n      }\n    }\n\n    if (!consumerGroup) {\n      throw new KafkaJSNonRetriableError(\n        'Consumer group was not initialized, consumer#run must be called first'\n      )\n    }\n\n    consumerGroup.pause(topicPartitions)\n  }\n\n  /**\n   * Returns the list of topic partitions paused on this consumer\n   *\n   * @type {import(\"../../types\").Consumer[\"paused\"]}\n   */\n  const paused = () => {\n    if (!consumerGroup) {\n      return []\n    }\n\n    return consumerGroup.paused()\n  }\n\n  /**\n   * @type {import(\"../../types\").Consumer[\"resume\"]}\n   * @param topicPartitions\n   *  Example: [{ topic: 'topic-name', partitions: [1, 2] }]\n   */\n  const resume = (topicPartitions = []) => {\n    for (const topicPartition of topicPartitions) {\n      if (!topicPartition || !topicPartition.topic) {\n        throw new KafkaJSNonRetriableError(\n          `Invalid topic ${(topicPartition && topicPartition.topic) || topicPartition}`\n        )\n      } else if (\n        typeof topicPartition.partitions !== 'undefined' &&\n        (!Array.isArray(topicPartition.partitions) || topicPartition.partitions.some(isNaN))\n      ) {\n        throw new KafkaJSNonRetriableError(\n          `Array of valid partitions required to resume specific partitions instead of ${topicPartition.partitions}`\n        )\n      }\n    }\n\n    if (!consumerGroup) {\n      throw new KafkaJSNonRetriableError(\n        'Consumer group was not initialized, consumer#run must be called first'\n      )\n    }\n\n    consumerGroup.resume(topicPartitions)\n  }\n\n  /**\n   * @return {Object} logger\n   */\n  const getLogger = () => logger\n\n  return {\n    connect,\n    disconnect,\n    subscribe,\n    stop,\n    run,\n    commitOffsets,\n    seek,\n    describeGroup,\n    pause,\n    paused,\n    resume,\n    on,\n    events,\n    logger: getLogger,\n  }\n}\n", "const sleep = require('./sleep')\nconst { KafkaJSTimeout } = require('../errors')\n\nmodule.exports = (\n  fn,\n  { delay = 50, maxWait = 10000, timeoutMessage = 'Timeout', ignoreTimeout = false } = {}\n) => {\n  let timeoutId\n  let totalWait = 0\n  let fulfilled = false\n\n  const checkCondition = async (resolve, reject) => {\n    totalWait += delay\n    if (fulfilled) {\n      return\n    }\n\n    await sleep(delay)\n\n    try {\n      const result = await fn(totalWait)\n      if (result) {\n        fulfilled = true\n        clearTimeout(timeoutId)\n        return resolve(result)\n      }\n\n      checkCondition(resolve, reject)\n    } catch (e) {\n      fulfilled = true\n      clearTimeout(timeoutId)\n      reject(e)\n    }\n  }\n\n  return new Promise((resolve, reject) => {\n    checkCondition(resolve, reject)\n\n    if (ignoreTimeout) {\n      return\n    }\n\n    timeoutId = setTimeout(() => {\n      if (!fulfilled) {\n        fulfilled = true\n        return reject(new KafkaJSTimeout(timeoutMessage))\n      }\n    }, maxWait)\n  })\n}\n", "module.exports = async (array, groupFn) => {\n  const result = new Map()\n\n  for (const item of array) {\n    const group = await Promise.resolve(groupFn(item))\n    result.set(group, result.has(group) ? [...result.get(group), item] : [item])\n  }\n\n  return result\n}\n", "const swapObject = require('../utils/swapObject')\nconst networkEvents = require('../network/instrumentationEvents')\nconst InstrumentationEventType = require('../instrumentation/eventType')\nconst adminType = InstrumentationEventType('admin')\n\nconst events = {\n  CONNECT: adminType('connect'),\n  DISCONNECT: adminType('disconnect'),\n  REQUEST: adminType(networkEvents.NETWORK_REQUEST),\n  REQUEST_TIMEOUT: adminType(networkEvents.NETWORK_REQUEST_TIMEOUT),\n  REQUEST_QUEUE_SIZE: adminType(networkEvents.NETWORK_REQUEST_QUEUE_SIZE),\n}\n\nconst wrappedEvents = {\n  [events.REQUEST]: networkEvents.NETWORK_REQUEST,\n  [events.REQUEST_TIMEOUT]: networkEvents.NETWORK_REQUEST_TIMEOUT,\n  [events.REQUEST_QUEUE_SIZE]: networkEvents.NETWORK_REQUEST_QUEUE_SIZE,\n}\n\nconst reversedWrappedEvents = swapObject(wrappedEvents)\nconst unwrap = eventName => wrappedEvents[eventName] || eventName\nconst wrap = eventName => reversedWrappedEvents[eventName] || eventName\n\nmodule.exports = {\n  events,\n  wrap,\n  unwrap,\n}\n", "/**\n * @see https://github.com/apache/kafka/blob/a15387f34d142684859c2a57fcbef25edcdce25a/clients/src/main/java/org/apache/kafka/common/resource/ResourceType.java#L25-L31\n * @typedef {number} ACLResourceTypes\n *\n * Enum for ACL Resource Types\n * @readonly\n * @enum {ACLResourceTypes}\n */\n\nmodule.exports = {\n  /**\n   * Represents any ResourceType which this client cannot understand,\n   * perhaps because this client is too old.\n   */\n  UNKNOWN: 0,\n  /**\n   * In a filter, matches any ResourceType.\n   */\n  ANY: 1,\n  /**\n   * A Kafka topic.\n   * @see http://kafka.apache.org/documentation/#topicconfigs\n   */\n  TOPIC: 2,\n  /**\n   * A consumer group.\n   * @see http://kafka.apache.org/documentation/#consumerconfigs\n   */\n  GROUP: 3,\n  /**\n   * The cluster as a whole.\n   */\n  CLUSTER: 4,\n  /**\n   * A transactional ID.\n   */\n  TRANSACTIONAL_ID: 5,\n  /**\n   * A token ID.\n   */\n  DELEGATION_TOKEN: 6,\n}\n", "// From:\n// https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/common/acl/AclOperation.java#L44\n\n/**\n * @typedef {number} ACLOperationTypes\n *\n * Enum for ACL Operations Types\n * @readonly\n * @enum {ACLOperationTypes}\n */\nmodule.exports = {\n  /**\n   * Represents any AclOperation which this client cannot understand, perhaps because this\n   * client is too old.\n   */\n  UNKNOWN: 0,\n  /**\n   * In a filter, matches any AclOperation.\n   */\n  ANY: 1,\n  /**\n   * ALL operation.\n   */\n  ALL: 2,\n  /**\n   * READ operation.\n   */\n  READ: 3,\n  /**\n   * WRITE operation.\n   */\n  WRITE: 4,\n  /**\n   * CREATE operation.\n   */\n  CREATE: 5,\n  /**\n   * DELETE operation.\n   */\n  DELETE: 6,\n  /**\n   * ALTER operation.\n   */\n  ALTER: 7,\n  /**\n   * DESCRIBE operation.\n   */\n  DESCRIBE: 8,\n  /**\n   * CLUSTER_ACTION operation.\n   */\n  CLUSTER_ACTION: 9,\n  /**\n   * DESCRIBE_CONFIGS operation.\n   */\n  DESCRIBE_CONFIGS: 10,\n  /**\n   * ALTER_CONFIGS operation.\n   */\n  ALTER_CONFIGS: 11,\n  /**\n   * IDEMPOTENT_WRITE operation.\n   */\n  IDEMPOTENT_WRITE: 12,\n}\n", "// From:\n// https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/common/acl/AclPermissionType.java/#L31\n\n/**\n * @typedef {number} ACLPermissionTypes\n *\n * Enum for Permission Types\n * @readonly\n * @enum {ACLPermissionTypes}\n */\nmodule.exports = {\n  /**\n   * Represents any AclPermissionType which this client cannot understand,\n   * perhaps because this client is too old.\n   */\n  UNKNOWN: 0,\n  /**\n   * In a filter, matches any AclPermissionType.\n   */\n  ANY: 1,\n  /**\n   * Disallows access.\n   */\n  DENY: 2,\n  /**\n   * Grants access.\n   */\n  ALLOW: 3,\n}\n", "// From:\n// https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/common/resource/PatternType.java#L32\n\n/**\n * @typedef {number} ACLResourcePatternTypes\n *\n * Enum for ACL Resource Pattern Type\n * @readonly\n * @enum {ACLResourcePatternTypes}\n */\nmodule.exports = {\n  /**\n   * Represents any PatternType which this client cannot understand, perhaps because this client is too old.\n   */\n  UNKNOWN: 0,\n  /**\n   * In a filter, matches any resource pattern type.\n   */\n  ANY: 1,\n  /**\n   * In a filter, will perform pattern matching.\n   *\n   * e.g. Given a filter of {@code ResourcePatternFilter(TOPIC, \"payments.received\", MATCH)`}, the filter match\n   * any {@link ResourcePattern} that matches topic 'payments.received'. This might include:\n   * <ul>\n   *     <li>A Literal pattern with the same type and name, e.g. {@code ResourcePattern(TOPIC, \"payments.received\", LITERAL)}</li>\n   *     <li>A Wildcard pattern with the same type, e.g. {@code ResourcePattern(TOPIC, \"*\", LITERAL)}</li>\n   *     <li>A Prefixed pattern with the same type and where the name is a matching prefix, e.g. {@code ResourcePattern(TOPIC, \"payments.\", PREFIXED)}</li>\n   * </ul>\n   */\n  MATCH: 2,\n  /**\n   * A literal resource name.\n   *\n   * A literal name defines the full name of a resource, e.g. topic with name 'foo', or group with name 'bob'.\n   *\n   * The special wildcard character {@code *} can be used to represent a resource with any name.\n   */\n  LITERAL: 3,\n  /**\n   * A prefixed resource name.\n   *\n   * A prefixed name defines a prefix for a resource, e.g. topics with names that start with 'foo'.\n   */\n  PREFIXED: 4,\n}\n", "const createRetry = require('../retry')\nconst waitFor = require('../utils/waitFor')\nconst groupBy = require('../utils/groupBy')\nconst createConsumer = require('../consumer')\nconst InstrumentationEventEmitter = require('../instrumentation/emitter')\nconst { events, wrap: wrapEvent, unwrap: unwrapEvent } = require('./instrumentationEvents')\nconst { LEVELS } = require('../loggers')\nconst {\n  KafkaJSNonRetriableError,\n  KafkaJSDeleteGroupsError,\n  KafkaJSBrokerNotFound,\n  KafkaJSDeleteTopicRecordsError,\n  KafkaJSAggregateError,\n} = require('../errors')\nconst { staleMetadata } = require('../protocol/error')\nconst CONFIG_RESOURCE_TYPES = require('../protocol/configResourceTypes')\nconst ACL_RESOURCE_TYPES = require('../protocol/aclResourceTypes')\nconst ACL_OPERATION_TYPES = require('../protocol/aclOperationTypes')\nconst ACL_PERMISSION_TYPES = require('../protocol/aclPermissionTypes')\nconst RESOURCE_PATTERN_TYPES = require('../protocol/resourcePatternTypes')\nconst { EARLIEST_OFFSET, LATEST_OFFSET } = require('../constants')\n\nconst { CONNECT, DISCONNECT } = events\n\nconst NO_CONTROLLER_ID = -1\n\nconst { values, keys, entries } = Object\nconst eventNames = values(events)\nconst eventKeys = keys(events)\n  .map(key => `admin.events.${key}`)\n  .join(', ')\n\nconst retryOnLeaderNotAvailable = (fn, opts = {}) => {\n  const callback = async () => {\n    try {\n      return await fn()\n    } catch (e) {\n      if (e.type !== 'LEADER_NOT_AVAILABLE') {\n        throw e\n      }\n      return false\n    }\n  }\n\n  return waitFor(callback, opts)\n}\n\nconst isConsumerGroupRunning = description => ['Empty', 'Dead'].includes(description.state)\nconst findTopicPartitions = async (cluster, topic) => {\n  await cluster.addTargetTopic(topic)\n  await cluster.refreshMetadataIfNecessary()\n\n  return cluster\n    .findTopicPartitionMetadata(topic)\n    .map(({ partitionId }) => partitionId)\n    .sort()\n}\nconst indexByPartition = array =>\n  array.reduce(\n    (obj, { partition, ...props }) => Object.assign(obj, { [partition]: { ...props } }),\n    {}\n  )\n\n/**\n *\n * @param {Object} params\n * @param {import(\"../../types\").Logger} params.logger\n * @param {InstrumentationEventEmitter} [params.instrumentationEmitter]\n * @param {import('../../types').RetryOptions} params.retry\n * @param {import(\"../../types\").Cluster} params.cluster\n *\n * @returns {import(\"../../types\").Admin}\n */\nmodule.exports = ({\n  logger: rootLogger,\n  instrumentationEmitter: rootInstrumentationEmitter,\n  retry,\n  cluster,\n}) => {\n  const logger = rootLogger.namespace('Admin')\n  const instrumentationEmitter = rootInstrumentationEmitter || new InstrumentationEventEmitter()\n\n  /**\n   * @returns {Promise}\n   */\n  const connect = async () => {\n    await cluster.connect()\n    instrumentationEmitter.emit(CONNECT)\n  }\n\n  /**\n   * @return {Promise}\n   */\n  const disconnect = async () => {\n    await cluster.disconnect()\n    instrumentationEmitter.emit(DISCONNECT)\n  }\n\n  /**\n   * @return {Promise}\n   */\n  const listTopics = async () => {\n    const { topicMetadata } = await cluster.metadata()\n    const topics = topicMetadata.map(t => t.topic)\n    return topics\n  }\n\n  /**\n   * @param {Object} request\n   * @param {array} request.topics\n   * @param {boolean} [request.validateOnly=false]\n   * @param {number} [request.timeout=5000]\n   * @param {boolean} [request.waitForLeaders=true]\n   * @return {Promise}\n   */\n  const createTopics = async ({ topics, validateOnly, timeout, waitForLeaders = true }) => {\n    if (!topics || !Array.isArray(topics)) {\n      throw new KafkaJSNonRetriableError(`Invalid topics array ${topics}`)\n    }\n\n    if (topics.filter(({ topic }) => typeof topic !== 'string').length > 0) {\n      throw new KafkaJSNonRetriableError(\n        'Invalid topics array, the topic names have to be a valid string'\n      )\n    }\n\n    const topicNames = new Set(topics.map(({ topic }) => topic))\n    if (topicNames.size < topics.length) {\n      throw new KafkaJSNonRetriableError(\n        'Invalid topics array, it cannot have multiple entries for the same topic'\n      )\n    }\n\n    for (const { topic, configEntries } of topics) {\n      if (configEntries == null) {\n        continue\n      }\n\n      if (!Array.isArray(configEntries)) {\n        throw new KafkaJSNonRetriableError(\n          `Invalid configEntries for topic \"${topic}\", must be an array`\n        )\n      }\n\n      configEntries.forEach((entry, index) => {\n        if (typeof entry !== 'object' || entry == null) {\n          throw new KafkaJSNonRetriableError(\n            `Invalid configEntries for topic \"${topic}\". Entry ${index} must be an object`\n          )\n        }\n\n        for (const requiredProperty of ['name', 'value']) {\n          if (\n            !Object.prototype.hasOwnProperty.call(entry, requiredProperty) ||\n            typeof entry[requiredProperty] !== 'string'\n          ) {\n            throw new KafkaJSNonRetriableError(\n              `Invalid configEntries for topic \"${topic}\". Entry ${index} must have a valid \"${requiredProperty}\" property`\n            )\n          }\n        }\n      })\n    }\n\n    const retrier = createRetry(retry)\n\n    return retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await cluster.refreshMetadata()\n        const broker = await cluster.findControllerBroker()\n        await broker.createTopics({ topics, validateOnly, timeout })\n\n        if (waitForLeaders) {\n          const topicNamesArray = Array.from(topicNames.values())\n          await retryOnLeaderNotAvailable(async () => await broker.metadata(topicNamesArray), {\n            delay: 100,\n            maxWait: timeout,\n            timeoutMessage: 'Timed out while waiting for topic leaders',\n          })\n        }\n\n        return true\n      } catch (e) {\n        if (e.type === 'NOT_CONTROLLER') {\n          logger.warn('Could not create topics', { error: e.message, retryCount, retryTime })\n          throw e\n        }\n\n        if (e instanceof KafkaJSAggregateError) {\n          if (e.errors.every(error => error.type === 'TOPIC_ALREADY_EXISTS')) {\n            return false\n          }\n        }\n\n        bail(e)\n      }\n    })\n  }\n  /**\n   * @param {array} topicPartitions\n   * @param {boolean} [validateOnly=false]\n   * @param {number} [timeout=5000]\n   * @return {Promise<void>}\n   */\n  const createPartitions = async ({ topicPartitions, validateOnly, timeout }) => {\n    if (!topicPartitions || !Array.isArray(topicPartitions)) {\n      throw new KafkaJSNonRetriableError(`Invalid topic partitions array ${topicPartitions}`)\n    }\n    if (topicPartitions.length === 0) {\n      throw new KafkaJSNonRetriableError(`Empty topic partitions array`)\n    }\n\n    if (topicPartitions.filter(({ topic }) => typeof topic !== 'string').length > 0) {\n      throw new KafkaJSNonRetriableError(\n        'Invalid topic partitions array, the topic names have to be a valid string'\n      )\n    }\n\n    const topicNames = new Set(topicPartitions.map(({ topic }) => topic))\n    if (topicNames.size < topicPartitions.length) {\n      throw new KafkaJSNonRetriableError(\n        'Invalid topic partitions array, it cannot have multiple entries for the same topic'\n      )\n    }\n\n    const retrier = createRetry(retry)\n\n    return retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await cluster.refreshMetadata()\n        const broker = await cluster.findControllerBroker()\n        await broker.createPartitions({ topicPartitions, validateOnly, timeout })\n      } catch (e) {\n        if (e.type === 'NOT_CONTROLLER') {\n          logger.warn('Could not create topics', { error: e.message, retryCount, retryTime })\n          throw e\n        }\n\n        bail(e)\n      }\n    })\n  }\n\n  /**\n   * @param {string[]} topics\n   * @param {number} [timeout=5000]\n   * @return {Promise}\n   */\n  const deleteTopics = async ({ topics, timeout }) => {\n    if (!topics || !Array.isArray(topics)) {\n      throw new KafkaJSNonRetriableError(`Invalid topics array ${topics}`)\n    }\n\n    if (topics.filter(topic => typeof topic !== 'string').length > 0) {\n      throw new KafkaJSNonRetriableError('Invalid topics array, the names must be a valid string')\n    }\n\n    const retrier = createRetry(retry)\n\n    return retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await cluster.refreshMetadata()\n        const broker = await cluster.findControllerBroker()\n        await broker.deleteTopics({ topics, timeout })\n\n        // Remove deleted topics\n        for (const topic of topics) {\n          cluster.targetTopics.delete(topic)\n        }\n\n        await cluster.refreshMetadata()\n      } catch (e) {\n        if (['NOT_CONTROLLER', 'UNKNOWN_TOPIC_OR_PARTITION'].includes(e.type)) {\n          logger.warn('Could not delete topics', { error: e.message, retryCount, retryTime })\n          throw e\n        }\n\n        if (e.type === 'REQUEST_TIMED_OUT') {\n          logger.error(\n            'Could not delete topics, check if \"delete.topic.enable\" is set to \"true\" (the default value is \"false\") or increase the timeout',\n            {\n              error: e.message,\n              retryCount,\n              retryTime,\n            }\n          )\n        }\n\n        bail(e)\n      }\n    })\n  }\n\n  /**\n   * @param {string} topic\n   */\n\n  const fetchTopicOffsets = async topic => {\n    if (!topic || typeof topic !== 'string') {\n      throw new KafkaJSNonRetriableError(`Invalid topic ${topic}`)\n    }\n\n    const retrier = createRetry(retry)\n\n    return retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await cluster.addTargetTopic(topic)\n        await cluster.refreshMetadataIfNecessary()\n\n        const metadata = cluster.findTopicPartitionMetadata(topic)\n        const high = await cluster.fetchTopicsOffset([\n          {\n            topic,\n            fromBeginning: false,\n            partitions: metadata.map(p => ({ partition: p.partitionId })),\n          },\n        ])\n\n        const low = await cluster.fetchTopicsOffset([\n          {\n            topic,\n            fromBeginning: true,\n            partitions: metadata.map(p => ({ partition: p.partitionId })),\n          },\n        ])\n\n        const { partitions: highPartitions } = high.pop()\n        const { partitions: lowPartitions } = low.pop()\n        return highPartitions.map(({ partition, offset }) => ({\n          partition,\n          offset,\n          high: offset,\n          low: lowPartitions.find(({ partition: lowPartition }) => lowPartition === partition)\n            .offset,\n        }))\n      } catch (e) {\n        if (e.type === 'UNKNOWN_TOPIC_OR_PARTITION') {\n          await cluster.refreshMetadata()\n          throw e\n        }\n\n        bail(e)\n      }\n    })\n  }\n\n  /**\n   * @param {string} topic\n   * @param {number} [timestamp]\n   */\n\n  const fetchTopicOffsetsByTimestamp = async (topic, timestamp) => {\n    if (!topic || typeof topic !== 'string') {\n      throw new KafkaJSNonRetriableError(`Invalid topic ${topic}`)\n    }\n\n    const retrier = createRetry(retry)\n\n    return retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await cluster.addTargetTopic(topic)\n        await cluster.refreshMetadataIfNecessary()\n\n        const metadata = cluster.findTopicPartitionMetadata(topic)\n        const partitions = metadata.map(p => ({ partition: p.partitionId }))\n\n        const high = await cluster.fetchTopicsOffset([\n          {\n            topic,\n            fromBeginning: false,\n            partitions,\n          },\n        ])\n        const { partitions: highPartitions } = high.pop()\n\n        const offsets = await cluster.fetchTopicsOffset([\n          {\n            topic,\n            fromTimestamp: timestamp,\n            partitions,\n          },\n        ])\n        const { partitions: lowPartitions } = offsets.pop()\n\n        return lowPartitions.map(({ partition, offset }) => ({\n          partition,\n          offset:\n            parseInt(offset, 10) >= 0\n              ? offset\n              : highPartitions.find(({ partition: highPartition }) => highPartition === partition)\n                  .offset,\n        }))\n      } catch (e) {\n        if (e.type === 'UNKNOWN_TOPIC_OR_PARTITION') {\n          await cluster.refreshMetadata()\n          throw e\n        }\n\n        bail(e)\n      }\n    })\n  }\n\n  /**\n   * Fetch offsets for a topic or multiple topics\n   *\n   * Note: set either topic or topics but not both.\n   *\n   * @param {string} groupId\n   * @param {string[]} topics - list of topics to fetch offsets for, defaults to `[]` which fetches all topics for `groupId`.\n   * @param {boolean} [resolveOffsets=false]\n   * @return {Promise}\n   */\n  const fetchOffsets = async ({ groupId, topics, resolveOffsets = false }) => {\n    if (!groupId) {\n      throw new KafkaJSNonRetriableError(`Invalid groupId ${groupId}`)\n    }\n\n    if (!topics) {\n      topics = []\n    }\n\n    if (!Array.isArray(topics)) {\n      throw new KafkaJSNonRetriableError('Expected topics array to be set')\n    }\n\n    const coordinator = await cluster.findGroupCoordinator({ groupId })\n    const topicsToFetch = await Promise.all(\n      topics.map(async topic => {\n        const partitions = await findTopicPartitions(cluster, topic)\n        const partitionsToFetch = partitions.map(partition => ({ partition }))\n        return { topic, partitions: partitionsToFetch }\n      })\n    )\n    let { responses: consumerOffsets } = await coordinator.offsetFetch({\n      groupId,\n      topics: topicsToFetch,\n    })\n\n    if (resolveOffsets) {\n      consumerOffsets = await Promise.all(\n        consumerOffsets.map(async ({ topic, partitions }) => {\n          const indexedOffsets = indexByPartition(await fetchTopicOffsets(topic))\n          const recalculatedPartitions = partitions.map(({ offset, partition, ...props }) => {\n            let resolvedOffset = offset\n            if (Number(offset) === EARLIEST_OFFSET) {\n              resolvedOffset = indexedOffsets[partition].low\n            }\n            if (Number(offset) === LATEST_OFFSET) {\n              resolvedOffset = indexedOffsets[partition].high\n            }\n            return {\n              partition,\n              offset: resolvedOffset,\n              ...props,\n            }\n          })\n\n          await setOffsets({ groupId, topic, partitions: recalculatedPartitions })\n\n          return {\n            topic,\n            partitions: recalculatedPartitions,\n          }\n        })\n      )\n    }\n\n    return consumerOffsets.map(({ topic, partitions }) => {\n      const completePartitions = partitions.map(({ partition, offset, metadata }) => ({\n        partition,\n        offset,\n        metadata: metadata || null,\n      }))\n\n      return { topic, partitions: completePartitions }\n    })\n  }\n\n  /**\n   * @param {string} groupId\n   * @param {string} topic\n   * @param {boolean} [earliest=false]\n   * @return {Promise}\n   */\n  const resetOffsets = async ({ groupId, topic, earliest = false }) => {\n    if (!groupId) {\n      throw new KafkaJSNonRetriableError(`Invalid groupId ${groupId}`)\n    }\n\n    if (!topic) {\n      throw new KafkaJSNonRetriableError(`Invalid topic ${topic}`)\n    }\n\n    const partitions = await findTopicPartitions(cluster, topic)\n    const partitionsToSeek = partitions.map(partition => ({\n      partition,\n      offset: cluster.defaultOffset({ fromBeginning: earliest }),\n    }))\n\n    return setOffsets({ groupId, topic, partitions: partitionsToSeek })\n  }\n\n  /**\n   * @param {string} groupId\n   * @param {string} topic\n   * @param {Array<SeekEntry>} partitions\n   * @return {Promise}\n   *\n   * @typedef {Object} SeekEntry\n   * @property {number} partition\n   * @property {string} offset\n   */\n  const setOffsets = async ({ groupId, topic, partitions }) => {\n    if (!groupId) {\n      throw new KafkaJSNonRetriableError(`Invalid groupId ${groupId}`)\n    }\n\n    if (!topic) {\n      throw new KafkaJSNonRetriableError(`Invalid topic ${topic}`)\n    }\n\n    if (!partitions || partitions.length === 0) {\n      throw new KafkaJSNonRetriableError(`Invalid partitions`)\n    }\n\n    const consumer = createConsumer({\n      logger: rootLogger.namespace('Admin', LEVELS.NOTHING),\n      cluster,\n      groupId,\n    })\n\n    await consumer.subscribe({ topic, fromBeginning: true })\n    const description = await consumer.describeGroup()\n\n    if (!isConsumerGroupRunning(description)) {\n      throw new KafkaJSNonRetriableError(\n        `The consumer group must have no running instances, current state: ${description.state}`\n      )\n    }\n\n    return new Promise((resolve, reject) => {\n      consumer.on(consumer.events.FETCH, async () =>\n        consumer\n          .stop()\n          .then(resolve)\n          .catch(reject)\n      )\n\n      consumer\n        .run({\n          eachBatchAutoResolve: false,\n          eachBatch: async () => true,\n        })\n        .catch(reject)\n\n      // This consumer doesn't need to consume any data\n      consumer.pause([{ topic }])\n\n      for (const seekData of partitions) {\n        consumer.seek({ topic, ...seekData })\n      }\n    })\n  }\n\n  const isBrokerConfig = type =>\n    [CONFIG_RESOURCE_TYPES.BROKER, CONFIG_RESOURCE_TYPES.BROKER_LOGGER].includes(type)\n\n  /**\n   * Broker configs can only be returned by the target broker\n   *\n   * @see\n   * https://github.com/apache/kafka/blob/821c1ac6641845aeca96a43bc2b946ecec5cba4f/clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java#L3783\n   * https://github.com/apache/kafka/blob/821c1ac6641845aeca96a43bc2b946ecec5cba4f/clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java#L2027\n   *\n   * @param {Broker} defaultBroker. Broker used in case the configuration is not a broker config\n   */\n  const groupResourcesByBroker = ({ resources, defaultBroker }) =>\n    groupBy(resources, async ({ type, name: nodeId }) => {\n      return isBrokerConfig(type)\n        ? await cluster.findBroker({ nodeId: String(nodeId) })\n        : defaultBroker\n    })\n\n  /**\n   * @param {Array<ResourceConfigQuery>} resources\n   * @param {boolean} [includeSynonyms=false]\n   * @return {Promise}\n   *\n   * @typedef {Object} ResourceConfigQuery\n   * @property {ConfigResourceType} type\n   * @property {string} name\n   * @property {Array<String>} [configNames=[]]\n   */\n  const describeConfigs = async ({ resources, includeSynonyms }) => {\n    if (!resources || !Array.isArray(resources)) {\n      throw new KafkaJSNonRetriableError(`Invalid resources array ${resources}`)\n    }\n\n    if (resources.length === 0) {\n      throw new KafkaJSNonRetriableError('Resources array cannot be empty')\n    }\n\n    const validResourceTypes = Object.values(CONFIG_RESOURCE_TYPES)\n    const invalidType = resources.find(r => !validResourceTypes.includes(r.type))\n\n    if (invalidType) {\n      throw new KafkaJSNonRetriableError(\n        `Invalid resource type ${invalidType.type}: ${JSON.stringify(invalidType)}`\n      )\n    }\n\n    const invalidName = resources.find(r => !r.name || typeof r.name !== 'string')\n\n    if (invalidName) {\n      throw new KafkaJSNonRetriableError(\n        `Invalid resource name ${invalidName.name}: ${JSON.stringify(invalidName)}`\n      )\n    }\n\n    const invalidConfigs = resources.find(\n      r => !Array.isArray(r.configNames) && r.configNames != null\n    )\n\n    if (invalidConfigs) {\n      const { configNames } = invalidConfigs\n      throw new KafkaJSNonRetriableError(\n        `Invalid resource configNames ${configNames}: ${JSON.stringify(invalidConfigs)}`\n      )\n    }\n\n    const retrier = createRetry(retry)\n\n    return retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await cluster.refreshMetadata()\n        const controller = await cluster.findControllerBroker()\n        const resourcerByBroker = await groupResourcesByBroker({\n          resources,\n          defaultBroker: controller,\n        })\n\n        const describeConfigsAction = async broker => {\n          const targetBroker = broker || controller\n          return targetBroker.describeConfigs({\n            resources: resourcerByBroker.get(targetBroker),\n            includeSynonyms,\n          })\n        }\n\n        const brokers = Array.from(resourcerByBroker.keys())\n        const responses = await Promise.all(brokers.map(describeConfigsAction))\n        const responseResources = responses.reduce(\n          (result, { resources }) => [...result, ...resources],\n          []\n        )\n\n        return { resources: responseResources }\n      } catch (e) {\n        if (e.type === 'NOT_CONTROLLER') {\n          logger.warn('Could not describe configs', { error: e.message, retryCount, retryTime })\n          throw e\n        }\n\n        bail(e)\n      }\n    })\n  }\n\n  /**\n   * @param {Array<ResourceConfig>} resources\n   * @param {boolean} [validateOnly=false]\n   * @return {Promise}\n   *\n   * @typedef {Object} ResourceConfig\n   * @property {ConfigResourceType} type\n   * @property {string} name\n   * @property {Array<ResourceConfigEntry>} configEntries\n   *\n   * @typedef {Object} ResourceConfigEntry\n   * @property {string} name\n   * @property {string} value\n   */\n  const alterConfigs = async ({ resources, validateOnly }) => {\n    if (!resources || !Array.isArray(resources)) {\n      throw new KafkaJSNonRetriableError(`Invalid resources array ${resources}`)\n    }\n\n    if (resources.length === 0) {\n      throw new KafkaJSNonRetriableError('Resources array cannot be empty')\n    }\n\n    const validResourceTypes = Object.values(CONFIG_RESOURCE_TYPES)\n    const invalidType = resources.find(r => !validResourceTypes.includes(r.type))\n\n    if (invalidType) {\n      throw new KafkaJSNonRetriableError(\n        `Invalid resource type ${invalidType.type}: ${JSON.stringify(invalidType)}`\n      )\n    }\n\n    const invalidName = resources.find(r => !r.name || typeof r.name !== 'string')\n\n    if (invalidName) {\n      throw new KafkaJSNonRetriableError(\n        `Invalid resource name ${invalidName.name}: ${JSON.stringify(invalidName)}`\n      )\n    }\n\n    const invalidConfigs = resources.find(r => !Array.isArray(r.configEntries))\n\n    if (invalidConfigs) {\n      const { configEntries } = invalidConfigs\n      throw new KafkaJSNonRetriableError(\n        `Invalid resource configEntries ${configEntries}: ${JSON.stringify(invalidConfigs)}`\n      )\n    }\n\n    const invalidConfigValue = resources.find(r =>\n      r.configEntries.some(e => typeof e.name !== 'string' || typeof e.value !== 'string')\n    )\n\n    if (invalidConfigValue) {\n      throw new KafkaJSNonRetriableError(\n        `Invalid resource config value: ${JSON.stringify(invalidConfigValue)}`\n      )\n    }\n\n    const retrier = createRetry(retry)\n\n    return retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await cluster.refreshMetadata()\n        const controller = await cluster.findControllerBroker()\n        const resourcerByBroker = await groupResourcesByBroker({\n          resources,\n          defaultBroker: controller,\n        })\n\n        const alterConfigsAction = async broker => {\n          const targetBroker = broker || controller\n          return targetBroker.alterConfigs({\n            resources: resourcerByBroker.get(targetBroker),\n            validateOnly: !!validateOnly,\n          })\n        }\n\n        const brokers = Array.from(resourcerByBroker.keys())\n        const responses = await Promise.all(brokers.map(alterConfigsAction))\n        const responseResources = responses.reduce(\n          (result, { resources }) => [...result, ...resources],\n          []\n        )\n\n        return { resources: responseResources }\n      } catch (e) {\n        if (e.type === 'NOT_CONTROLLER') {\n          logger.warn('Could not alter configs', { error: e.message, retryCount, retryTime })\n          throw e\n        }\n\n        bail(e)\n      }\n    })\n  }\n\n  /**\n   * Fetch metadata for provided topics.\n   *\n   * If no topics are provided fetch metadata for all topics.\n   * @see https://kafka.apache.org/protocol#The_Messages_Metadata\n   *\n   * @param {Object} [options]\n   * @param {string[]} [options.topics]\n   * @return {Promise<TopicsMetadata>}\n   *\n   * @typedef {Object} TopicsMetadata\n   * @property {Array<TopicMetadata>} topics\n   *\n   * @typedef {Object} TopicMetadata\n   * @property {String} name\n   * @property {Array<PartitionMetadata>} partitions\n   *\n   * @typedef {Object} PartitionMetadata\n   * @property {number} partitionErrorCode Response error code\n   * @property {number} partitionId Topic partition id\n   * @property {number} leader  The id of the broker acting as leader for this partition.\n   * @property {Array<number>} replicas The set of all nodes that host this partition.\n   * @property {Array<number>} isr The set of nodes that are in sync with the leader for this partition.\n   */\n  const fetchTopicMetadata = async ({ topics = [] } = {}) => {\n    if (topics) {\n      topics.forEach(topic => {\n        if (!topic || typeof topic !== 'string') {\n          throw new KafkaJSNonRetriableError(`Invalid topic ${topic}`)\n        }\n      })\n    }\n\n    const metadata = await cluster.metadata({ topics })\n\n    return {\n      topics: metadata.topicMetadata.map(topicMetadata => ({\n        name: topicMetadata.topic,\n        partitions: topicMetadata.partitionMetadata,\n      })),\n    }\n  }\n\n  /**\n   * Describe cluster\n   *\n   * @return {Promise<ClusterMetadata>}\n   *\n   * @typedef {Object} ClusterMetadata\n   * @property {Array<Broker>} brokers\n   * @property {Number} controller Current controller id. Returns null if unknown.\n   * @property {String} clusterId\n   *\n   * @typedef {Object} Broker\n   * @property {Number} nodeId\n   * @property {String} host\n   * @property {Number} port\n   */\n  const describeCluster = async () => {\n    const { brokers: nodes, clusterId, controllerId } = await cluster.metadata({ topics: [] })\n    const brokers = nodes.map(({ nodeId, host, port }) => ({\n      nodeId,\n      host,\n      port,\n    }))\n    const controller =\n      controllerId == null || controllerId === NO_CONTROLLER_ID ? null : controllerId\n\n    return {\n      brokers,\n      controller,\n      clusterId,\n    }\n  }\n\n  /**\n   * List groups in a broker\n   *\n   * @return {Promise<ListGroups>}\n   *\n   * @typedef {Object} ListGroups\n   * @property {Array<ListGroup>} groups\n   *\n   * @typedef {Object} ListGroup\n   * @property {string} groupId\n   * @property {string} protocolType\n   */\n  const listGroups = async () => {\n    await cluster.refreshMetadata()\n    let groups = []\n    for (var nodeId in cluster.brokerPool.brokers) {\n      const broker = await cluster.findBroker({ nodeId })\n      const response = await broker.listGroups()\n      groups = groups.concat(response.groups)\n    }\n\n    return { groups }\n  }\n\n  /**\n   * Describe groups by group ids\n   * @param {Array<string>} groupIds\n   *\n   * @typedef {Object} GroupDescriptions\n   * @property {Array<GroupDescription>} groups\n   *\n   * @return {Promise<GroupDescriptions>}\n   */\n  const describeGroups = async groupIds => {\n    const coordinatorsForGroup = await Promise.all(\n      groupIds.map(async groupId => {\n        const coordinator = await cluster.findGroupCoordinator({ groupId })\n        return {\n          coordinator,\n          groupId,\n        }\n      })\n    )\n\n    const groupsByCoordinator = Object.values(\n      coordinatorsForGroup.reduce((coordinators, { coordinator, groupId }) => {\n        const group = coordinators[coordinator.nodeId]\n\n        if (group) {\n          coordinators[coordinator.nodeId] = {\n            ...group,\n            groupIds: [...group.groupIds, groupId],\n          }\n        } else {\n          coordinators[coordinator.nodeId] = { coordinator, groupIds: [groupId] }\n        }\n        return coordinators\n      }, {})\n    )\n\n    const responses = await Promise.all(\n      groupsByCoordinator.map(async ({ coordinator, groupIds }) => {\n        const retrier = createRetry(retry)\n        const { groups } = await retrier(() => coordinator.describeGroups({ groupIds }))\n        return groups\n      })\n    )\n\n    const groups = [].concat.apply([], responses)\n\n    return { groups }\n  }\n\n  /**\n   * Delete groups in a broker\n   *\n   * @param {string[]} [groupIds]\n   * @return {Promise<DeleteGroups>}\n   *\n   * @typedef {Array} DeleteGroups\n   * @property {string} groupId\n   * @property {number} errorCode\n   */\n  const deleteGroups = async groupIds => {\n    if (!groupIds || !Array.isArray(groupIds)) {\n      throw new KafkaJSNonRetriableError(`Invalid groupIds array ${groupIds}`)\n    }\n\n    const invalidGroupId = groupIds.some(g => typeof g !== 'string')\n\n    if (invalidGroupId) {\n      throw new KafkaJSNonRetriableError(`Invalid groupId name: ${JSON.stringify(invalidGroupId)}`)\n    }\n\n    const retrier = createRetry(retry)\n\n    let results = []\n\n    let clonedGroupIds = groupIds.slice()\n\n    return retrier(async (bail, retryCount, retryTime) => {\n      try {\n        if (clonedGroupIds.length === 0) return []\n\n        await cluster.refreshMetadata()\n\n        const brokersPerGroups = {}\n        const brokersPerNode = {}\n        for (const groupId of clonedGroupIds) {\n          const broker = await cluster.findGroupCoordinator({ groupId })\n          if (brokersPerGroups[broker.nodeId] === undefined) brokersPerGroups[broker.nodeId] = []\n          brokersPerGroups[broker.nodeId].push(groupId)\n          brokersPerNode[broker.nodeId] = broker\n        }\n\n        const res = await Promise.all(\n          Object.keys(brokersPerNode).map(\n            async nodeId => await brokersPerNode[nodeId].deleteGroups(brokersPerGroups[nodeId])\n          )\n        )\n\n        const errors = res\n          .flatMap(({ results }) =>\n            results.map(({ groupId, errorCode, error }) => {\n              return { groupId, errorCode, error }\n            })\n          )\n          .filter(({ errorCode }) => errorCode !== 0)\n\n        clonedGroupIds = errors.map(({ groupId }) => groupId)\n\n        if (errors.length > 0) throw new KafkaJSDeleteGroupsError('Error in DeleteGroups', errors)\n\n        results = res.flatMap(({ results }) => results)\n\n        return results\n      } catch (e) {\n        if (e.type === 'NOT_CONTROLLER' || e.type === 'COORDINATOR_NOT_AVAILABLE') {\n          logger.warn('Could not delete groups', { error: e.message, retryCount, retryTime })\n          throw e\n        }\n\n        bail(e)\n      }\n    })\n  }\n\n  /**\n   * Delete topic records up to the selected partition offsets\n   *\n   * @param {string} topic\n   * @param {Array<SeekEntry>} partitions\n   * @return {Promise}\n   *\n   * @typedef {Object} SeekEntry\n   * @property {number} partition\n   * @property {string} offset\n   */\n  const deleteTopicRecords = async ({ topic, partitions }) => {\n    if (!topic || typeof topic !== 'string') {\n      throw new KafkaJSNonRetriableError(`Invalid topic \"${topic}\"`)\n    }\n\n    if (!partitions || partitions.length === 0) {\n      throw new KafkaJSNonRetriableError(`Invalid partitions`)\n    }\n\n    const partitionsByBroker = cluster.findLeaderForPartitions(\n      topic,\n      partitions.map(p => p.partition)\n    )\n\n    const partitionsFound = values(partitionsByBroker).flat()\n    const topicOffsets = await fetchTopicOffsets(topic)\n\n    const leaderNotFoundErrors = []\n    partitions.forEach(({ partition, offset }) => {\n      // throw if no leader found for partition\n      if (!partitionsFound.includes(partition)) {\n        leaderNotFoundErrors.push({\n          partition,\n          offset,\n          error: new KafkaJSBrokerNotFound('Could not find the leader for the partition', {\n            retriable: false,\n          }),\n        })\n        return\n      }\n      const { low } = topicOffsets.find(p => p.partition === partition) || {\n        high: undefined,\n        low: undefined,\n      }\n      // warn in case of offset below low watermark\n      if (parseInt(offset) < parseInt(low) && parseInt(offset) !== -1) {\n        logger.warn(\n          'The requested offset is before the earliest offset maintained on the partition - no records will be deleted from this partition',\n          {\n            topic,\n            partition,\n            offset,\n          }\n        )\n      }\n    })\n\n    if (leaderNotFoundErrors.length > 0) {\n      throw new KafkaJSDeleteTopicRecordsError({ topic, partitions: leaderNotFoundErrors })\n    }\n\n    const seekEntriesByBroker = entries(partitionsByBroker).reduce(\n      (obj, [nodeId, nodePartitions]) => {\n        obj[nodeId] = {\n          topic,\n          partitions: partitions.filter(p => nodePartitions.includes(p.partition)),\n        }\n        return obj\n      },\n      {}\n    )\n\n    const retrier = createRetry(retry)\n    return retrier(async bail => {\n      try {\n        const partitionErrors = []\n\n        const brokerRequests = entries(seekEntriesByBroker).map(\n          ([nodeId, { topic, partitions }]) => async () => {\n            const broker = await cluster.findBroker({ nodeId })\n            await broker.deleteRecords({ topics: [{ topic, partitions }] })\n            // remove successful entry so it's ignored on retry\n            delete seekEntriesByBroker[nodeId]\n          }\n        )\n\n        await Promise.all(\n          brokerRequests.map(request =>\n            request().catch(e => {\n              if (e.name === 'KafkaJSDeleteTopicRecordsError') {\n                e.partitions.forEach(({ partition, offset, error }) => {\n                  partitionErrors.push({\n                    partition,\n                    offset,\n                    error,\n                  })\n                })\n              } else {\n                // then it's an unknown error, not from the broker response\n                throw e\n              }\n            })\n          )\n        )\n\n        if (partitionErrors.length > 0) {\n          throw new KafkaJSDeleteTopicRecordsError({\n            topic,\n            partitions: partitionErrors,\n          })\n        }\n      } catch (e) {\n        if (\n          e.retriable &&\n          e.partitions.some(\n            ({ error }) => staleMetadata(error) || error.name === 'KafkaJSMetadataNotLoaded'\n          )\n        ) {\n          await cluster.refreshMetadata()\n        }\n        throw e\n      }\n    })\n  }\n\n  /**\n   * @param {Array<ACLEntry>} acl\n   * @return {Promise<void>}\n   *\n   * @typedef {Object} ACLEntry\n   */\n  const createAcls = async ({ acl }) => {\n    if (!acl || !Array.isArray(acl)) {\n      throw new KafkaJSNonRetriableError(`Invalid ACL array ${acl}`)\n    }\n    if (acl.length === 0) {\n      throw new KafkaJSNonRetriableError('Empty ACL array')\n    }\n\n    // Validate principal\n    if (acl.some(({ principal }) => typeof principal !== 'string')) {\n      throw new KafkaJSNonRetriableError(\n        'Invalid ACL array, the principals have to be a valid string'\n      )\n    }\n\n    // Validate host\n    if (acl.some(({ host }) => typeof host !== 'string')) {\n      throw new KafkaJSNonRetriableError('Invalid ACL array, the hosts have to be a valid string')\n    }\n\n    // Validate resourceName\n    if (acl.some(({ resourceName }) => typeof resourceName !== 'string')) {\n      throw new KafkaJSNonRetriableError(\n        'Invalid ACL array, the resourceNames have to be a valid string'\n      )\n    }\n\n    let invalidType\n    // Validate operation\n    const validOperationTypes = Object.values(ACL_OPERATION_TYPES)\n    invalidType = acl.find(i => !validOperationTypes.includes(i.operation))\n\n    if (invalidType) {\n      throw new KafkaJSNonRetriableError(\n        `Invalid operation type ${invalidType.operation}: ${JSON.stringify(invalidType)}`\n      )\n    }\n\n    // Validate resourcePatternTypes\n    const validResourcePatternTypes = Object.values(RESOURCE_PATTERN_TYPES)\n    invalidType = acl.find(i => !validResourcePatternTypes.includes(i.resourcePatternType))\n\n    if (invalidType) {\n      throw new KafkaJSNonRetriableError(\n        `Invalid resource pattern type ${invalidType.resourcePatternType}: ${JSON.stringify(\n          invalidType\n        )}`\n      )\n    }\n\n    // Validate permissionTypes\n    const validPermissionTypes = Object.values(ACL_PERMISSION_TYPES)\n    invalidType = acl.find(i => !validPermissionTypes.includes(i.permissionType))\n\n    if (invalidType) {\n      throw new KafkaJSNonRetriableError(\n        `Invalid permission type ${invalidType.permissionType}: ${JSON.stringify(invalidType)}`\n      )\n    }\n\n    // Validate resourceTypes\n    const validResourceTypes = Object.values(ACL_RESOURCE_TYPES)\n    invalidType = acl.find(i => !validResourceTypes.includes(i.resourceType))\n\n    if (invalidType) {\n      throw new KafkaJSNonRetriableError(\n        `Invalid resource type ${invalidType.resourceType}: ${JSON.stringify(invalidType)}`\n      )\n    }\n\n    const retrier = createRetry(retry)\n\n    return retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await cluster.refreshMetadata()\n        const broker = await cluster.findControllerBroker()\n        await broker.createAcls({ acl })\n\n        return true\n      } catch (e) {\n        if (e.type === 'NOT_CONTROLLER') {\n          logger.warn('Could not create ACL', { error: e.message, retryCount, retryTime })\n          throw e\n        }\n\n        bail(e)\n      }\n    })\n  }\n\n  /**\n   * @param {ACLResourceTypes} resourceType The type of resource\n   * @param {string} resourceName The name of the resource\n   * @param {ACLResourcePatternTypes} resourcePatternType The resource pattern type filter\n   * @param {string} principal The principal name\n   * @param {string} host The hostname\n   * @param {ACLOperationTypes} operation The type of operation\n   * @param {ACLPermissionTypes} permissionType The type of permission\n   * @return {Promise<void>}\n   *\n   * @typedef {number} ACLResourceTypes\n   * @typedef {number} ACLResourcePatternTypes\n   * @typedef {number} ACLOperationTypes\n   * @typedef {number} ACLPermissionTypes\n   */\n  const describeAcls = async ({\n    resourceType,\n    resourceName,\n    resourcePatternType,\n    principal,\n    host,\n    operation,\n    permissionType,\n  }) => {\n    // Validate principal\n    if (typeof principal !== 'string' && typeof principal !== 'undefined') {\n      throw new KafkaJSNonRetriableError(\n        'Invalid principal, the principal have to be a valid string'\n      )\n    }\n\n    // Validate host\n    if (typeof host !== 'string' && typeof host !== 'undefined') {\n      throw new KafkaJSNonRetriableError('Invalid host, the host have to be a valid string')\n    }\n\n    // Validate resourceName\n    if (typeof resourceName !== 'string' && typeof resourceName !== 'undefined') {\n      throw new KafkaJSNonRetriableError(\n        'Invalid resourceName, the resourceName have to be a valid string'\n      )\n    }\n\n    // Validate operation\n    const validOperationTypes = Object.values(ACL_OPERATION_TYPES)\n    if (!validOperationTypes.includes(operation)) {\n      throw new KafkaJSNonRetriableError(`Invalid operation type ${operation}`)\n    }\n\n    // Validate resourcePatternType\n    const validResourcePatternTypes = Object.values(RESOURCE_PATTERN_TYPES)\n    if (!validResourcePatternTypes.includes(resourcePatternType)) {\n      throw new KafkaJSNonRetriableError(\n        `Invalid resource pattern filter type ${resourcePatternType}`\n      )\n    }\n\n    // Validate permissionType\n    const validPermissionTypes = Object.values(ACL_PERMISSION_TYPES)\n    if (!validPermissionTypes.includes(permissionType)) {\n      throw new KafkaJSNonRetriableError(`Invalid permission type ${permissionType}`)\n    }\n\n    // Validate resourceType\n    const validResourceTypes = Object.values(ACL_RESOURCE_TYPES)\n    if (!validResourceTypes.includes(resourceType)) {\n      throw new KafkaJSNonRetriableError(`Invalid resource type ${resourceType}`)\n    }\n\n    const retrier = createRetry(retry)\n\n    return retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await cluster.refreshMetadata()\n        const broker = await cluster.findControllerBroker()\n        const { resources } = await broker.describeAcls({\n          resourceType,\n          resourceName,\n          resourcePatternType,\n          principal,\n          host,\n          operation,\n          permissionType,\n        })\n        return { resources }\n      } catch (e) {\n        if (e.type === 'NOT_CONTROLLER') {\n          logger.warn('Could not describe ACL', { error: e.message, retryCount, retryTime })\n          throw e\n        }\n\n        bail(e)\n      }\n    })\n  }\n\n  /**\n   * @param {Array<ACLFilter>} filters\n   * @return {Promise<void>}\n   *\n   * @typedef {Object} ACLFilter\n   */\n  const deleteAcls = async ({ filters }) => {\n    if (!filters || !Array.isArray(filters)) {\n      throw new KafkaJSNonRetriableError(`Invalid ACL Filter array ${filters}`)\n    }\n\n    if (filters.length === 0) {\n      throw new KafkaJSNonRetriableError('Empty ACL Filter array')\n    }\n\n    // Validate principal\n    if (\n      filters.some(\n        ({ principal }) => typeof principal !== 'string' && typeof principal !== 'undefined'\n      )\n    ) {\n      throw new KafkaJSNonRetriableError(\n        'Invalid ACL Filter array, the principals have to be a valid string'\n      )\n    }\n\n    // Validate host\n    if (filters.some(({ host }) => typeof host !== 'string' && typeof host !== 'undefined')) {\n      throw new KafkaJSNonRetriableError(\n        'Invalid ACL Filter array, the hosts have to be a valid string'\n      )\n    }\n\n    // Validate resourceName\n    if (\n      filters.some(\n        ({ resourceName }) =>\n          typeof resourceName !== 'string' && typeof resourceName !== 'undefined'\n      )\n    ) {\n      throw new KafkaJSNonRetriableError(\n        'Invalid ACL Filter array, the resourceNames have to be a valid string'\n      )\n    }\n\n    let invalidType\n    // Validate operation\n    const validOperationTypes = Object.values(ACL_OPERATION_TYPES)\n    invalidType = filters.find(i => !validOperationTypes.includes(i.operation))\n\n    if (invalidType) {\n      throw new KafkaJSNonRetriableError(\n        `Invalid operation type ${invalidType.operation}: ${JSON.stringify(invalidType)}`\n      )\n    }\n\n    // Validate resourcePatternTypes\n    const validResourcePatternTypes = Object.values(RESOURCE_PATTERN_TYPES)\n    invalidType = filters.find(i => !validResourcePatternTypes.includes(i.resourcePatternType))\n\n    if (invalidType) {\n      throw new KafkaJSNonRetriableError(\n        `Invalid resource pattern type ${invalidType.resourcePatternType}: ${JSON.stringify(\n          invalidType\n        )}`\n      )\n    }\n\n    // Validate permissionTypes\n    const validPermissionTypes = Object.values(ACL_PERMISSION_TYPES)\n    invalidType = filters.find(i => !validPermissionTypes.includes(i.permissionType))\n\n    if (invalidType) {\n      throw new KafkaJSNonRetriableError(\n        `Invalid permission type ${invalidType.permissionType}: ${JSON.stringify(invalidType)}`\n      )\n    }\n\n    // Validate resourceTypes\n    const validResourceTypes = Object.values(ACL_RESOURCE_TYPES)\n    invalidType = filters.find(i => !validResourceTypes.includes(i.resourceType))\n\n    if (invalidType) {\n      throw new KafkaJSNonRetriableError(\n        `Invalid resource type ${invalidType.resourceType}: ${JSON.stringify(invalidType)}`\n      )\n    }\n\n    const retrier = createRetry(retry)\n\n    return retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await cluster.refreshMetadata()\n        const broker = await cluster.findControllerBroker()\n        const { filterResponses } = await broker.deleteAcls({ filters })\n        return { filterResponses }\n      } catch (e) {\n        if (e.type === 'NOT_CONTROLLER') {\n          logger.warn('Could not delete ACL', { error: e.message, retryCount, retryTime })\n          throw e\n        }\n\n        bail(e)\n      }\n    })\n  }\n\n  /**\n   * Alter the replicas partitions are assigned to for a topic\n   * @param {Object} request\n   * @param {import(\"../../types\").IPartitionReassignment[]} request.topics topics and the paritions to be reassigned\n   * @param {number} [request.timeout]\n   * @returns {Promise}\n   */\n  const alterPartitionReassignments = async ({ topics, timeout }) => {\n    if (!topics || !Array.isArray(topics)) {\n      throw new KafkaJSNonRetriableError(`Invalid topics array ${topics}`)\n    }\n\n    if (topics.filter(({ topic }) => typeof topic !== 'string').length > 0) {\n      throw new KafkaJSNonRetriableError(\n        'Invalid topics array, the topic names have to be a valid string'\n      )\n    }\n\n    const topicNames = new Set(topics.map(({ topic }) => topic))\n    if (topicNames.size < topics.length) {\n      throw new KafkaJSNonRetriableError(\n        'Invalid topics array, it cannot have multiple entries for the same topic'\n      )\n    }\n\n    for (const { topic, partitionAssignment } of topics) {\n      if (!partitionAssignment || !Array.isArray(partitionAssignment)) {\n        throw new KafkaJSNonRetriableError(\n          `Invalid partitions array: ${partitionAssignment} for topic: ${topic}`\n        )\n      }\n\n      for (const { partition, replicas } of partitionAssignment) {\n        if (\n          partition === null ||\n          partition === undefined ||\n          typeof partition !== 'number' ||\n          partition < 0\n        ) {\n          throw new KafkaJSNonRetriableError(\n            `Invalid partitions index: ${partition} for topic: ${topic}`\n          )\n        }\n\n        if (!replicas || !Array.isArray(replicas)) {\n          throw new KafkaJSNonRetriableError(\n            `Invalid replica assignment: ${replicas} for topic: ${topic} on partition: ${partition}`\n          )\n        }\n\n        if (replicas.filter(replica => typeof replica !== 'number' || replica < 0).length >= 1) {\n          throw new KafkaJSNonRetriableError(\n            `Invalid replica assignment: ${replicas} for topic: ${topic} on partition: ${partition}. Replicas must be a non negative number`\n          )\n        }\n      }\n    }\n\n    const retrier = createRetry(retry)\n\n    return retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await cluster.refreshMetadata()\n        const broker = await cluster.findControllerBroker()\n        await broker.alterPartitionReassignments({ topics, timeout })\n      } catch (e) {\n        if (e.type === 'NOT_CONTROLLER') {\n          logger.warn('Could not reassign partitions', { error: e.message, retryCount, retryTime })\n          throw e\n        }\n\n        bail(e)\n      }\n    })\n  }\n\n  /**\n   * List the partition reassignments in progress.\n   * If a partition is not going through a reassignment, its AddingReplicas and RemovingReplicas fields will simply be empty.\n   * If a partition doesn't exist, no response will be returned for it.\n   * @param {Object} request\n   * @param {import(\"../../types\").TopicPartitions[]} request.topics topics and the paritions to be returned, if this is null will return all the topics.\n   * @param {number} [request.timeout]\n   * @returns {Promise<import(\"../../types\").ListPartitionReassignmentsResponse>}\n   */\n  const listPartitionReassignments = async ({ topics = null, timeout }) => {\n    if (topics) {\n      if (!Array.isArray(topics)) {\n        throw new KafkaJSNonRetriableError(`Invalid topics array ${topics}`)\n      }\n\n      if (topics.filter(({ topic }) => typeof topic !== 'string').length > 0) {\n        throw new KafkaJSNonRetriableError(\n          'Invalid topics array, the topic names have to be a valid string'\n        )\n      }\n\n      const topicNames = new Set(topics.map(({ topic }) => topic))\n      if (topicNames.size < topics.length) {\n        throw new KafkaJSNonRetriableError(\n          'Invalid topics array, it cannot have multiple entries for the same topic'\n        )\n      }\n\n      for (const { topic, partitions } of topics) {\n        if (!partitions || !Array.isArray(partitions)) {\n          throw new KafkaJSNonRetriableError(\n            `Invalid partition array: ${partitions} for topic: ${topic}`\n          )\n        }\n\n        if (\n          partitions.filter(partition => typeof partition !== 'number' || partition < 0).length >= 1\n        ) {\n          throw new KafkaJSNonRetriableError(\n            `Invalid partition array: ${partitions} for topic: ${topic}. The partition indices have to be a valid number greater than 0.`\n          )\n        }\n      }\n    }\n\n    const retrier = createRetry(retry)\n\n    return retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await cluster.refreshMetadata()\n        const broker = await cluster.findControllerBroker()\n        const response = await broker.listPartitionReassignments({ topics, timeout })\n\n        return { topics: response.topics }\n      } catch (e) {\n        if (e.type === 'NOT_CONTROLLER') {\n          logger.warn('Could not reassign partitions', { error: e.message, retryCount, retryTime })\n          throw e\n        }\n\n        bail(e)\n      }\n    })\n  }\n\n  /** @type {import(\"../../types\").Admin[\"on\"]} */\n  const on = (eventName, listener) => {\n    if (!eventNames.includes(eventName)) {\n      throw new KafkaJSNonRetriableError(`Event name should be one of ${eventKeys}`)\n    }\n\n    return instrumentationEmitter.addListener(unwrapEvent(eventName), event => {\n      event.type = wrapEvent(event.type)\n      Promise.resolve(listener(event)).catch(e => {\n        logger.error(`Failed to execute listener: ${e.message}`, {\n          eventName,\n          stack: e.stack,\n        })\n      })\n    })\n  }\n\n  /**\n   * @return {Object} logger\n   */\n  const getLogger = () => logger\n\n  return {\n    connect,\n    disconnect,\n    listTopics,\n    createTopics,\n    deleteTopics,\n    createPartitions,\n    fetchTopicMetadata,\n    describeCluster,\n    events,\n    fetchOffsets,\n    fetchTopicOffsets,\n    fetchTopicOffsetsByTimestamp,\n    setOffsets,\n    resetOffsets,\n    describeConfigs,\n    alterConfigs,\n    on,\n    logger: getLogger,\n    listGroups,\n    describeGroups,\n    deleteGroups,\n    describeAcls,\n    deleteAcls,\n    createAcls,\n    deleteTopicRecords,\n    alterPartitionReassignments,\n    listPartitionReassignments,\n  }\n}\n", "const KEEP_ALIVE_DELAY = 60000 // in ms\n\n/**\n * @returns {import(\"../../types\").ISocketFactory}\n */\nmodule.exports = () => {\n  const net = require('net')\n  const tls = require('tls')\n\n  return ({ host, port, ssl, onConnect }) => {\n    const socket = ssl\n      ? tls.connect(\n          Object.assign({ host, port }, !net.isIP(host) ? { servername: host } : {}, ssl),\n          onConnect\n        )\n      : net.connect({ host, port }, onConnect)\n\n    socket.setKeepAlive(true, KEEP_ALIVE_DELAY)\n\n    return socket\n  }\n}\n", "module.exports = fn => {\n  let called = false\n\n  return (...args) => {\n    if (!called) {\n      called = true\n      return fn(...args)\n    }\n  }\n}\n", "const {\n  createLogger,\n  LEVELS: { INFO },\n} = require('./loggers')\n\nconst InstrumentationEventEmitter = require('./instrumentation/emitter')\nconst LoggerConsole = require('./loggers/console')\nconst Cluster = require('./cluster')\nconst createProducer = require('./producer')\nconst createConsumer = require('./consumer')\nconst createAdmin = require('./admin')\nconst ISOLATION_LEVEL = require('./protocol/isolationLevel')\nconst defaultSocketFactory = require('./network/socketFactory')\nconst once = require('./utils/once')\nconst websiteUrl = require('./utils/websiteUrl')\n\nconst PRIVATE = {\n  CREATE_CLUSTER: Symbol('private:Kafka:createCluster'),\n  CLUSTER_RETRY: Symbol('private:Kafka:clusterRetry'),\n  LOGGER: Symbol('private:Kafka:logger'),\n  OFFSETS: Symbol('private:Kafka:offsets'),\n}\n\nconst DEFAULT_METADATA_MAX_AGE = 300000\nconst warnOfDefaultPartitioner = once(logger => {\n  if (process.env.KAFKAJS_NO_PARTITIONER_WARNING == null) {\n    logger.warn(\n      `KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at ${websiteUrl(\n        'docs/migration-guide-v2.0.0',\n        'producer-new-default-partitioner'\n      )} for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\"`\n    )\n  }\n})\n\nmodule.exports = class Client {\n  /**\n   * @param {Object} options\n   * @param {Array<string>} options.brokers example: ['127.0.0.1:9092', '127.0.0.1:9094']\n   * @param {Object} options.ssl\n   * @param {Object} options.sasl\n   * @param {string} options.clientId\n   * @param {number} [options.connectionTimeout=1000] - in milliseconds\n   * @param {number} options.authenticationTimeout - in milliseconds\n   * @param {number} options.reauthenticationThreshold - in milliseconds\n   * @param {number} [options.requestTimeout=30000] - in milliseconds\n   * @param {boolean} [options.enforceRequestTimeout]\n   * @param {import(\"../types\").RetryOptions} [options.retry]\n   * @param {import(\"../types\").ISocketFactory} [options.socketFactory]\n   */\n  constructor({\n    brokers,\n    ssl,\n    sasl,\n    clientId,\n    connectionTimeout = 1000,\n    authenticationTimeout,\n    reauthenticationThreshold,\n    requestTimeout,\n    enforceRequestTimeout = true,\n    retry,\n    socketFactory = defaultSocketFactory(),\n    logLevel = INFO,\n    logCreator = LoggerConsole,\n  }) {\n    this[PRIVATE.OFFSETS] = new Map()\n    this[PRIVATE.LOGGER] = createLogger({ level: logLevel, logCreator })\n    this[PRIVATE.CLUSTER_RETRY] = retry\n    this[PRIVATE.CREATE_CLUSTER] = ({\n      metadataMaxAge,\n      allowAutoTopicCreation = true,\n      maxInFlightRequests = null,\n      instrumentationEmitter = null,\n      isolationLevel,\n    }) =>\n      new Cluster({\n        logger: this[PRIVATE.LOGGER],\n        retry: this[PRIVATE.CLUSTER_RETRY],\n        offsets: this[PRIVATE.OFFSETS],\n        socketFactory,\n        brokers,\n        ssl,\n        sasl,\n        clientId,\n        connectionTimeout,\n        authenticationTimeout,\n        reauthenticationThreshold,\n        requestTimeout,\n        enforceRequestTimeout,\n        metadataMaxAge,\n        instrumentationEmitter,\n        allowAutoTopicCreation,\n        maxInFlightRequests,\n        isolationLevel,\n      })\n  }\n\n  /**\n   * @public\n   */\n  producer({\n    createPartitioner,\n    retry,\n    metadataMaxAge = DEFAULT_METADATA_MAX_AGE,\n    allowAutoTopicCreation,\n    idempotent,\n    transactionalId,\n    transactionTimeout,\n    maxInFlightRequests,\n  } = {}) {\n    const instrumentationEmitter = new InstrumentationEventEmitter()\n    const cluster = this[PRIVATE.CREATE_CLUSTER]({\n      metadataMaxAge,\n      allowAutoTopicCreation,\n      maxInFlightRequests,\n      instrumentationEmitter,\n    })\n\n    if (createPartitioner == null) {\n      warnOfDefaultPartitioner(this[PRIVATE.LOGGER])\n    }\n\n    return createProducer({\n      retry: { ...this[PRIVATE.CLUSTER_RETRY], ...retry },\n      logger: this[PRIVATE.LOGGER],\n      cluster,\n      createPartitioner,\n      idempotent,\n      transactionalId,\n      transactionTimeout,\n      instrumentationEmitter,\n    })\n  }\n\n  /**\n   * @public\n   */\n  consumer({\n    groupId,\n    partitionAssigners,\n    metadataMaxAge = DEFAULT_METADATA_MAX_AGE,\n    sessionTimeout,\n    rebalanceTimeout,\n    heartbeatInterval,\n    maxBytesPerPartition,\n    minBytes,\n    maxBytes,\n    maxWaitTimeInMs,\n    retry = { retries: 5 },\n    allowAutoTopicCreation,\n    maxInFlightRequests,\n    readUncommitted = false,\n    rackId = '',\n  } = {}) {\n    const isolationLevel = readUncommitted\n      ? ISOLATION_LEVEL.READ_UNCOMMITTED\n      : ISOLATION_LEVEL.READ_COMMITTED\n\n    const instrumentationEmitter = new InstrumentationEventEmitter()\n    const cluster = this[PRIVATE.CREATE_CLUSTER]({\n      metadataMaxAge,\n      allowAutoTopicCreation,\n      maxInFlightRequests,\n      isolationLevel,\n      instrumentationEmitter,\n    })\n\n    return createConsumer({\n      retry: { ...this[PRIVATE.CLUSTER_RETRY], ...retry },\n      logger: this[PRIVATE.LOGGER],\n      cluster,\n      groupId,\n      partitionAssigners,\n      sessionTimeout,\n      rebalanceTimeout,\n      heartbeatInterval,\n      maxBytesPerPartition,\n      minBytes,\n      maxBytes,\n      maxWaitTimeInMs,\n      isolationLevel,\n      instrumentationEmitter,\n      rackId,\n      metadataMaxAge,\n    })\n  }\n\n  /**\n   * @public\n   */\n  admin({ retry } = {}) {\n    const instrumentationEmitter = new InstrumentationEventEmitter()\n    const cluster = this[PRIVATE.CREATE_CLUSTER]({\n      allowAutoTopicCreation: false,\n      instrumentationEmitter,\n    })\n\n    return createAdmin({\n      retry: { ...this[PRIVATE.CLUSTER_RETRY], ...retry },\n      logger: this[PRIVATE.LOGGER],\n      instrumentationEmitter,\n      cluster,\n    })\n  }\n\n  /**\n   * @public\n   */\n  logger() {\n    return this[PRIVATE.LOGGER]\n  }\n}\n", "const Kafka = require('./src')\nconst PartitionAssigners = require('./src/consumer/assigners')\nconst AssignerProtocol = require('./src/consumer/assignerProtocol')\nconst Partitioners = require('./src/producer/partitioners')\nconst Compression = require('./src/protocol/message/compression')\nconst ConfigResourceTypes = require('./src/protocol/configResourceTypes')\nconst ConfigSource = require('./src/protocol/configSource')\nconst AclResourceTypes = require('./src/protocol/aclResourceTypes')\nconst AclOperationTypes = require('./src/protocol/aclOperationTypes')\nconst AclPermissionTypes = require('./src/protocol/aclPermissionTypes')\nconst ResourcePatternTypes = require('./src/protocol/resourcePatternTypes')\nconst { isRebalancing, isKafkaJSError, ...errors } = require('./src/errors')\nconst { LEVELS } = require('./src/loggers')\n\nmodule.exports = {\n  Kafka,\n  PartitionAssigners,\n  AssignerProtocol,\n  Partitioners,\n  logLevel: LEVELS,\n  CompressionTypes: Compression.Types,\n  CompressionCodecs: Compression.Codecs,\n  ConfigResourceTypes,\n  AclResourceTypes,\n  AclOperationTypes,\n  AclPermissionTypes,\n  ResourcePatternTypes,\n  ConfigSource,\n  ...errors,\n}\n", "/* eslint-disable no-console */\nconst { ConfigResourceTypes, Kafka } = require(\"kafkajs\");\n\n/**\n * Removes topics in BigMac given the following\n * @param {*} brokerString - Comma delimited list of brokers\n * @param {*} namespace - String in the format of `--${event.project}--`, only used for temp branches for easy identification and cleanup\n */\nexports.listTopics = async function (brokerString, namespace) {\n  const brokers = brokerString.split(\",\");\n\n  const kafka = new Kafka({\n    clientId: \"admin\",\n    brokers: brokers,\n    ssl: true,\n  });\n  var admin = kafka.admin();\n\n  await admin.connect();\n\n  const currentTopics = await admin.listTopics();\n  var lingeringTopics = currentTopics.filter(\n    (topic) =>\n      topic.startsWith(namespace) ||\n      topic.startsWith(`_confluent-ksql-${namespace}`)\n  );\n\n  await admin.disconnect();\n  return lingeringTopics;\n};\n\n/**\n * Generates topics in BigMac given the following\n * @param { string[] } brokers - List of brokers\n * @param {{ topic: string, numPartitions: number, replicationFactor: number }[]}\n *   desiredTopicConfigs - array of topics to create or update.\n *   The `topic` property should include any namespace.\n */\nexports.createTopics = async function (brokers, desiredTopicConfigs) {\n  const kafka = new Kafka({\n    clientId: \"admin\",\n    brokers,\n    ssl: true,\n  });\n  var admin = kafka.admin();\n  await admin.connect();\n\n  // Fetch topic names from MSK, filtering out __ internal management topic\n  const listTopicResponse = await admin.listTopics();\n  const existingTopicNames = listTopicResponse.filter(\n    (name) => !name.startsWith(\"_\")\n  );\n\n  console.log(\"Existing topics:\", JSON.stringify(existingTopicNames, null, 2));\n\n  // Fetch the metadata for those topics from MSK\n  const fetchTopicResponse = await admin.fetchTopicMetadata({\n    topics: existingTopicNames,\n  });\n  const existingTopicConfigs = fetchTopicResponse.topics;\n  console.log(\n    \"Topics Metadata:\",\n    JSON.stringify(existingTopicConfigs, null, 2)\n  );\n\n  // Any desired topics whose names don't exist in MSK need to be created\n  const topicsToCreate = desiredTopicConfigs.filter(\n    (desired) => !existingTopicNames.includes(desired.topic)\n  );\n\n  /*\n   * Any topics which do exist, but with fewer partitions than desired,\n   * need to be updated. Partitions can't be removed, only added.\n   */\n  const topicsToUpdate = desiredTopicConfigs.filter((desired) =>\n    existingTopicConfigs.some(\n      (existing) =>\n        desired.topic === existing.name &&\n        desired.numPartitions > existing.partitions.length\n    )\n  );\n\n  // Format the request to update those topics (by creating partitions)\n  const partitionsToCreate = topicsToUpdate.map((topic) => ({\n    topic: topic.topic,\n    count: topic.numPartitions,\n  }));\n\n  // Describe existing topics for informational logs\n  let existingTopicDescriptions = [];\n  if (existingTopicConfigs.length > 0) {\n    const resourcesToDescribe = existingTopicConfigs.map((topic) => ({\n      name: topic.name,\n      type: ConfigResourceTypes.TOPIC,\n    }));\n    existingTopicDescriptions = await admin.describeConfigs({\n      resources: resourcesToDescribe,\n    });\n  }\n\n  console.log(\"Topics to Create:\", JSON.stringify(topicsToCreate, null, 2));\n  console.log(\"Topics to Update:\", JSON.stringify(topicsToUpdate, null, 2));\n  console.log(\n    \"Partitions to Create:\",\n    JSON.stringify(partitionsToCreate, null, 2)\n  );\n  console.log(\n    \"Topic configuration options:\",\n    JSON.stringify(existingTopicDescriptions, null, 2)\n  );\n\n  // Create all the new topics\n  await admin.createTopics({ topics: topicsToCreate });\n\n  // Create all the new partitions\n  if (partitionsToCreate.length > 0) {\n    await admin.createPartitions({ topicPartitions: partitionsToCreate });\n  }\n\n  await admin.disconnect();\n};\n\n/**\n * Deletes all topics for an ephemeral (`--` prefixed) namespace\n * @param { string[] } brokers - List of brokers\n * @param {string} topicNamespace\n */\nexports.deleteTopics = async function (brokers, topicNamespace) {\n  if (!topicNamespace.startsWith(\"--\")) {\n    throw \"ERROR:  The deleteTopics function only operates against topics that begin with --.\";\n  }\n\n  const kafka = new Kafka({\n    clientId: \"admin\",\n    brokers,\n    ssl: true,\n    requestTimeout: 295000, // 5s short of the lambda function's timeout\n  });\n  var admin = kafka.admin();\n\n  await admin.connect();\n\n  const existingTopicNames = await admin.listTopics();\n  console.log(`All existing topics: ${existingTopicNames}`);\n  var topicsToDelete = existingTopicNames.filter(\n    (name) =>\n      name.startsWith(topicNamespace) ||\n      name.startsWith(`_confluent-ksql-${topicNamespace}`)\n  );\n  console.log(`Deleting topics:  ${topicsToDelete}`);\n\n  await admin.deleteTopics({\n    topics: topicsToDelete,\n  });\n\n  await admin.disconnect();\n  return topicsToDelete;\n};\n", "const topics = require(\"../libs/topics-lib.js\");\n\n/**\n * String in the format of `--${event.project}--${event.stage}--`\n *\n * Only used for temp branches for easy identification and cleanup.\n */\nconst namespace = process.env.topicNamespace;\nconst brokers = process.env.brokerString?.split(\",\") ?? [];\n\nconst condensedTopicList = [\n  {\n    // topics for the mfp service's connector\n    topicPrefix: \"aws.mdct.mfp\",\n    version: \".v0\",\n    numPartitions: 1,\n    replicationFactor: 3,\n    topics: [\".wp-reports\", \".wp-form\", \".sar-reports\", \".sar-form\"],\n  },\n];\n\n/**\n * Handler triggered on deploy by the serverless js to create known topics in bigmac\n * @param {*} event\n * @param {*} _context\n * @param {*} _callback\n */\nexports.handler = async function (event, _context, _callback) {\n  console.log(\"Received event:\", JSON.stringify(event, null, 2)); // eslint-disable-line no-console\n\n  const desiredTopicConfigs = condensedTopicList.flatMap((element) =>\n    element.topics.map((topic) => ({\n      topic: `${namespace}${element.topicPrefix}${topic}${element.version}`,\n      numPartitions: element.numPartitions,\n      replicationFactor: element.replicationFactor,\n    }))\n  );\n\n  await topics.createTopics(brokers, desiredTopicConfigs);\n};\n"],
./.cdk/cdk.out/asset.418c8bdceca7ec106b8a86070f63155d5f932e977d0ea96d09296574d17509f8/index.js.map:4: TODO: validate topics not null/empty\n    const fetch = this.lookupRequest(apiKeys.Fetch, requests.Fetch)\n\n    // Shuffle topic-partitions to ensure fair response allocation across partitions (KIP-74)\n    const flattenedTopicPartitions = topics.reduce((topicPartitions, { topic, partitions }) => {\n      partitions.forEach(partition => {\n        topicPartitions.push({ topic, partition })\n      })\n      return topicPartitions\n    }, [])\n\n    const shuffledTopicPartitions = shuffle(flattenedTopicPartitions)\n\n    // Consecutive partitions for the same topic can be combined into a single `topic` entry\n    const consolidatedTopicPartitions = shuffledTopicPartitions.reduce(\n      (topicPartitions, { topic, partition }) => {\n        const last = topicPartitions[topicPartitions.length - 1]\n\n        if (last != null && last.topic === topic) {\n          topicPartitions[topicPartitions.length - 1].partitions.push(partition)\n        } else {\n          topicPartitions.push({ topic, partitions: [partition] })\n        }\n\n        return topicPartitions\n      },\n      []\n    )\n\n    return await this[PRIVATE.SEND_REQUEST](\n      fetch({\n        replicaId,\n        isolationLevel,\n        maxWaitTime,\n        minBytes,\n        maxBytes,\n        topics: consolidatedTopicPartitions,\n        rackId,\n      })\n    )\n  }\n\n  /**\n   * @public\n   * @param {object} request\n   * @param {string} request.groupId The group id\n   * @param {number} request.groupGenerationId The generation of the group\n   * @param {string} request.memberId The member id assigned by the group coordinator\n   * @returns {Promise}\n   */\n  async heartbeat({ groupId, groupGenerationId, memberId }) {\n    const heartbeat = this.lookupRequest(apiKeys.Heartbeat, requests.Heartbeat)\n    return await this[PRIVATE.SEND_REQUEST](heartbeat({ groupId, groupGenerationId, memberId }))\n  }\n\n  /**\n   * @public\n   * @param {object} request\n   * @param {string} request.groupId The unique group id\n   * @param {import(\"../protocol/coordinatorTypes\").CoordinatorType} request.coordinatorType The type of coordinator to find\n   * @returns {Promise}\n   */\n  async findGroupCoordinator({ groupId, coordinatorType }) {\n    // TODO: validate groupId, mandatory\n    const findCoordinator = this.lookupRequest(apiKeys.GroupCoordinator, requests.GroupCoordinator)\n    return await this[PRIVATE.SEND_REQUEST](findCoordinator({ groupId, coordinatorType }))\n  }\n\n  /**\n   * @public\n   * @param {object} request\n   * @param {string} request.groupId The unique group id\n   * @param {number} request.sessionTimeout The coordinator considers the consumer dead if it receives\n   *                                no heartbeat after this timeout in ms\n   * @param {number} request.rebalanceTimeout The maximum time that the coordinator will wait for each member\n   *                                  to rejoin when rebalancing the group\n   * @param {string} [request.memberId=\"\"] The assigned consumer id or an empty string for a new consumer\n   * @param {string} [request.protocolType=\"consumer\"] Unique name for class of protocols implemented by group\n   * @param {Array} request.groupProtocols List of protocols that the member supports (assignment strategy)\n   *                                [{ name: 'AssignerName', metadata: '{\"version\": 1, \"topics\": []}' }]\n   * @returns {Promise}\n   */\n  async joinGroup({\n    groupId,\n    sessionTimeout,\n    rebalanceTimeout,\n    memberId = '',\n    protocolType = 'consumer',\n    groupProtocols,\n  }) {\n    const joinGroup = this.lookupRequest(apiKeys.JoinGroup, requests.JoinGroup)\n    const makeRequest = (assignedMemberId = memberId) =>\n      this[PRIVATE.SEND_REQUEST](\n        joinGroup({\n          groupId,\n          sessionTimeout,\n          rebalanceTimeout,\n          memberId: assignedMemberId,\n          protocolType,\n          groupProtocols,\n        })\n      )\n\n    try {\n      return await makeRequest()\n    } catch (error) {\n      if (error.name === 'KafkaJSMemberIdRequired') {\n        return makeRequest(error.memberId)\n      }\n\n      throw error\n    }\n  }\n\n  /**\n   * @public\n   * @param {object} request\n   * @param {string} request.groupId\n   * @param {string} request.memberId\n   * @returns {Promise}\n   */\n  async leaveGroup({ groupId, memberId }) {\n    const leaveGroup = this.lookupRequest(apiKeys.LeaveGroup, requests.LeaveGroup)\n    return await this[PRIVATE.SEND_REQUEST](leaveGroup({ groupId, memberId }))\n  }\n\n  /**\n   * @public\n   * @param {object} request\n   * @param {string} request.groupId\n   * @param {number} request.generationId\n   * @param {string} request.memberId\n   * @param {object} request.groupAssignment\n   * @returns {Promise}\n   */\n  async syncGroup({ groupId, generationId, memberId, groupAssignment }) {\n    const syncGroup = this.lookupRequest(apiKeys.SyncGroup, requests.SyncGroup)\n    return await this[PRIVATE.SEND_REQUEST](\n      syncGroup({\n        groupId,\n        generationId,\n        memberId,\n        groupAssignment,\n      })\n    )\n  }\n\n  /**\n   * @public\n   * @param {object} request\n   * @param {number} request.replicaId=-1 Broker id of the follower. For normal consumers, use -1\n   * @param {number} request.isolationLevel=1 This setting controls the visibility of transactional records (default READ_COMMITTED, Kafka >0.11 only)\n   * @param {TopicPartitionOffset[]} request.topics e.g:\n   *\n   * @typedef {Object} TopicPartitionOffset\n   * @property {string} topic\n   * @property {PartitionOffset[]} partitions\n   *\n   * @typedef {Object} PartitionOffset\n   * @property {number} partition\n   * @property {number} [timestamp=-1]\n   *\n   *\n   * @returns {Promise}\n   */\n  async listOffsets({ replicaId, isolationLevel, topics }) {\n    const listOffsets = this.lookupRequest(apiKeys.ListOffsets, requests.ListOffsets)\n    const result = await this[PRIVATE.SEND_REQUEST](\n      listOffsets({ replicaId, isolationLevel, topics })\n    )\n\n    // ListOffsets >= v1 will return a single `offset` rather than an array of `offsets` (ListOffsets V0).\n    // Normalize to just return `offset`.\n    for (const response of result.responses) {\n      response.partitions = response.partitions.map(({ offsets, ...partitionData }) => {\n        return offsets ? { ...partitionData, offset: offsets.pop() } : partitionData\n      })\n    }\n\n    return result\n  }\n\n  /**\n   * @public\n   * @param {object} request\n   * @param {string} request.groupId\n   * @param {number} request.groupGenerationId\n   * @param {string} request.memberId\n   * @param {number} [request.retentionTime=-1] -1 signals to the broker that its default configuration\n   *                                    should be used.\n   * @param {object} request.topics Topics to commit offsets, e.g:\n   *                  [\n   *                    {\n   *                      topic: 'topic-name',\n   *                      partitions: [\n   *                        { partition: 0, offset: '11' }\n   *                      ]\n   *                    }\n   *                  ]\n   * @returns {Promise}\n   */\n  async offsetCommit({ groupId, groupGenerationId, memberId, retentionTime, topics }) {\n    const offsetCommit = this.lookupRequest(apiKeys.OffsetCommit, requests.OffsetCommit)\n    return await this[PRIVATE.SEND_REQUEST](\n      offsetCommit({\n        groupId,\n        groupGenerationId,\n        memberId,\n        retentionTime,\n        topics,\n      })\n    )\n  }\n\n  /**\n   * @public\n   * @param {object} request\n   * @param {string} request.groupId\n   * @param {object} request.topics - If the topic array is null fetch offsets for all topics. e.g:\n   *                  [\n   *                    {\n   *                      topic: 'topic-name',\n   *                      partitions: [\n   *                        { partition: 0 }\n   *                      ]\n   *                    }\n   *                  ]\n   * @returns {Promise}\n   */\n  async offsetFetch({ groupId, topics }) {\n    const offsetFetch = this.lookupRequest(apiKeys.OffsetFetch, requests.OffsetFetch)\n    return await this[PRIVATE.SEND_REQUEST](offsetFetch({ groupId, topics }))\n  }\n\n  /**\n   * @public\n   * @param {object} request\n   * @param {Array} request.groupIds\n   * @returns {Promise}\n   */\n  async describeGroups({ groupIds }) {\n    const describeGroups = this.lookupRequest(apiKeys.DescribeGroups, requests.DescribeGroups)\n    return await this[PRIVATE.SEND_REQUEST](describeGroups({ groupIds }))\n  }\n\n  /**\n   * @public\n   * @param {object} request\n   * @param {Array} request.topics e.g:\n   *                 [\n   *                   {\n   *                     topic: 'topic-name',\n   *                     numPartitions: 1,\n   *                     replicationFactor: 1\n   *                   }\n   *                 ]\n   * @param {boolean} [request.validateOnly=false] If this is true, the request will be validated, but the topic\n   *                                       won't be created\n   * @param {number} [request.timeout=5000] The time in ms to wait for a topic to be completely created\n   *                                on the controller node\n   * @returns {Promise}\n   */\n  async createTopics({ topics, validateOnly = false, timeout = 5000 }) {\n    const createTopics = this.lookupRequest(apiKeys.CreateTopics, requests.CreateTopics)\n    return await this[PRIVATE.SEND_REQUEST](createTopics({ topics, validateOnly, timeout }))\n  }\n\n  /**\n   * @public\n   * @param {object} request\n   * @param {Array} request.topicPartitions e.g:\n   *                 [\n   *                   {\n   *                     topic: 'topic-name',\n   *                     count: 3,\n   *                     assignments: []\n   *                   }\n   *                 ]\n   * @param {boolean} [request.validateOnly=false] If this is true, the request will be validated, but the topic\n   *                                       won't be created\n   * @param {number} [request.timeout=5000] The time in ms to wait for a topic to be completely created\n   *                                on the controller node\n   * @returns {Promise<void>}\n   */\n  async createPartitions({ topicPartitions, validateOnly = false, timeout = 5000 }) {\n    const createPartitions = this.lookupRequest(apiKeys.CreatePartitions, requests.CreatePartitions)\n    return await this[PRIVATE.SEND_REQUEST](\n      createPartitions({ topicPartitions, validateOnly, timeout })\n    )\n  }\n\n  /**\n   * @public\n   * @param {object} request\n   * @param {string[]} request.topics An array of topics to be deleted\n   * @param {number} [request.timeout=5000] The time in ms to wait for a topic to be completely deleted on the\n   *                                controller node.\n   * @returns {Promise}\n   */\n  async deleteTopics({ topics, timeout = 5000 }) {\n    const deleteTopics = this.lookupRequest(apiKeys.DeleteTopics, requests.DeleteTopics)\n    return await this[PRIVATE.SEND_REQUEST](deleteTopics({ topics, timeout }))\n  }\n\n  /**\n   * @public\n   * @param {object} request\n   * @param {import(\"../../types\").ResourceConfigQuery[]} request.resources\n   *                                 [{\n   *                                   type: RESOURCE_TYPES.TOPIC,\n   *                                   name: 'topic-name',\n   *                                   configNames: ['compression.type', 'retention.ms']\n   *                                 }]\n   * @param {boolean} [request.includeSynonyms=false]\n   * @returns {Promise}\n   */\n  async describeConfigs({ resources, includeSynonyms = false }) {\n    const describeConfigs = this.lookupRequest(apiKeys.DescribeConfigs, requests.DescribeConfigs)\n    return await this[PRIVATE.SEND_REQUEST](describeConfigs({ resources, includeSynonyms }))\n  }\n\n  /**\n   * @public\n   * @param {object} request\n   * @param {import(\"../../types\").IResourceConfig[]} request.resources\n   *                                 [{\n   *                                  type: RESOURCE_TYPES.TOPIC,\n   *                                  name: 'topic-name',\n   *                                  configEntries: [\n   *                                    {\n   *                                      name: 'cleanup.policy',\n   *                                      value: 'compact'\n   *                                    }\n   *                                  ]\n   *                                 }]\n   * @param {boolean} [request.validateOnly=false]\n   * @returns {Promise}\n   */\n  async alterConfigs({ resources, validateOnly = false }) {\n    const alterConfigs = this.lookupRequest(apiKeys.AlterConfigs, requests.AlterConfigs)\n    return await this[PRIVATE.SEND_REQUEST](alterConfigs({ resources, validateOnly }))\n  }\n\n  /**\n   * Send an `InitProducerId` request to fetch a PID and bump the producer epoch.\n   *\n   * Request should be made to the transaction coordinator.\n   * @public\n   * @param {object} request\n   * @param {number} request.transactionTimeout The time in ms to wait for before aborting idle transactions\n   * @param {number} [request.transactionalId] The transactional id or null if the producer is not transactional\n   * @returns {Promise}\n   */\n  async initProducerId({ transactionalId, transactionTimeout }) {\n    const initProducerId = this.lookupRequest(apiKeys.InitProducerId, requests.InitProducerId)\n    return await this[PRIVATE.SEND_REQUEST](initProducerId({ transactionalId, transactionTimeout }))\n  }\n\n  /**\n   * Send an `AddPartitionsToTxn` request to mark a TopicPartition as participating in the transaction.\n   *\n   * Request should be made to the transaction coordinator.\n   * @public\n   * @param {object} request\n   * @param {string} request.transactionalId The transactional id corresponding to the transaction.\n   * @param {number} request.producerId Current producer id in use by the transactional id.\n   * @param {number} request.producerEpoch Current epoch associated with the producer id.\n   * @param {object[]} request.topics e.g:\n   *                  [\n   *                    {\n   *                      topic: 'topic-name',\n   *                      partitions: [ 0, 1]\n   *                    }\n   *                  ]\n   * @returns {Promise}\n   */\n  async addPartitionsToTxn({ transactionalId, producerId, producerEpoch, topics }) {\n    const addPartitionsToTxn = this.lookupRequest(\n      apiKeys.AddPartitionsToTxn,\n      requests.AddPartitionsToTxn\n    )\n    return await this[PRIVATE.SEND_REQUEST](\n      addPartitionsToTxn({ transactionalId, producerId, producerEpoch, topics })\n    )\n  }\n\n  /**\n   * Send an `AddOffsetsToTxn` request.\n   *\n   * Request should be made to the transaction coordinator.\n   * @public\n   * @param {object} request\n   * @param {string} request.transactionalId The transactional id corresponding to the transaction.\n   * @param {number} request.producerId Current producer id in use by the transactional id.\n   * @param {number} request.producerEpoch Current epoch associated with the producer id.\n   * @param {string} request.groupId The unique group identifier (for the consumer group)\n   * @returns {Promise}\n   */\n  async addOffsetsToTxn({ transactionalId, producerId, producerEpoch, groupId }) {\n    const addOffsetsToTxn = this.lookupRequest(apiKeys.AddOffsetsToTxn, requests.AddOffsetsToTxn)\n    return await this[PRIVATE.SEND_REQUEST](\n      addOffsetsToTxn({ transactionalId, producerId, producerEpoch, groupId })\n    )\n  }\n\n  /**\n   * Send a `TxnOffsetCommit` request to persist the offsets in the `__consumer_offsets` topics.\n   *\n   * Request should be made to the consumer coordinator.\n   * @public\n   * @param {object} request\n   * @param {OffsetCommitTopic[]} request.topics\n   * @param {string} request.transactionalId The transactional id corresponding to the transaction.\n   * @param {string} request.groupId The unique group identifier (for the consumer group)\n   * @param {number} request.producerId Current producer id in use by the transactional id.\n   * @param {number} request.producerEpoch Current epoch associated with the producer id.\n   * @param {OffsetCommitTopic[]} request.topics\n   *\n   * @typedef {Object} OffsetCommitTopic\n   * @property {string} topic\n   * @property {OffsetCommitTopicPartition[]} partitions\n   *\n   * @typedef {Object} OffsetCommitTopicPartition\n   * @property {number} partition\n   * @property {number} offset\n   * @property {string} [metadata]\n   *\n   * @returns {Promise}\n   */\n  async txnOffsetCommit({ transactionalId, groupId, producerId, producerEpoch, topics }) {\n    const txnOffsetCommit = this.lookupRequest(apiKeys.TxnOffsetCommit, requests.TxnOffsetCommit)\n    return await this[PRIVATE.SEND_REQUEST](\n      txnOffsetCommit({ transactionalId, groupId, producerId, producerEpoch, topics })\n    )\n  }\n\n  /**\n   * Send an `EndTxn` request to indicate transaction should be committed or aborted.\n   *\n   * Request should be made to the transaction coordinator.\n   * @public\n   * @param {object} request\n   * @param {string} request.transactionalId The transactional id corresponding to the transaction.\n   * @param {number} request.producerId Current producer id in use by the transactional id.\n   * @param {number} request.producerEpoch Current epoch associated with the producer id.\n   * @param {boolean} request.transactionResult The result of the transaction (false = ABORT, true = COMMIT)\n   * @returns {Promise}\n   */\n  async endTxn({ transactionalId, producerId, producerEpoch, transactionResult }) {\n    const endTxn = this.lookupRequest(apiKeys.EndTxn, requests.EndTxn)\n    return await this[PRIVATE.SEND_REQUEST](\n      endTxn({ transactionalId, producerId, producerEpoch, transactionResult })\n    )\n  }\n\n  /**\n   * Send request for list of groups\n   * @public\n   * @returns {Promise}\n   */\n  async listGroups() {\n    const listGroups = this.lookupRequest(apiKeys.ListGroups, requests.ListGroups)\n    return await this[PRIVATE.SEND_REQUEST](listGroups())\n  }\n\n  /**\n   * Send request to delete groups\n   * @param {string[]} groupIds\n   * @public\n   * @returns {Promise}\n   */\n  async deleteGroups(groupIds) {\n    const deleteGroups = this.lookupRequest(apiKeys.DeleteGroups, requests.DeleteGroups)\n    return await this[PRIVATE.SEND_REQUEST](deleteGroups(groupIds))\n  }\n\n  /**\n   * Send request to delete records\n   * @public\n   * @param {object} request\n   * @param {TopicPartitionRecords[]} request.topics\n   *                          [\n   *                            {\n   *                              topic: 'my-topic-name',\n   *                              partitions: [\n   *                                { partition: 0, offset 2 },\n   *                                { partition: 1, offset 4 },\n   *                              ],\n   *                            }\n   *                          ]\n   * @returns {Promise<Array>} example:\n   *                          {\n   *                            throttleTime: 0\n   *                           [\n   *                              {\n   *                                topic: 'my-topic-name',\n   *                                partitions: [\n   *                                 { partition: 0, lowWatermark: '2n', errorCode: 0 },\n   *                                 { partition: 1, lowWatermark: '4n', errorCode: 0 },\n   *                               ],\n   *                             },\n   *                           ]\n   *                          }\n   *\n   * @typedef {object} TopicPartitionRecords\n   * @property {string} topic\n   * @property {PartitionRecord[]} partitions\n   *\n   * @typedef {object} PartitionRecord\n   * @property {number} partition\n   * @property {number} offset\n   */\n  async deleteRecords({ topics }) {\n    const deleteRecords = this.lookupRequest(apiKeys.DeleteRecords, requests.DeleteRecords)\n    return await this[PRIVATE.SEND_REQUEST](deleteRecords({ topics }))\n  }\n\n  /**\n   * @public\n   * @param {object} request\n   * @param {import(\"../../types\").AclEntry[]} request.acl e.g:\n   *                 [\n   *                   {\n   *                     resourceType: AclResourceTypes.TOPIC,\n   *                     resourceName: 'topic-name',\n   *                     resourcePatternType: ResourcePatternTypes.LITERAL,\n   *                     principal: 'User:bob',\n   *                     host: '*',\n   *                     operation: AclOperationTypes.ALL,\n   *                     permissionType: AclPermissionTypes.DENY,\n   *                   }\n   *                 ]\n   * @returns {Promise<void>}\n   */\n  async createAcls({ acl }) {\n    const createAcls = this.lookupRequest(apiKeys.CreateAcls, requests.CreateAcls)\n    return await this[PRIVATE.SEND_REQUEST](createAcls({ creations: acl }))\n  }\n\n  /**\n   * @public\n   * @param {import(\"../../types\").AclEntry} aclEntry\n   * @returns {Promise<void>}\n   */\n  async describeAcls({\n    resourceType,\n    resourceName,\n    resourcePatternType,\n    principal,\n    host,\n    operation,\n    permissionType,\n  }) {\n    const describeAcls = this.lookupRequest(apiKeys.DescribeAcls, requests.DescribeAcls)\n    return await this[PRIVATE.SEND_REQUEST](\n      describeAcls({\n        resourceType,\n        resourceName,\n        resourcePatternType,\n        principal,\n        host,\n        operation,\n        permissionType,\n      })\n    )\n  }\n\n  /**\n   * @public\n   * @param {Object} request\n   * @param {import(\"../../types\").AclEntry[]} request.filters\n   * @returns {Promise<void>}\n   */\n  async deleteAcls({ filters }) {\n    const deleteAcls = this.lookupRequest(apiKeys.DeleteAcls, requests.DeleteAcls)\n    return await this[PRIVATE.SEND_REQUEST](deleteAcls({ filters }))\n  }\n\n  /**\n   * @public\n   * @param {Object} request\n   * @param {import(\"../../types\").PartitionReassignment[]} request.topics\n   * @param {number} [request.timeout]\n   * @returns {Promise}\n   */\n  async alterPartitionReassignments({ topics, timeout }) {\n    const alterPartitionReassignments = this.lookupRequest(\n      apiKeys.AlterPartitionReassignments,\n      requests.AlterPartitionReassignments\n    )\n    return await this[PRIVATE.SEND_REQUEST](alterPartitionReassignments({ topics, timeout }))\n  }\n\n  /**\n   * @public\n   * @param {Object} request\n   * @param {import(\"../../types\").TopicPartitions[]} request.topics can be null\n   * @param {number} [request.timeout]\n   * @returns {Promise}\n   */\n  async listPartitionReassignments({ topics = null, timeout }) {\n    const listPartitionReassignments = this.lookupRequest(\n      apiKeys.ListPartitionReassignments,\n      requests.ListPartitionReassignments\n    )\n    return await this[PRIVATE.SEND_REQUEST](listPartitionReassignments({ topics, timeout }))\n  }\n\n  /**\n   * @private\n   */\n  async [PRIVATE.SEND_REQUEST](protocolRequest) {\n    try {\n      return await this.connectionPool.send(protocolRequest)\n    } catch (e) {\n      if (e.name === 'KafkaJSConnectionClosedError') {\n        await this.disconnect()\n      }\n\n      throw e\n    }\n  }\n}\n", "module.exports = {\n  maxRetryTime: 1000,\n  initialRetryTime: 50,\n  factor: 0.02, // randomization factor\n  multiplier: 1.5, // exponential factor\n  retries: 15, // max retries\n}\n", "module.exports = {\n  maxRetryTime: 30 * 1000,\n  initialRetryTime: 300,\n  factor: 0.2, // randomization factor\n  multiplier: 2, // exponential factor\n  retries: 5, // max retries\n}\n", "const { KafkaJSNumberOfRetriesExceeded, KafkaJSNonRetriableError } = require('../errors')\n\nconst isTestMode = process.env.NODE_ENV === 'test'\nconst RETRY_DEFAULT = isTestMode ? require('./defaults.test') : require('./defaults')\n\nconst random = (min, max) => {\n  return Math.random() * (max - min) + min\n}\n\nconst randomFromRetryTime = (factor, retryTime) => {\n  const delta = factor * retryTime\n  return Math.ceil(random(retryTime - delta, retryTime + delta))\n}\n\nconst UNRECOVERABLE_ERRORS = ['RangeError', 'ReferenceError', 'SyntaxError', 'TypeError']\nconst isErrorUnrecoverable = e => UNRECOVERABLE_ERRORS.includes(e.name)\nconst isErrorRetriable = error =>\n  (error.retriable || error.retriable !== false) && !isErrorUnrecoverable(error)\n\nconst createRetriable = (configs, resolve, reject, fn) => {\n  let aborted = false\n  const { factor, multiplier, maxRetryTime, retries } = configs\n\n  const bail = error => {\n    aborted = true\n    reject(error || new Error('Aborted'))\n  }\n\n  const calculateExponentialRetryTime = retryTime => {\n    return Math.min(randomFromRetryTime(factor, retryTime) * multiplier, maxRetryTime)\n  }\n\n  const retry = (retryTime, retryCount = 0) => {\n    if (aborted) return\n\n    const nextRetryTime = calculateExponentialRetryTime(retryTime)\n    const shouldRetry = retryCount < retries\n\n    const scheduleRetry = () => {\n      setTimeout(() => retry(nextRetryTime, retryCount + 1), retryTime)\n    }\n\n    fn(bail, retryCount, retryTime)\n      .then(resolve)\n      .catch(e => {\n        if (isErrorRetriable(e)) {\n          if (shouldRetry) {\n            scheduleRetry()\n          } else {\n            reject(\n              new KafkaJSNumberOfRetriesExceeded(e, { retryCount, retryTime, cause: e.cause || e })\n            )\n          }\n        } else {\n          reject(new KafkaJSNonRetriableError(e, { cause: e.cause || e }))\n        }\n      })\n  }\n\n  return retry\n}\n\n/**\n * @typedef {(fn: (bail: (err: Error) => void, retryCount: number, retryTime: number) => any) => Promise<ReturnType<fn>>} Retrier\n */\n\n/**\n * @param {import(\"../../types\").RetryOptions} [opts]\n * @returns {Retrier}\n */\nmodule.exports = (opts = {}) => fn => {\n  return new Promise((resolve, reject) => {\n    const configs = Object.assign({}, RETRY_DEFAULT, opts)\n    const start = createRetriable(configs, resolve, reject, fn)\n    start(randomFromRetryTime(configs.factor, configs.initialRetryTime))\n  })\n}\n", "module.exports = (a, b) => {\n  const result = []\n  const length = a.length\n  let i = 0\n\n  while (i < length) {\n    if (b.indexOf(a[i]) === -1) {\n      result.push(a[i])\n    }\n    i += 1\n  }\n\n  return result\n}\n", "const Broker = require('../broker')\nconst createRetry = require('../retry')\nconst shuffle = require('../utils/shuffle')\nconst arrayDiff = require('../utils/arrayDiff')\nconst { KafkaJSBrokerNotFound, KafkaJSProtocolError } = require('../errors')\n\nconst { keys, assign, values } = Object\nconst hasBrokerBeenReplaced = (broker, { host, port, rack }) =>\n  broker.connectionPool.host !== host ||\n  broker.connectionPool.port !== port ||\n  broker.connectionPool.rack !== rack\n\nmodule.exports = class BrokerPool {\n  /**\n   * @param {object} options\n   * @param {import(\"./connectionPoolBuilder\").ConnectionPoolBuilder} options.connectionPoolBuilder\n   * @param {import(\"../../types\").Logger} options.logger\n   * @param {import(\"../../types\").RetryOptions} [options.retry]\n   * @param {boolean} [options.allowAutoTopicCreation]\n   * @param {number} [options.authenticationTimeout]\n   * @param {number} [options.metadataMaxAge]\n   */\n  constructor({\n    connectionPoolBuilder,\n    logger,\n    retry,\n    allowAutoTopicCreation,\n    authenticationTimeout,\n    metadataMaxAge,\n  }) {\n    this.rootLogger = logger\n    this.connectionPoolBuilder = connectionPoolBuilder\n    this.metadataMaxAge = metadataMaxAge || 0\n    this.logger = logger.namespace('BrokerPool')\n    this.retrier = createRetry(assign({}, retry))\n\n    this.createBroker = options =>\n      new Broker({\n        allowAutoTopicCreation,\n        authenticationTimeout,\n        ...options,\n      })\n\n    this.brokers = {}\n    /** @type {Broker | undefined} */\n    this.seedBroker = undefined\n    /** @type {import(\"../../types\").BrokerMetadata | null} */\n    this.metadata = null\n    this.metadataExpireAt = null\n    this.versions = null\n  }\n\n  /**\n   * @public\n   * @returns {Boolean}\n   */\n  hasConnectedBrokers() {\n    const brokers = values(this.brokers)\n    return (\n      !!brokers.find(broker => broker.isConnected()) ||\n      (this.seedBroker ? this.seedBroker.isConnected() : false)\n    )\n  }\n\n  async createSeedBroker() {\n    if (this.seedBroker) {\n      await this.seedBroker.disconnect()\n    }\n\n    const connectionPool = await this.connectionPoolBuilder.build()\n\n    this.seedBroker = this.createBroker({\n      connectionPool,\n      logger: this.rootLogger,\n    })\n  }\n\n  /**\n   * @public\n   * @returns {Promise<void>}\n   */\n  async connect() {\n    if (this.hasConnectedBrokers()) {\n      return\n    }\n\n    if (!this.seedBroker) {\n      await this.createSeedBroker()\n    }\n\n    return this.retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await this.seedBroker.connect()\n        this.versions = this.seedBroker.versions\n      } catch (e) {\n        if (e.name === 'KafkaJSConnectionError' || e.type === 'ILLEGAL_SASL_STATE') {\n          // Connection builder will always rotate the seed broker\n          await this.createSeedBroker()\n          this.logger.error(\n            `Failed to connect to seed broker, trying another broker from the list: ${e.message}`,\n            { retryCount, retryTime }\n          )\n        } else {\n          this.logger.error(e.message, { retryCount, retryTime })\n        }\n\n        if (e.retriable) throw e\n        bail(e)\n      }\n    })\n  }\n\n  /**\n   * @public\n   * @returns {Promise}\n   */\n  async disconnect() {\n    this.seedBroker && (await this.seedBroker.disconnect())\n    await Promise.all(values(this.brokers).map(broker => broker.disconnect()))\n\n    this.brokers = {}\n    this.metadata = null\n    this.versions = null\n  }\n\n  /**\n   * @public\n   * @param {Object} destination\n   * @param {string} destination.host\n   * @param {number} destination.port\n   */\n  removeBroker({ host, port }) {\n    const removedBroker = values(this.brokers).find(\n      broker => broker.connectionPool.host === host && broker.connectionPool.port === port\n    )\n\n    if (removedBroker) {\n      delete this.brokers[removedBroker.nodeId]\n      this.metadataExpireAt = null\n\n      if (this.seedBroker.nodeId === removedBroker.nodeId) {\n        this.seedBroker = shuffle(values(this.brokers))[0]\n      }\n    }\n  }\n\n  /**\n   * @public\n   * @param {Array<String>} topics\n   * @returns {Promise<null>}\n   */\n  async refreshMetadata(topics) {\n    const broker = await this.findConnectedBroker()\n    const { host: seedHost, port: seedPort } = this.seedBroker.connectionPool\n\n    return this.retrier(async (bail, retryCount, retryTime) => {\n      try {\n        this.metadata = await broker.metadata(topics)\n        this.metadataExpireAt = Date.now() + this.metadataMaxAge\n\n        const replacedBrokers = []\n\n        this.brokers = await this.metadata.brokers.reduce(\n          async (resultPromise, { nodeId, host, port, rack }) => {\n            const result = await resultPromise\n\n            if (result[nodeId]) {\n              if (!hasBrokerBeenReplaced(result[nodeId], { host, port, rack })) {\n                return result\n              }\n\n              replacedBrokers.push(result[nodeId])\n            }\n\n            if (host === seedHost && port === seedPort) {\n              this.seedBroker.nodeId = nodeId\n              this.seedBroker.connectionPool.rack = rack\n              return assign(result, {\n                [nodeId]: this.seedBroker,\n              })\n            }\n\n            return assign(result, {\n              [nodeId]: this.createBroker({\n                logger: this.rootLogger,\n                versions: this.versions,\n                connectionPool: await this.connectionPoolBuilder.build({ host, port, rack }),\n                nodeId,\n              }),\n            })\n          },\n          this.brokers\n        )\n\n        const freshBrokerIds = this.metadata.brokers.map(({ nodeId }) => `${nodeId}`).sort()\n        const currentBrokerIds = keys(this.brokers).sort()\n        const unusedBrokerIds = arrayDiff(currentBrokerIds, freshBrokerIds)\n\n        const brokerDisconnects = unusedBrokerIds.map(nodeId => {\n          const broker = this.brokers[nodeId]\n          return broker.disconnect().then(() => {\n            delete this.brokers[nodeId]\n          })\n        })\n\n        const replacedBrokersDisconnects = replacedBrokers.map(broker => broker.disconnect())\n        await Promise.all([...brokerDisconnects, ...replacedBrokersDisconnects])\n      } catch (e) {\n        if (e.type === 'LEADER_NOT_AVAILABLE') {\n          throw e\n        }\n\n        bail(e)\n      }\n    })\n  }\n\n  /**\n   * Only refreshes metadata if the data is stale according to the `metadataMaxAge` param or does not contain information about the provided topics\n   *\n   * @public\n   * @param {Array<String>} topics\n   * @returns {Promise<null>}\n   */\n  async refreshMetadataIfNecessary(topics) {\n    const shouldRefresh =\n      this.metadata == null ||\n      this.metadataExpireAt == null ||\n      Date.now() > this.metadataExpireAt ||\n      !topics.every(topic =>\n        this.metadata.topicMetadata.some(topicMetadata => topicMetadata.topic === topic)\n      )\n\n    if (shouldRefresh) {\n      return this.refreshMetadata(topics)\n    }\n  }\n\n  /** @type {() => string[]} */\n  getNodeIds() {\n    return keys(this.brokers)\n  }\n\n  /**\n   * @public\n   * @param {object} options\n   * @param {string} options.nodeId\n   * @returns {Promise<Broker>}\n   */\n  async findBroker({ nodeId }) {\n    const broker = this.brokers[nodeId]\n\n    if (!broker) {\n      throw new KafkaJSBrokerNotFound(`Broker ${nodeId} not found in the cached metadata`)\n    }\n\n    await this.connectBroker(broker)\n    return broker\n  }\n\n  /**\n   * @public\n   * @param {(params: { nodeId: string, broker: Broker }) => Promise<T>} callback\n   * @returns {Promise<T>}\n   * @template T\n   */\n  async withBroker(callback) {\n    const brokers = shuffle(keys(this.brokers))\n    if (brokers.length === 0) {\n      throw new KafkaJSBrokerNotFound('No brokers in the broker pool')\n    }\n\n    for (const nodeId of brokers) {\n      const broker = await this.findBroker({ nodeId })\n      try {\n        return await callback({ nodeId, broker })\n      } catch (e) {}\n    }\n\n    return null\n  }\n\n  /**\n   * @public\n   * @returns {Promise<Broker>}\n   */\n  async findConnectedBroker() {\n    const nodeIds = shuffle(keys(this.brokers))\n    const connectedBrokerId = nodeIds.find(nodeId => this.brokers[nodeId].isConnected())\n\n    if (connectedBrokerId) {\n      return await this.findBroker({ nodeId: connectedBrokerId })\n    }\n\n    // Cycle through the nodes until one connects\n    for (const nodeId of nodeIds) {\n      try {\n        return await this.findBroker({ nodeId })\n      } catch (e) {}\n    }\n\n    // Failed to connect to all known brokers, metadata might be old\n    await this.connect()\n    return this.seedBroker\n  }\n\n  /**\n   * @private\n   * @param {Broker} broker\n   * @returns {Promise<null>}\n   */\n  async connectBroker(broker) {\n    if (broker.isConnected()) {\n      return\n    }\n\n    return this.retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await broker.connect()\n      } catch (e) {\n        if (e.name === 'KafkaJSConnectionError' || e.type === 'ILLEGAL_SASL_STATE') {\n          await broker.disconnect()\n        }\n\n        // To avoid reconnecting to an unavailable host, we bail on connection errors\n        // and refresh metadata on a higher level before reconnecting\n        if (e.name === 'KafkaJSConnectionError') {\n          return bail(e)\n        }\n\n        if (e.type === 'ILLEGAL_SASL_STATE') {\n          // Rebuild the connection pool since it can't recover from illegal SASL state\n          broker.connectionPool = await this.connectionPoolBuilder.build({\n            host: broker.connectionPool.host,\n            port: broker.connectionPool.port,\n            rack: broker.connectionPool.rack,\n          })\n\n          this.logger.error(`Failed to connect to broker, reconnecting`, { retryCount, retryTime })\n          throw new KafkaJSProtocolError(e, { retriable: true })\n        }\n\n        if (e.retriable) throw e\n        this.logger.error(e, { retryCount, retryTime, stack: e.stack })\n        bail(e)\n      }\n    })\n  }\n}\n", "/**\n * @template T\n * @param { (...args: any) => Promise<T> } [asyncFunction]\n * Promise returning function that will only ever be invoked sequentially.\n * @returns { (...args: any) => Promise<T> }\n * Function that may invoke asyncFunction if there is not a currently executing invocation.\n * Returns promise from the currently executing invocation.\n */\nmodule.exports = asyncFunction => {\n  let promise = null\n\n  return (...args) => {\n    if (promise == null) {\n      promise = asyncFunction(...args).finally(() => (promise = null))\n    }\n    return promise\n  }\n}\n", "/**\n * @param {Object} options\n * @param {import(\"../../types\").ISocketFactory} options.socketFactory\n * @param {string} options.host\n * @param {number} options.port\n * @param {Object} options.ssl\n * @param {() => void} options.onConnect\n * @param {(data: Buffer) => void} options.onData\n * @param {() => void} options.onEnd\n * @param {(err: Error) => void} options.onError\n * @param {() => void} options.onTimeout\n */\nmodule.exports = ({\n  socketFactory,\n  host,\n  port,\n  ssl,\n  onConnect,\n  onData,\n  onEnd,\n  onError,\n  onTimeout,\n}) => {\n  const socket = socketFactory({ host, port, ssl, onConnect })\n\n  socket.on('data', onData)\n  socket.on('end', onEnd)\n  socket.on('error', onError)\n  socket.on('timeout', onTimeout)\n\n  return socket\n}\n", "const Encoder = require('./encoder')\n\nmodule.exports = async ({ correlationId, clientId, request: { apiKey, apiVersion, encode } }) => {\n  const payload = await encode()\n  const requestPayload = new Encoder()\n    .writeInt16(apiKey)\n    .writeInt16(apiVersion)\n    .writeInt32(correlationId)\n    .writeString(clientId)\n    .writeEncoder(payload)\n\n  return new Encoder().writeInt32(requestPayload.size()).writeEncoder(requestPayload)\n}\n", "const EARLIEST_OFFSET = -2\nconst LATEST_OFFSET = -1\nconst INT_32_MAX_VALUE = Math.pow(2, 31) - 1\n\nmodule.exports = {\n  EARLIEST_OFFSET,\n  LATEST_OFFSET,\n  INT_32_MAX_VALUE,\n}\n", "module.exports = () => ({\n  KAFKAJS_DEBUG_PROTOCOL_BUFFERS: process.env.KAFKAJS_DEBUG_PROTOCOL_BUFFERS,\n  KAFKAJS_DEBUG_EXTENDED_PROTOCOL_BUFFERS: process.env.KAFKAJS_DEBUG_EXTENDED_PROTOCOL_BUFFERS,\n})\n", "/** @type {<T1 extends string>(namespace: T1) => <T2 extends string>(type: T2) => `${T1}.${T2}`} */\nmodule.exports = namespace => type => `${namespace}.${type}`\n", "const InstrumentationEventType = require('../instrumentation/eventType')\nconst eventType = InstrumentationEventType('network')\n\nmodule.exports = {\n  NETWORK_REQUEST: eventType('request'),\n  NETWORK_REQUEST_TIMEOUT: eventType('request_timeout'),\n  NETWORK_REQUEST_QUEUE_SIZE: eventType('request_queue_size'),\n}\n", "const { KafkaJSRequestTimeoutError, KafkaJSNonRetriableError } = require('../../errors')\nconst events = require('../instrumentationEvents')\n\nconst PRIVATE = {\n  STATE: Symbol('private:SocketRequest:state'),\n  EMIT_EVENT: Symbol('private:SocketRequest:emitEvent'),\n}\n\nconst REQUEST_STATE = {\n  PENDING: Symbol('PENDING'),\n  SENT: Symbol('SENT'),\n  COMPLETED: Symbol('COMPLETED'),\n  REJECTED: Symbol('REJECTED'),\n}\n\n/**\n * SocketRequest abstracts the life cycle of a socket request, making it easier to track\n * request durations and to have individual timeouts per request.\n *\n * @typedef {Object} SocketRequest\n * @property {number} createdAt\n * @property {number} sentAt\n * @property {number} pendingDuration\n * @property {number} duration\n * @property {number} requestTimeout\n * @property {string} broker\n * @property {string} clientId\n * @property {RequestEntry} entry\n * @property {boolean} expectResponse\n * @property {Function} send\n * @property {Function} timeout\n *\n * @typedef {Object} RequestEntry\n * @property {string} apiKey\n * @property {string} apiName\n * @property {number} apiVersion\n * @property {number} correlationId\n * @property {Function} resolve\n * @property {Function} reject\n */\nmodule.exports = class SocketRequest {\n  /**\n   * @param {Object} options\n   * @param {number} options.requestTimeout\n   * @param {string} options.broker - e.g: 127.0.0.1:9092\n   * @param {string} options.clientId\n   * @param {RequestEntry} options.entry\n   * @param {boolean} options.expectResponse\n   * @param {Function} options.send\n   * @param {() => void} options.timeout\n   * @param {import(\"../../instrumentation/emitter\")} [options.instrumentationEmitter=null]\n   */\n  constructor({\n    requestTimeout,\n    broker,\n    clientId,\n    entry,\n    expectResponse,\n    send,\n    timeout,\n    instrumentationEmitter = null,\n  }) {\n    this.createdAt = Date.now()\n    this.requestTimeout = requestTimeout\n    this.broker = broker\n    this.clientId = clientId\n    this.entry = entry\n    this.correlationId = entry.correlationId\n    this.expectResponse = expectResponse\n    this.sendRequest = send\n    this.timeoutHandler = timeout\n\n    this.sentAt = null\n    this.duration = null\n    this.pendingDuration = null\n\n    this[PRIVATE.STATE] = REQUEST_STATE.PENDING\n    this[PRIVATE.EMIT_EVENT] = (eventName, payload) =>\n      instrumentationEmitter && instrumentationEmitter.emit(eventName, payload)\n  }\n\n  send() {\n    this.throwIfInvalidState({\n      accepted: [REQUEST_STATE.PENDING],\n      next: REQUEST_STATE.SENT,\n    })\n\n    this.sendRequest()\n    this.sentAt = Date.now()\n    this.pendingDuration = this.sentAt - this.createdAt\n    this[PRIVATE.STATE] = REQUEST_STATE.SENT\n  }\n\n  timeoutRequest() {\n    const { apiName, apiKey, apiVersion } = this.entry\n    const requestInfo = `${apiName}(key: ${apiKey}, version: ${apiVersion})`\n    const eventData = {\n      broker: this.broker,\n      clientId: this.clientId,\n      correlationId: this.correlationId,\n      createdAt: this.createdAt,\n      sentAt: this.sentAt,\n      pendingDuration: this.pendingDuration,\n    }\n\n    this.timeoutHandler()\n    this.rejected(new KafkaJSRequestTimeoutError(`Request ${requestInfo} timed out`, eventData))\n    this[PRIVATE.EMIT_EVENT](events.NETWORK_REQUEST_TIMEOUT, {\n      ...eventData,\n      apiName,\n      apiKey,\n      apiVersion,\n    })\n  }\n\n  completed({ size, payload }) {\n    this.throwIfInvalidState({\n      accepted: [REQUEST_STATE.SENT],\n      next: REQUEST_STATE.COMPLETED,\n    })\n\n    const { entry, correlationId, broker, clientId, createdAt, sentAt, pendingDuration } = this\n\n    this[PRIVATE.STATE] = REQUEST_STATE.COMPLETED\n    this.duration = Date.now() - this.sentAt\n    entry.resolve({ correlationId, entry, size, payload })\n\n    this[PRIVATE.EMIT_EVENT](events.NETWORK_REQUEST, {\n      broker,\n      clientId,\n      correlationId,\n      size,\n      createdAt,\n      sentAt,\n      pendingDuration,\n      duration: this.duration,\n      apiName: entry.apiName,\n      apiKey: entry.apiKey,\n      apiVersion: entry.apiVersion,\n    })\n  }\n\n  rejected(error) {\n    this.throwIfInvalidState({\n      accepted: [REQUEST_STATE.PENDING, REQUEST_STATE.SENT],\n      next: REQUEST_STATE.REJECTED,\n    })\n\n    this[PRIVATE.STATE] = REQUEST_STATE.REJECTED\n    this.duration = Date.now() - this.sentAt\n    this.entry.reject(error)\n  }\n\n  /**\n   * @private\n   */\n  throwIfInvalidState({ accepted, next }) {\n    if (accepted.includes(this[PRIVATE.STATE])) {\n      return\n    }\n\n    const current = this[PRIVATE.STATE].toString()\n\n    throw new KafkaJSNonRetriableError(\n      `Invalid state, can't transition from ${current} to ${next.toString()}`\n    )\n  }\n}\n", "const { EventEmitter } = require('events')\nconst SocketRequest = require('./socketRequest')\nconst events = require('../instrumentationEvents')\nconst { KafkaJSInvariantViolation } = require('../../errors')\n\nconst PRIVATE = {\n  EMIT_QUEUE_SIZE_EVENT: Symbol('private:RequestQueue:emitQueueSizeEvent'),\n  EMIT_REQUEST_QUEUE_EMPTY: Symbol('private:RequestQueue:emitQueueEmpty'),\n}\n\nconst REQUEST_QUEUE_EMPTY = 'requestQueueEmpty'\nconst CHECK_PENDING_REQUESTS_INTERVAL = 10\n\nmodule.exports = class RequestQueue extends EventEmitter {\n  /**\n   * @param {Object} options\n   * @param {number} options.maxInFlightRequests\n   * @param {number} options.requestTimeout\n   * @param {boolean} options.enforceRequestTimeout\n   * @param {string} options.clientId\n   * @param {string} options.broker\n   * @param {import(\"../../../types\").Logger} options.logger\n   * @param {import(\"../../instrumentation/emitter\")} [options.instrumentationEmitter=null]\n   * @param {() => boolean} [options.isConnected]\n   */\n  constructor({\n    instrumentationEmitter = null,\n    maxInFlightRequests,\n    requestTimeout,\n    enforceRequestTimeout,\n    clientId,\n    broker,\n    logger,\n    isConnected = () => true,\n  }) {\n    super()\n    this.instrumentationEmitter = instrumentationEmitter\n    this.maxInFlightRequests = maxInFlightRequests\n    this.requestTimeout = requestTimeout\n    this.enforceRequestTimeout = enforceRequestTimeout\n    this.clientId = clientId\n    this.broker = broker\n    this.logger = logger\n    this.isConnected = isConnected\n\n    this.inflight = new Map()\n    this.pending = []\n\n    /**\n     * Until when this request queue is throttled and shouldn't send requests\n     *\n     * The value represents the timestamp of the end of the throttling in ms-since-epoch. If the value\n     * is smaller than the current timestamp no throttling is active.\n     *\n     * @type {number}\n     */\n    this.throttledUntil = -1\n\n    /**\n     * Timeout id if we have scheduled a check for pending requests due to client-side throttling\n     *\n     * @type {null|NodeJS.Timeout}\n     */\n    this.throttleCheckTimeoutId = null\n\n    this[PRIVATE.EMIT_REQUEST_QUEUE_EMPTY] = () => {\n      if (this.pending.length === 0 && this.inflight.size === 0) {\n        this.emit(REQUEST_QUEUE_EMPTY)\n      }\n    }\n\n    this[PRIVATE.EMIT_QUEUE_SIZE_EVENT] = () => {\n      instrumentationEmitter &&\n        instrumentationEmitter.emit(events.NETWORK_REQUEST_QUEUE_SIZE, {\n          broker: this.broker,\n          clientId: this.clientId,\n          queueSize: this.pending.length,\n        })\n\n      this[PRIVATE.EMIT_REQUEST_QUEUE_EMPTY]()\n    }\n  }\n\n  /**\n   * @public\n   */\n  scheduleRequestTimeoutCheck() {\n    if (this.enforceRequestTimeout) {\n      this.destroy()\n\n      this.requestTimeoutIntervalId = setInterval(() => {\n        this.inflight.forEach(request => {\n          if (Date.now() - request.sentAt > request.requestTimeout) {\n            request.timeoutRequest()\n          }\n        })\n\n        if (!this.isConnected()) {\n          this.destroy()\n        }\n      }, Math.min(this.requestTimeout, 100))\n    }\n  }\n\n  maybeThrottle(clientSideThrottleTime) {\n    if (clientSideThrottleTime !== null && clientSideThrottleTime > 0) {\n      this.logger.debug(`Client side throttling in effect for ${clientSideThrottleTime}ms`)\n      const minimumThrottledUntil = Date.now() + clientSideThrottleTime\n      this.throttledUntil = Math.max(minimumThrottledUntil, this.throttledUntil)\n    }\n  }\n\n  createSocketRequest(pushedRequest) {\n    const { correlationId } = pushedRequest.entry\n    const defaultRequestTimeout = this.requestTimeout\n    const customRequestTimeout = pushedRequest.requestTimeout\n\n    // Some protocol requests have custom request timeouts (e.g JoinGroup, Fetch, etc). The custom\n    // timeouts are influenced by user configurations, which can be lower than the default requestTimeout\n    const requestTimeout = Math.max(defaultRequestTimeout, customRequestTimeout || 0)\n\n    const socketRequest = new SocketRequest({\n      entry: pushedRequest.entry,\n      expectResponse: pushedRequest.expectResponse,\n      broker: this.broker,\n      clientId: this.clientId,\n      instrumentationEmitter: this.instrumentationEmitter,\n      requestTimeout,\n      send: () => {\n        if (this.inflight.has(correlationId)) {\n          throw new KafkaJSInvariantViolation('Correlation id already exists')\n        }\n        this.inflight.set(correlationId, socketRequest)\n        pushedRequest.sendRequest()\n      },\n      timeout: () => {\n        this.inflight.delete(correlationId)\n        this.checkPendingRequests()\n        // Try to emit REQUEST_QUEUE_EMPTY. Otherwise, waitForPendingRequests may stuck forever\n        this[PRIVATE.EMIT_REQUEST_QUEUE_EMPTY]()\n      },\n    })\n\n    return socketRequest\n  }\n\n  /**\n   * @typedef {Object} PushedRequest\n   * @property {import(\"./socketRequest\").RequestEntry} entry\n   * @property {boolean} expectResponse\n   * @property {Function} sendRequest\n   * @property {number} [requestTimeout]\n   *\n   * @public\n   * @param {PushedRequest} pushedRequest\n   */\n  push(pushedRequest) {\n    const { correlationId } = pushedRequest.entry\n    const socketRequest = this.createSocketRequest(pushedRequest)\n\n    if (this.canSendSocketRequestImmediately()) {\n      this.sendSocketRequest(socketRequest)\n      return\n    }\n\n    this.pending.push(socketRequest)\n    this.scheduleCheckPendingRequests()\n\n    this.logger.debug(`Request enqueued`, {\n      clientId: this.clientId,\n      broker: this.broker,\n      correlationId,\n    })\n\n    this[PRIVATE.EMIT_QUEUE_SIZE_EVENT]()\n  }\n\n  /**\n   * @param {SocketRequest} socketRequest\n   */\n  sendSocketRequest(socketRequest) {\n    socketRequest.send()\n\n    if (!socketRequest.expectResponse) {\n      this.logger.debug(`Request does not expect a response, resolving immediately`, {\n        clientId: this.clientId,\n        broker: this.broker,\n        correlationId: socketRequest.correlationId,\n      })\n\n      this.inflight.delete(socketRequest.correlationId)\n      socketRequest.completed({ size: 0, payload: null })\n    }\n  }\n\n  /**\n   * @public\n   * @param {object} response\n   * @param {number} response.correlationId\n   * @param {Buffer} response.payload\n   * @param {number} response.size\n   */\n  fulfillRequest({ correlationId, payload, size }) {\n    const socketRequest = this.inflight.get(correlationId)\n    this.inflight.delete(correlationId)\n    this.checkPendingRequests()\n\n    if (socketRequest) {\n      socketRequest.completed({ size, payload })\n    } else {\n      this.logger.warn(`Response without match`, {\n        clientId: this.clientId,\n        broker: this.broker,\n        correlationId,\n      })\n    }\n\n    this[PRIVATE.EMIT_REQUEST_QUEUE_EMPTY]()\n  }\n\n  /**\n   * @public\n   * @param {Error} error\n   */\n  rejectAll(error) {\n    const requests = [...this.inflight.values(), ...this.pending]\n\n    for (const socketRequest of requests) {\n      socketRequest.rejected(error)\n      this.inflight.delete(socketRequest.correlationId)\n    }\n\n    this.pending = []\n    this.inflight.clear()\n    this[PRIVATE.EMIT_QUEUE_SIZE_EVENT]()\n  }\n\n  /**\n   * @public\n   */\n  waitForPendingRequests() {\n    return new Promise(resolve => {\n      if (this.pending.length === 0 && this.inflight.size === 0) {\n        return resolve()\n      }\n\n      this.logger.debug('Waiting for pending requests', {\n        clientId: this.clientId,\n        broker: this.broker,\n        currentInflightRequests: this.inflight.size,\n        currentPendingQueueSize: this.pending.length,\n      })\n\n      this.once(REQUEST_QUEUE_EMPTY, () => resolve())\n    })\n  }\n\n  /**\n   * @public\n   */\n  destroy() {\n    clearInterval(this.requestTimeoutIntervalId)\n    clearTimeout(this.throttleCheckTimeoutId)\n    this.throttleCheckTimeoutId = null\n  }\n\n  canSendSocketRequestImmediately() {\n    const shouldEnqueue =\n      (this.maxInFlightRequests != null && this.inflight.size >= this.maxInFlightRequests) ||\n      this.throttledUntil > Date.now()\n\n    return !shouldEnqueue\n  }\n\n  /**\n   * Check and process pending requests either now or in the future\n   *\n   * This function will send out as many pending requests as possible taking throttling and\n   * in-flight limits into account.\n   */\n  checkPendingRequests() {\n    while (this.pending.length > 0 && this.canSendSocketRequestImmediately()) {\n      const pendingRequest = this.pending.shift() // first in first out\n      this.sendSocketRequest(pendingRequest)\n\n      this.logger.debug(`Consumed pending request`, {\n        clientId: this.clientId,\n        broker: this.broker,\n        correlationId: pendingRequest.correlationId,\n        pendingDuration: pendingRequest.pendingDuration,\n        currentPendingQueueSize: this.pending.length,\n      })\n\n      this[PRIVATE.EMIT_QUEUE_SIZE_EVENT]()\n    }\n\n    this.scheduleCheckPendingRequests()\n  }\n\n  /**\n   * Ensure that pending requests will be checked in the future\n   *\n   * If there is a client-side throttling in place this will ensure that we will check\n   * the pending request queue eventually.\n   */\n  scheduleCheckPendingRequests() {\n    // If we're throttled: Schedule checkPendingRequests when the throttle\n    // should be resolved. If there is already something scheduled we assume that that\n    // will be fine, and potentially fix up a new timeout if needed at that time.\n    // Note that if we're merely \"overloaded\" by having too many inflight requests\n    // we will anyways check the queue when one of them gets fulfilled.\n    let scheduleAt = this.throttledUntil - Date.now()\n    if (!this.throttleCheckTimeoutId) {\n      if (this.pending.length > 0) {\n        scheduleAt = scheduleAt > 0 ? scheduleAt : CHECK_PENDING_REQUESTS_INTERVAL\n      }\n      this.throttleCheckTimeoutId = setTimeout(() => {\n        this.throttleCheckTimeoutId = null\n        this.checkPendingRequests()\n      }, scheduleAt)\n    }\n  }\n}\n", "const CONNECTION_STATUS = {\n  CONNECTED: 'connected',\n  DISCONNECTING: 'disconnecting',\n  DISCONNECTED: 'disconnected',\n}\n\nconst CONNECTED_STATUS = [CONNECTION_STATUS.CONNECTED, CONNECTION_STATUS.DISCONNECTING]\n\nmodule.exports = {\n  CONNECTION_STATUS,\n  CONNECTED_STATUS,\n}\n", "/**\n * http://www.ietf.org/rfc/rfc2595.txt\n *\n * The mechanism consists of a single message from the client to the\n * server.  The client sends the authorization identity (identity to\n * login as), followed by a US-ASCII NUL character, followed by the\n * authentication identity (identity whose password will be used),\n * followed by a US-ASCII NUL character, followed by the clear-text\n * password.  The client may leave the authorization identity empty to\n * indicate that it is the same as the authentication identity.\n *\n * The server will verify the authentication identity and password with\n * the system authentication database and verify that the authentication\n * credentials permit the client to login as the authorization identity.\n * If both steps succeed, the user is logged in.\n */\n\nconst Encoder = require('../../encoder')\n\nconst US_ASCII_NULL_CHAR = '\\u0000'\n\nmodule.exports = ({ authorizationIdentity = null, username, password }) => ({\n  encode: async () => {\n    return new Encoder().writeBytes(\n      [authorizationIdentity, username, password].join(US_ASCII_NULL_CHAR)\n    ).buffer\n  },\n})\n", "module.exports = {\n  decode: async () => true,\n  parse: async () => true,\n}\n", "module.exports = {\n  request: require('./request'),\n  response: require('./response'),\n}\n", "const { request, response } = require('../../protocol/sasl/plain')\nconst { KafkaJSSASLAuthenticationError } = require('../../errors')\n\nconst plainAuthenticatorProvider = sasl => ({ host, port, logger, saslAuthenticate }) => {\n  return {\n    authenticate: async () => {\n      if (sasl.username == null || sasl.password == null) {\n        throw new KafkaJSSASLAuthenticationError('SASL Plain: Invalid username or password')\n      }\n\n      const broker = `${host}:${port}`\n\n      try {\n        logger.debug('Authenticate with SASL PLAIN', { broker })\n        await saslAuthenticate({ request: request(sasl), response })\n        logger.debug('SASL PLAIN authentication successful', { broker })\n      } catch (e) {\n        const error = new KafkaJSSASLAuthenticationError(\n          `SASL PLAIN authentication failed: ${e.message}`\n        )\n        logger.error(error.message, { broker })\n        throw error\n      }\n    },\n  }\n}\n\nmodule.exports = plainAuthenticatorProvider\n", "/**\n * https://tools.ietf.org/html/rfc5802\n *\n * First, the client sends the \"client-first-message\" containing:\n *\n *  -> a GS2 header consisting of a flag indicating whether channel\n * binding is supported-but-not-used, not supported, or used, and an\n * optional SASL authorization identity;\n *\n *  -> SCRAM username and a random, unique nonce attributes.\n *\n * Note that the client's first message will always start with \"n\", \"y\",\n * or \"p\"; otherwise, the message is invalid and authentication MUST\n * fail.  This is important, as it allows for GS2 extensibility (e.g.,\n * to add support for security layers).\n */\n\nconst Encoder = require('../../../encoder')\n\nmodule.exports = ({ clientFirstMessage }) => ({\n  encode: async () => new Encoder().writeBytes(clientFirstMessage).buffer,\n})\n", "/* eslint no-unused-vars: [\"error\", { \"varsIgnorePattern\": \"_\" }] */\n\nconst Decoder = require('../../../decoder')\n\nconst ENTRY_REGEX = /^([rsiev])=(.*)$/\n\nmodule.exports = {\n  decode: async rawData => {\n    return new Decoder(rawData).readBytes()\n  },\n  parse: async data => {\n    const processed = data\n      .toString()\n      .split(',')\n      .map(str => {\n        const [_, key, value] = str.match(ENTRY_REGEX)\n        return [key, value]\n      })\n      .reduce((obj, entry) => ({ ...obj, [entry[0]]: entry[1] }), {})\n\n    return { original: data.toString(), ...processed }\n  },\n}\n", "const Encoder = require('../../../encoder')\n\nmodule.exports = ({ finalMessage }) => ({\n  encode: async () => new Encoder().writeBytes(finalMessage).buffer,\n})\n", "module.exports = require('../firstMessage/response')\n", "module.exports = {\n  firstMessage: {\n    request: require('./firstMessage/request'),\n    response: require('./firstMessage/response'),\n  },\n  finalMessage: {\n    request: require('./finalMessage/request'),\n    response: require('./finalMessage/response'),\n  },\n}\n", "const crypto = require('crypto')\nconst scram = require('../../protocol/sasl/scram')\nconst { KafkaJSSASLAuthenticationError, KafkaJSNonRetriableError } = require('../../errors')\n\nconst GS2_HEADER = 'n,,'\n\nconst EQUAL_SIGN_REGEX = /=/g\nconst COMMA_SIGN_REGEX = /,/g\n\nconst URLSAFE_BASE64_PLUS_REGEX = /\\+/g\nconst URLSAFE_BASE64_SLASH_REGEX = /\\//g\nconst URLSAFE_BASE64_TRAILING_EQUAL_REGEX = /=+$/\n\nconst HMAC_CLIENT_KEY = 'Client Key'\nconst HMAC_SERVER_KEY = 'Server Key'\n\nconst DIGESTS = {\n  SHA256: {\n    length: 32,\n    type: 'sha256',\n    minIterations: 4096,\n  },\n  SHA512: {\n    length: 64,\n    type: 'sha512',\n    minIterations: 4096,\n  },\n}\n\nconst encode64 = str => Buffer.from(str).toString('base64')\n\nclass SCRAM {\n  /**\n   * From https://tools.ietf.org/html/rfc5802#section-5.1\n   *\n   * The characters ',' or '=' in usernames are sent as '=2C' and\n   * '=3D' respectively.  If the server receives a username that\n   * contains '=' not followed by either '2C' or '3D', then the\n   * server MUST fail the authentication.\n   *\n   * @returns {String}\n   */\n  static sanitizeString(str) {\n    return str.replace(EQUAL_SIGN_REGEX, '=3D').replace(COMMA_SIGN_REGEX, '=2C')\n  }\n\n  /**\n   * In cryptography, a nonce is an arbitrary number that can be used just once.\n   * It is similar in spirit to a nonce * word, hence the name. It is often a random or pseudo-random\n   * number issued in an authentication protocol to * ensure that old communications cannot be reused\n   * in replay attacks.\n   *\n   * @returns {String}\n   */\n  static nonce() {\n    return crypto\n      .randomBytes(16)\n      .toString('base64')\n      .replace(URLSAFE_BASE64_PLUS_REGEX, '-') // make it url safe\n      .replace(URLSAFE_BASE64_SLASH_REGEX, '_')\n      .replace(URLSAFE_BASE64_TRAILING_EQUAL_REGEX, '')\n      .toString('ascii')\n  }\n\n  /**\n   * Hi() is, essentially, PBKDF2 [RFC2898] with HMAC() as the\n   * pseudorandom function (PRF) and with dkLen == output length of\n   * HMAC() == output length of H()\n   *\n   * @returns {Promise<Buffer>}\n   */\n  static hi(password, salt, iterations, digestDefinition) {\n    return new Promise((resolve, reject) => {\n      crypto.pbkdf2(\n        password,\n        salt,\n        iterations,\n        digestDefinition.length,\n        digestDefinition.type,\n        (err, derivedKey) => (err ? reject(err) : resolve(derivedKey))\n      )\n    })\n  }\n\n  /**\n   * Apply the exclusive-or operation to combine the octet string\n   * on the left of this operator with the octet string on the right of\n   * this operator.  The length of the output and each of the two\n   * inputs will be the same for this use\n   *\n   * @returns {Buffer}\n   */\n  static xor(left, right) {\n    const bufferA = Buffer.from(left)\n    const bufferB = Buffer.from(right)\n    const length = Buffer.byteLength(bufferA)\n\n    if (length !== Buffer.byteLength(bufferB)) {\n      throw new KafkaJSNonRetriableError('Buffers must be of the same length')\n    }\n\n    const result = []\n    for (let i = 0; i < length; i++) {\n      result.push(bufferA[i] ^ bufferB[i])\n    }\n\n    return Buffer.from(result)\n  }\n\n  /**\n   * @param {SASLOptions} sasl\n   * @param {Logger} logger\n   * @param {Function} saslAuthenticate\n   * @param {DigestDefinition} digestDefinition\n   */\n  constructor(sasl, host, port, logger, saslAuthenticate, digestDefinition) {\n    this.sasl = sasl\n    this.host = host\n    this.port = port\n    this.logger = logger\n    this.saslAuthenticate = saslAuthenticate\n    this.digestDefinition = digestDefinition\n\n    const digestType = digestDefinition.type.toUpperCase()\n    this.PREFIX = `SASL SCRAM ${digestType} authentication`\n\n    this.currentNonce = SCRAM.nonce()\n  }\n\n  async authenticate() {\n    const { PREFIX } = this\n    const broker = `${this.host}:${this.port}`\n\n    if (this.sasl.username == null || this.sasl.password == null) {\n      throw new KafkaJSSASLAuthenticationError(`${this.PREFIX}: Invalid username or password`)\n    }\n\n    try {\n      this.logger.debug('Exchanging first client message', { broker })\n      const clientMessageResponse = await this.sendClientFirstMessage()\n\n      this.logger.debug('Sending final message', { broker })\n      const finalResponse = await this.sendClientFinalMessage(clientMessageResponse)\n\n      if (finalResponse.e) {\n        throw new Error(finalResponse.e)\n      }\n\n      const serverKey = await this.serverKey(clientMessageResponse)\n      const serverSignature = this.serverSignature(serverKey, clientMessageResponse)\n\n      if (finalResponse.v !== serverSignature) {\n        throw new Error('Invalid server signature in server final message')\n      }\n\n      this.logger.debug(`${PREFIX} successful`, { broker })\n    } catch (e) {\n      const error = new KafkaJSSASLAuthenticationError(`${PREFIX} failed: ${e.message}`)\n      this.logger.error(error.message, { broker })\n      throw error\n    }\n  }\n\n  /**\n   * @private\n   */\n  async sendClientFirstMessage() {\n    const clientFirstMessage = `${GS2_HEADER}${this.firstMessageBare()}`\n    const request = scram.firstMessage.request({ clientFirstMessage })\n    const response = scram.firstMessage.response\n\n    return this.saslAuthenticate({\n      request,\n      response,\n    })\n  }\n\n  /**\n   * @private\n   */\n  async sendClientFinalMessage(clientMessageResponse) {\n    const { PREFIX } = this\n    const iterations = parseInt(clientMessageResponse.i, 10)\n    const { minIterations } = this.digestDefinition\n\n    if (!clientMessageResponse.r.startsWith(this.currentNonce)) {\n      throw new KafkaJSSASLAuthenticationError(\n        `${PREFIX} failed: Invalid server nonce, it does not start with the client nonce`\n      )\n    }\n\n    if (iterations < minIterations) {\n      throw new KafkaJSSASLAuthenticationError(\n        `${PREFIX} failed: Requested iterations ${iterations} is less than the minimum ${minIterations}`\n      )\n    }\n\n    const finalMessageWithoutProof = this.finalMessageWithoutProof(clientMessageResponse)\n    const clientProof = await this.clientProof(clientMessageResponse)\n    const finalMessage = `${finalMessageWithoutProof},p=${clientProof}`\n    const request = scram.finalMessage.request({ finalMessage })\n    const response = scram.finalMessage.response\n\n    return this.saslAuthenticate({\n      request,\n      response,\n    })\n  }\n\n  /**\n   * @private\n   */\n  async clientProof(clientMessageResponse) {\n    const clientKey = await this.clientKey(clientMessageResponse)\n    const storedKey = this.H(clientKey)\n    const clientSignature = this.clientSignature(storedKey, clientMessageResponse)\n    return encode64(SCRAM.xor(clientKey, clientSignature))\n  }\n\n  /**\n   * @private\n   */\n  async clientKey(clientMessageResponse) {\n    const saltedPassword = await this.saltPassword(clientMessageResponse)\n    return this.HMAC(saltedPassword, HMAC_CLIENT_KEY)\n  }\n\n  /**\n   * @private\n   */\n  async serverKey(clientMessageResponse) {\n    const saltedPassword = await this.saltPassword(clientMessageResponse)\n    return this.HMAC(saltedPassword, HMAC_SERVER_KEY)\n  }\n\n  /**\n   * @private\n   */\n  clientSignature(storedKey, clientMessageResponse) {\n    return this.HMAC(storedKey, this.authMessage(clientMessageResponse))\n  }\n\n  /**\n   * @private\n   */\n  serverSignature(serverKey, clientMessageResponse) {\n    return encode64(this.HMAC(serverKey, this.authMessage(clientMessageResponse)))\n  }\n\n  /**\n   * @private\n   */\n  authMessage(clientMessageResponse) {\n    return [\n      this.firstMessageBare(),\n      clientMessageResponse.original,\n      this.finalMessageWithoutProof(clientMessageResponse),\n    ].join(',')\n  }\n\n  /**\n   * @private\n   */\n  async saltPassword(clientMessageResponse) {\n    const salt = Buffer.from(clientMessageResponse.s, 'base64')\n    const iterations = parseInt(clientMessageResponse.i, 10)\n    return SCRAM.hi(this.encodedPassword(), salt, iterations, this.digestDefinition)\n  }\n\n  /**\n   * @private\n   */\n  firstMessageBare() {\n    return `n=${this.encodedUsername()},r=${this.currentNonce}`\n  }\n\n  /**\n   * @private\n   */\n  finalMessageWithoutProof(clientMessageResponse) {\n    const rnonce = clientMessageResponse.r\n    return `c=${encode64(GS2_HEADER)},r=${rnonce}`\n  }\n\n  /**\n   * @private\n   */\n  encodedUsername() {\n    const { username } = this.sasl\n    return SCRAM.sanitizeString(username).toString('utf-8')\n  }\n\n  /**\n   * @private\n   */\n  encodedPassword() {\n    const { password } = this.sasl\n    return password.toString('utf-8')\n  }\n\n  /**\n   * @private\n   */\n  H(data) {\n    return crypto\n      .createHash(this.digestDefinition.type)\n      .update(data)\n      .digest()\n  }\n\n  /**\n   * @private\n   */\n  HMAC(key, data) {\n    return crypto\n      .createHmac(this.digestDefinition.type, key)\n      .update(data)\n      .digest()\n  }\n}\n\nmodule.exports = {\n  DIGESTS,\n  SCRAM,\n}\n", "const { SCRAM, DIGESTS } = require('./scram')\n\nconst scram256AuthenticatorProvider = sasl => ({ host, port, logger, saslAuthenticate }) => {\n  const scram = new SCRAM(sasl, host, port, logger, saslAuthenticate, DIGESTS.SHA256)\n  return {\n    authenticate: async () => await scram.authenticate(),\n  }\n}\n\nmodule.exports = scram256AuthenticatorProvider\n", "const { SCRAM, DIGESTS } = require('./scram')\n\nconst scram512AuthenticatorProvider = sasl => ({ host, port, logger, saslAuthenticate }) => {\n  const scram = new SCRAM(sasl, host, port, logger, saslAuthenticate, DIGESTS.SHA512)\n  return {\n    authenticate: async () => await scram.authenticate(),\n  }\n}\n\nmodule.exports = scram512AuthenticatorProvider\n", "const Encoder = require('../../encoder')\n\nconst US_ASCII_NULL_CHAR = '\\u0000'\n\nmodule.exports = ({ authorizationIdentity, accessKeyId, secretAccessKey, sessionToken = '' }) => ({\n  encode: async () => {\n    return new Encoder().writeBytes(\n      [authorizationIdentity, accessKeyId, secretAccessKey, sessionToken].join(US_ASCII_NULL_CHAR)\n    ).buffer\n  },\n})\n", "module.exports = {\n  decode: async () => true,\n  parse: async () => true,\n}\n", "module.exports = {\n  request: require('./request'),\n  response: require('./response'),\n}\n", "const { request, response } = require('../../protocol/sasl/awsIam')\nconst { KafkaJSSASLAuthenticationError } = require('../../errors')\n\nconst awsIAMAuthenticatorProvider = sasl => ({ host, port, logger, saslAuthenticate }) => {\n  return {\n    authenticate: async () => {\n      if (!sasl.authorizationIdentity) {\n        throw new KafkaJSSASLAuthenticationError('SASL AWS-IAM: Missing authorizationIdentity')\n      }\n      if (!sasl.accessKeyId) {\n        throw new KafkaJSSASLAuthenticationError('SASL AWS-IAM: Missing accessKeyId')\n      }\n      if (!sasl.secretAccessKey) {\n        throw new KafkaJSSASLAuthenticationError('SASL AWS-IAM: Missing secretAccessKey')\n      }\n      if (!sasl.sessionToken) {\n        sasl.sessionToken = ''\n      }\n\n      const broker = `${host}:${port}`\n\n      try {\n        logger.debug('Authenticate with SASL AWS-IAM', { broker })\n        await saslAuthenticate({ request: request(sasl), response })\n        logger.debug('SASL AWS-IAM authentication successful', { broker })\n      } catch (e) {\n        const error = new KafkaJSSASLAuthenticationError(\n          `SASL AWS-IAM authentication failed: ${e.message}`\n        )\n        logger.error(error.message, { broker })\n        throw error\n      }\n    },\n  }\n}\n\nmodule.exports = awsIAMAuthenticatorProvider\n", "/**\n * http://www.ietf.org/rfc/rfc5801.txt\n *\n * See org.apache.kafka.common.security.oauthbearer.internals.OAuthBearerClientInitialResponse\n * for official Java client implementation.\n *\n * The mechanism consists of a message from the client to the server.\n * The client sends the \"n,\"\" GS header, followed by the authorizationIdentitty\n * prefixed by \"a=\" (if present), followed by \",\", followed by a US-ASCII SOH\n * character, followed by \"auth=Bearer \", followed by the token value, followed\n * by US-ASCII SOH character, followed by SASL extensions in OAuth \"friendly\"\n * format and then closed by two additionals US-ASCII SOH characters.\n *\n * SASL extensions are optional an must be expressed as key-value pairs in an\n * object. Each expression is converted as, the extension entry key, followed\n * by \"=\", followed by extension entry value. Each extension is separated by a\n * US-ASCII SOH character. If extensions are not present, their relative part\n * in the message, including the US-ASCII SOH character, is omitted.\n *\n * The client may leave the authorization identity empty to\n * indicate that it is the same as the authentication identity.\n *\n * The server will verify the authentication token and verify that the\n * authentication credentials permit the client to login as the authorization\n * identity. If both steps succeed, the user is logged in.\n */\n\nconst Encoder = require('../../encoder')\n\nconst SEPARATOR = '\\u0001' // SOH - Start Of Header ASCII\n\nfunction formatExtensions(extensions) {\n  let msg = ''\n\n  if (extensions == null) {\n    return msg\n  }\n\n  let prefix = ''\n  for (const k in extensions) {\n    msg += `${prefix}${k}=${extensions[k]}`\n    prefix = SEPARATOR\n  }\n\n  return msg\n}\n\nmodule.exports = async ({ authorizationIdentity = null }, oauthBearerToken) => {\n  const authzid = authorizationIdentity == null ? '' : `\"a=${authorizationIdentity}`\n  let ext = formatExtensions(oauthBearerToken.extensions)\n  if (ext.length > 0) {\n    ext = `${SEPARATOR}${ext}`\n  }\n\n  const oauthMsg = `n,${authzid},${SEPARATOR}auth=Bearer ${oauthBearerToken.value}${ext}${SEPARATOR}${SEPARATOR}`\n\n  return {\n    encode: async () => {\n      return new Encoder().writeBytes(Buffer.from(oauthMsg)).buffer\n    },\n  }\n}\n", "module.exports = {\n  decode: async () => true,\n  parse: async () => true,\n}\n", "module.exports = {\n  request: require('./request'),\n  response: require('./response'),\n}\n", "/**\n * The sasl object must include a property named oauthBearerProvider, an\n * async function that is used to return the OAuth bearer token.\n *\n * The OAuth bearer token must be an object with properties value and\n * (optionally) extensions, that will be sent during the SASL/OAUTHBEARER\n * request.\n *\n * The implementation of the oauthBearerProvider must take care that tokens are\n * reused and refreshed when appropriate.\n */\n\nconst { request } = require('../../protocol/sasl/oauthBearer')\nconst { KafkaJSSASLAuthenticationError } = require('../../errors')\n\nconst oauthBearerAuthenticatorProvider = sasl => ({ host, port, logger, saslAuthenticate }) => {\n  return {\n    authenticate: async () => {\n      const { oauthBearerProvider } = sasl\n\n      if (oauthBearerProvider == null) {\n        throw new KafkaJSSASLAuthenticationError(\n          'SASL OAUTHBEARER: Missing OAuth bearer token provider'\n        )\n      }\n\n      const oauthBearerToken = await oauthBearerProvider()\n\n      if (oauthBearerToken.value == null) {\n        throw new KafkaJSSASLAuthenticationError('SASL OAUTHBEARER: Invalid OAuth bearer token')\n      }\n\n      const broker = `${host}:${port}`\n\n      try {\n        logger.debug('Authenticate with SASL OAUTHBEARER', { broker })\n        await saslAuthenticate({ request: await request(sasl, oauthBearerToken) })\n        logger.debug('SASL OAUTHBEARER authentication successful', { broker })\n      } catch (e) {\n        const error = new KafkaJSSASLAuthenticationError(\n          `SASL OAUTHBEARER authentication failed: ${e.message}`\n        )\n        logger.error(error.message, { broker })\n        throw error\n      }\n    },\n  }\n}\n\nmodule.exports = oauthBearerAuthenticatorProvider\n", "const { requests, lookup } = require('../../protocol/requests')\nconst apiKeys = require('../../protocol/requests/apiKeys')\nconst plainAuthenticatorProvider = require('./plain')\nconst scram256AuthenticatorProvider = require('./scram256')\nconst scram512AuthenticatorProvider = require('./scram512')\nconst awsIAMAuthenticatorProvider = require('./awsIam')\nconst oauthBearerAuthenticatorProvider = require('./oauthBearer')\nconst { KafkaJSSASLAuthenticationError } = require('../../errors')\n\nconst BUILT_IN_AUTHENTICATION_PROVIDERS = {\n  AWS: awsIAMAuthenticatorProvider,\n  PLAIN: plainAuthenticatorProvider,\n  OAUTHBEARER: oauthBearerAuthenticatorProvider,\n  'SCRAM-SHA-256': scram256AuthenticatorProvider,\n  'SCRAM-SHA-512': scram512AuthenticatorProvider,\n}\n\nconst UNLIMITED_SESSION_LIFETIME = '0'\n\nmodule.exports = class SASLAuthenticator {\n  constructor(connection, logger, versions, supportAuthenticationProtocol) {\n    this.connection = connection\n    this.logger = logger\n    this.sessionLifetime = UNLIMITED_SESSION_LIFETIME\n\n    const lookupRequest = lookup(versions)\n    this.saslHandshake = lookupRequest(apiKeys.SaslHandshake, requests.SaslHandshake)\n    this.protocolAuthentication = supportAuthenticationProtocol\n      ? lookupRequest(apiKeys.SaslAuthenticate, requests.SaslAuthenticate)\n      : null\n  }\n\n  async authenticate() {\n    const mechanism = this.connection.sasl.mechanism.toUpperCase()\n    const handshake = await this.connection.send(this.saslHandshake({ mechanism }))\n    if (!handshake.enabledMechanisms.includes(mechanism)) {\n      throw new KafkaJSSASLAuthenticationError(\n        `SASL ${mechanism} mechanism is not supported by the server`\n      )\n    }\n\n    const saslAuthenticate = async ({ request, response }) => {\n      if (this.protocolAuthentication) {\n        const requestAuthBytes = await request.encode()\n        const authResponse = await this.connection.send(\n          this.protocolAuthentication({ authBytes: requestAuthBytes })\n        )\n\n        // `0` is a string because `sessionLifetimeMs` is an int64 encoded as string.\n        // This is not present in SaslAuthenticateV0, so we default to `\"0\"`\n        this.sessionLifetime = authResponse.sessionLifetimeMs || UNLIMITED_SESSION_LIFETIME\n\n        if (!response) {\n          return\n        }\n\n        const { authBytes: responseAuthBytes } = authResponse\n        const payloadDecoded = await response.decode(responseAuthBytes)\n        return response.parse(payloadDecoded)\n      }\n\n      return this.connection.sendAuthRequest({ request, response })\n    }\n\n    if (\n      !this.connection.sasl.authenticationProvider &&\n      Object.keys(BUILT_IN_AUTHENTICATION_PROVIDERS).includes(mechanism)\n    ) {\n      this.connection.sasl.authenticationProvider = BUILT_IN_AUTHENTICATION_PROVIDERS[mechanism](\n        this.connection.sasl\n      )\n    }\n    await this.connection.sasl\n      .authenticationProvider({\n        host: this.connection.host,\n        port: this.connection.port,\n        logger: this.logger.namespace(`SaslAuthenticator-${mechanism}`),\n        saslAuthenticate,\n      })\n      .authenticate()\n  }\n}\n", "const createSocket = require('./socket')\nconst createRequest = require('../protocol/request')\nconst Decoder = require('../protocol/decoder')\nconst { KafkaJSConnectionError, KafkaJSConnectionClosedError } = require('../errors')\nconst { INT_32_MAX_VALUE } = require('../constants')\nconst getEnv = require('../env')\nconst RequestQueue = require('./requestQueue')\nconst { CONNECTION_STATUS, CONNECTED_STATUS } = require('./connectionStatus')\nconst sharedPromiseTo = require('../utils/sharedPromiseTo')\nconst Long = require('../utils/long')\nconst SASLAuthenticator = require('../broker/saslAuthenticator')\nconst apiKeys = require('../protocol/requests/apiKeys')\n\nconst requestInfo = ({ apiName, apiKey, apiVersion }) =>\n  `${apiName}(key: ${apiKey}, version: ${apiVersion})`\n\n/**\n * @param request - request from protocol\n * @returns {boolean}\n */\nconst isAuthenticatedRequest = request => {\n  return ![apiKeys.ApiVersions, apiKeys.SaslHandshake, apiKeys.SaslAuthenticate].includes(\n    request.apiKey\n  )\n}\n\nconst PRIVATE = {\n  SHOULD_REAUTHENTICATE: Symbol('private:Connection:shouldReauthenticate'),\n  AUTHENTICATE: Symbol('private:Connection:authenticate'),\n}\n\nmodule.exports = class Connection {\n  /**\n   * @param {Object} options\n   * @param {string} options.host\n   * @param {number} options.port\n   * @param {import(\"../../types\").Logger} options.logger\n   * @param {import(\"../../types\").ISocketFactory} options.socketFactory\n   * @param {string} [options.clientId='kafkajs']\n   * @param {number} options.requestTimeout The maximum amount of time the client will wait for the response of a request,\n   *                                in milliseconds\n   * @param {string} [options.rack=null]\n   * @param {Object} [options.ssl=null] Options for the TLS Secure Context. It accepts all options,\n   *                            usually \"cert\", \"key\" and \"ca\". More information at\n   *                            https://nodejs.org/api/tls.html#tls_tls_createsecurecontext_options\n   * @param {Object} [options.sasl=null] Attributes used for SASL authentication. Options based on the\n   *                             key \"mechanism\". Connection is not actively using the SASL attributes\n   *                             but acting as a data object for this information\n   * @param {number} [options.reauthenticationThreshold=10000]\n   * @param {number} options.connectionTimeout The connection timeout, in milliseconds\n   * @param {boolean} [options.enforceRequestTimeout]\n   * @param {number} [options.maxInFlightRequests=null] The maximum number of unacknowledged requests on a connection before\n   *                                            enqueuing\n   * @param {import(\"../instrumentation/emitter\")} [options.instrumentationEmitter=null]\n   */\n  constructor({\n    host,\n    port,\n    logger,\n    socketFactory,\n    requestTimeout,\n    reauthenticationThreshold = 10000,\n    rack = null,\n    ssl = null,\n    sasl = null,\n    clientId = 'kafkajs',\n    connectionTimeout,\n    enforceRequestTimeout = true,\n    maxInFlightRequests = null,\n    instrumentationEmitter = null,\n  }) {\n    this.host = host\n    this.port = port\n    this.rack = rack\n    this.clientId = clientId\n    this.broker = `${this.host}:${this.port}`\n    this.logger = logger.namespace('Connection')\n\n    this.socketFactory = socketFactory\n    this.ssl = ssl\n    this.sasl = sasl\n\n    this.requestTimeout = requestTimeout\n    this.connectionTimeout = connectionTimeout\n    this.reauthenticationThreshold = reauthenticationThreshold\n\n    this.bytesBuffered = 0\n    this.bytesNeeded = Decoder.int32Size()\n    this.chunks = []\n\n    this.connectionStatus = CONNECTION_STATUS.DISCONNECTED\n    this.correlationId = 0\n    this.requestQueue = new RequestQueue({\n      instrumentationEmitter,\n      maxInFlightRequests,\n      requestTimeout,\n      enforceRequestTimeout,\n      clientId,\n      broker: this.broker,\n      logger: logger.namespace('RequestQueue'),\n      isConnected: () => this.isConnected(),\n    })\n\n    this.versions = null\n\n    this.authHandlers = null\n    this.authExpectResponse = false\n\n    const log = level => (message, extra = {}) => {\n      const logFn = this.logger[level]\n      logFn(message, { broker: this.broker, clientId, ...extra })\n    }\n\n    this.logDebug = log('debug')\n    this.logError = log('error')\n\n    const env = getEnv()\n    this.shouldLogBuffers = env.KAFKAJS_DEBUG_PROTOCOL_BUFFERS === '1'\n    this.shouldLogFetchBuffer =\n      this.shouldLogBuffers && env.KAFKAJS_DEBUG_EXTENDED_PROTOCOL_BUFFERS === '1'\n\n    this.authenticatedAt = null\n    this.sessionLifetime = Long.ZERO\n    this.supportAuthenticationProtocol = null\n\n    /**\n     * @private\n     * @returns {Promise}\n     */\n    this[PRIVATE.AUTHENTICATE] = sharedPromiseTo(async () => {\n      if (this.sasl && !this.isAuthenticated()) {\n        const authenticator = new SASLAuthenticator(\n          this,\n          this.logger,\n          this.versions,\n          this.supportAuthenticationProtocol\n        )\n\n        await authenticator.authenticate()\n        this.authenticatedAt = process.hrtime()\n        this.sessionLifetime = Long.fromValue(authenticator.sessionLifetime)\n      }\n    })\n  }\n\n  getSupportAuthenticationProtocol() {\n    return this.supportAuthenticationProtocol\n  }\n\n  setSupportAuthenticationProtocol(isSupported) {\n    this.supportAuthenticationProtocol = isSupported\n  }\n\n  setVersions(versions) {\n    this.versions = versions\n  }\n\n  isConnected() {\n    return CONNECTED_STATUS.includes(this.connectionStatus)\n  }\n\n  /**\n   * @public\n   * @returns {Promise}\n   */\n  connect() {\n    return new Promise((resolve, reject) => {\n      if (this.isConnected()) {\n        return resolve(true)\n      }\n\n      this.authenticatedAt = null\n\n      let timeoutId\n\n      const onConnect = () => {\n        clearTimeout(timeoutId)\n        this.connectionStatus = CONNECTION_STATUS.CONNECTED\n        this.requestQueue.scheduleRequestTimeoutCheck()\n        resolve(true)\n      }\n\n      const onData = data => {\n        this.processData(data)\n      }\n\n      const onEnd = async () => {\n        clearTimeout(timeoutId)\n\n        const wasConnected = this.isConnected()\n\n        if (this.authHandlers) {\n          this.authHandlers.onError()\n        } else if (wasConnected) {\n          this.logDebug('Kafka server has closed connection')\n          this.rejectRequests(\n            new KafkaJSConnectionClosedError('Closed connection', {\n              host: this.host,\n              port: this.port,\n            })\n          )\n        }\n\n        await this.disconnect()\n      }\n\n      const onError = async e => {\n        clearTimeout(timeoutId)\n\n        const error = new KafkaJSConnectionError(`Connection error: ${e.message}`, {\n          broker: `${this.host}:${this.port}`,\n          code: e.code,\n        })\n\n        this.logError(error.message, { stack: e.stack })\n        this.rejectRequests(error)\n        await this.disconnect()\n\n        reject(error)\n      }\n\n      const onTimeout = async () => {\n        const error = new KafkaJSConnectionError('Connection timeout', {\n          broker: `${this.host}:${this.port}`,\n        })\n\n        this.logError(error.message)\n        this.rejectRequests(error)\n        await this.disconnect()\n        reject(error)\n      }\n\n      this.logDebug(`Connecting`, {\n        ssl: !!this.ssl,\n        sasl: !!this.sasl,\n      })\n\n      try {\n        timeoutId = setTimeout(onTimeout, this.connectionTimeout)\n        this.socket = createSocket({\n          socketFactory: this.socketFactory,\n          host: this.host,\n          port: this.port,\n          ssl: this.ssl,\n          onConnect,\n          onData,\n          onEnd,\n          onError,\n          onTimeout,\n        })\n      } catch (e) {\n        clearTimeout(timeoutId)\n        reject(\n          new KafkaJSConnectionError(`Failed to connect: ${e.message}`, {\n            broker: `${this.host}:${this.port}`,\n          })\n        )\n      }\n    })\n  }\n\n  /**\n   * @public\n   * @returns {Promise}\n   */\n  async disconnect() {\n    this.authenticatedAt = null\n    this.connectionStatus = CONNECTION_STATUS.DISCONNECTING\n    this.logDebug('disconnecting...')\n\n    await this.requestQueue.waitForPendingRequests()\n    this.requestQueue.destroy()\n\n    if (this.socket) {\n      this.socket.end()\n      this.socket.unref()\n    }\n\n    this.connectionStatus = CONNECTION_STATUS.DISCONNECTED\n    this.logDebug('disconnected')\n    return true\n  }\n\n  /**\n   * @public\n   * @returns {boolean}\n   */\n  isAuthenticated() {\n    return this.authenticatedAt != null && !this[PRIVATE.SHOULD_REAUTHENTICATE]()\n  }\n\n  /***\n   * @private\n   */\n  [PRIVATE.SHOULD_REAUTHENTICATE]() {\n    if (this.sessionLifetime.equals(Long.ZERO)) {\n      return false\n    }\n\n    if (this.authenticatedAt == null) {\n      return true\n    }\n\n    const [secondsSince, remainingNanosSince] = process.hrtime(this.authenticatedAt)\n    const millisSince = Long.fromValue(secondsSince)\n      .multiply(1000)\n      .add(Long.fromValue(remainingNanosSince).divide(1000000))\n\n    const reauthenticateAt = millisSince.add(this.reauthenticationThreshold)\n    return reauthenticateAt.greaterThanOrEqual(this.sessionLifetime)\n  }\n\n  /** @public */\n  async authenticate() {\n    await this[PRIVATE.AUTHENTICATE]()\n  }\n\n  /**\n   * @public\n   * @returns {Promise}\n   */\n  sendAuthRequest({ request, response }) {\n    this.authExpectResponse = !!response\n\n    /**\n     * TODO: rewrite removing the async promise executor\n     */\n\n    /* eslint-disable no-async-promise-executor */\n    return new Promise(async (resolve, reject) => {\n      this.authHandlers = {\n        onSuccess: rawData => {\n          this.authHandlers = null\n          this.authExpectResponse = false\n\n          response\n            .decode(rawData)\n            .then(data => response.parse(data))\n            .then(resolve)\n            .catch(reject)\n        },\n        onError: () => {\n          this.authHandlers = null\n          this.authExpectResponse = false\n\n          reject(\n            new KafkaJSConnectionError('Connection closed by the server', {\n              broker: `${this.host}:${this.port}`,\n            })\n          )\n        },\n      }\n\n      try {\n        const requestPayload = await request.encode()\n\n        this.failIfNotConnected()\n        this.socket.write(requestPayload, 'binary')\n      } catch (e) {\n        reject(e)\n      }\n    })\n  }\n\n  /**\n   * @public\n   * @param {object} protocol\n   * @param {object} protocol.request It is defined by the protocol and consists of an object with \"apiKey\",\n   *                         \"apiVersion\", \"apiName\" and an \"encode\" function. The encode function\n   *                         must return an instance of Encoder\n   *\n   * @param {object} protocol.response It is defined by the protocol and consists of an object with two functions:\n   *                          \"decode\" and \"parse\"\n   *\n   * @param {number} [protocol.requestTimeout=null] Override for the default requestTimeout\n   * @param {boolean} [protocol.logResponseError=true] Whether to log errors\n   * @returns {Promise<data>} where data is the return of \"response#parse\"\n   */\n  async send({ request, response, requestTimeout = null, logResponseError = true }) {\n    if (!this.isAuthenticated() && isAuthenticatedRequest(request)) {\n      await this[PRIVATE.AUTHENTICATE]()\n    }\n\n    this.failIfNotConnected()\n\n    const expectResponse = !request.expectResponse || request.expectResponse()\n    const sendRequest = async () => {\n      const { clientId } = this\n      const correlationId = this.nextCorrelationId()\n\n      const requestPayload = await createRequest({ request, correlationId, clientId })\n      const { apiKey, apiName, apiVersion } = request\n      this.logDebug(`Request ${requestInfo(request)}`, {\n        correlationId,\n        expectResponse,\n        size: Buffer.byteLength(requestPayload.buffer),\n      })\n\n      return new Promise((resolve, reject) => {\n        try {\n          this.failIfNotConnected()\n          const entry = { apiKey, apiName, apiVersion, correlationId, resolve, reject }\n\n          this.requestQueue.push({\n            entry,\n            expectResponse,\n            requestTimeout,\n            sendRequest: () => {\n              this.socket.write(requestPayload.buffer, 'binary')\n            },\n          })\n        } catch (e) {\n          reject(e)\n        }\n      })\n    }\n\n    const { correlationId, size, entry, payload } = await sendRequest()\n\n    if (!expectResponse) {\n      return\n    }\n\n    try {\n      const payloadDecoded = await response.decode(payload)\n\n      /**\n       * @see KIP-219\n       * If the response indicates that the client-side needs to throttle, do that.\n       */\n      this.requestQueue.maybeThrottle(payloadDecoded.clientSideThrottleTime)\n\n      const data = await response.parse(payloadDecoded)\n      const isFetchApi = entry.apiName === 'Fetch'\n      this.logDebug(`Response ${requestInfo(entry)}`, {\n        correlationId,\n        size,\n        data: isFetchApi && !this.shouldLogFetchBuffer ? '[filtered]' : data,\n      })\n\n      return data\n    } catch (e) {\n      if (logResponseError) {\n        this.logError(`Response ${requestInfo(entry)}`, {\n          error: e.message,\n          correlationId,\n          size,\n        })\n      }\n\n      const isBuffer = Buffer.isBuffer(payload)\n      this.logDebug(`Response ${requestInfo(entry)}`, {\n        error: e.message,\n        correlationId,\n        payload:\n          isBuffer && !this.shouldLogBuffers ? { type: 'Buffer', data: '[filtered]' } : payload,\n      })\n\n      throw e\n    }\n  }\n\n  /**\n   * @private\n   */\n  failIfNotConnected() {\n    if (!this.isConnected()) {\n      throw new KafkaJSConnectionError('Not connected', {\n        broker: `${this.host}:${this.port}`,\n      })\n    }\n  }\n\n  /**\n   * @private\n   */\n  nextCorrelationId() {\n    if (this.correlationId >= INT_32_MAX_VALUE) {\n      this.correlationId = 0\n    }\n\n    return this.correlationId++\n  }\n\n  /**\n   * @private\n   */\n  processData(rawData) {\n    if (this.authHandlers && !this.authExpectResponse) {\n      return this.authHandlers.onSuccess(rawData)\n    }\n\n    // Accumulate the new chunk\n    this.chunks.push(rawData)\n    this.bytesBuffered += Buffer.byteLength(rawData)\n\n    // Process data if there are enough bytes to read the expected response size,\n    // otherwise keep buffering\n    while (this.bytesNeeded <= this.bytesBuffered) {\n      const buffer = this.chunks.length > 1 ? Buffer.concat(this.chunks) : this.chunks[0]\n      const decoder = new Decoder(buffer)\n      const expectedResponseSize = decoder.readInt32()\n\n      // Return early if not enough bytes to read the full response\n      if (!decoder.canReadBytes(expectedResponseSize)) {\n        this.chunks = [buffer]\n        this.bytesBuffered = Buffer.byteLength(buffer)\n        this.bytesNeeded = Decoder.int32Size() + expectedResponseSize\n        return\n      }\n\n      const response = new Decoder(decoder.readBytes(expectedResponseSize))\n\n      // Reset the buffered chunks as the rest of the bytes\n      const remainderBuffer = decoder.readAll()\n      this.chunks = [remainderBuffer]\n      this.bytesBuffered = Buffer.byteLength(remainderBuffer)\n      this.bytesNeeded = Decoder.int32Size()\n\n      if (this.authHandlers) {\n        const rawResponseSize = Decoder.int32Size() + expectedResponseSize\n        const rawResponseBuffer = buffer.slice(0, rawResponseSize)\n        return this.authHandlers.onSuccess(rawResponseBuffer)\n      }\n\n      const correlationId = response.readInt32()\n      const payload = response.readAll()\n\n      this.requestQueue.fulfillRequest({\n        size: expectedResponseSize,\n        correlationId,\n        payload,\n      })\n    }\n  }\n\n  /**\n   * @private\n   */\n  rejectRequests(error) {\n    this.requestQueue.rejectAll(error)\n  }\n}\n", "const apiKeys = require('../protocol/requests/apiKeys')\nconst Connection = require('./connection')\n\nmodule.exports = class ConnectionPool {\n  /**\n   * @param {ConstructorParameters<typeof Connection>[0]} options\n   */\n  constructor(options) {\n    this.logger = options.logger.namespace('ConnectionPool')\n    this.connectionTimeout = options.connectionTimeout\n    this.host = options.host\n    this.port = options.port\n    this.rack = options.rack\n    this.ssl = options.ssl\n    this.sasl = options.sasl\n    this.clientId = options.clientId\n    this.socketFactory = options.socketFactory\n\n    this.pool = new Array(2).fill().map(() => new Connection(options))\n  }\n\n  isConnected() {\n    return this.pool.some(c => c.isConnected())\n  }\n\n  isAuthenticated() {\n    return this.pool.some(c => c.isAuthenticated())\n  }\n\n  setSupportAuthenticationProtocol(isSupported) {\n    this.map(c => c.setSupportAuthenticationProtocol(isSupported))\n  }\n\n  setVersions(versions) {\n    this.map(c => c.setVersions(versions))\n  }\n\n  map(callback) {\n    return this.pool.map(c => callback(c))\n  }\n\n  async send(protocolRequest) {\n    const connection = await this.getConnectionByRequest(protocolRequest)\n    return connection.send(protocolRequest)\n  }\n\n  getConnectionByRequest({ request: { apiKey } }) {\n    const index = { [apiKeys.Fetch]: 1 }[apiKey] || 0\n    return this.getConnection(index)\n  }\n\n  async getConnection(index = 0) {\n    const connection = this.pool[index]\n\n    if (!connection.isConnected()) {\n      await connection.connect()\n    }\n\n    return connection\n  }\n\n  async destroy() {\n    await Promise.all(this.map(c => c.disconnect()))\n  }\n}\n", "const { KafkaJSConnectionError, KafkaJSNonRetriableError } = require('../errors')\nconst ConnectionPool = require('../network/connectionPool')\n\n/**\n * @typedef {Object} ConnectionPoolBuilder\n * @property {(destination?: { host?: string, port?: number, rack?: string }) => Promise<ConnectionPool>} build\n */\n\n/**\n * @param {Object} options\n * @param {import(\"../../types\").ISocketFactory} [options.socketFactory]\n * @param {string[]|(() => string[])} options.brokers\n * @param {Object} [options.ssl]\n * @param {Object} [options.sasl]\n * @param {string} options.clientId\n * @param {number} options.requestTimeout\n * @param {boolean} [options.enforceRequestTimeout]\n * @param {number} [options.connectionTimeout]\n * @param {number} [options.maxInFlightRequests]\n * @param {import(\"../../types\").RetryOptions} [options.retry]\n * @param {import(\"../../types\").Logger} options.logger\n * @param {import(\"../instrumentation/emitter\")} [options.instrumentationEmitter]\n * @param {number} [options.reauthenticationThreshold]\n * @returns {ConnectionPoolBuilder}\n */\nmodule.exports = ({\n  socketFactory,\n  brokers,\n  ssl,\n  sasl,\n  clientId,\n  requestTimeout,\n  enforceRequestTimeout,\n  connectionTimeout,\n  maxInFlightRequests,\n  logger,\n  instrumentationEmitter = null,\n  reauthenticationThreshold,\n}) => {\n  let index = 0\n\n  const isValidBroker = broker => {\n    return broker && typeof broker === 'string' && broker.length > 0\n  }\n\n  const validateBrokers = brokers => {\n    if (!brokers) {\n      throw new KafkaJSNonRetriableError(`Failed to connect: brokers should not be null`)\n    }\n\n    if (Array.isArray(brokers)) {\n      if (!brokers.length) {\n        throw new KafkaJSNonRetriableError(`Failed to connect: brokers array is empty`)\n      }\n\n      brokers.forEach((broker, index) => {\n        if (!isValidBroker(broker)) {\n          throw new KafkaJSNonRetriableError(\n            `Failed to connect: broker at index ${index} is invalid \"${typeof broker}\"`\n          )\n        }\n      })\n    }\n  }\n\n  const getBrokers = async () => {\n    let list\n\n    if (typeof brokers === 'function') {\n      try {\n        list = await brokers()\n      } catch (e) {\n        const wrappedError = new KafkaJSConnectionError(\n          `Failed to connect: \"config.brokers\" threw: ${e.message}`\n        )\n        wrappedError.stack = `${wrappedError.name}\\n  Caused by: ${e.stack}`\n        throw wrappedError\n      }\n    } else {\n      list = brokers\n    }\n\n    validateBrokers(list)\n\n    return list\n  }\n\n  return {\n    build: async ({ host, port, rack } = {}) => {\n      if (!host) {\n        const list = await getBrokers()\n\n        const randomBroker = list[index++ % list.length]\n\n        host = randomBroker.split(':')[0]\n        port = Number(randomBroker.split(':')[1])\n      }\n\n      return new ConnectionPool({\n        host,\n        port,\n        rack,\n        sasl,\n        ssl,\n        clientId,\n        socketFactory,\n        connectionTimeout,\n        requestTimeout,\n        enforceRequestTimeout,\n        maxInFlightRequests,\n        instrumentationEmitter,\n        logger,\n        reauthenticationThreshold,\n      })\n    },\n  }\n}\n", "const BrokerPool = require('./brokerPool')\nconst Lock = require('../utils/lock')\nconst sharedPromiseTo = require('../utils/sharedPromiseTo')\nconst createRetry = require('../retry')\nconst connectionPoolBuilder = require('./connectionPoolBuilder')\nconst { EARLIEST_OFFSET, LATEST_OFFSET } = require('../constants')\nconst {\n  KafkaJSError,\n  KafkaJSBrokerNotFound,\n  KafkaJSMetadataNotLoaded,\n  KafkaJSTopicMetadataNotLoaded,\n  KafkaJSGroupCoordinatorNotFound,\n} = require('../errors')\nconst COORDINATOR_TYPES = require('../protocol/coordinatorTypes')\n\nconst { keys } = Object\n\nconst mergeTopics = (obj, { topic, partitions }) => ({\n  ...obj,\n  [topic]: [...(obj[topic] || []), ...partitions],\n})\n\nconst PRIVATE = {\n  CONNECT: Symbol('private:Cluster:connect'),\n  REFRESH_METADATA: Symbol('private:Cluster:refreshMetadata'),\n  REFRESH_METADATA_IF_NECESSARY: Symbol('private:Cluster:refreshMetadataIfNecessary'),\n  FIND_CONTROLLER_BROKER: Symbol('private:Cluster:findControllerBroker'),\n}\n\nmodule.exports = class Cluster {\n  /**\n   * @param {Object} options\n   * @param {Array<string>} options.brokers example: ['127.0.0.1:9092', '127.0.0.1:9094']\n   * @param {Object} options.ssl\n   * @param {Object} options.sasl\n   * @param {string} options.clientId\n   * @param {number} options.connectionTimeout - in milliseconds\n   * @param {number} options.authenticationTimeout - in milliseconds\n   * @param {number} options.reauthenticationThreshold - in milliseconds\n   * @param {number} [options.requestTimeout=30000] - in milliseconds\n   * @param {boolean} [options.enforceRequestTimeout]\n   * @param {number} options.metadataMaxAge - in milliseconds\n   * @param {boolean} options.allowAutoTopicCreation\n   * @param {number} options.maxInFlightRequests\n   * @param {number} options.isolationLevel\n   * @param {import(\"../../types\").RetryOptions} options.retry\n   * @param {import(\"../../types\").Logger} options.logger\n   * @param {import(\"../../types\").ISocketFactory} options.socketFactory\n   * @param {Map} [options.offsets]\n   * @param {import(\"../instrumentation/emitter\")} [options.instrumentationEmitter=null]\n   */\n  constructor({\n    logger: rootLogger,\n    socketFactory,\n    brokers,\n    ssl,\n    sasl,\n    clientId,\n    connectionTimeout,\n    authenticationTimeout,\n    reauthenticationThreshold,\n    requestTimeout = 30000,\n    enforceRequestTimeout,\n    metadataMaxAge,\n    retry,\n    allowAutoTopicCreation,\n    maxInFlightRequests,\n    isolationLevel,\n    instrumentationEmitter = null,\n    offsets = new Map(),\n  }) {\n    this.rootLogger = rootLogger\n    this.logger = rootLogger.namespace('Cluster')\n    this.retrier = createRetry(retry)\n    this.connectionPoolBuilder = connectionPoolBuilder({\n      logger: rootLogger,\n      instrumentationEmitter,\n      socketFactory,\n      brokers,\n      ssl,\n      sasl,\n      clientId,\n      connectionTimeout,\n      requestTimeout,\n      enforceRequestTimeout,\n      maxInFlightRequests,\n      reauthenticationThreshold,\n    })\n\n    this.targetTopics = new Set()\n    this.mutatingTargetTopics = new Lock({\n      description: `updating target topics`,\n      timeout: requestTimeout,\n    })\n    this.isolationLevel = isolationLevel\n    this.brokerPool = new BrokerPool({\n      connectionPoolBuilder: this.connectionPoolBuilder,\n      logger: this.rootLogger,\n      retry,\n      allowAutoTopicCreation,\n      authenticationTimeout,\n      metadataMaxAge,\n    })\n    this.committedOffsetsByGroup = offsets\n\n    this[PRIVATE.CONNECT] = sharedPromiseTo(async () => {\n      return await this.brokerPool.connect()\n    })\n\n    this[PRIVATE.REFRESH_METADATA] = sharedPromiseTo(async () => {\n      return await this.brokerPool.refreshMetadata(Array.from(this.targetTopics))\n    })\n\n    this[PRIVATE.REFRESH_METADATA_IF_NECESSARY] = sharedPromiseTo(async () => {\n      return await this.brokerPool.refreshMetadataIfNecessary(Array.from(this.targetTopics))\n    })\n\n    this[PRIVATE.FIND_CONTROLLER_BROKER] = sharedPromiseTo(async () => {\n      const { metadata } = this.brokerPool\n\n      if (!metadata || metadata.controllerId == null) {\n        throw new KafkaJSMetadataNotLoaded('Topic metadata not loaded')\n      }\n\n      const broker = await this.findBroker({ nodeId: metadata.controllerId })\n\n      if (!broker) {\n        throw new KafkaJSBrokerNotFound(\n          `Controller broker with id ${metadata.controllerId} not found in the cached metadata`\n        )\n      }\n\n      return broker\n    })\n  }\n\n  isConnected() {\n    return this.brokerPool.hasConnectedBrokers()\n  }\n\n  /**\n   * @public\n   * @returns {Promise<void>}\n   */\n  async connect() {\n    await this[PRIVATE.CONNECT]()\n  }\n\n  /**\n   * @public\n   * @returns {Promise<void>}\n   */\n  async disconnect() {\n    await this.brokerPool.disconnect()\n  }\n\n  /**\n   * @public\n   * @param {object} destination\n   * @param {String} destination.host\n   * @param {Number} destination.port\n   */\n  removeBroker({ host, port }) {\n    this.brokerPool.removeBroker({ host, port })\n  }\n\n  /**\n   * @public\n   * @returns {Promise<void>}\n   */\n  async refreshMetadata() {\n    await this[PRIVATE.REFRESH_METADATA]()\n  }\n\n  /**\n   * @public\n   * @returns {Promise<void>}\n   */\n  async refreshMetadataIfNecessary() {\n    await this[PRIVATE.REFRESH_METADATA_IF_NECESSARY]()\n  }\n\n  /**\n   * @public\n   * @returns {Promise<import(\"../../types\").BrokerMetadata>}\n   */\n  async metadata({ topics = [] } = {}) {\n    return this.retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await this.brokerPool.refreshMetadataIfNecessary(topics)\n        return this.brokerPool.withBroker(async ({ broker }) => broker.metadata(topics))\n      } catch (e) {\n        if (e.type === 'LEADER_NOT_AVAILABLE') {\n          throw e\n        }\n\n        bail(e)\n      }\n    })\n  }\n\n  /**\n   * @public\n   * @param {string} topic\n   * @return {Promise}\n   */\n  async addTargetTopic(topic) {\n    return this.addMultipleTargetTopics([topic])\n  }\n\n  /**\n   * @public\n   * @param {string[]} topics\n   * @return {Promise}\n   */\n  async addMultipleTargetTopics(topics) {\n    await this.mutatingTargetTopics.acquire()\n\n    try {\n      const previousSize = this.targetTopics.size\n      const previousTopics = new Set(this.targetTopics)\n      for (const topic of topics) {\n        this.targetTopics.add(topic)\n      }\n\n      const hasChanged = previousSize !== this.targetTopics.size || !this.brokerPool.metadata\n\n      if (hasChanged) {\n        try {\n          await this.refreshMetadata()\n        } catch (e) {\n          if (\n            e.type === 'INVALID_TOPIC_EXCEPTION' ||\n            e.type === 'UNKNOWN_TOPIC_OR_PARTITION' ||\n            e.type === 'TOPIC_AUTHORIZATION_FAILED'\n          ) {\n            this.targetTopics = previousTopics\n          }\n\n          throw e\n        }\n      }\n    } finally {\n      await this.mutatingTargetTopics.release()\n    }\n  }\n\n  /** @type {() => string[]} */\n  getNodeIds() {\n    return this.brokerPool.getNodeIds()\n  }\n\n  /**\n   * @public\n   * @param {object} options\n   * @param {string} options.nodeId\n   * @returns {Promise<import(\"../../types\").Broker>}\n   */\n  async findBroker({ nodeId }) {\n    try {\n      return await this.brokerPool.findBroker({ nodeId })\n    } catch (e) {\n      // The client probably has stale metadata\n      if (\n        e.name === 'KafkaJSBrokerNotFound' ||\n        e.name === 'KafkaJSLockTimeout' ||\n        e.name === 'KafkaJSConnectionError'\n      ) {\n        await this.refreshMetadata()\n      }\n\n      throw e\n    }\n  }\n\n  /**\n   * @public\n   * @returns {Promise<import(\"../../types\").Broker>}\n   */\n  async findControllerBroker() {\n    return await this[PRIVATE.FIND_CONTROLLER_BROKER]()\n  }\n\n  /**\n   * @public\n   * @param {string} topic\n   * @returns {import(\"../../types\").PartitionMetadata[]} Example:\n   *                   [{\n   *                     isr: [2],\n   *                     leader: 2,\n   *                     partitionErrorCode: 0,\n   *                     partitionId: 0,\n   *                     replicas: [2],\n   *                   }]\n   */\n  findTopicPartitionMetadata(topic) {\n    const { metadata } = this.brokerPool\n    if (!metadata || !metadata.topicMetadata) {\n      throw new KafkaJSTopicMetadataNotLoaded('Topic metadata not loaded', { topic })\n    }\n\n    const topicMetadata = metadata.topicMetadata.find(t => t.topic === topic)\n    return topicMetadata ? topicMetadata.partitionMetadata : []\n  }\n\n  /**\n   * @public\n   * @param {string} topic\n   * @param {(number|string)[]} partitions\n   * @returns {Object} Object with leader and partitions. For partitions 0 and 5\n   *                   the result could be:\n   *                     { '0': [0], '2': [5] }\n   *\n   *                   where the key is the nodeId.\n   */\n  findLeaderForPartitions(topic, partitions) {\n    const partitionMetadata = this.findTopicPartitionMetadata(topic)\n    return partitions.reduce((result, id) => {\n      const partitionId = parseInt(id, 10)\n      const metadata = partitionMetadata.find(p => p.partitionId === partitionId)\n\n      if (!metadata) {\n        return result\n      }\n\n      if (metadata.leader === null || metadata.leader === undefined) {\n        throw new KafkaJSError('Invalid partition metadata', { topic, partitionId, metadata })\n      }\n\n      const { leader } = metadata\n      const current = result[leader] || []\n      return { ...result, [leader]: [...current, partitionId] }\n    }, {})\n  }\n\n  /**\n   * @public\n   * @param {object} params\n   * @param {string} params.groupId\n   * @param {import(\"../protocol/coordinatorTypes\").CoordinatorType} [params.coordinatorType=0]\n   * @returns {Promise<import(\"../../types\").Broker>}\n   */\n  async findGroupCoordinator({ groupId, coordinatorType = COORDINATOR_TYPES.GROUP }) {\n    return this.retrier(async (bail, retryCount, retryTime) => {\n      try {\n        const { coordinator } = await this.findGroupCoordinatorMetadata({\n          groupId,\n          coordinatorType,\n        })\n        return await this.findBroker({ nodeId: coordinator.nodeId })\n      } catch (e) {\n        // A new broker can join the cluster before we have the chance\n        // to refresh metadata\n        if (e.name === 'KafkaJSBrokerNotFound' || e.type === 'GROUP_COORDINATOR_NOT_AVAILABLE') {\n          this.logger.debug(`${e.message}, refreshing metadata and trying again...`, {\n            groupId,\n            retryCount,\n            retryTime,\n          })\n\n          await this.refreshMetadata()\n          throw e\n        }\n\n        if (e.code === 'ECONNREFUSED') {\n          // During maintenance the current coordinator can go down; findBroker will\n          // refresh metadata and re-throw the error. findGroupCoordinator has to re-throw\n          // the error to go through the retry cycle.\n          throw e\n        }\n\n        bail(e)\n      }\n    })\n  }\n\n  /**\n   * @public\n   * @param {object} params\n   * @param {string} params.groupId\n   * @param {import(\"../protocol/coordinatorTypes\").CoordinatorType} [params.coordinatorType=0]\n   * @returns {Promise<Object>}\n   */\n  async findGroupCoordinatorMetadata({ groupId, coordinatorType }) {\n    const brokerMetadata = await this.brokerPool.withBroker(async ({ nodeId, broker }) => {\n      return await this.retrier(async (bail, retryCount, retryTime) => {\n        try {\n          const brokerMetadata = await broker.findGroupCoordinator({ groupId, coordinatorType })\n          this.logger.debug('Found group coordinator', {\n            broker: brokerMetadata.host,\n            nodeId: brokerMetadata.coordinator.nodeId,\n          })\n          return brokerMetadata\n        } catch (e) {\n          this.logger.debug('Tried to find group coordinator', {\n            nodeId,\n            groupId,\n            error: e,\n          })\n\n          if (e.type === 'GROUP_COORDINATOR_NOT_AVAILABLE') {\n            this.logger.debug('Group coordinator not available, retrying...', {\n              nodeId,\n              retryCount,\n              retryTime,\n            })\n\n            throw e\n          }\n\n          bail(e)\n        }\n      })\n    })\n\n    if (brokerMetadata) {\n      return brokerMetadata\n    }\n\n    throw new KafkaJSGroupCoordinatorNotFound('Failed to find group coordinator')\n  }\n\n  /**\n   * @param {object} topicConfiguration\n   * @returns {number}\n   */\n  defaultOffset({ fromBeginning }) {\n    return fromBeginning ? EARLIEST_OFFSET : LATEST_OFFSET\n  }\n\n  /**\n   * @public\n   * @param {Array<Object>} topics\n   *                          [\n   *                            {\n   *                              topic: 'my-topic-name',\n   *                              partitions: [{ partition: 0 }],\n   *                              fromBeginning: false\n   *                            }\n   *                          ]\n   * @returns {Promise<import(\"../../types\").TopicOffsets[]>} example:\n   *                          [\n   *                            {\n   *                              topic: 'my-topic-name',\n   *                              partitions: [\n   *                                { partition: 0, offset: '1' },\n   *                                { partition: 1, offset: '2' },\n   *                                { partition: 2, offset: '1' },\n   *                              ],\n   *                            },\n   *                          ]\n   */\n  async fetchTopicsOffset(topics) {\n    const partitionsPerBroker = {}\n    const topicConfigurations = {}\n\n    const addDefaultOffset = topic => partition => {\n      const { timestamp } = topicConfigurations[topic]\n      return { ...partition, timestamp }\n    }\n\n    // Index all topics and partitions per leader (nodeId)\n    for (const topicData of topics) {\n      const { topic, partitions, fromBeginning, fromTimestamp } = topicData\n      const partitionsPerLeader = this.findLeaderForPartitions(\n        topic,\n        partitions.map(p => p.partition)\n      )\n      const timestamp =\n        fromTimestamp != null ? fromTimestamp : this.defaultOffset({ fromBeginning })\n\n      topicConfigurations[topic] = { timestamp }\n\n      keys(partitionsPerLeader).forEach(nodeId => {\n        partitionsPerBroker[nodeId] = partitionsPerBroker[nodeId] || {}\n        partitionsPerBroker[nodeId][topic] = partitions.filter(p =>\n          partitionsPerLeader[nodeId].includes(p.partition)\n        )\n      })\n    }\n\n    // Create a list of requests to fetch the offset of all partitions\n    const requests = keys(partitionsPerBroker).map(async nodeId => {\n      const broker = await this.findBroker({ nodeId })\n      const partitions = partitionsPerBroker[nodeId]\n\n      const { responses: topicOffsets } = await broker.listOffsets({\n        isolationLevel: this.isolationLevel,\n        topics: keys(partitions).map(topic => ({\n          topic,\n          partitions: partitions[topic].map(addDefaultOffset(topic)),\n        })),\n      })\n\n      return topicOffsets\n    })\n\n    // Execute all requests, merge and normalize the responses\n    const responses = await Promise.all(requests)\n    const partitionsPerTopic = responses.flat().reduce(mergeTopics, {})\n\n    return keys(partitionsPerTopic).map(topic => ({\n      topic,\n      partitions: partitionsPerTopic[topic].map(({ partition, offset }) => ({\n        partition,\n        offset,\n      })),\n    }))\n  }\n\n  /**\n   * Retrieve the object mapping for committed offsets for a single consumer group\n   * @param {object} options\n   * @param {string} options.groupId\n   * @returns {Object}\n   */\n  committedOffsets({ groupId }) {\n    if (!this.committedOffsetsByGroup.has(groupId)) {\n      this.committedOffsetsByGroup.set(groupId, {})\n    }\n\n    return this.committedOffsetsByGroup.get(groupId)\n  }\n\n  /**\n   * Mark offset as committed for a single consumer group's topic-partition\n   * @param {object} options\n   * @param {string} options.groupId\n   * @param {string} options.topic\n   * @param {string|number} options.partition\n   * @param {string} options.offset\n   */\n  markOffsetAsCommitted({ groupId, topic, partition, offset }) {\n    const committedOffsets = this.committedOffsets({ groupId })\n\n    committedOffsets[topic] = committedOffsets[topic] || {}\n    committedOffsets[topic][partition] = offset\n  }\n}\n", "/* eslint-disable */\nconst Long = require('../../../utils/long')\n\n// Based on the kafka client 0.10.2 murmur2 implementation\n// https://github.com/apache/kafka/blob/0.10.2/clients/src/main/java/org/apache/kafka/common/utils/Utils.java#L364\n\nconst SEED = Long.fromValue(0x9747b28c)\n\n// 'm' and 'r' are mixing constants generated offline.\n// They're not really 'magic', they just happen to work well.\nconst M = Long.fromValue(0x5bd1e995)\nconst R = Long.fromValue(24)\n\nmodule.exports = key => {\n  const data = Buffer.isBuffer(key) ? key : Buffer.from(String(key))\n  const length = data.length\n\n  // Initialize the hash to a random value\n  let h = Long.fromValue(SEED.xor(length))\n  let length4 = Math.floor(length / 4)\n\n  for (let i = 0; i < length4; i++) {\n    const i4 = i * 4\n    let k =\n      (data[i4 + 0] & 0xff) +\n      ((data[i4 + 1] & 0xff) << 8) +\n      ((data[i4 + 2] & 0xff) << 16) +\n      ((data[i4 + 3] & 0xff) << 24)\n    k = Long.fromValue(k)\n    k = k.multiply(M)\n    k = k.xor(k.toInt() >>> R)\n    k = Long.fromValue(k).multiply(M)\n    h = h.multiply(M)\n    h = h.xor(k)\n  }\n\n  // Handle the last few bytes of the input array\n  switch (length % 4) {\n    case 3:\n      h = h.xor((data[(length & ~3) + 2] & 0xff) << 16)\n    case 2:\n      h = h.xor((data[(length & ~3) + 1] & 0xff) << 8)\n    case 1:\n      h = h.xor(data[length & ~3] & 0xff)\n      h = h.multiply(M)\n  }\n\n  h = h.xor(h.toInt() >>> 13)\n  h = h.multiply(M)\n  h = h.xor(h.toInt() >>> 15)\n\n  return h.toInt()\n}\n", "const { KafkaJSNonRetriableError } = require('../../../errors')\n\nconst toNodeCompatible = crypto => ({\n  randomBytes: size => crypto.getRandomValues(Buffer.allocUnsafe(size)),\n})\n\nlet cryptoImplementation = null\nif (global && global.crypto) {\n  cryptoImplementation =\n    global.crypto.randomBytes === undefined ? toNodeCompatible(global.crypto) : global.crypto\n} else if (global && global.msCrypto) {\n  cryptoImplementation = toNodeCompatible(global.msCrypto)\n} else if (global && !global.crypto) {\n  cryptoImplementation = require('crypto')\n}\n\nconst MAX_BYTES = 65536\n\nmodule.exports = size => {\n  if (size > MAX_BYTES) {\n    throw new KafkaJSNonRetriableError(\n      `Byte length (${size}) exceeds the max number of bytes of entropy available (${MAX_BYTES})`\n    )\n  }\n\n  if (!cryptoImplementation) {\n    throw new KafkaJSNonRetriableError('No available crypto implementation')\n  }\n\n  return cryptoImplementation.randomBytes(size)\n}\n", "const randomBytes = require('./randomBytes')\n\n// Based on the java client 0.10.2\n// https://github.com/apache/kafka/blob/0.10.2/clients/src/main/java/org/apache/kafka/clients/producer/internals/DefaultPartitioner.java\n\n/**\n * A cheap way to deterministically convert a number to a positive value. When the input is\n * positive, the original value is returned. When the input number is negative, the returned\n * positive value is the original value bit AND against 0x7fffffff which is not its absolutely\n * value.\n */\nconst toPositive = x => x & 0x7fffffff\n\n/**\n * The default partitioning strategy:\n *  - If a partition is specified in the message, use it\n *  - If no partition is specified but a key is present choose a partition based on a hash of the key\n *  - If no partition or key is present choose a partition in a round-robin fashion\n */\nmodule.exports = murmur2 => () => {\n  const counters = {}\n\n  return ({ topic, partitionMetadata, message }) => {\n    if (!(topic in counters)) {\n      counters[topic] = randomBytes(32).readUInt32BE(0)\n    }\n    const numPartitions = partitionMetadata.length\n    const availablePartitions = partitionMetadata.filter(p => p.leader >= 0)\n    const numAvailablePartitions = availablePartitions.length\n\n    if (message.partition !== null && message.partition !== undefined) {\n      return message.partition\n    }\n\n    if (message.key !== null && message.key !== undefined) {\n      return toPositive(murmur2(message.key)) % numPartitions\n    }\n\n    if (numAvailablePartitions > 0) {\n      const i = toPositive(++counters[topic]) % numAvailablePartitions\n      return availablePartitions[i].partitionId\n    }\n\n    // no partitions are available, give a non-available partition\n    return toPositive(++counters[topic]) % numPartitions\n  }\n}\n", "const murmur2 = require('./murmur2')\nconst createDefaultPartitioner = require('../legacy/partitioner')\n\nmodule.exports = createDefaultPartitioner(murmur2)\n", "/* eslint-disable */\n\n// Based on the kafka client 0.10.2 murmur2 implementation\n// https://github.com/apache/kafka/blob/0.10.2/clients/src/main/java/org/apache/kafka/common/utils/Utils.java#L364\n\nconst SEED = 0x9747b28c\n\n// 'm' and 'r' are mixing constants generated offline.\n// They're not really 'magic', they just happen to work well.\nconst M = 0x5bd1e995\nconst R = 24\n\nmodule.exports = key => {\n  const data = Buffer.isBuffer(key) ? key : Buffer.from(String(key))\n  const length = data.length\n\n  // Initialize the hash to a random value\n  let h = SEED ^ length\n  let length4 = length / 4\n\n  for (let i = 0; i < length4; i++) {\n    const i4 = i * 4\n    let k =\n      (data[i4 + 0] & 0xff) +\n      ((data[i4 + 1] & 0xff) << 8) +\n      ((data[i4 + 2] & 0xff) << 16) +\n      ((data[i4 + 3] & 0xff) << 24)\n    k *= M\n    k ^= k >>> R\n    k *= M\n    h *= M\n    h ^= k\n  }\n\n  // Handle the last few bytes of the input array\n  switch (length % 4) {\n    case 3:\n      h ^= (data[(length & ~3) + 2] & 0xff) << 16\n    case 2:\n      h ^= (data[(length & ~3) + 1] & 0xff) << 8\n    case 1:\n      h ^= data[length & ~3] & 0xff\n      h *= M\n  }\n\n  h ^= h >>> 13\n  h *= M\n  h ^= h >>> 15\n\n  return h\n}\n", "const murmur2 = require('./murmur2')\nconst createLegacyPartitioner = require('./partitioner')\n\nmodule.exports = createLegacyPartitioner(murmur2)\n", "const DefaultPartitioner = require('./default')\nconst LegacyPartitioner = require('./legacy')\n\nmodule.exports = {\n  DefaultPartitioner,\n  LegacyPartitioner,\n  /**\n   * @deprecated Use DefaultPartitioner instead\n   *\n   * The JavaCompatiblePartitioner was renamed DefaultPartitioner\n   * and made to be the default in 2.0.0.\n   */\n  JavaCompatiblePartitioner: DefaultPartitioner,\n}\n", "module.exports = {\n  UNINITIALIZED: 'UNINITIALIZED',\n  READY: 'READY',\n  TRANSACTING: 'TRANSACTING',\n  COMMITTING: 'COMMITTING',\n  ABORTING: 'ABORTING',\n}\n", "const { EventEmitter } = require('events')\nconst { KafkaJSNonRetriableError } = require('../../errors')\nconst STATES = require('./transactionStates')\n\nconst VALID_STATE_TRANSITIONS = {\n  [STATES.UNINITIALIZED]: [STATES.READY],\n  [STATES.READY]: [STATES.READY, STATES.TRANSACTING],\n  [STATES.TRANSACTING]: [STATES.COMMITTING, STATES.ABORTING],\n  [STATES.COMMITTING]: [STATES.READY],\n  [STATES.ABORTING]: [STATES.READY],\n}\n\nmodule.exports = ({ logger, initialState = STATES.UNINITIALIZED }) => {\n  let currentState = initialState\n\n  const guard = (object, method, { legalStates, async: isAsync = true }) => {\n    if (!object[method]) {\n      throw new KafkaJSNonRetriableError(`Cannot add guard on missing method \"${method}\"`)\n    }\n\n    return (...args) => {\n      const fn = object[method]\n\n      if (!legalStates.includes(currentState)) {\n        const error = new KafkaJSNonRetriableError(\n          `Transaction state exception: Cannot call \"${method}\" in state \"${currentState}\"`\n        )\n\n        if (isAsync) {\n          return Promise.reject(error)\n        } else {\n          throw error\n        }\n      }\n\n      return fn.apply(object, args)\n    }\n  }\n\n  const stateMachine = Object.assign(new EventEmitter(), {\n    /**\n     * Create a clone of \"object\" where we ensure state machine is in correct state\n     * prior to calling any of the configured methods\n     * @param {Object} object The object whose methods we will guard\n     * @param {Object} methodStateMapping Keys are method names on \"object\"\n     * @param {string[]} methodStateMapping.legalStates Legal states for this method\n     * @param {boolean=true} methodStateMapping.async Whether this method is async (throw vs reject)\n     */\n    createGuarded(object, methodStateMapping) {\n      const guardedMethods = Object.keys(methodStateMapping).reduce((guards, method) => {\n        guards[method] = guard(object, method, methodStateMapping[method])\n        return guards\n      }, {})\n\n      return { ...object, ...guardedMethods }\n    },\n    /**\n     * Transition safely to a new state\n     */\n    transitionTo(state) {\n      logger.debug(`Transaction state transition ${currentState} --> ${state}`)\n\n      if (!VALID_STATE_TRANSITIONS[currentState].includes(state)) {\n        throw new KafkaJSNonRetriableError(\n          `Transaction state exception: Invalid transition ${currentState} --> ${state}`\n        )\n      }\n\n      stateMachine.emit('transition', { to: state, from: currentState })\n      currentState = state\n    },\n\n    state() {\n      return currentState\n    },\n  })\n\n  return stateMachine\n}\n", "const createRetry = require('../../retry')\nconst Lock = require('../../utils/lock')\nconst { KafkaJSNonRetriableError } = require('../../errors')\nconst COORDINATOR_TYPES = require('../../protocol/coordinatorTypes')\nconst createStateMachine = require('./transactionStateMachine')\nconst { INT_32_MAX_VALUE } = require('../../constants')\nconst assert = require('assert')\n\nconst STATES = require('./transactionStates')\nconst NO_PRODUCER_ID = -1\nconst SEQUENCE_START = 0\nconst INIT_PRODUCER_RETRIABLE_PROTOCOL_ERRORS = [\n  'NOT_COORDINATOR_FOR_GROUP',\n  'GROUP_COORDINATOR_NOT_AVAILABLE',\n  'GROUP_LOAD_IN_PROGRESS',\n  /**\n   * The producer might have crashed and never committed the transaction; retry the\n   * request so Kafka can abort the current transaction\n   * @see https://github.com/apache/kafka/blob/201da0542726472d954080d54bc585b111aaf86f/clients/src/main/java/org/apache/kafka/clients/producer/internals/TransactionManager.java#L1001-L1002\n   */\n  'CONCURRENT_TRANSACTIONS',\n]\nconst COMMIT_RETRIABLE_PROTOCOL_ERRORS = [\n  'UNKNOWN_TOPIC_OR_PARTITION',\n  'COORDINATOR_LOAD_IN_PROGRESS',\n]\nconst COMMIT_STALE_COORDINATOR_PROTOCOL_ERRORS = ['COORDINATOR_NOT_AVAILABLE', 'NOT_COORDINATOR']\n\n/**\n * @typedef {Object} EosManager\n */\n\n/**\n * Manage behavior for an idempotent producer and transactions.\n *\n * @returns {EosManager}\n */\nmodule.exports = ({\n  logger,\n  cluster,\n  transactionTimeout = 60000,\n  transactional,\n  transactionalId,\n}) => {\n  if (transactional && !transactionalId) {\n    throw new KafkaJSNonRetriableError('Cannot manage transactions without a transactionalId')\n  }\n\n  const retrier = createRetry(cluster.retry)\n\n  /**\n   * Current producer ID\n   */\n  let producerId = NO_PRODUCER_ID\n\n  /**\n   * Current producer epoch\n   */\n  let producerEpoch = 0\n\n  /**\n   * Idempotent production requires that the producer track the sequence number of messages.\n   *\n   * Sequences are sent with every Record Batch and tracked per Topic-Partition\n   */\n  let producerSequence = {}\n\n  /**\n   * Idempotent production requires a mutex lock per broker to serialize requests with sequence number handling\n   */\n  let brokerMutexLocks = {}\n\n  /**\n   * Topic partitions already participating in the transaction\n   */\n  let transactionTopicPartitions = {}\n\n  /**\n   * Offsets have been added to the transaction\n   */\n  let hasOffsetsAddedToTransaction = false\n\n  const stateMachine = createStateMachine({ logger })\n  stateMachine.on('transition', ({ to }) => {\n    if (to === STATES.READY) {\n      transactionTopicPartitions = {}\n      hasOffsetsAddedToTransaction = false\n    }\n  })\n\n  const findTransactionCoordinator = () => {\n    return cluster.findGroupCoordinator({\n      groupId: transactionalId,\n      coordinatorType: COORDINATOR_TYPES.TRANSACTION,\n    })\n  }\n\n  const transactionalGuard = () => {\n    if (!transactional) {\n      throw new KafkaJSNonRetriableError('Method unavailable if non-transactional')\n    }\n  }\n\n  /**\n   * A transaction is ongoing when offsets or partitions added to it\n   *\n   * @returns {boolean}\n   */\n  const isOngoing = () => {\n    return (\n      hasOffsetsAddedToTransaction ||\n      Object.entries(transactionTopicPartitions).some(([, partitions]) => {\n        return Object.entries(partitions).some(\n          ([, isPartitionAddedToTransaction]) => isPartitionAddedToTransaction\n        )\n      })\n    )\n  }\n\n  const eosManager = stateMachine.createGuarded(\n    {\n      /**\n       * Get the current producer id\n       * @returns {number}\n       */\n      getProducerId() {\n        return producerId\n      },\n\n      /**\n       * Get the current producer epoch\n       * @returns {number}\n       */\n      getProducerEpoch() {\n        return producerEpoch\n      },\n\n      getTransactionalId() {\n        return transactionalId\n      },\n\n      /**\n       * Initialize the idempotent producer by making an `InitProducerId` request.\n       * Overwrites any existing state in this transaction manager\n       */\n      async initProducerId() {\n        return retrier(async (bail, retryCount, retryTime) => {\n          try {\n            await cluster.refreshMetadataIfNecessary()\n\n            // If non-transactional we can request the PID from any broker\n            const broker = await (transactional\n              ? findTransactionCoordinator()\n              : cluster.findControllerBroker())\n\n            const result = await broker.initProducerId({\n              transactionalId: transactional ? transactionalId : undefined,\n              transactionTimeout,\n            })\n\n            stateMachine.transitionTo(STATES.READY)\n            producerId = result.producerId\n            producerEpoch = result.producerEpoch\n            producerSequence = {}\n            brokerMutexLocks = {}\n\n            logger.debug('Initialized producer id & epoch', { producerId, producerEpoch })\n          } catch (e) {\n            if (INIT_PRODUCER_RETRIABLE_PROTOCOL_ERRORS.includes(e.type)) {\n              if (e.type === 'CONCURRENT_TRANSACTIONS') {\n                logger.debug('There is an ongoing transaction on this transactionId, retrying', {\n                  error: e.message,\n                  stack: e.stack,\n                  transactionalId,\n                  retryCount,\n                  retryTime,\n                })\n              }\n\n              throw e\n            }\n\n            bail(e)\n          }\n        })\n      },\n\n      /**\n       * Get the current sequence for a given Topic-Partition. Defaults to 0.\n       *\n       * @param {string} topic\n       * @param {string} partition\n       * @returns {number}\n       */\n      getSequence(topic, partition) {\n        if (!eosManager.isInitialized()) {\n          return SEQUENCE_START\n        }\n\n        producerSequence[topic] = producerSequence[topic] || {}\n        producerSequence[topic][partition] = producerSequence[topic][partition] || SEQUENCE_START\n\n        return producerSequence[topic][partition]\n      },\n\n      /**\n       * Update the sequence for a given Topic-Partition.\n       *\n       * Do nothing if not yet initialized (not idempotent)\n       * @param {string} topic\n       * @param {string} partition\n       * @param {number} increment\n       */\n      updateSequence(topic, partition, increment) {\n        if (!eosManager.isInitialized()) {\n          return\n        }\n\n        const previous = eosManager.getSequence(topic, partition)\n        let sequence = previous + increment\n\n        // Sequence is defined as Int32 in the Record Batch,\n        // so theoretically should need to rotate here\n        if (sequence >= INT_32_MAX_VALUE) {\n          logger.debug(\n            `Sequence for ${topic} ${partition} exceeds max value (${sequence}). Rotating to 0.`\n          )\n          sequence = 0\n        }\n\n        producerSequence[topic][partition] = sequence\n      },\n\n      /**\n       * Begin a transaction\n       */\n      beginTransaction() {\n        transactionalGuard()\n        stateMachine.transitionTo(STATES.TRANSACTING)\n      },\n\n      /**\n       * Add partitions to a transaction if they are not already marked as participating.\n       *\n       * Should be called prior to sending any messages during a transaction\n       * @param {TopicData[]} topicData\n       *\n       * @typedef {Object} TopicData\n       * @property {string} topic\n       * @property {object[]} partitions\n       * @property {number} partitions[].partition\n       */\n      async addPartitionsToTransaction(topicData) {\n        transactionalGuard()\n        const newTopicPartitions = {}\n\n        topicData.forEach(({ topic, partitions }) => {\n          transactionTopicPartitions[topic] = transactionTopicPartitions[topic] || {}\n\n          partitions.forEach(({ partition }) => {\n            if (!transactionTopicPartitions[topic][partition]) {\n              newTopicPartitions[topic] = newTopicPartitions[topic] || []\n              newTopicPartitions[topic].push(partition)\n            }\n          })\n        })\n\n        const topics = Object.keys(newTopicPartitions).map(topic => ({\n          topic,\n          partitions: newTopicPartitions[topic],\n        }))\n\n        if (topics.length) {\n          const broker = await findTransactionCoordinator()\n          await broker.addPartitionsToTxn({ transactionalId, producerId, producerEpoch, topics })\n        }\n\n        topics.forEach(({ topic, partitions }) => {\n          partitions.forEach(partition => {\n            transactionTopicPartitions[topic][partition] = true\n          })\n        })\n      },\n\n      /**\n       * Commit the ongoing transaction\n       */\n      async commit() {\n        transactionalGuard()\n        stateMachine.transitionTo(STATES.COMMITTING)\n\n        if (!isOngoing()) {\n          logger.debug('No partitions or offsets registered, not sending EndTxn')\n\n          stateMachine.transitionTo(STATES.READY)\n          return\n        }\n\n        const broker = await findTransactionCoordinator()\n        await broker.endTxn({\n          producerId,\n          producerEpoch,\n          transactionalId,\n          transactionResult: true,\n        })\n\n        stateMachine.transitionTo(STATES.READY)\n      },\n\n      /**\n       * Abort the ongoing transaction\n       */\n      async abort() {\n        transactionalGuard()\n        stateMachine.transitionTo(STATES.ABORTING)\n\n        if (!isOngoing()) {\n          logger.debug('No partitions or offsets registered, not sending EndTxn')\n\n          stateMachine.transitionTo(STATES.READY)\n          return\n        }\n\n        const broker = await findTransactionCoordinator()\n        await broker.endTxn({\n          producerId,\n          producerEpoch,\n          transactionalId,\n          transactionResult: false,\n        })\n\n        stateMachine.transitionTo(STATES.READY)\n      },\n\n      /**\n       * Whether the producer id has already been initialized\n       */\n      isInitialized() {\n        return producerId !== NO_PRODUCER_ID\n      },\n\n      isTransactional() {\n        return transactional\n      },\n\n      isInTransaction() {\n        return stateMachine.state() === STATES.TRANSACTING\n      },\n\n      async acquireBrokerLock(broker) {\n        if (this.isInitialized()) {\n          brokerMutexLocks[broker.nodeId] =\n            brokerMutexLocks[broker.nodeId] || new Lock({ timeout: 0xffff })\n          await brokerMutexLocks[broker.nodeId].acquire()\n        }\n      },\n\n      releaseBrokerLock(broker) {\n        if (this.isInitialized()) brokerMutexLocks[broker.nodeId].release()\n      },\n\n      /**\n       * Mark the provided offsets as participating in the transaction for the given consumer group.\n       *\n       * This allows us to commit an offset as consumed only if the transaction passes.\n       * @param {string} consumerGroupId The unique group identifier\n       * @param {OffsetCommitTopic[]} topics The unique group identifier\n       * @returns {Promise}\n       *\n       * @typedef {Object} OffsetCommitTopic\n       * @property {string} topic\n       * @property {OffsetCommitTopicPartition[]} partitions\n       *\n       * @typedef {Object} OffsetCommitTopicPartition\n       * @property {number} partition\n       * @property {number} offset\n       */\n      async sendOffsets({ consumerGroupId, topics }) {\n        assert(consumerGroupId, 'Missing consumerGroupId')\n        assert(topics, 'Missing offset topics')\n\n        const transactionCoordinator = await findTransactionCoordinator()\n\n        // Do we need to add offsets if we've already done so for this consumer group?\n        await transactionCoordinator.addOffsetsToTxn({\n          transactionalId,\n          producerId,\n          producerEpoch,\n          groupId: consumerGroupId,\n        })\n\n        hasOffsetsAddedToTransaction = true\n\n        let groupCoordinator = await cluster.findGroupCoordinator({\n          groupId: consumerGroupId,\n          coordinatorType: COORDINATOR_TYPES.GROUP,\n        })\n\n        return retrier(async (bail, retryCount, retryTime) => {\n          try {\n            await groupCoordinator.txnOffsetCommit({\n              transactionalId,\n              producerId,\n              producerEpoch,\n              groupId: consumerGroupId,\n              topics,\n            })\n          } catch (e) {\n            if (COMMIT_RETRIABLE_PROTOCOL_ERRORS.includes(e.type)) {\n              logger.debug('Group coordinator is not ready yet, retrying', {\n                error: e.message,\n                stack: e.stack,\n                transactionalId,\n                retryCount,\n                retryTime,\n              })\n\n              throw e\n            }\n\n            if (\n              COMMIT_STALE_COORDINATOR_PROTOCOL_ERRORS.includes(e.type) ||\n              e.code === 'ECONNREFUSED'\n            ) {\n              logger.debug(\n                'Invalid group coordinator, finding new group coordinator and retrying',\n                {\n                  error: e.message,\n                  stack: e.stack,\n                  transactionalId,\n                  retryCount,\n                  retryTime,\n                }\n              )\n\n              groupCoordinator = await cluster.findGroupCoordinator({\n                groupId: consumerGroupId,\n                coordinatorType: COORDINATOR_TYPES.GROUP,\n              })\n\n              throw e\n            }\n\n            bail(e)\n          }\n        })\n      },\n    },\n\n    /**\n     * Transaction state guards\n     */\n    {\n      initProducerId: { legalStates: [STATES.UNINITIALIZED, STATES.READY] },\n      beginTransaction: { legalStates: [STATES.READY], async: false },\n      addPartitionsToTransaction: { legalStates: [STATES.TRANSACTING] },\n      sendOffsets: { legalStates: [STATES.TRANSACTING] },\n      commit: { legalStates: [STATES.TRANSACTING] },\n      abort: { legalStates: [STATES.TRANSACTING] },\n    }\n  )\n\n  return eosManager\n}\n", "module.exports = ({ topic, partitionMetadata, messages, partitioner }) => {\n  if (partitionMetadata.length === 0) {\n    return {}\n  }\n\n  return messages.reduce((result, message) => {\n    const partition = partitioner({ topic, partitionMetadata, message })\n    const current = result[partition] || []\n    return Object.assign(result, { [partition]: [...current, message] })\n  }, {})\n}\n", "module.exports = topicDataForBroker => {\n  return topicDataForBroker.map(\n    ({ topic, partitions, messagesPerPartition, sequencePerPartition }) => ({\n      topic,\n      partitions: partitions.map(partition => ({\n        partition,\n        messages: messagesPerPartition[partition],\n      })),\n    })\n  )\n}\n", "module.exports = ({ topics }) =>\n  topics.flatMap(({ topicName, partitions }) =>\n    partitions.map(partition => ({ topicName, ...partition }))\n  )\n", "const { KafkaJSMetadataNotLoaded } = require('../errors')\nconst { staleMetadata } = require('../protocol/error')\nconst groupMessagesPerPartition = require('./groupMessagesPerPartition')\nconst createTopicData = require('./createTopicData')\nconst responseSerializer = require('./responseSerializer')\n\nconst { keys } = Object\n\n/**\n * @param {Object} options\n * @param {import(\"../../types\").Logger} options.logger\n * @param {import(\"../../types\").Cluster} options.cluster\n * @param {ReturnType<import(\"../../types\").ICustomPartitioner>} options.partitioner\n * @param {import(\"./eosManager\").EosManager} options.eosManager\n * @param {import(\"../retry\").Retrier} options.retrier\n */\nmodule.exports = ({ logger, cluster, partitioner, eosManager, retrier }) => {\n  return async ({ acks, timeout, compression, topicMessages }) => {\n    /** @type {Map<import(\"../../types\").Broker, any[]>} */\n    const responsePerBroker = new Map()\n\n    /** @param {Map<import(\"../../types\").Broker, any[]>} responsePerBroker */\n    const createProducerRequests = async responsePerBroker => {\n      const topicMetadata = new Map()\n\n      await cluster.refreshMetadataIfNecessary()\n\n      for (const { topic, messages } of topicMessages) {\n        const partitionMetadata = cluster.findTopicPartitionMetadata(topic)\n\n        if (partitionMetadata.length === 0) {\n          logger.debug('Producing to topic without metadata', {\n            topic,\n            targetTopics: Array.from(cluster.targetTopics),\n          })\n\n          throw new KafkaJSMetadataNotLoaded('Producing to topic without metadata')\n        }\n\n        const messagesPerPartition = groupMessagesPerPartition({\n          topic,\n          partitionMetadata,\n          messages,\n          partitioner,\n        })\n\n        const partitions = keys(messagesPerPartition)\n        const partitionsPerLeader = cluster.findLeaderForPartitions(topic, partitions)\n        const leaders = keys(partitionsPerLeader)\n\n        topicMetadata.set(topic, {\n          partitionsPerLeader,\n          messagesPerPartition,\n        })\n\n        for (const nodeId of leaders) {\n          const broker = await cluster.findBroker({ nodeId })\n          if (!responsePerBroker.has(broker)) {\n            responsePerBroker.set(broker, null)\n          }\n        }\n      }\n\n      const brokers = Array.from(responsePerBroker.keys())\n      const brokersWithoutResponse = brokers.filter(broker => !responsePerBroker.get(broker))\n\n      return brokersWithoutResponse.map(async broker => {\n        const entries = Array.from(topicMetadata.entries())\n        const topicDataForBroker = entries\n          .filter(([_, { partitionsPerLeader }]) => !!partitionsPerLeader[broker.nodeId])\n          .map(([topic, { partitionsPerLeader, messagesPerPartition, sequencePerPartition }]) => ({\n            topic,\n            partitions: partitionsPerLeader[broker.nodeId],\n            messagesPerPartition,\n          }))\n\n        const topicData = createTopicData(topicDataForBroker)\n\n        await eosManager.acquireBrokerLock(broker)\n        try {\n          if (eosManager.isTransactional()) {\n            await eosManager.addPartitionsToTransaction(topicData)\n          }\n\n          topicData.forEach(({ topic, partitions }) => {\n            partitions.forEach(entry => {\n              entry['firstSequence'] = eosManager.getSequence(topic, entry.partition)\n              eosManager.updateSequence(topic, entry.partition, entry.messages.length)\n            })\n          })\n\n          let response\n          try {\n            response = await broker.produce({\n              transactionalId: eosManager.isTransactional()\n                ? eosManager.getTransactionalId()\n                : undefined,\n              producerId: eosManager.getProducerId(),\n              producerEpoch: eosManager.getProducerEpoch(),\n              acks,\n              timeout,\n              compression,\n              topicData,\n            })\n          } catch (e) {\n            topicData.forEach(({ topic, partitions }) => {\n              partitions.forEach(entry => {\n                eosManager.updateSequence(topic, entry.partition, -entry.messages.length)\n              })\n            })\n            throw e\n          }\n\n          const expectResponse = acks !== 0\n          const formattedResponse = expectResponse ? responseSerializer(response) : []\n\n          responsePerBroker.set(broker, formattedResponse)\n        } catch (e) {\n          responsePerBroker.delete(broker)\n          throw e\n        } finally {\n          await eosManager.releaseBrokerLock(broker)\n        }\n      })\n    }\n\n    return retrier(async (bail, retryCount, retryTime) => {\n      const topics = topicMessages.map(({ topic }) => topic)\n      await cluster.addMultipleTargetTopics(topics)\n\n      try {\n        const requests = await createProducerRequests(responsePerBroker)\n        await Promise.all(requests)\n        return Array.from(responsePerBroker.values()).flat()\n      } catch (e) {\n        if (e.name === 'KafkaJSConnectionClosedError') {\n          cluster.removeBroker({ host: e.host, port: e.port })\n        }\n\n        if (!cluster.isConnected()) {\n          logger.debug(`Cluster has disconnected, reconnecting: ${e.message}`, {\n            retryCount,\n            retryTime,\n          })\n          await cluster.connect()\n          await cluster.refreshMetadata()\n          throw e\n        }\n\n        // This is necessary in case the metadata is stale and the number of partitions\n        // for this topic has increased in the meantime\n        if (\n          staleMetadata(e) ||\n          e.name === 'KafkaJSMetadataNotLoaded' ||\n          e.name === 'KafkaJSConnectionError' ||\n          e.name === 'KafkaJSConnectionClosedError' ||\n          (e.name === 'KafkaJSProtocolError' && e.retriable)\n        ) {\n          logger.error(`Failed to send messages: ${e.message}`, { retryCount, retryTime })\n          await cluster.refreshMetadata()\n          throw e\n        }\n\n        logger.error(`${e.message}`, { retryCount, retryTime })\n        if (e.retriable) throw e\n        bail(e)\n      }\n    })\n  }\n}\n", "const createSendMessages = require('./sendMessages')\nconst { KafkaJSError, KafkaJSNonRetriableError } = require('../errors')\nconst { CONNECTION_STATUS } = require('../network/connectionStatus')\n\nmodule.exports = ({\n  logger,\n  cluster,\n  partitioner,\n  eosManager,\n  idempotent,\n  retrier,\n  getConnectionStatus,\n}) => {\n  const sendMessages = createSendMessages({\n    logger,\n    cluster,\n    retrier,\n    partitioner,\n    eosManager,\n  })\n\n  const validateConnectionStatus = () => {\n    const connectionStatus = getConnectionStatus()\n\n    switch (connectionStatus) {\n      case CONNECTION_STATUS.DISCONNECTING:\n        throw new KafkaJSNonRetriableError(\n          `The producer is disconnecting; therefore, it can't safely accept messages anymore`\n        )\n      case CONNECTION_STATUS.DISCONNECTED:\n        throw new KafkaJSError('The producer is disconnected')\n    }\n  }\n\n  /**\n   * @typedef {Object} TopicMessages\n   * @property {string} topic\n   * @property {Array} messages An array of objects with \"key\" and \"value\", example:\n   *                         [{ key: 'my-key', value: 'my-value'}]\n   *\n   * @typedef {Object} SendBatchRequest\n   * @property {Array<TopicMessages>} topicMessages\n   * @property {number} [acks=-1] Control the number of required acks.\n   *                           -1 = all replicas must acknowledge\n   *                            0 = no acknowledgments\n   *                            1 = only waits for the leader to acknowledge\n   *\n   * @property {number} [timeout=30000] The time to await a response in ms\n   * @property {Compression.Types} [compression=Compression.Types.None] Compression codec\n   *\n   * @param {SendBatchRequest}\n   * @returns {Promise}\n   */\n  const sendBatch = async ({ acks = -1, timeout, compression, topicMessages = [] }) => {\n    if (topicMessages.some(({ topic }) => !topic)) {\n      throw new KafkaJSNonRetriableError(`Invalid topic`)\n    }\n\n    if (idempotent && acks !== -1) {\n      throw new KafkaJSNonRetriableError(\n        `Not requiring ack for all messages invalidates the idempotent producer's EoS guarantees`\n      )\n    }\n\n    for (const { topic, messages } of topicMessages) {\n      if (!messages) {\n        throw new KafkaJSNonRetriableError(\n          `Invalid messages array [${messages}] for topic \"${topic}\"`\n        )\n      }\n\n      const messageWithoutValue = messages.find(message => message.value === undefined)\n      if (messageWithoutValue) {\n        throw new KafkaJSNonRetriableError(\n          `Invalid message without value for topic \"${topic}\": ${JSON.stringify(\n            messageWithoutValue\n          )}`\n        )\n      }\n    }\n\n    validateConnectionStatus()\n    const mergedTopicMessages = topicMessages.reduce((merged, { topic, messages }) => {\n      const index = merged.findIndex(({ topic: mergedTopic }) => topic === mergedTopic)\n\n      if (index === -1) {\n        merged.push({ topic, messages })\n      } else {\n        merged[index].messages = [...merged[index].messages, ...messages]\n      }\n\n      return merged\n    }, [])\n\n    return await sendMessages({\n      acks,\n      timeout,\n      compression,\n      topicMessages: mergedTopicMessages,\n    })\n  }\n\n  /**\n   * @param {ProduceRequest} ProduceRequest\n   * @returns {Promise}\n   *\n   * @typedef {Object} ProduceRequest\n   * @property {string} topic\n   * @property {Array} messages An array of objects with \"key\" and \"value\", example:\n   *                         [{ key: 'my-key', value: 'my-value'}]\n   * @property {number} [acks=-1] Control the number of required acks.\n   *                           -1 = all replicas must acknowledge\n   *                            0 = no acknowledgments\n   *                            1 = only waits for the leader to acknowledge\n   * @property {number} [timeout=30000] The time to await a response in ms\n   * @property {Compression.Types} [compression=Compression.Types.None] Compression codec\n   */\n  const send = async ({ acks, timeout, compression, topic, messages }) => {\n    const topicMessage = { topic, messages }\n    return sendBatch({\n      acks,\n      timeout,\n      compression,\n      topicMessages: [topicMessage],\n    })\n  }\n\n  return {\n    send,\n    sendBatch,\n  }\n}\n", "const { keys } = Object\nmodule.exports = object =>\n  keys(object).reduce((result, key) => ({ ...result, [object[key]]: key }), {})\n", "const swapObject = require('../utils/swapObject')\nconst networkEvents = require('../network/instrumentationEvents')\nconst InstrumentationEventType = require('../instrumentation/eventType')\nconst producerType = InstrumentationEventType('producer')\n\nconst events = {\n  CONNECT: producerType('connect'),\n  DISCONNECT: producerType('disconnect'),\n  REQUEST: producerType(networkEvents.NETWORK_REQUEST),\n  REQUEST_TIMEOUT: producerType(networkEvents.NETWORK_REQUEST_TIMEOUT),\n  REQUEST_QUEUE_SIZE: producerType(networkEvents.NETWORK_REQUEST_QUEUE_SIZE),\n}\n\nconst wrappedEvents = {\n  [events.REQUEST]: networkEvents.NETWORK_REQUEST,\n  [events.REQUEST_TIMEOUT]: networkEvents.NETWORK_REQUEST_TIMEOUT,\n  [events.REQUEST_QUEUE_SIZE]: networkEvents.NETWORK_REQUEST_QUEUE_SIZE,\n}\n\nconst reversedWrappedEvents = swapObject(wrappedEvents)\nconst unwrap = eventName => wrappedEvents[eventName] || eventName\nconst wrap = eventName => reversedWrappedEvents[eventName] || eventName\n\nmodule.exports = {\n  events,\n  wrap,\n  unwrap,\n}\n", "const createRetry = require('../retry')\nconst { CONNECTION_STATUS } = require('../network/connectionStatus')\nconst { DefaultPartitioner } = require('./partitioners/')\nconst InstrumentationEventEmitter = require('../instrumentation/emitter')\nconst createEosManager = require('./eosManager')\nconst createMessageProducer = require('./messageProducer')\nconst { events, wrap: wrapEvent, unwrap: unwrapEvent } = require('./instrumentationEvents')\nconst { KafkaJSNonRetriableError } = require('../errors')\n\nconst { values, keys } = Object\nconst eventNames = values(events)\nconst eventKeys = keys(events)\n  .map(key => `producer.events.${key}`)\n  .join(', ')\n\nconst { CONNECT, DISCONNECT } = events\n\n/**\n *\n * @param {Object} params\n * @param {import('../../types').Cluster} params.cluster\n * @param {import('../../types').Logger} params.logger\n * @param {import('../../types').ICustomPartitioner} [params.createPartitioner]\n * @param {import('../../types').RetryOptions} [params.retry]\n * @param {boolean} [params.idempotent]\n * @param {string} [params.transactionalId]\n * @param {number} [params.transactionTimeout]\n * @param {InstrumentationEventEmitter} [params.instrumentationEmitter]\n *\n * @returns {import('../../types').Producer}\n */\nmodule.exports = ({\n  cluster,\n  logger: rootLogger,\n  createPartitioner = DefaultPartitioner,\n  retry,\n  idempotent = false,\n  transactionalId,\n  transactionTimeout,\n  instrumentationEmitter: rootInstrumentationEmitter,\n}) => {\n  let connectionStatus = CONNECTION_STATUS.DISCONNECTED\n  retry = retry || { retries: idempotent ? Number.MAX_SAFE_INTEGER : 5 }\n\n  if (idempotent && retry.retries < 1) {\n    throw new KafkaJSNonRetriableError(\n      'Idempotent producer must allow retries to protect against transient errors'\n    )\n  }\n\n  const logger = rootLogger.namespace('Producer')\n\n  if (idempotent && retry.retries < Number.MAX_SAFE_INTEGER) {\n    logger.warn('Limiting retries for the idempotent producer may invalidate EoS guarantees')\n  }\n\n  const partitioner = createPartitioner()\n  const retrier = createRetry(Object.assign({}, cluster.retry, retry))\n  const instrumentationEmitter = rootInstrumentationEmitter || new InstrumentationEventEmitter()\n  const idempotentEosManager = createEosManager({\n    logger,\n    cluster,\n    transactionTimeout,\n    transactional: false,\n    transactionalId,\n  })\n\n  const { send, sendBatch } = createMessageProducer({\n    logger,\n    cluster,\n    partitioner,\n    eosManager: idempotentEosManager,\n    idempotent,\n    retrier,\n    getConnectionStatus: () => connectionStatus,\n  })\n\n  let transactionalEosManager\n\n  /** @type {import(\"../../types\").Producer[\"on\"]} */\n  const on = (eventName, listener) => {\n    if (!eventNames.includes(eventName)) {\n      throw new KafkaJSNonRetriableError(`Event name should be one of ${eventKeys}`)\n    }\n\n    return instrumentationEmitter.addListener(unwrapEvent(eventName), event => {\n      event.type = wrapEvent(event.type)\n      Promise.resolve(listener(event)).catch(e => {\n        logger.error(`Failed to execute listener: ${e.message}`, {\n          eventName,\n          stack: e.stack,\n        })\n      })\n    })\n  }\n\n  /**\n   * Begin a transaction. The returned object contains methods to send messages\n   * to the transaction and end the transaction by committing or aborting.\n   *\n   * Only messages sent on the transaction object will participate in the transaction.\n   *\n   * Calling any of the transactional methods after the transaction has ended\n   * will raise an exception (use `isActive` to ascertain if ended).\n   * @returns {Promise<Transaction>}\n   *\n   * @typedef {Object} Transaction\n   * @property {Function} send  Identical to the producer \"send\" method\n   * @property {Function} sendBatch Identical to the producer \"sendBatch\" method\n   * @property {Function} abort Abort the transaction\n   * @property {Function} commit  Commit the transaction\n   * @property {Function} isActive  Whether the transaction is active\n   */\n  const transaction = async () => {\n    if (!transactionalId) {\n      throw new KafkaJSNonRetriableError('Must provide transactional id for transactional producer')\n    }\n\n    let transactionDidEnd = false\n    transactionalEosManager =\n      transactionalEosManager ||\n      createEosManager({\n        logger,\n        cluster,\n        transactionTimeout,\n        transactional: true,\n        transactionalId,\n      })\n\n    if (transactionalEosManager.isInTransaction()) {\n      throw new KafkaJSNonRetriableError(\n        'There is already an ongoing transaction for this producer. Please end the transaction before beginning another.'\n      )\n    }\n\n    // We only initialize the producer id once\n    if (!transactionalEosManager.isInitialized()) {\n      await transactionalEosManager.initProducerId()\n    }\n    transactionalEosManager.beginTransaction()\n\n    const { send: sendTxn, sendBatch: sendBatchTxn } = createMessageProducer({\n      logger,\n      cluster,\n      partitioner,\n      retrier,\n      eosManager: transactionalEosManager,\n      idempotent: true,\n      getConnectionStatus: () => connectionStatus,\n    })\n\n    const isActive = () => transactionalEosManager.isInTransaction() && !transactionDidEnd\n\n    const transactionGuard = fn => (...args) => {\n      if (!isActive()) {\n        return Promise.reject(\n          new KafkaJSNonRetriableError('Cannot continue to use transaction once ended')\n        )\n      }\n\n      return fn(...args)\n    }\n\n    return {\n      sendBatch: transactionGuard(sendBatchTxn),\n      send: transactionGuard(sendTxn),\n      /**\n       * Abort the ongoing transaction.\n       *\n       * @throws {KafkaJSNonRetriableError} If transaction has ended\n       */\n      abort: transactionGuard(async () => {\n        await transactionalEosManager.abort()\n        transactionDidEnd = true\n      }),\n      /**\n       * Commit the ongoing transaction.\n       *\n       * @throws {KafkaJSNonRetriableError} If transaction has ended\n       */\n      commit: transactionGuard(async () => {\n        await transactionalEosManager.commit()\n        transactionDidEnd = true\n      }),\n      /**\n       * Sends a list of specified offsets to the consumer group coordinator, and also marks those offsets as part of the current transaction.\n       *\n       * @throws {KafkaJSNonRetriableError} If transaction has ended\n       */\n      sendOffsets: transactionGuard(async ({ consumerGroupId, topics }) => {\n        await transactionalEosManager.sendOffsets({ consumerGroupId, topics })\n\n        for (const topicOffsets of topics) {\n          const { topic, partitions } = topicOffsets\n          for (const { partition, offset } of partitions) {\n            cluster.markOffsetAsCommitted({\n              groupId: consumerGroupId,\n              topic,\n              partition,\n              offset,\n            })\n          }\n        }\n      }),\n      isActive,\n    }\n  }\n\n  /**\n   * @returns {Object} logger\n   */\n  const getLogger = () => logger\n\n  return {\n    /**\n     * @returns {Promise}\n     */\n    connect: async () => {\n      await cluster.connect()\n      connectionStatus = CONNECTION_STATUS.CONNECTED\n      instrumentationEmitter.emit(CONNECT)\n\n      if (idempotent && !idempotentEosManager.isInitialized()) {\n        await idempotentEosManager.initProducerId()\n      }\n    },\n    /**\n     * @return {Promise}\n     */\n    disconnect: async () => {\n      connectionStatus = CONNECTION_STATUS.DISCONNECTING\n      await cluster.disconnect()\n      connectionStatus = CONNECTION_STATUS.DISCONNECTED\n      instrumentationEmitter.emit(DISCONNECT)\n    },\n    isIdempotent: () => {\n      return idempotent\n    },\n    events,\n    on,\n    send,\n    sendBatch,\n    transaction,\n    logger: getLogger,\n  }\n}\n", "module.exports = timeInMs =>\n  new Promise(resolve => {\n    setTimeout(resolve, timeInMs)\n  })\n", "const Long = require('../../utils/long')\n\nmodule.exports = offset => (!offset && offset !== 0) || Long.fromValue(offset).isNegative()\n", "const isInvalidOffset = require('./isInvalidOffset')\nconst { keys, assign } = Object\n\nconst indexPartitions = (obj, { partition, offset }) => assign(obj, { [partition]: offset })\nconst indexTopics = (obj, { topic, partitions }) =>\n  assign(obj, { [topic]: partitions.reduce(indexPartitions, {}) })\n\nmodule.exports = (consumerOffsets, topicOffsets) => {\n  const indexedConsumerOffsets = consumerOffsets.reduce(indexTopics, {})\n  const indexedTopicOffsets = topicOffsets.reduce(indexTopics, {})\n\n  return keys(indexedConsumerOffsets).map(topic => {\n    const partitions = indexedConsumerOffsets[topic]\n    return {\n      topic,\n      partitions: keys(partitions).map(partition => {\n        const offset = partitions[partition]\n        const resolvedOffset = isInvalidOffset(offset)\n          ? indexedTopicOffsets[topic][partition]\n          : offset\n\n        return { partition: Number(partition), offset: resolvedOffset }\n      }),\n    }\n  })\n}\n", "const swapObject = require('../utils/swapObject')\nconst InstrumentationEventType = require('../instrumentation/eventType')\nconst networkEvents = require('../network/instrumentationEvents')\nconst consumerType = InstrumentationEventType('consumer')\n\n/** @type {import('types').ConsumerEvents} */\nconst events = {\n  HEARTBEAT: consumerType('heartbeat'),\n  COMMIT_OFFSETS: consumerType('commit_offsets'),\n  GROUP_JOIN: consumerType('group_join'),\n  FETCH: consumerType('fetch'),\n  FETCH_START: consumerType('fetch_start'),\n  START_BATCH_PROCESS: consumerType('start_batch_process'),\n  END_BATCH_PROCESS: consumerType('end_batch_process'),\n  CONNECT: consumerType('connect'),\n  DISCONNECT: consumerType('disconnect'),\n  STOP: consumerType('stop'),\n  CRASH: consumerType('crash'),\n  REBALANCING: consumerType('rebalancing'),\n  RECEIVED_UNSUBSCRIBED_TOPICS: consumerType('received_unsubscribed_topics'),\n  REQUEST: consumerType(networkEvents.NETWORK_REQUEST),\n  REQUEST_TIMEOUT: consumerType(networkEvents.NETWORK_REQUEST_TIMEOUT),\n  REQUEST_QUEUE_SIZE: consumerType(networkEvents.NETWORK_REQUEST_QUEUE_SIZE),\n}\n\nconst wrappedEvents = {\n  [events.REQUEST]: networkEvents.NETWORK_REQUEST,\n  [events.REQUEST_TIMEOUT]: networkEvents.NETWORK_REQUEST_TIMEOUT,\n  [events.REQUEST_QUEUE_SIZE]: networkEvents.NETWORK_REQUEST_QUEUE_SIZE,\n}\n\nconst reversedWrappedEvents = swapObject(wrappedEvents)\nconst unwrap = eventName => wrappedEvents[eventName] || eventName\nconst wrap = eventName => reversedWrappedEvents[eventName] || eventName\n\nmodule.exports = {\n  events,\n  wrap,\n  unwrap,\n}\n", "const Long = require('../../utils/long')\nconst isInvalidOffset = require('./isInvalidOffset')\nconst initializeConsumerOffsets = require('./initializeConsumerOffsets')\nconst {\n  events: { COMMIT_OFFSETS },\n} = require('../instrumentationEvents')\n\nconst { keys, assign } = Object\nconst indexTopics = topics => topics.reduce((obj, topic) => assign(obj, { [topic]: {} }), {})\n\nconst PRIVATE = {\n  COMMITTED_OFFSETS: Symbol('private:OffsetManager:committedOffsets'),\n}\nmodule.exports = class OffsetManager {\n  /**\n   * @param {Object} options\n   * @param {import(\"../../../types\").Cluster} options.cluster\n   * @param {import(\"../../../types\").Broker} options.coordinator\n   * @param {import(\"../../../types\").IMemberAssignment} options.memberAssignment\n   * @param {boolean} options.autoCommit\n   * @param {number | null} options.autoCommitInterval\n   * @param {number | null} options.autoCommitThreshold\n   * @param {{[topic: string]: { fromBeginning: boolean }}} options.topicConfigurations\n   * @param {import(\"../../instrumentation/emitter\")} options.instrumentationEmitter\n   * @param {string} options.groupId\n   * @param {number} options.generationId\n   * @param {string} options.memberId\n   */\n  constructor({\n    cluster,\n    coordinator,\n    memberAssignment,\n    autoCommit,\n    autoCommitInterval,\n    autoCommitThreshold,\n    topicConfigurations,\n    instrumentationEmitter,\n    groupId,\n    generationId,\n    memberId,\n  }) {\n    this.cluster = cluster\n    this.coordinator = coordinator\n\n    // memberAssignment format:\n    // {\n    //   'topic1': [0, 1, 2, 3],\n    //   'topic2': [0, 1, 2, 3, 4, 5],\n    // }\n    this.memberAssignment = memberAssignment\n\n    this.topicConfigurations = topicConfigurations\n    this.instrumentationEmitter = instrumentationEmitter\n    this.groupId = groupId\n    this.generationId = generationId\n    this.memberId = memberId\n\n    this.autoCommit = autoCommit\n    this.autoCommitInterval = autoCommitInterval\n    this.autoCommitThreshold = autoCommitThreshold\n    this.lastCommit = Date.now()\n\n    this.topics = keys(memberAssignment)\n    this.clearAllOffsets()\n  }\n\n  /**\n   * @param {string} topic\n   * @param {number} partition\n   * @returns {Long}\n   */\n  nextOffset(topic, partition) {\n    if (!this.resolvedOffsets[topic][partition]) {\n      this.resolvedOffsets[topic][partition] = this.committedOffsets()[topic][partition]\n    }\n\n    let offset = this.resolvedOffsets[topic][partition]\n    if (isInvalidOffset(offset)) {\n      offset = '0'\n    }\n\n    return Long.fromValue(offset)\n  }\n\n  /**\n   * @returns {Promise<import(\"../../../types\").Broker>}\n   */\n  async getCoordinator() {\n    if (!this.coordinator.isConnected()) {\n      this.coordinator = await this.cluster.findBroker(this.coordinator)\n    }\n\n    return this.coordinator\n  }\n\n  /**\n   * @param {import(\"../../../types\").TopicPartition} topicPartition\n   */\n  resetOffset({ topic, partition }) {\n    this.resolvedOffsets[topic][partition] = this.committedOffsets()[topic][partition]\n  }\n\n  /**\n   * @param {import(\"../../../types\").TopicPartitionOffset} topicPartitionOffset\n   */\n  resolveOffset({ topic, partition, offset }) {\n    this.resolvedOffsets[topic][partition] = Long.fromValue(offset)\n      .add(1)\n      .toString()\n  }\n\n  /**\n   * @returns {Long}\n   */\n  countResolvedOffsets() {\n    const committedOffsets = this.committedOffsets()\n\n    const subtractOffsets = (resolvedOffset, committedOffset) => {\n      const resolvedOffsetLong = Long.fromValue(resolvedOffset)\n      return isInvalidOffset(committedOffset)\n        ? resolvedOffsetLong\n        : resolvedOffsetLong.subtract(Long.fromValue(committedOffset))\n    }\n\n    const subtractPartitionOffsets = (resolvedTopicOffsets, committedTopicOffsets) =>\n      keys(resolvedTopicOffsets).map(partition =>\n        subtractOffsets(resolvedTopicOffsets[partition], committedTopicOffsets[partition])\n      )\n\n    const subtractTopicOffsets = topic =>\n      subtractPartitionOffsets(this.resolvedOffsets[topic], committedOffsets[topic])\n\n    const offsetsDiff = this.topics.flatMap(subtractTopicOffsets)\n    return offsetsDiff.reduce((sum, offset) => sum.add(offset), Long.fromValue(0))\n  }\n\n  /**\n   * @param {import(\"../../../types\").TopicPartition} topicPartition\n   */\n  async setDefaultOffset({ topic, partition }) {\n    const { groupId, generationId, memberId } = this\n    const defaultOffset = this.cluster.defaultOffset(this.topicConfigurations[topic])\n    const coordinator = await this.getCoordinator()\n\n    await coordinator.offsetCommit({\n      groupId,\n      memberId,\n      groupGenerationId: generationId,\n      topics: [\n        {\n          topic,\n          partitions: [{ partition, offset: defaultOffset }],\n        },\n      ],\n    })\n\n    this.clearOffsets({ topic, partition })\n  }\n\n  /**\n   * Commit the given offset to the topic/partition. If the consumer isn't assigned to the given\n   * topic/partition this method will be a NO-OP.\n   *\n   * @param {import(\"../../../types\").TopicPartitionOffset} topicPartitionOffset\n   */\n  async seek({ topic, partition, offset }) {\n    if (!this.memberAssignment[topic] || !this.memberAssignment[topic].includes(partition)) {\n      return\n    }\n\n    if (!this.autoCommit) {\n      this.resolveOffset({\n        topic,\n        partition,\n        offset: Long.fromValue(offset)\n          .subtract(1)\n          .toString(),\n      })\n      return\n    }\n\n    const { groupId, generationId, memberId } = this\n    const coordinator = await this.getCoordinator()\n\n    await coordinator.offsetCommit({\n      groupId,\n      memberId,\n      groupGenerationId: generationId,\n      topics: [\n        {\n          topic,\n          partitions: [{ partition, offset }],\n        },\n      ],\n    })\n\n    this.clearOffsets({ topic, partition })\n  }\n\n  async commitOffsetsIfNecessary() {\n    const now = Date.now()\n\n    const timeoutReached =\n      this.autoCommitInterval != null && now >= this.lastCommit + this.autoCommitInterval\n\n    const thresholdReached =\n      this.autoCommitThreshold != null &&\n      this.countResolvedOffsets().gte(Long.fromValue(this.autoCommitThreshold))\n\n    if (timeoutReached || thresholdReached) {\n      return this.commitOffsets()\n    }\n  }\n\n  /**\n   * Return all locally resolved offsets which are not marked as committed, by topic-partition.\n   * @returns {import('../../../types').OffsetsByTopicPartition}\n   */\n  uncommittedOffsets() {\n    const offsets = topic => keys(this.resolvedOffsets[topic])\n    const emptyPartitions = ({ partitions }) => partitions.length > 0\n    const toPartitions = topic => partition => ({\n      partition,\n      offset: this.resolvedOffsets[topic][partition],\n    })\n    const changedOffsets = topic => ({ partition, offset }) => {\n      return (\n        offset !== this.committedOffsets()[topic][partition] &&\n        Long.fromValue(offset).greaterThanOrEqual(0)\n      )\n    }\n\n    // Select and format updated partitions\n    const topicsWithPartitionsToCommit = this.topics\n      .map(topic => ({\n        topic,\n        partitions: offsets(topic)\n          .map(toPartitions(topic))\n          .filter(changedOffsets(topic)),\n      }))\n      .filter(emptyPartitions)\n\n    return { topics: topicsWithPartitionsToCommit }\n  }\n\n  async commitOffsets(offsets = {}) {\n    const { groupId, generationId, memberId } = this\n    const { topics = this.uncommittedOffsets().topics } = offsets\n\n    if (topics.length === 0) {\n      this.lastCommit = Date.now()\n      return\n    }\n\n    const payload = {\n      groupId,\n      memberId,\n      groupGenerationId: generationId,\n      topics,\n    }\n\n    try {\n      const coordinator = await this.getCoordinator()\n      await coordinator.offsetCommit(payload)\n      this.instrumentationEmitter.emit(COMMIT_OFFSETS, payload)\n\n      // Update local reference of committed offsets\n      topics.forEach(({ topic, partitions }) => {\n        const updatedOffsets = partitions.reduce(\n          (obj, { partition, offset }) => assign(obj, { [partition]: offset }),\n          {}\n        )\n\n        this[PRIVATE.COMMITTED_OFFSETS][topic] = assign(\n          {},\n          this.committedOffsets()[topic],\n          updatedOffsets\n        )\n      })\n\n      this.lastCommit = Date.now()\n    } catch (e) {\n      // metadata is stale, the coordinator has changed due to a restart or\n      // broker reassignment\n      if (e.type === 'NOT_COORDINATOR_FOR_GROUP') {\n        await this.cluster.refreshMetadata()\n      }\n\n      throw e\n    }\n  }\n\n  async resolveOffsets() {\n    const { groupId } = this\n    const invalidOffset = topic => partition => {\n      return isInvalidOffset(this.committedOffsets()[topic][partition])\n    }\n\n    const pendingPartitions = this.topics\n      .map(topic => ({\n        topic,\n        partitions: this.memberAssignment[topic]\n          .filter(invalidOffset(topic))\n          .map(partition => ({ partition })),\n      }))\n      .filter(t => t.partitions.length > 0)\n\n    if (pendingPartitions.length === 0) {\n      return\n    }\n\n    const coordinator = await this.getCoordinator()\n    const { responses: consumerOffsets } = await coordinator.offsetFetch({\n      groupId,\n      topics: pendingPartitions,\n    })\n\n    const unresolvedPartitions = consumerOffsets.map(({ topic, partitions }) =>\n      assign(\n        {\n          topic,\n          partitions: partitions\n            .filter(({ offset }) => isInvalidOffset(offset))\n            .map(({ partition }) => assign({ partition })),\n        },\n        this.topicConfigurations[topic]\n      )\n    )\n\n    const indexPartitions = (obj, { partition, offset }) => {\n      return assign(obj, { [partition]: offset })\n    }\n\n    const hasUnresolvedPartitions = () => unresolvedPartitions.some(t => t.partitions.length > 0)\n\n    let offsets = consumerOffsets\n    if (hasUnresolvedPartitions()) {\n      const topicOffsets = await this.cluster.fetchTopicsOffset(unresolvedPartitions)\n      offsets = initializeConsumerOffsets(consumerOffsets, topicOffsets)\n    }\n\n    offsets.forEach(({ topic, partitions }) => {\n      this.committedOffsets()[topic] = partitions.reduce(indexPartitions, {\n        ...this.committedOffsets()[topic],\n      })\n    })\n  }\n\n  /**\n   * @private\n   * @param {import(\"../../../types\").TopicPartition} topicPartition\n   */\n  clearOffsets({ topic, partition }) {\n    delete this.committedOffsets()[topic][partition]\n    delete this.resolvedOffsets[topic][partition]\n  }\n\n  /**\n   * @private\n   */\n  clearAllOffsets() {\n    const committedOffsets = this.committedOffsets()\n\n    for (const topic in committedOffsets) {\n      delete committedOffsets[topic]\n    }\n\n    for (const topic of this.topics) {\n      committedOffsets[topic] = {}\n    }\n\n    this.resolvedOffsets = indexTopics(this.topics)\n  }\n\n  committedOffsets() {\n    if (!this[PRIVATE.COMMITTED_OFFSETS]) {\n      this[PRIVATE.COMMITTED_OFFSETS] = this.groupId\n        ? this.cluster.committedOffsets({ groupId: this.groupId })\n        : {}\n    }\n\n    return this[PRIVATE.COMMITTED_OFFSETS]\n  }\n}\n", "const Long = require('../utils/long')\nconst ABORTED_MESSAGE_KEY = Buffer.from([0, 0, 0, 0])\n\nconst isAbortMarker = ({ key }) => {\n  // Handle null/undefined keys.\n  if (!key) return false\n  // Cast key to buffer defensively\n  return Buffer.from(key).equals(ABORTED_MESSAGE_KEY)\n}\n\n/**\n * Remove messages marked as aborted according to the aborted transactions list.\n *\n * Start of an aborted transaction is determined by message offset.\n * End of an aborted transaction is determined by control messages.\n * @param {Message[]} messages\n * @param {Transaction[]} [abortedTransactions]\n * @returns {Message[]} Messages which did not participate in an aborted transaction\n *\n * @typedef {object} Message\n * @param {Buffer} key\n * @param {lastOffset} key  Int64\n * @param {RecordBatch}  batchContext\n *\n * @typedef {object} Transaction\n * @param {string} firstOffset  Int64\n * @param {string} producerId  Int64\n *\n * @typedef {object} RecordBatch\n * @param {string}  producerId  Int64\n * @param {boolean}  inTransaction\n */\nmodule.exports = ({ messages, abortedTransactions }) => {\n  const currentAbortedTransactions = new Map()\n\n  if (!abortedTransactions || !abortedTransactions.length) {\n    return messages\n  }\n\n  const remainingAbortedTransactions = [...abortedTransactions]\n\n  return messages.filter(message => {\n    // If the message offset is GTE the first offset of the next aborted transaction\n    // then we have stepped into an aborted transaction.\n    if (\n      remainingAbortedTransactions.length &&\n      Long.fromValue(message.offset).gte(remainingAbortedTransactions[0].firstOffset)\n    ) {\n      const { producerId } = remainingAbortedTransactions.shift()\n      currentAbortedTransactions.set(producerId, true)\n    }\n\n    const { producerId, inTransaction } = message.batchContext\n\n    if (isAbortMarker(message)) {\n      // Transaction is over, we no longer need to ignore messages from this producer\n      currentAbortedTransactions.delete(producerId)\n    } else if (currentAbortedTransactions.has(producerId) && inTransaction) {\n      return false\n    }\n\n    return true\n  })\n}\n", "const Long = require('../utils/long')\nconst filterAbortedMessages = require('./filterAbortedMessages')\n\n/**\n * A batch collects messages returned from a single fetch call.\n *\n * A batch could contain _multiple_ Kafka RecordBatches.\n */\nmodule.exports = class Batch {\n  constructor(topic, fetchedOffset, partitionData) {\n    this.fetchedOffset = fetchedOffset\n    const longFetchedOffset = Long.fromValue(this.fetchedOffset)\n    const { abortedTransactions, messages } = partitionData\n\n    this.topic = topic\n    this.partition = partitionData.partition\n    this.highWatermark = partitionData.highWatermark\n\n    this.rawMessages = messages\n    // Apparently fetch can return different offsets than the target offset provided to the fetch API.\n    // Discard messages that are not in the requested offset\n    // https://github.com/apache/kafka/blob/bf237fa7c576bd141d78fdea9f17f65ea269c290/clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java#L912\n    this.messagesWithinOffset = this.rawMessages.filter(message =>\n      Long.fromValue(message.offset).gte(longFetchedOffset)\n    )\n\n    // 1. Don't expose aborted messages\n    // 2. Don't expose control records\n    // @see https://kafka.apache.org/documentation/#controlbatch\n    this.messages = filterAbortedMessages({\n      messages: this.messagesWithinOffset,\n      abortedTransactions,\n    }).filter(message => !message.isControlRecord)\n  }\n\n  isEmpty() {\n    return this.messages.length === 0\n  }\n\n  isEmptyIncludingFiltered() {\n    return this.messagesWithinOffset.length === 0\n  }\n\n  /**\n   * If the batch contained raw messages (i.e was not truly empty) but all messages were filtered out due to\n   * log compaction, control records or other reasons\n   */\n  isEmptyDueToFiltering() {\n    return this.isEmpty() && this.rawMessages.length > 0\n  }\n\n  isEmptyControlRecord() {\n    return (\n      this.isEmpty() && this.messagesWithinOffset.some(({ isControlRecord }) => isControlRecord)\n    )\n  }\n\n  /**\n   * With compressed messages, it's possible for the returned messages to have offsets smaller than the starting offset.\n   * These messages will be filtered out (i.e. they are not even included in this.messagesWithinOffset)\n   * If these are the only messages, the batch will appear as an empty batch.\n   *\n   * isEmpty() and isEmptyIncludingFiltered() will always return true if the batch is empty,\n   * but this method will only return true if the batch is empty due to log compacted messages.\n   *\n   * @returns boolean True if the batch is empty, because of log compacted messages in the partition.\n   */\n  isEmptyDueToLogCompactedMessages() {\n    const hasMessages = this.rawMessages.length > 0\n    return hasMessages && this.isEmptyIncludingFiltered()\n  }\n\n  firstOffset() {\n    return this.isEmptyIncludingFiltered() ? null : this.messagesWithinOffset[0].offset\n  }\n\n  lastOffset() {\n    if (this.isEmptyDueToLogCompactedMessages()) {\n      return this.fetchedOffset\n    }\n\n    if (this.isEmptyIncludingFiltered()) {\n      return Long.fromValue(this.highWatermark)\n        .add(-1)\n        .toString()\n    }\n\n    return this.messagesWithinOffset[this.messagesWithinOffset.length - 1].offset\n  }\n\n  /**\n   * Returns the lag based on the last offset in the batch (also known as \"high\")\n   */\n  offsetLag() {\n    const lastOffsetOfPartition = Long.fromValue(this.highWatermark).add(-1)\n    const lastConsumedOffset = Long.fromValue(this.lastOffset())\n    return lastOffsetOfPartition.add(lastConsumedOffset.multiply(-1)).toString()\n  }\n\n  /**\n   * Returns the lag based on the first offset in the batch\n   */\n  offsetLagLow() {\n    if (this.isEmptyIncludingFiltered()) {\n      return '0'\n    }\n\n    const lastOffsetOfPartition = Long.fromValue(this.highWatermark).add(-1)\n    const firstConsumedOffset = Long.fromValue(this.firstOffset())\n    return lastOffsetOfPartition.add(firstConsumedOffset.multiply(-1)).toString()\n  }\n}\n", "module.exports = class SeekOffsets extends Map {\n  getKey(topic, partition) {\n    return JSON.stringify([topic, partition])\n  }\n\n  set(topic, partition, offset) {\n    const key = this.getKey(topic, partition)\n    super.set(key, offset)\n  }\n\n  has(topic, partition) {\n    const key = this.getKey(topic, partition)\n    return super.has(key)\n  }\n\n  pop(topic, partition) {\n    if (this.size === 0 || !this.has(topic, partition)) {\n      return\n    }\n\n    const key = this.getKey(topic, partition)\n    const offset = this.get(key)\n\n    this.delete(key)\n    return { topic, partition, offset }\n  }\n}\n", "const createState = topic => ({\n  topic,\n  paused: new Set(),\n  pauseAll: false,\n  resumed: new Set(),\n})\n\nmodule.exports = class SubscriptionState {\n  constructor() {\n    this.assignedPartitionsByTopic = {}\n    this.subscriptionStatesByTopic = {}\n  }\n\n  /**\n   * Replace the current assignment with a new set of assignments\n   *\n   * @param {Array<TopicPartitions>} topicPartitions Example: [{ topic: 'topic-name', partitions: [1, 2] }]\n   */\n  assign(topicPartitions = []) {\n    this.assignedPartitionsByTopic = topicPartitions.reduce(\n      (assigned, { topic, partitions = [] }) => {\n        return { ...assigned, [topic]: { topic, partitions } }\n      },\n      {}\n    )\n  }\n\n  /**\n   * @param {Array<TopicPartitions>} topicPartitions Example: [{ topic: 'topic-name', partitions: [1, 2] }]\n   */\n  pause(topicPartitions = []) {\n    topicPartitions.forEach(({ topic, partitions }) => {\n      const state = this.subscriptionStatesByTopic[topic] || createState(topic)\n\n      if (typeof partitions === 'undefined') {\n        state.paused.clear()\n        state.resumed.clear()\n        state.pauseAll = true\n      } else if (Array.isArray(partitions)) {\n        partitions.forEach(partition => {\n          state.paused.add(partition)\n          state.resumed.delete(partition)\n        })\n        state.pauseAll = false\n      }\n\n      this.subscriptionStatesByTopic[topic] = state\n    })\n  }\n\n  /**\n   * @param {Array<TopicPartitions>} topicPartitions Example: [{ topic: 'topic-name', partitions: [1, 2] }]\n   */\n  resume(topicPartitions = []) {\n    topicPartitions.forEach(({ topic, partitions }) => {\n      const state = this.subscriptionStatesByTopic[topic] || createState(topic)\n\n      if (typeof partitions === 'undefined') {\n        state.paused.clear()\n        state.resumed.clear()\n        state.pauseAll = false\n      } else if (Array.isArray(partitions)) {\n        partitions.forEach(partition => {\n          state.paused.delete(partition)\n\n          if (state.pauseAll) {\n            state.resumed.add(partition)\n          }\n        })\n      }\n\n      this.subscriptionStatesByTopic[topic] = state\n    })\n  }\n\n  /**\n   * @returns {Array<import(\"../../types\").TopicPartitions>} topicPartitions\n   * Example: [{ topic: 'topic-name', partitions: [1, 2] }]\n   */\n  assigned() {\n    return Object.values(this.assignedPartitionsByTopic).map(({ topic, partitions }) => ({\n      topic,\n      partitions: partitions.sort(),\n    }))\n  }\n\n  /**\n   * @returns {Array<import(\"../../types\").TopicPartitions>} topicPartitions\n   * Example: [{ topic: 'topic-name', partitions: [1, 2] }]\n   */\n  active() {\n    return Object.values(this.assignedPartitionsByTopic).map(({ topic, partitions }) => ({\n      topic,\n      partitions: partitions.filter(partition => !this.isPaused(topic, partition)).sort(),\n    }))\n  }\n\n  /**\n   * @returns {Array<import(\"../../types\").TopicPartitions>} topicPartitions\n   * Example: [{ topic: 'topic-name', partitions: [1, 2] }]\n   */\n  paused() {\n    return Object.values(this.assignedPartitionsByTopic)\n      .map(({ topic, partitions }) => ({\n        topic,\n        partitions: partitions.filter(partition => this.isPaused(topic, partition)).sort(),\n      }))\n      .filter(({ partitions }) => partitions.length !== 0)\n  }\n\n  isPaused(topic, partition) {\n    const state = this.subscriptionStatesByTopic[topic]\n\n    if (!state) {\n      return false\n    }\n\n    const partitionResumed = state.resumed.has(partition)\n    const partitionPaused = state.paused.has(partition)\n\n    return (state.pauseAll && !partitionResumed) || partitionPaused\n  }\n}\n", "const Encoder = require('../protocol/encoder')\nconst Decoder = require('../protocol/decoder')\n\nconst MemberMetadata = {\n  /**\n   * @param {Object} metadata\n   * @param {number} metadata.version\n   * @param {Array<string>} metadata.topics\n   * @param {Buffer} [metadata.userData=Buffer.alloc(0)]\n   *\n   * @returns Buffer\n   */\n  encode({ version, topics, userData = Buffer.alloc(0) }) {\n    return new Encoder()\n      .writeInt16(version)\n      .writeArray(topics)\n      .writeBytes(userData).buffer\n  },\n\n  /**\n   * @param {Buffer} buffer\n   * @returns {Object}\n   */\n  decode(buffer) {\n    const decoder = new Decoder(buffer)\n    return {\n      version: decoder.readInt16(),\n      topics: decoder.readArray(d => d.readString()),\n      userData: decoder.readBytes(),\n    }\n  },\n}\n\nconst MemberAssignment = {\n  /**\n   * @param {object} options\n   * @param {number} options.version\n   * @param {Object<String,Array>} options.assignment, example:\n   *                               {\n   *                                 'topic-A': [0, 2, 4, 6],\n   *                                 'topic-B': [0, 2],\n   *                               }\n   * @param {Buffer} [options.userData=Buffer.alloc(0)]\n   *\n   * @returns Buffer\n   */\n  encode({ version, assignment, userData = Buffer.alloc(0) }) {\n    return new Encoder()\n      .writeInt16(version)\n      .writeArray(\n        Object.keys(assignment).map(topic =>\n          new Encoder().writeString(topic).writeArray(assignment[topic])\n        )\n      )\n      .writeBytes(userData).buffer\n  },\n\n  /**\n   * @param {Buffer} buffer\n   * @returns {Object|null}\n   */\n  decode(buffer) {\n    const decoder = new Decoder(buffer)\n    const decodePartitions = d => d.readInt32()\n    const decodeAssignment = d => ({\n      topic: d.readString(),\n      partitions: d.readArray(decodePartitions),\n    })\n    const indexAssignment = (obj, { topic, partitions }) =>\n      Object.assign(obj, { [topic]: partitions })\n\n    if (!decoder.canReadInt16()) {\n      return null\n    }\n\n    return {\n      version: decoder.readInt16(),\n      assignment: decoder.readArray(decodeAssignment).reduce(indexAssignment, {}),\n      userData: decoder.readBytes(),\n    }\n  },\n}\n\nmodule.exports = {\n  MemberMetadata,\n  MemberAssignment,\n}\n", "const sleep = require('../utils/sleep')\nconst websiteUrl = require('../utils/websiteUrl')\nconst arrayDiff = require('../utils/arrayDiff')\nconst createRetry = require('../retry')\nconst sharedPromiseTo = require('../utils/sharedPromiseTo')\n\nconst OffsetManager = require('./offsetManager')\nconst Batch = require('./batch')\nconst SeekOffsets = require('./seekOffsets')\nconst SubscriptionState = require('./subscriptionState')\nconst {\n  events: { GROUP_JOIN, HEARTBEAT, CONNECT, RECEIVED_UNSUBSCRIBED_TOPICS },\n} = require('./instrumentationEvents')\nconst { MemberAssignment } = require('./assignerProtocol')\nconst {\n  KafkaJSError,\n  KafkaJSNonRetriableError,\n  KafkaJSStaleTopicMetadataAssignment,\n  isRebalancing,\n} = require('../errors')\n\nconst { keys } = Object\n\nconst STALE_METADATA_ERRORS = [\n  'LEADER_NOT_AVAILABLE',\n  // Fetch before v9 uses NOT_LEADER_FOR_PARTITION\n  'NOT_LEADER_FOR_PARTITION',\n  // Fetch after v9 uses {FENCED,UNKNOWN}_LEADER_EPOCH\n  'FENCED_LEADER_EPOCH',\n  'UNKNOWN_LEADER_EPOCH',\n  'UNKNOWN_TOPIC_OR_PARTITION',\n]\n\nconst PRIVATE = {\n  JOIN: Symbol('private:ConsumerGroup:join'),\n  SYNC: Symbol('private:ConsumerGroup:sync'),\n  SHARED_HEARTBEAT: Symbol('private:ConsumerGroup:sharedHeartbeat'),\n}\n\nmodule.exports = class ConsumerGroup {\n  /**\n   * @param {object} options\n   * @param {import('../../types').RetryOptions} options.retry\n   * @param {import('../../types').Cluster} options.cluster\n   * @param {string} options.groupId\n   * @param {string[]} options.topics\n   * @param {Record<string, { fromBeginning?: boolean }>} options.topicConfigurations\n   * @param {import('../../types').Logger} options.logger\n   * @param {import('../instrumentation/emitter')} options.instrumentationEmitter\n   * @param {import('../../types').Assigner[]} options.assigners\n   * @param {number} options.sessionTimeout\n   * @param {number} options.rebalanceTimeout\n   * @param {number} options.maxBytesPerPartition\n   * @param {number} options.minBytes\n   * @param {number} options.maxBytes\n   * @param {number} options.maxWaitTimeInMs\n   * @param {boolean} options.autoCommit\n   * @param {number} options.autoCommitInterval\n   * @param {number} options.autoCommitThreshold\n   * @param {number} options.isolationLevel\n   * @param {string} options.rackId\n   * @param {number} options.metadataMaxAge\n   */\n  constructor({\n    retry,\n    cluster,\n    groupId,\n    topics,\n    topicConfigurations,\n    logger,\n    instrumentationEmitter,\n    assigners,\n    sessionTimeout,\n    rebalanceTimeout,\n    maxBytesPerPartition,\n    minBytes,\n    maxBytes,\n    maxWaitTimeInMs,\n    autoCommit,\n    autoCommitInterval,\n    autoCommitThreshold,\n    isolationLevel,\n    rackId,\n    metadataMaxAge,\n  }) {\n    /** @type {import(\"../../types\").Cluster} */\n    this.cluster = cluster\n    this.groupId = groupId\n    this.topics = topics\n    this.topicsSubscribed = topics\n    this.topicConfigurations = topicConfigurations\n    this.logger = logger.namespace('ConsumerGroup')\n    this.instrumentationEmitter = instrumentationEmitter\n    this.retrier = createRetry(Object.assign({}, retry))\n    this.assigners = assigners\n    this.sessionTimeout = sessionTimeout\n    this.rebalanceTimeout = rebalanceTimeout\n    this.maxBytesPerPartition = maxBytesPerPartition\n    this.minBytes = minBytes\n    this.maxBytes = maxBytes\n    this.maxWaitTime = maxWaitTimeInMs\n    this.autoCommit = autoCommit\n    this.autoCommitInterval = autoCommitInterval\n    this.autoCommitThreshold = autoCommitThreshold\n    this.isolationLevel = isolationLevel\n    this.rackId = rackId\n    this.metadataMaxAge = metadataMaxAge\n\n    this.seekOffset = new SeekOffsets()\n    this.coordinator = null\n    this.generationId = null\n    this.leaderId = null\n    this.memberId = null\n    this.members = null\n    this.groupProtocol = null\n\n    this.partitionsPerSubscribedTopic = null\n    /**\n     * Preferred read replica per topic and partition\n     *\n     * Each of the partitions tracks the preferred read replica (`nodeId`) and a timestamp\n     * until when that preference is valid.\n     *\n     * @type {{[topicName: string]: {[partition: number]: {nodeId: number, expireAt: number}}}}\n     */\n    this.preferredReadReplicasPerTopicPartition = {}\n    this.offsetManager = null\n    this.subscriptionState = new SubscriptionState()\n\n    this.lastRequest = Date.now()\n\n    this[PRIVATE.SHARED_HEARTBEAT] = sharedPromiseTo(async ({ interval }) => {\n      const { groupId, generationId, memberId } = this\n      const now = Date.now()\n\n      if (memberId && now >= this.lastRequest + interval) {\n        const payload = {\n          groupId,\n          memberId,\n          groupGenerationId: generationId,\n        }\n\n        await this.coordinator.heartbeat(payload)\n        this.instrumentationEmitter.emit(HEARTBEAT, payload)\n        this.lastRequest = Date.now()\n      }\n    })\n  }\n\n  isLeader() {\n    return this.leaderId && this.memberId === this.leaderId\n  }\n\n  getNodeIds() {\n    return this.cluster.getNodeIds()\n  }\n\n  async connect() {\n    await this.cluster.connect()\n    this.instrumentationEmitter.emit(CONNECT)\n    await this.cluster.refreshMetadataIfNecessary()\n  }\n\n  async [PRIVATE.JOIN]() {\n    const { groupId, sessionTimeout, rebalanceTimeout } = this\n\n    this.coordinator = await this.cluster.findGroupCoordinator({ groupId })\n\n    const groupData = await this.coordinator.joinGroup({\n      groupId,\n      sessionTimeout,\n      rebalanceTimeout,\n      memberId: this.memberId || '',\n      groupProtocols: this.assigners.map(assigner =>\n        assigner.protocol({\n          topics: this.topicsSubscribed,\n        })\n      ),\n    })\n\n    this.generationId = groupData.generationId\n    this.leaderId = groupData.leaderId\n    this.memberId = groupData.memberId\n    this.members = groupData.members\n    this.groupProtocol = groupData.groupProtocol\n  }\n\n  async leave() {\n    const { groupId, memberId } = this\n    if (memberId) {\n      await this.coordinator.leaveGroup({ groupId, memberId })\n      this.memberId = null\n    }\n  }\n\n  async [PRIVATE.SYNC]() {\n    let assignment = []\n    const {\n      groupId,\n      generationId,\n      memberId,\n      members,\n      groupProtocol,\n      topics,\n      topicsSubscribed,\n      coordinator,\n    } = this\n\n    if (this.isLeader()) {\n      this.logger.debug('Chosen as group leader', { groupId, generationId, memberId, topics })\n      const assigner = this.assigners.find(({ name }) => name === groupProtocol)\n\n      if (!assigner) {\n        throw new KafkaJSNonRetriableError(\n          `Unsupported partition assigner \"${groupProtocol}\", the assigner wasn't found in the assigners list`\n        )\n      }\n\n      await this.cluster.refreshMetadata()\n      assignment = await assigner.assign({ members, topics: topicsSubscribed })\n\n      this.logger.debug('Group assignment', {\n        groupId,\n        generationId,\n        groupProtocol,\n        assignment,\n        topics: topicsSubscribed,\n      })\n    }\n\n    // Keep track of the partitions for the subscribed topics\n    this.partitionsPerSubscribedTopic = this.generatePartitionsPerSubscribedTopic()\n    const { memberAssignment } = await this.coordinator.syncGroup({\n      groupId,\n      generationId,\n      memberId,\n      groupAssignment: assignment,\n    })\n\n    const decodedMemberAssignment = MemberAssignment.decode(memberAssignment)\n    const decodedAssignment =\n      decodedMemberAssignment != null ? decodedMemberAssignment.assignment : {}\n\n    this.logger.debug('Received assignment', {\n      groupId,\n      generationId,\n      memberId,\n      memberAssignment: decodedAssignment,\n    })\n\n    const assignedTopics = keys(decodedAssignment)\n    const topicsNotSubscribed = arrayDiff(assignedTopics, topicsSubscribed)\n\n    if (topicsNotSubscribed.length > 0) {\n      const payload = {\n        groupId,\n        generationId,\n        memberId,\n        assignedTopics,\n        topicsSubscribed,\n        topicsNotSubscribed,\n      }\n\n      this.instrumentationEmitter.emit(RECEIVED_UNSUBSCRIBED_TOPICS, payload)\n      this.logger.warn('Consumer group received unsubscribed topics', {\n        ...payload,\n        helpUrl: websiteUrl(\n          'docs/faq',\n          'why-am-i-receiving-messages-for-topics-i-m-not-subscribed-to'\n        ),\n      })\n    }\n\n    // Remove unsubscribed topics from the list\n    const safeAssignment = arrayDiff(assignedTopics, topicsNotSubscribed)\n    const currentMemberAssignment = safeAssignment.map(topic => ({\n      topic,\n      partitions: decodedAssignment[topic],\n    }))\n\n    // Check if the consumer is aware of all assigned partitions\n    for (const assignment of currentMemberAssignment) {\n      const { topic, partitions: assignedPartitions } = assignment\n      const knownPartitions = this.partitionsPerSubscribedTopic.get(topic)\n      const isAwareOfAllAssignedPartitions = assignedPartitions.every(partition =>\n        knownPartitions.includes(partition)\n      )\n\n      if (!isAwareOfAllAssignedPartitions) {\n        this.logger.warn('Consumer is not aware of all assigned partitions, refreshing metadata', {\n          groupId,\n          generationId,\n          memberId,\n          topic,\n          knownPartitions,\n          assignedPartitions,\n        })\n\n        // If the consumer is not aware of all assigned partitions, refresh metadata\n        // and update the list of partitions per subscribed topic. It's enough to perform\n        // this operation once since refresh metadata will update metadata for all topics\n        await this.cluster.refreshMetadata()\n        this.partitionsPerSubscribedTopic = this.generatePartitionsPerSubscribedTopic()\n        break\n      }\n    }\n\n    this.topics = currentMemberAssignment.map(({ topic }) => topic)\n    this.subscriptionState.assign(currentMemberAssignment)\n    this.offsetManager = new OffsetManager({\n      cluster: this.cluster,\n      topicConfigurations: this.topicConfigurations,\n      instrumentationEmitter: this.instrumentationEmitter,\n      memberAssignment: currentMemberAssignment.reduce(\n        (partitionsByTopic, { topic, partitions }) => ({\n          ...partitionsByTopic,\n          [topic]: partitions,\n        }),\n        {}\n      ),\n      autoCommit: this.autoCommit,\n      autoCommitInterval: this.autoCommitInterval,\n      autoCommitThreshold: this.autoCommitThreshold,\n      coordinator,\n      groupId,\n      generationId,\n      memberId,\n    })\n  }\n\n  joinAndSync() {\n    const startJoin = Date.now()\n    return this.retrier(async bail => {\n      try {\n        await this[PRIVATE.JOIN]()\n        await this[PRIVATE.SYNC]()\n\n        const memberAssignment = this.assigned().reduce(\n          (result, { topic, partitions }) => ({ ...result, [topic]: partitions }),\n          {}\n        )\n\n        const payload = {\n          groupId: this.groupId,\n          memberId: this.memberId,\n          leaderId: this.leaderId,\n          isLeader: this.isLeader(),\n          memberAssignment,\n          groupProtocol: this.groupProtocol,\n          duration: Date.now() - startJoin,\n        }\n\n        this.instrumentationEmitter.emit(GROUP_JOIN, payload)\n        this.logger.info('Consumer has joined the group', payload)\n      } catch (e) {\n        if (isRebalancing(e)) {\n          // Rebalance in progress isn't a retriable protocol error since the consumer\n          // has to go through find coordinator and join again before it can\n          // actually retry the operation. We wrap the original error in a retriable error\n          // here instead in order to restart the join + sync sequence using the retrier.\n          throw new KafkaJSError(e)\n        }\n\n        if (e.type === 'UNKNOWN_MEMBER_ID') {\n          this.memberId = null\n          throw new KafkaJSError(e)\n        }\n\n        bail(e)\n      }\n    })\n  }\n\n  /**\n   * @param {import(\"../../types\").TopicPartition} topicPartition\n   */\n  resetOffset({ topic, partition }) {\n    this.offsetManager.resetOffset({ topic, partition })\n  }\n\n  /**\n   * @param {import(\"../../types\").TopicPartitionOffset} topicPartitionOffset\n   */\n  resolveOffset({ topic, partition, offset }) {\n    this.offsetManager.resolveOffset({ topic, partition, offset })\n  }\n\n  /**\n   * Update the consumer offset for the given topic/partition. This will be used\n   * on the next fetch. If this API is invoked for the same topic/partition more\n   * than once, the latest offset will be used on the next fetch.\n   *\n   * @param {import(\"../../types\").TopicPartitionOffset} topicPartitionOffset\n   */\n  seek({ topic, partition, offset }) {\n    this.seekOffset.set(topic, partition, offset)\n  }\n\n  pause(topicPartitions) {\n    this.logger.info(`Pausing fetching from ${topicPartitions.length} topics`, {\n      topicPartitions,\n    })\n    this.subscriptionState.pause(topicPartitions)\n  }\n\n  resume(topicPartitions) {\n    this.logger.info(`Resuming fetching from ${topicPartitions.length} topics`, {\n      topicPartitions,\n    })\n    this.subscriptionState.resume(topicPartitions)\n  }\n\n  assigned() {\n    return this.subscriptionState.assigned()\n  }\n\n  paused() {\n    return this.subscriptionState.paused()\n  }\n\n  /**\n   * @param {string} topic\n   * @param {string} partition\n   * @returns {boolean} whether the specified topic-partition are paused or not\n   */\n  isPaused(topic, partition) {\n    return this.subscriptionState.isPaused(topic, partition)\n  }\n\n  async commitOffsetsIfNecessary() {\n    await this.offsetManager.commitOffsetsIfNecessary()\n  }\n\n  async commitOffsets(offsets) {\n    await this.offsetManager.commitOffsets(offsets)\n  }\n\n  uncommittedOffsets() {\n    return this.offsetManager.uncommittedOffsets()\n  }\n\n  async heartbeat({ interval }) {\n    return this[PRIVATE.SHARED_HEARTBEAT]({ interval })\n  }\n\n  async fetch(nodeId) {\n    try {\n      await this.cluster.refreshMetadataIfNecessary()\n      this.checkForStaleAssignment()\n\n      let topicPartitions = this.subscriptionState.assigned()\n      topicPartitions = this.filterPartitionsByNode(nodeId, topicPartitions)\n\n      await this.seekOffsets(topicPartitions)\n\n      const committedOffsets = this.offsetManager.committedOffsets()\n      const activeTopicPartitions = this.getActiveTopicPartitions()\n\n      const requests = topicPartitions\n        .map(({ topic, partitions }) => ({\n          topic,\n          partitions: partitions\n            .filter(\n              partition =>\n                /**\n                 * When recovering from OffsetOutOfRange, each partition can recover\n                 * concurrently, which invalidates resolved and committed offsets as part\n                 * of the recovery mechanism (see OffsetManager.clearOffsets). In concurrent\n                 * scenarios this can initiate a new fetch with invalid offsets.\n                 *\n                 * This was further highlighted by https://github.com/tulios/kafkajs/pull/570,\n                 * which increased concurrency, making this more likely to happen.\n                 *\n                 * This is solved by only making requests for partitions with initialized offsets.\n                 *\n                 * See the following pull request which explains the context of the problem:\n                 * @issue https://github.com/tulios/kafkajs/pull/578\n                 */\n                committedOffsets[topic][partition] != null &&\n                activeTopicPartitions[topic].has(partition)\n            )\n            .map(partition => ({\n              partition,\n              fetchOffset: this.offsetManager.nextOffset(topic, partition).toString(),\n              maxBytes: this.maxBytesPerPartition,\n            })),\n        }))\n        .filter(({ partitions }) => partitions.length)\n\n      if (!requests.length) {\n        await sleep(this.maxWaitTime)\n        return []\n      }\n\n      const broker = await this.cluster.findBroker({ nodeId })\n\n      const { responses } = await broker.fetch({\n        maxWaitTime: this.maxWaitTime,\n        minBytes: this.minBytes,\n        maxBytes: this.maxBytes,\n        isolationLevel: this.isolationLevel,\n        topics: requests,\n        rackId: this.rackId,\n      })\n\n      return responses.flatMap(({ topicName, partitions }) => {\n        const topicRequestData = requests.find(({ topic }) => topic === topicName)\n\n        let preferredReadReplicas = this.preferredReadReplicasPerTopicPartition[topicName]\n        if (!preferredReadReplicas) {\n          this.preferredReadReplicasPerTopicPartition[topicName] = preferredReadReplicas = {}\n        }\n\n        return partitions\n          .filter(\n            ({ partition }) =>\n              !this.seekOffset.has(topicName, partition) &&\n              !this.subscriptionState.isPaused(topicName, partition)\n          )\n          .map(partitionData => {\n            const { partition, preferredReadReplica } = partitionData\n\n            if (preferredReadReplica != null && preferredReadReplica !== -1) {\n              const { nodeId: currentPreferredReadReplica } = preferredReadReplicas[partition] || {}\n              if (currentPreferredReadReplica !== preferredReadReplica) {\n                this.logger.info(`Preferred read replica is now ${preferredReadReplica}`, {\n                  groupId: this.groupId,\n                  memberId: this.memberId,\n                  topic: topicName,\n                  partition,\n                })\n              }\n              preferredReadReplicas[partition] = {\n                nodeId: preferredReadReplica,\n                expireAt: Date.now() + this.metadataMaxAge,\n              }\n            }\n\n            const partitionRequestData = topicRequestData.partitions.find(\n              ({ partition }) => partition === partitionData.partition\n            )\n\n            const fetchedOffset = partitionRequestData.fetchOffset\n            return new Batch(topicName, fetchedOffset, partitionData)\n          })\n      })\n    } catch (e) {\n      await this.recoverFromFetch(e)\n      return []\n    }\n  }\n\n  async recoverFromFetch(e) {\n    if (STALE_METADATA_ERRORS.includes(e.type) || e.name === 'KafkaJSTopicMetadataNotLoaded') {\n      this.logger.debug('Stale cluster metadata, refreshing...', {\n        groupId: this.groupId,\n        memberId: this.memberId,\n        error: e.message,\n      })\n\n      await this.cluster.refreshMetadata()\n      await this.joinAndSync()\n      return\n    }\n\n    if (e.name === 'KafkaJSStaleTopicMetadataAssignment') {\n      this.logger.warn(`${e.message}, resync group`, {\n        groupId: this.groupId,\n        memberId: this.memberId,\n        topic: e.topic,\n        unknownPartitions: e.unknownPartitions,\n      })\n\n      await this.joinAndSync()\n      return\n    }\n\n    if (e.name === 'KafkaJSOffsetOutOfRange') {\n      await this.recoverFromOffsetOutOfRange(e)\n      return\n    }\n\n    if (e.name === 'KafkaJSConnectionClosedError') {\n      this.cluster.removeBroker({ host: e.host, port: e.port })\n      return\n    }\n\n    if (e.name === 'KafkaJSBrokerNotFound' || e.name === 'KafkaJSConnectionClosedError') {\n      this.logger.debug(`${e.message}, refreshing metadata and retrying...`)\n      await this.cluster.refreshMetadata()\n      return\n    }\n\n    throw e\n  }\n\n  async recoverFromOffsetOutOfRange(e) {\n    // If we are fetching from a follower try with the leader before resetting offsets\n    const preferredReadReplicas = this.preferredReadReplicasPerTopicPartition[e.topic]\n    if (preferredReadReplicas && typeof preferredReadReplicas[e.partition] === 'number') {\n      this.logger.info('Offset out of range while fetching from follower, retrying with leader', {\n        topic: e.topic,\n        partition: e.partition,\n        groupId: this.groupId,\n        memberId: this.memberId,\n      })\n      delete preferredReadReplicas[e.partition]\n    } else {\n      this.logger.error('Offset out of range, resetting to default offset', {\n        topic: e.topic,\n        partition: e.partition,\n        groupId: this.groupId,\n        memberId: this.memberId,\n      })\n\n      await this.offsetManager.setDefaultOffset({\n        topic: e.topic,\n        partition: e.partition,\n      })\n    }\n  }\n\n  generatePartitionsPerSubscribedTopic() {\n    const map = new Map()\n\n    for (const topic of this.topicsSubscribed) {\n      const partitions = this.cluster\n        .findTopicPartitionMetadata(topic)\n        .map(m => m.partitionId)\n        .sort()\n\n      map.set(topic, partitions)\n    }\n\n    return map\n  }\n\n  checkForStaleAssignment() {\n    if (!this.partitionsPerSubscribedTopic) {\n      return\n    }\n\n    const newPartitionsPerSubscribedTopic = this.generatePartitionsPerSubscribedTopic()\n\n    for (const [topic, partitions] of newPartitionsPerSubscribedTopic) {\n      const diff = arrayDiff(partitions, this.partitionsPerSubscribedTopic.get(topic))\n\n      if (diff.length > 0) {\n        throw new KafkaJSStaleTopicMetadataAssignment('Topic has been updated', {\n          topic,\n          unknownPartitions: diff,\n        })\n      }\n    }\n  }\n\n  async seekOffsets(topicPartitions) {\n    for (const { topic, partitions } of topicPartitions) {\n      for (const partition of partitions) {\n        const seekEntry = this.seekOffset.pop(topic, partition)\n        if (!seekEntry) {\n          continue\n        }\n\n        this.logger.debug('Seek offset', {\n          groupId: this.groupId,\n          memberId: this.memberId,\n          seek: seekEntry,\n        })\n        await this.offsetManager.seek(seekEntry)\n      }\n    }\n\n    await this.offsetManager.resolveOffsets()\n  }\n\n  hasSeekOffset({ topic, partition }) {\n    return this.seekOffset.has(topic, partition)\n  }\n\n  /**\n   * For each of the partitions find the best nodeId to read it from\n   *\n   * @param {string} topic\n   * @param {number[]} partitions\n   * @returns {{[nodeId: number]: number[]}} per-node assignment of partitions\n   * @see Cluster~findLeaderForPartitions\n   */\n  // Invariant: The resulting object has each partition referenced exactly once\n  findReadReplicaForPartitions(topic, partitions) {\n    const partitionMetadata = this.cluster.findTopicPartitionMetadata(topic)\n    const preferredReadReplicas = this.preferredReadReplicasPerTopicPartition[topic]\n    return partitions.reduce((result, id) => {\n      const partitionId = parseInt(id, 10)\n      const metadata = partitionMetadata.find(p => p.partitionId === partitionId)\n      if (!metadata) {\n        return result\n      }\n\n      if (metadata.leader == null) {\n        throw new KafkaJSError('Invalid partition metadata', { topic, partitionId, metadata })\n      }\n\n      // Pick the preferred replica if there is one, and it isn't known to be offline, otherwise the leader.\n      let nodeId = metadata.leader\n      if (preferredReadReplicas) {\n        const { nodeId: preferredReadReplica, expireAt } = preferredReadReplicas[partitionId] || {}\n        if (Date.now() >= expireAt) {\n          this.logger.debug('Preferred read replica information has expired, using leader', {\n            topic,\n            partitionId,\n            groupId: this.groupId,\n            memberId: this.memberId,\n            preferredReadReplica,\n            leader: metadata.leader,\n          })\n          // Drop the entry\n          delete preferredReadReplicas[partitionId]\n        } else if (preferredReadReplica != null) {\n          // Valid entry, check whether it is not offline\n          // Note that we don't delete the preference here, and rather hope that eventually that replica comes online again\n          const offlineReplicas = metadata.offlineReplicas\n          if (Array.isArray(offlineReplicas) && offlineReplicas.includes(nodeId)) {\n            this.logger.debug('Preferred read replica is offline, using leader', {\n              topic,\n              partitionId,\n              groupId: this.groupId,\n              memberId: this.memberId,\n              preferredReadReplica,\n              leader: metadata.leader,\n            })\n          } else {\n            nodeId = preferredReadReplica\n          }\n        }\n      }\n      const current = result[nodeId] || []\n      return { ...result, [nodeId]: [...current, partitionId] }\n    }, {})\n  }\n\n  filterPartitionsByNode(nodeId, topicPartitions) {\n    return topicPartitions.map(({ topic, partitions }) => ({\n      topic,\n      partitions: this.findReadReplicaForPartitions(topic, partitions)[nodeId] || [],\n    }))\n  }\n\n  getActiveTopicPartitions() {\n    const activeSubscriptionState = this.subscriptionState.active()\n\n    const activeTopicPartitions = {}\n    activeSubscriptionState.forEach(({ topic, partitions }) => {\n      activeTopicPartitions[topic] = new Set(partitions)\n    })\n\n    return activeTopicPartitions\n  }\n}\n", "/**\n * @param {number} count\n * @param {(index: number) => T} [callback]\n * @template T\n */\nconst seq = (count, callback = x => x) =>\n  new Array(count).fill(0).map((_, index) => callback(index))\n\nmodule.exports = seq\n", "const EventEmitter = require('events')\n\n/**\n * Fetches data from all assigned nodes, waits for workerQueue to drain and repeats.\n *\n * @param {object} options\n * @param {number} options.nodeId\n * @param {import('./workerQueue').WorkerQueue} options.workerQueue\n * @param {Map<string, string[]>} options.partitionAssignments\n * @param {(nodeId: number) => Promise<T[]>} options.fetch\n * @param {import('../../types').Logger} options.logger\n * @template T\n */\nconst createFetcher = ({\n  nodeId,\n  workerQueue,\n  partitionAssignments,\n  fetch,\n  logger: rootLogger,\n}) => {\n  const logger = rootLogger.namespace(`Fetcher ${nodeId}`)\n  const emitter = new EventEmitter()\n  let isRunning = false\n\n  const getWorkerQueue = () => workerQueue\n  const assignmentKey = ({ topic, partition }) => `${topic}|${partition}`\n  const getAssignedFetcher = batch => partitionAssignments.get(assignmentKey(batch))\n  const assignTopicPartition = batch => partitionAssignments.set(assignmentKey(batch), nodeId)\n  const unassignTopicPartition = batch => partitionAssignments.delete(assignmentKey(batch))\n  const filterUnassignedBatches = batches =>\n    batches.filter(batch => {\n      const assignedFetcher = getAssignedFetcher(batch)\n      if (assignedFetcher != null && assignedFetcher !== nodeId) {\n        logger.info(\n          'Filtering out batch due to partition already being processed by another fetcher',\n          {\n            topic: batch.topic,\n            partition: batch.partition,\n            assignedFetcher: assignedFetcher,\n            fetcher: nodeId,\n          }\n        )\n        return false\n      }\n\n      return true\n    })\n\n  const start = async () => {\n    if (isRunning) return\n    isRunning = true\n\n    while (isRunning) {\n      try {\n        const batches = await fetch(nodeId)\n        if (isRunning) {\n          const availableBatches = filterUnassignedBatches(batches)\n\n          if (availableBatches.length > 0) {\n            availableBatches.forEach(assignTopicPartition)\n            try {\n              await workerQueue.push(...availableBatches)\n            } finally {\n              availableBatches.forEach(unassignTopicPartition)\n            }\n          }\n        }\n      } catch (error) {\n        isRunning = false\n        emitter.emit('end')\n        throw error\n      }\n    }\n    emitter.emit('end')\n  }\n\n  const stop = async () => {\n    if (!isRunning) return\n    isRunning = false\n    await new Promise(resolve => emitter.once('end', () => resolve()))\n  }\n\n  return { start, stop, getWorkerQueue }\n}\n\nmodule.exports = createFetcher\n", "/**\n * @typedef {(batch: T, metadata: { workerId: number }) => Promise<void>} Handler\n * @template T\n *\n * @typedef {ReturnType<typeof createWorker>} Worker\n */\n\nconst sharedPromiseTo = require('../utils/sharedPromiseTo')\n\n/**\n * @param {{ handler: Handler<T>, workerId: number }} options\n * @template T\n */\nconst createWorker = ({ handler, workerId }) => {\n  /**\n   * Takes batches from next() until it returns undefined.\n   *\n   * @param {{ next: () => { batch: T, resolve: () => void, reject: (e: Error) => void } | undefined }} param0\n   * @returns {Promise<void>}\n   */\n  const run = sharedPromiseTo(async ({ next }) => {\n    while (true) {\n      const item = next()\n      if (!item) break\n\n      const { batch, resolve, reject } = item\n\n      try {\n        await handler(batch, { workerId })\n        resolve()\n      } catch (error) {\n        reject(error)\n      }\n    }\n  })\n\n  return { run }\n}\n\nmodule.exports = createWorker\n", "/**\n * @typedef {ReturnType<typeof createWorkerQueue>} WorkerQueue\n */\n\n/**\n * @param {object} options\n * @param {import('./worker').Worker<T>[]} options.workers\n * @template T\n */\nconst createWorkerQueue = ({ workers }) => {\n  /** @type {{ batch: T, resolve: (value?: any) => void, reject: (e: Error) => void}[]} */\n  const queue = []\n\n  const getWorkers = () => workers\n\n  /**\n   * Waits until workers have processed all batches in the queue.\n   *\n   * @param {...T} batches\n   * @returns {Promise<void>}\n   */\n  const push = async (...batches) => {\n    const promises = batches.map(\n      batch => new Promise((resolve, reject) => queue.push({ batch, resolve, reject }))\n    )\n\n    workers.forEach(worker => worker.run({ next: () => queue.shift() }))\n\n    const results = await Promise.allSettled(promises)\n    const rejected = results.find(result => result.status === 'rejected')\n    if (rejected) {\n      // @ts-ignore\n      throw rejected.reason\n    }\n  }\n\n  return { push, getWorkers }\n}\n\nmodule.exports = createWorkerQueue\n", "const seq = require('../utils/seq')\nconst createFetcher = require('./fetcher')\nconst createWorker = require('./worker')\nconst createWorkerQueue = require('./workerQueue')\nconst { KafkaJSFetcherRebalanceError, KafkaJSNoBrokerAvailableError } = require('../errors')\n\n/** @typedef {ReturnType<typeof createFetchManager>} FetchManager */\n\n/**\n * @param {object} options\n * @param {import('../../types').Logger} options.logger\n * @param {() => number[]} options.getNodeIds\n * @param {(nodeId: number) => Promise<import('../../types').Batch[]>} options.fetch\n * @param {import('./worker').Handler<T>} options.handler\n * @param {number} [options.concurrency]\n * @template T\n */\nconst createFetchManager = ({\n  logger: rootLogger,\n  getNodeIds,\n  fetch,\n  handler,\n  concurrency = 1,\n}) => {\n  const logger = rootLogger.namespace('FetchManager')\n  const workers = seq(concurrency, workerId => createWorker({ handler, workerId }))\n  const workerQueue = createWorkerQueue({ workers })\n\n  let fetchers = []\n\n  const getFetchers = () => fetchers\n\n  const createFetchers = () => {\n    const nodeIds = getNodeIds()\n    const partitionAssignments = new Map()\n\n    if (nodeIds.length === 0) {\n      throw new KafkaJSNoBrokerAvailableError()\n    }\n\n    const validateShouldRebalance = () => {\n      const current = getNodeIds()\n      const hasChanged =\n        nodeIds.length !== current.length || nodeIds.some(nodeId => !current.includes(nodeId))\n      if (hasChanged && current.length !== 0) {\n        throw new KafkaJSFetcherRebalanceError()\n      }\n    }\n\n    const fetchers = nodeIds.map(nodeId =>\n      createFetcher({\n        nodeId,\n        workerQueue,\n        partitionAssignments,\n        fetch: async nodeId => {\n          validateShouldRebalance()\n          return fetch(nodeId)\n        },\n        logger,\n      })\n    )\n\n    logger.debug(`Created ${fetchers.length} fetchers`, { nodeIds, concurrency })\n    return fetchers\n  }\n\n  const start = async () => {\n    logger.debug('Starting...')\n\n    while (true) {\n      fetchers = createFetchers()\n\n      try {\n        await Promise.all(fetchers.map(fetcher => fetcher.start()))\n      } catch (error) {\n        await stop()\n\n        if (error instanceof KafkaJSFetcherRebalanceError) {\n          logger.debug('Rebalancing fetchers...')\n          continue\n        }\n\n        throw error\n      }\n\n      break\n    }\n  }\n\n  const stop = async () => {\n    logger.debug('Stopping fetchers...')\n    await Promise.all(fetchers.map(fetcher => fetcher.stop()))\n    logger.debug('Stopped fetchers')\n  }\n\n  return { start, stop, getFetchers }\n}\n\nmodule.exports = createFetchManager\n", "const { EventEmitter } = require('events')\nconst Long = require('../utils/long')\nconst createRetry = require('../retry')\nconst { isKafkaJSError, isRebalancing } = require('../errors')\n\nconst {\n  events: { FETCH, FETCH_START, START_BATCH_PROCESS, END_BATCH_PROCESS, REBALANCING },\n} = require('./instrumentationEvents')\nconst createFetchManager = require('./fetchManager')\n\nconst isSameOffset = (offsetA, offsetB) => Long.fromValue(offsetA).equals(Long.fromValue(offsetB))\nconst CONSUMING_START = 'consuming-start'\nconst CONSUMING_STOP = 'consuming-stop'\n\nmodule.exports = class Runner extends EventEmitter {\n  /**\n   * @param {object} options\n   * @param {import(\"../../types\").Logger} options.logger\n   * @param {import(\"./consumerGroup\")} options.consumerGroup\n   * @param {import(\"../instrumentation/emitter\")} options.instrumentationEmitter\n   * @param {boolean} [options.eachBatchAutoResolve=true]\n   * @param {number} options.concurrency\n   * @param {(payload: import(\"../../types\").EachBatchPayload) => Promise<void>} [options.eachBatch]\n   * @param {(payload: import(\"../../types\").EachMessagePayload) => Promise<void>} [options.eachMessage]\n   * @param {number} [options.heartbeatInterval]\n   * @param {(reason: Error) => void} options.onCrash\n   * @param {import(\"../../types\").RetryOptions} [options.retry]\n   * @param {boolean} [options.autoCommit=true]\n   */\n  constructor({\n    logger,\n    consumerGroup,\n    instrumentationEmitter,\n    eachBatchAutoResolve = true,\n    concurrency,\n    eachBatch,\n    eachMessage,\n    heartbeatInterval,\n    onCrash,\n    retry,\n    autoCommit = true,\n  }) {\n    super()\n    this.logger = logger.namespace('Runner')\n    this.consumerGroup = consumerGroup\n    this.instrumentationEmitter = instrumentationEmitter\n    this.eachBatchAutoResolve = eachBatchAutoResolve\n    this.eachBatch = eachBatch\n    this.eachMessage = eachMessage\n    this.heartbeatInterval = heartbeatInterval\n    this.retrier = createRetry(Object.assign({}, retry))\n    this.onCrash = onCrash\n    this.autoCommit = autoCommit\n    this.fetchManager = createFetchManager({\n      logger: this.logger,\n      getNodeIds: () => this.consumerGroup.getNodeIds(),\n      fetch: nodeId => this.fetch(nodeId),\n      handler: batch => this.handleBatch(batch),\n      concurrency,\n    })\n\n    this.running = false\n    this.consuming = false\n  }\n\n  get consuming() {\n    return this._consuming\n  }\n\n  set consuming(value) {\n    if (this._consuming !== value) {\n      this._consuming = value\n      this.emit(value ? CONSUMING_START : CONSUMING_STOP)\n    }\n  }\n\n  async start() {\n    if (this.running) {\n      return\n    }\n\n    try {\n      await this.consumerGroup.connect()\n      await this.consumerGroup.joinAndSync()\n    } catch (e) {\n      return this.onCrash(e)\n    }\n\n    this.running = true\n    this.scheduleFetchManager()\n  }\n\n  scheduleFetchManager() {\n    if (!this.running) {\n      this.consuming = false\n\n      this.logger.info('consumer not running, exiting', {\n        groupId: this.consumerGroup.groupId,\n        memberId: this.consumerGroup.memberId,\n      })\n\n      return\n    }\n\n    this.consuming = true\n\n    this.retrier(async (bail, retryCount, retryTime) => {\n      if (!this.running) {\n        return\n      }\n\n      try {\n        await this.fetchManager.start()\n      } catch (e) {\n        if (isRebalancing(e)) {\n          this.logger.warn('The group is rebalancing, re-joining', {\n            groupId: this.consumerGroup.groupId,\n            memberId: this.consumerGroup.memberId,\n            error: e.message,\n          })\n\n          this.instrumentationEmitter.emit(REBALANCING, {\n            groupId: this.consumerGroup.groupId,\n            memberId: this.consumerGroup.memberId,\n          })\n\n          await this.consumerGroup.joinAndSync()\n          return\n        }\n\n        if (e.type === 'UNKNOWN_MEMBER_ID') {\n          this.logger.error('The coordinator is not aware of this member, re-joining the group', {\n            groupId: this.consumerGroup.groupId,\n            memberId: this.consumerGroup.memberId,\n            error: e.message,\n          })\n\n          this.consumerGroup.memberId = null\n          await this.consumerGroup.joinAndSync()\n          return\n        }\n\n        if (e.name === 'KafkaJSNotImplemented') {\n          return bail(e)\n        }\n\n        if (e.name === 'KafkaJSNoBrokerAvailableError') {\n          return bail(e)\n        }\n\n        this.logger.debug('Error while scheduling fetch manager, trying again...', {\n          groupId: this.consumerGroup.groupId,\n          memberId: this.consumerGroup.memberId,\n          error: e.message,\n          stack: e.stack,\n          retryCount,\n          retryTime,\n        })\n\n        throw e\n      }\n    })\n      .then(() => {\n        this.scheduleFetchManager()\n      })\n      .catch(e => {\n        this.onCrash(e)\n        this.consuming = false\n        this.running = false\n      })\n  }\n\n  async stop() {\n    if (!this.running) {\n      return\n    }\n\n    this.logger.debug('stop consumer group', {\n      groupId: this.consumerGroup.groupId,\n      memberId: this.consumerGroup.memberId,\n    })\n\n    this.running = false\n\n    try {\n      await this.fetchManager.stop()\n      await this.waitForConsumer()\n      await this.consumerGroup.leave()\n    } catch (e) {}\n  }\n\n  waitForConsumer() {\n    return new Promise(resolve => {\n      if (!this.consuming) {\n        return resolve()\n      }\n\n      this.logger.debug('waiting for consumer to finish...', {\n        groupId: this.consumerGroup.groupId,\n        memberId: this.consumerGroup.memberId,\n      })\n\n      this.once(CONSUMING_STOP, () => resolve())\n    })\n  }\n\n  async heartbeat() {\n    try {\n      await this.consumerGroup.heartbeat({ interval: this.heartbeatInterval })\n    } catch (e) {\n      if (isRebalancing(e)) {\n        await this.autoCommitOffsets()\n      }\n      throw e\n    }\n  }\n\n  async processEachMessage(batch) {\n    const { topic, partition } = batch\n\n    const pause = () => {\n      this.consumerGroup.pause([{ topic, partitions: [partition] }])\n      return () => this.consumerGroup.resume([{ topic, partitions: [partition] }])\n    }\n    for (const message of batch.messages) {\n      if (!this.running || this.consumerGroup.hasSeekOffset({ topic, partition })) {\n        break\n      }\n\n      try {\n        await this.eachMessage({\n          topic,\n          partition,\n          message,\n          heartbeat: () => this.heartbeat(),\n          pause,\n        })\n      } catch (e) {\n        if (!isKafkaJSError(e)) {\n          this.logger.error(`Error when calling eachMessage`, {\n            topic,\n            partition,\n            offset: message.offset,\n            stack: e.stack,\n            error: e,\n          })\n        }\n\n        // In case of errors, commit the previously consumed offsets unless autoCommit is disabled\n        await this.autoCommitOffsets()\n        throw e\n      }\n\n      this.consumerGroup.resolveOffset({ topic, partition, offset: message.offset })\n      await this.heartbeat()\n      await this.autoCommitOffsetsIfNecessary()\n\n      if (this.consumerGroup.isPaused(topic, partition)) {\n        break\n      }\n    }\n  }\n\n  async processEachBatch(batch) {\n    const { topic, partition } = batch\n    const lastFilteredMessage = batch.messages[batch.messages.length - 1]\n\n    const pause = () => {\n      this.consumerGroup.pause([{ topic, partitions: [partition] }])\n      return () => this.consumerGroup.resume([{ topic, partitions: [partition] }])\n    }\n\n    try {\n      await this.eachBatch({\n        batch,\n        resolveOffset: offset => {\n          /**\n           * The transactional producer generates a control record after committing the transaction.\n           * The control record is the last record on the RecordBatch, and it is filtered before it\n           * reaches the eachBatch callback. When disabling auto-resolve, the user-land code won't\n           * be able to resolve the control record offset, since it never reaches the callback,\n           * causing stuck consumers as the consumer will never move the offset marker.\n           *\n           * When the last offset of the batch is resolved, we should automatically resolve\n           * the control record offset as this entry doesn't have any meaning to the user-land code,\n           * and won't interfere with the stream processing.\n           *\n           * @see https://github.com/apache/kafka/blob/9aa660786e46c1efbf5605a6a69136a1dac6edb9/clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java#L1499-L1505\n           */\n          const offsetToResolve =\n            lastFilteredMessage && isSameOffset(offset, lastFilteredMessage.offset)\n              ? batch.lastOffset()\n              : offset\n\n          this.consumerGroup.resolveOffset({ topic, partition, offset: offsetToResolve })\n        },\n        heartbeat: () => this.heartbeat(),\n        /**\n         * Pause consumption for the current topic-partition being processed\n         */\n        pause,\n        /**\n         * Commit offsets if provided. Otherwise commit most recent resolved offsets\n         * if the autoCommit conditions are met.\n         *\n         * @param {import('../../types').OffsetsByTopicPartition} [offsets] Optional.\n         */\n        commitOffsetsIfNecessary: async offsets => {\n          return offsets\n            ? this.consumerGroup.commitOffsets(offsets)\n            : this.consumerGroup.commitOffsetsIfNecessary()\n        },\n        uncommittedOffsets: () => this.consumerGroup.uncommittedOffsets(),\n        isRunning: () => this.running,\n        isStale: () => this.consumerGroup.hasSeekOffset({ topic, partition }),\n      })\n    } catch (e) {\n      if (!isKafkaJSError(e)) {\n        this.logger.error(`Error when calling eachBatch`, {\n          topic,\n          partition,\n          offset: batch.firstOffset(),\n          stack: e.stack,\n          error: e,\n        })\n      }\n\n      // eachBatch has a special resolveOffset which can be used\n      // to keep track of the messages\n      await this.autoCommitOffsets()\n      throw e\n    }\n\n    // resolveOffset for the last offset can be disabled to allow the users of eachBatch to\n    // stop their consumers without resolving unprocessed offsets (issues/18)\n    if (this.eachBatchAutoResolve) {\n      this.consumerGroup.resolveOffset({ topic, partition, offset: batch.lastOffset() })\n    }\n  }\n\n  async fetch(nodeId) {\n    if (!this.running) {\n      this.logger.debug('consumer not running, exiting', {\n        groupId: this.consumerGroup.groupId,\n        memberId: this.consumerGroup.memberId,\n      })\n\n      return []\n    }\n\n    const startFetch = Date.now()\n\n    this.instrumentationEmitter.emit(FETCH_START, { nodeId })\n\n    const batches = await this.consumerGroup.fetch(nodeId)\n\n    this.instrumentationEmitter.emit(FETCH, {\n      /**\n       * PR #570 removed support for the number of batches in this instrumentation event;\n       * The new implementation uses an async generation to deliver the batches, which makes\n       * this number impossible to get. The number is set to 0 to keep the event backward\n       * compatible until we bump KafkaJS to version 2, following the end of node 8 LTS.\n       *\n       * @since 2019-11-29\n       */\n      numberOfBatches: 0,\n      duration: Date.now() - startFetch,\n      nodeId,\n    })\n\n    if (batches.length === 0) {\n      await this.heartbeat()\n    }\n\n    return batches\n  }\n\n  async handleBatch(batch) {\n    if (!this.running) {\n      this.logger.debug('consumer not running, exiting', {\n        groupId: this.consumerGroup.groupId,\n        memberId: this.consumerGroup.memberId,\n      })\n\n      return\n    }\n\n    /** @param {import('./batch')} batch */\n    const onBatch = async batch => {\n      const startBatchProcess = Date.now()\n      const payload = {\n        topic: batch.topic,\n        partition: batch.partition,\n        highWatermark: batch.highWatermark,\n        offsetLag: batch.offsetLag(),\n        /**\n         * @since 2019-06-24 (>= 1.8.0)\n         *\n         * offsetLag returns the lag based on the latest offset in the batch, to\n         * keep the event backward compatible we just introduced \"offsetLagLow\"\n         * which calculates the lag based on the first offset in the batch\n         */\n        offsetLagLow: batch.offsetLagLow(),\n        batchSize: batch.messages.length,\n        firstOffset: batch.firstOffset(),\n        lastOffset: batch.lastOffset(),\n      }\n\n      /**\n       * If the batch contained only control records or only aborted messages then we still\n       * need to resolve and auto-commit to ensure the consumer can move forward.\n       *\n       * We also need to emit batch instrumentation events to allow any listeners keeping\n       * track of offsets to know about the latest point of consumption.\n       *\n       * Added in #1256\n       *\n       * @see https://github.com/apache/kafka/blob/9aa660786e46c1efbf5605a6a69136a1dac6edb9/clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java#L1499-L1505\n       */\n      if (batch.isEmptyDueToFiltering()) {\n        this.instrumentationEmitter.emit(START_BATCH_PROCESS, payload)\n\n        this.consumerGroup.resolveOffset({\n          topic: batch.topic,\n          partition: batch.partition,\n          offset: batch.lastOffset(),\n        })\n        await this.autoCommitOffsetsIfNecessary()\n\n        this.instrumentationEmitter.emit(END_BATCH_PROCESS, {\n          ...payload,\n          duration: Date.now() - startBatchProcess,\n        })\n\n        await this.heartbeat()\n        return\n      }\n\n      if (batch.isEmpty()) {\n        await this.heartbeat()\n        return\n      }\n\n      this.instrumentationEmitter.emit(START_BATCH_PROCESS, payload)\n\n      if (this.eachMessage) {\n        await this.processEachMessage(batch)\n      } else if (this.eachBatch) {\n        await this.processEachBatch(batch)\n      }\n\n      this.instrumentationEmitter.emit(END_BATCH_PROCESS, {\n        ...payload,\n        duration: Date.now() - startBatchProcess,\n      })\n\n      await this.autoCommitOffsets()\n      await this.heartbeat()\n    }\n\n    await onBatch(batch)\n  }\n\n  autoCommitOffsets() {\n    if (this.autoCommit) {\n      return this.consumerGroup.commitOffsets()\n    }\n  }\n\n  autoCommitOffsetsIfNecessary() {\n    if (this.autoCommit) {\n      return this.consumerGroup.commitOffsetsIfNecessary()\n    }\n  }\n\n  commitOffsets(offsets) {\n    if (!this.running) {\n      this.logger.debug('consumer not running, exiting', {\n        groupId: this.consumerGroup.groupId,\n        memberId: this.consumerGroup.memberId,\n        offsets,\n      })\n      return\n    }\n\n    return this.retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await this.consumerGroup.commitOffsets(offsets)\n      } catch (e) {\n        if (!this.running) {\n          this.logger.debug('consumer not running, exiting', {\n            error: e.message,\n            groupId: this.consumerGroup.groupId,\n            memberId: this.consumerGroup.memberId,\n            offsets,\n          })\n          return\n        }\n\n        if (e.name === 'KafkaJSNotImplemented') {\n          return bail(e)\n        }\n\n        this.logger.debug('Error while committing offsets, trying again...', {\n          groupId: this.consumerGroup.groupId,\n          memberId: this.consumerGroup.memberId,\n          error: e.message,\n          stack: e.stack,\n          retryCount,\n          retryTime,\n          offsets,\n        })\n\n        throw e\n      }\n    })\n  }\n}\n", "const { MemberMetadata, MemberAssignment } = require('../../assignerProtocol')\n\n/**\n * RoundRobinAssigner\n * @type {import('types').PartitionAssigner}\n */\nmodule.exports = ({ cluster }) => ({\n  name: 'RoundRobinAssigner',\n  version: 0,\n\n  /**\n   * Assign the topics to the provided members.\n   *\n   * The members array contains information about each member, `memberMetadata` is the result of the\n   * `protocol` operation.\n   *\n   * @param {object} group\n   * @param {import('types').GroupMember[]} group.members array of members, e.g:\n                              [{ memberId: 'test-5f93f5a3', memberMetadata: Buffer }]\n   * @param {string[]} group.topics\n   * @returns {Promise<import('types').GroupMemberAssignment[]>} object partitions per topic per member, e.g:\n   *                   [\n   *                     {\n   *                       memberId: 'test-5f93f5a3',\n   *                       memberAssignment: {\n   *                         'topic-A': [0, 2, 4, 6],\n   *                         'topic-B': [1],\n   *                       },\n   *                     },\n   *                     {\n   *                       memberId: 'test-3d3d5341',\n   *                       memberAssignment: {\n   *                         'topic-A': [1, 3, 5],\n   *                         'topic-B': [0, 2],\n   *                       },\n   *                     }\n   *                   ]\n   */\n  async assign({ members, topics }) {\n    const membersCount = members.length\n    const sortedMembers = members.map(({ memberId }) => memberId).sort()\n    const assignment = {}\n\n    const topicsPartitions = topics.flatMap(topic => {\n      const partitionMetadata = cluster.findTopicPartitionMetadata(topic)\n      return partitionMetadata.map(m => ({ topic: topic, partitionId: m.partitionId }))\n    })\n\n    topicsPartitions.forEach((topicPartition, i) => {\n      const assignee = sortedMembers[i % membersCount]\n\n      if (!assignment[assignee]) {\n        assignment[assignee] = Object.create(null)\n      }\n\n      if (!assignment[assignee][topicPartition.topic]) {\n        assignment[assignee][topicPartition.topic] = []\n      }\n\n      assignment[assignee][topicPartition.topic].push(topicPartition.partitionId)\n    })\n\n    return Object.keys(assignment).map(memberId => ({\n      memberId,\n      memberAssignment: MemberAssignment.encode({\n        version: this.version,\n        assignment: assignment[memberId],\n      }),\n    }))\n  },\n\n  protocol({ topics }) {\n    return {\n      name: this.name,\n      metadata: MemberMetadata.encode({\n        version: this.version,\n        topics,\n      }),\n    }\n  },\n})\n", "const roundRobin = require('./roundRobinAssigner')\n\nmodule.exports = {\n  roundRobin,\n}\n", "const Long = require('../utils/long')\nconst createRetry = require('../retry')\nconst { initialRetryTime } = require('../retry/defaults')\nconst ConsumerGroup = require('./consumerGroup')\nconst Runner = require('./runner')\nconst { events, wrap: wrapEvent, unwrap: unwrapEvent } = require('./instrumentationEvents')\nconst InstrumentationEventEmitter = require('../instrumentation/emitter')\nconst { KafkaJSNonRetriableError } = require('../errors')\nconst { roundRobin } = require('./assigners')\nconst { EARLIEST_OFFSET, LATEST_OFFSET } = require('../constants')\nconst ISOLATION_LEVEL = require('../protocol/isolationLevel')\nconst sharedPromiseTo = require('../utils/sharedPromiseTo')\n\nconst { keys, values } = Object\nconst { CONNECT, DISCONNECT, STOP, CRASH } = events\n\nconst eventNames = values(events)\nconst eventKeys = keys(events)\n  .map(key => `consumer.events.${key}`)\n  .join(', ')\n\nconst specialOffsets = [\n  Long.fromValue(EARLIEST_OFFSET).toString(),\n  Long.fromValue(LATEST_OFFSET).toString(),\n]\n\n/**\n * @param {Object} params\n * @param {import(\"../../types\").Cluster} params.cluster\n * @param {String} params.groupId\n * @param {import('../../types').RetryOptions} [params.retry]\n * @param {import('../../types').Logger} params.logger\n * @param {import('../../types').PartitionAssigner[]} [params.partitionAssigners]\n * @param {number} [params.sessionTimeout]\n * @param {number} [params.rebalanceTimeout]\n * @param {number} [params.heartbeatInterval]\n * @param {number} [params.maxBytesPerPartition]\n * @param {number} [params.minBytes]\n * @param {number} [params.maxBytes]\n * @param {number} [params.maxWaitTimeInMs]\n * @param {number} [params.isolationLevel]\n * @param {string} [params.rackId]\n * @param {InstrumentationEventEmitter} [params.instrumentationEmitter]\n * @param {number} params.metadataMaxAge\n *\n * @returns {import(\"../../types\").Consumer}\n */\nmodule.exports = ({\n  cluster,\n  groupId,\n  retry,\n  logger: rootLogger,\n  partitionAssigners = [roundRobin],\n  sessionTimeout = 30000,\n  rebalanceTimeout = 60000,\n  heartbeatInterval = 3000,\n  maxBytesPerPartition = 1048576, // 1MB\n  minBytes = 1,\n  maxBytes = 10485760, // 10MB\n  maxWaitTimeInMs = 5000,\n  isolationLevel = ISOLATION_LEVEL.READ_COMMITTED,\n  rackId = '',\n  instrumentationEmitter: rootInstrumentationEmitter,\n  metadataMaxAge,\n}) => {\n  if (!groupId) {\n    throw new KafkaJSNonRetriableError('Consumer groupId must be a non-empty string.')\n  }\n\n  const logger = rootLogger.namespace('Consumer')\n  const instrumentationEmitter = rootInstrumentationEmitter || new InstrumentationEventEmitter()\n  const assigners = partitionAssigners.map(createAssigner =>\n    createAssigner({ groupId, logger, cluster })\n  )\n\n  /** @type {Record<string, { fromBeginning?: boolean }>} */\n  const topics = {}\n  let runner = null\n  /** @type {ConsumerGroup} */\n  let consumerGroup = null\n  let restartTimeout = null\n\n  if (heartbeatInterval >= sessionTimeout) {\n    throw new KafkaJSNonRetriableError(\n      `Consumer heartbeatInterval (${heartbeatInterval}) must be lower than sessionTimeout (${sessionTimeout}). It is recommended to set heartbeatInterval to approximately a third of the sessionTimeout.`\n    )\n  }\n\n  /** @type {import(\"../../types\").Consumer[\"connect\"]} */\n  const connect = async () => {\n    await cluster.connect()\n    instrumentationEmitter.emit(CONNECT)\n  }\n\n  /** @type {import(\"../../types\").Consumer[\"disconnect\"]} */\n  const disconnect = async () => {\n    try {\n      await stop()\n      logger.debug('consumer has stopped, disconnecting', { groupId })\n      await cluster.disconnect()\n      instrumentationEmitter.emit(DISCONNECT)\n    } catch (e) {\n      logger.error(`Caught error when disconnecting the consumer: ${e.message}`, {\n        stack: e.stack,\n        groupId,\n      })\n      throw e\n    }\n  }\n\n  /** @type {import(\"../../types\").Consumer[\"stop\"]} */\n  const stop = sharedPromiseTo(async () => {\n    try {\n      if (runner) {\n        await runner.stop()\n        runner = null\n        consumerGroup = null\n        instrumentationEmitter.emit(STOP)\n      }\n\n      clearTimeout(restartTimeout)\n      logger.info('Stopped', { groupId })\n    } catch (e) {\n      logger.error(`Caught error when stopping the consumer: ${e.message}`, {\n        stack: e.stack,\n        groupId,\n      })\n\n      throw e\n    }\n  })\n\n  /** @type {import(\"../../types\").Consumer[\"subscribe\"]} */\n  const subscribe = async ({ topic, topics: subscriptionTopics, fromBeginning = false }) => {\n    if (consumerGroup) {\n      throw new KafkaJSNonRetriableError('Cannot subscribe to topic while consumer is running')\n    }\n\n    if (!topic && !subscriptionTopics) {\n      throw new KafkaJSNonRetriableError('Missing required argument \"topics\"')\n    }\n\n    if (subscriptionTopics != null && !Array.isArray(subscriptionTopics)) {\n      throw new KafkaJSNonRetriableError('Argument \"topics\" must be an array')\n    }\n\n    const subscriptions = subscriptionTopics || [topic]\n\n    for (const subscription of subscriptions) {\n      if (typeof subscription !== 'string' && !(subscription instanceof RegExp)) {\n        throw new KafkaJSNonRetriableError(\n          `Invalid topic ${subscription} (${typeof subscription}), the topic name has to be a String or a RegExp`\n        )\n      }\n    }\n\n    const hasRegexSubscriptions = subscriptions.some(subscription => subscription instanceof RegExp)\n    const metadata = hasRegexSubscriptions ? await cluster.metadata() : undefined\n\n    const topicsToSubscribe = []\n    for (const subscription of subscriptions) {\n      const isRegExp = subscription instanceof RegExp\n      if (isRegExp) {\n        const topicRegExp = subscription\n        const matchedTopics = metadata.topicMetadata\n          .map(({ topic: topicName }) => topicName)\n          .filter(topicName => topicRegExp.test(topicName))\n\n        logger.debug('Subscription based on RegExp', {\n          groupId,\n          topicRegExp: topicRegExp.toString(),\n          matchedTopics,\n        })\n\n        topicsToSubscribe.push(...matchedTopics)\n      } else {\n        topicsToSubscribe.push(subscription)\n      }\n    }\n\n    for (const t of topicsToSubscribe) {\n      topics[t] = { fromBeginning }\n    }\n\n    await cluster.addMultipleTargetTopics(topicsToSubscribe)\n  }\n\n  /** @type {import(\"../../types\").Consumer[\"run\"]} */\n  const run = async ({\n    autoCommit = true,\n    autoCommitInterval = null,\n    autoCommitThreshold = null,\n    eachBatchAutoResolve = true,\n    partitionsConsumedConcurrently: concurrency = 1,\n    eachBatch = null,\n    eachMessage = null,\n  } = {}) => {\n    if (consumerGroup) {\n      logger.warn('consumer#run was called, but the consumer is already running', { groupId })\n      return\n    }\n\n    const start = async onCrash => {\n      logger.info('Starting', { groupId })\n\n      consumerGroup = new ConsumerGroup({\n        logger: rootLogger,\n        topics: keys(topics),\n        topicConfigurations: topics,\n        retry,\n        cluster,\n        groupId,\n        assigners,\n        sessionTimeout,\n        rebalanceTimeout,\n        maxBytesPerPartition,\n        minBytes,\n        maxBytes,\n        maxWaitTimeInMs,\n        instrumentationEmitter,\n        isolationLevel,\n        rackId,\n        metadataMaxAge,\n        autoCommit,\n        autoCommitInterval,\n        autoCommitThreshold,\n      })\n\n      runner = new Runner({\n        logger: rootLogger,\n        consumerGroup,\n        instrumentationEmitter,\n        heartbeatInterval,\n        retry,\n        autoCommit,\n        eachBatchAutoResolve,\n        eachBatch,\n        eachMessage,\n        onCrash,\n        concurrency,\n      })\n\n      await runner.start()\n    }\n\n    const onCrash = async e => {\n      logger.error(`Crash: ${e.name}: ${e.message}`, {\n        groupId,\n        retryCount: e.retryCount,\n        stack: e.stack,\n      })\n\n      if (e.name === 'KafkaJSConnectionClosedError') {\n        cluster.removeBroker({ host: e.host, port: e.port })\n      }\n\n      await disconnect()\n\n      const getOriginalCause = error => {\n        if (error.cause) {\n          return getOriginalCause(error.cause)\n        }\n\n        return error\n      }\n\n      const isErrorRetriable =\n        e.name === 'KafkaJSNumberOfRetriesExceeded' || getOriginalCause(e).retriable === true\n      const shouldRestart =\n        isErrorRetriable &&\n        (!retry ||\n          !retry.restartOnFailure ||\n          (await retry.restartOnFailure(e).catch(error => {\n            logger.error(\n              'Caught error when invoking user-provided \"restartOnFailure\" callback. Defaulting to restarting.',\n              {\n                error: error.message || error,\n                cause: e.message || e,\n                groupId,\n              }\n            )\n\n            return true\n          })))\n\n      instrumentationEmitter.emit(CRASH, {\n        error: e,\n        groupId,\n        restart: shouldRestart,\n      })\n\n      if (shouldRestart) {\n        const retryTime = e.retryTime || (retry && retry.initialRetryTime) || initialRetryTime\n        logger.error(`Restarting the consumer in ${retryTime}ms`, {\n          retryCount: e.retryCount,\n          retryTime,\n          groupId,\n        })\n\n        restartTimeout = setTimeout(() => start(onCrash), retryTime)\n      }\n    }\n\n    await start(onCrash)\n  }\n\n  /** @type {import(\"../../types\").Consumer[\"on\"]} */\n  const on = (eventName, listener) => {\n    if (!eventNames.includes(eventName)) {\n      throw new KafkaJSNonRetriableError(`Event name should be one of ${eventKeys}`)\n    }\n\n    return instrumentationEmitter.addListener(unwrapEvent(eventName), event => {\n      event.type = wrapEvent(event.type)\n      Promise.resolve(listener(event)).catch(e => {\n        logger.error(`Failed to execute listener: ${e.message}`, {\n          eventName,\n          stack: e.stack,\n        })\n      })\n    })\n  }\n\n  /**\n   * @type {import(\"../../types\").Consumer[\"commitOffsets\"]}\n   * @param topicPartitions\n   *   Example: [{ topic: 'topic-name', partition: 0, offset: '1', metadata: 'event-id-3' }]\n   */\n  const commitOffsets = async (topicPartitions = []) => {\n    const commitsByTopic = topicPartitions.reduce(\n      (payload, { topic, partition, offset, metadata = null }) => {\n        if (!topic) {\n          throw new KafkaJSNonRetriableError(`Invalid topic ${topic}`)\n        }\n\n        if (isNaN(partition)) {\n          throw new KafkaJSNonRetriableError(\n            `Invalid partition, expected a number received ${partition}`\n          )\n        }\n\n        let commitOffset\n        try {\n          commitOffset = Long.fromValue(offset)\n        } catch (_) {\n          throw new KafkaJSNonRetriableError(`Invalid offset, expected a long received ${offset}`)\n        }\n\n        if (commitOffset.lessThan(0)) {\n          throw new KafkaJSNonRetriableError('Offset must not be a negative number')\n        }\n\n        if (metadata !== null && typeof metadata !== 'string') {\n          throw new KafkaJSNonRetriableError(\n            `Invalid offset metadata, expected string or null, received ${metadata}`\n          )\n        }\n\n        const topicCommits = payload[topic] || []\n\n        topicCommits.push({ partition, offset: commitOffset, metadata })\n\n        return { ...payload, [topic]: topicCommits }\n      },\n      {}\n    )\n\n    if (!consumerGroup) {\n      throw new KafkaJSNonRetriableError(\n        'Consumer group was not initialized, consumer#run must be called first'\n      )\n    }\n\n    const topics = Object.keys(commitsByTopic)\n\n    return runner.commitOffsets({\n      topics: topics.map(topic => {\n        return {\n          topic,\n          partitions: commitsByTopic[topic],\n        }\n      }),\n    })\n  }\n\n  /** @type {import(\"../../types\").Consumer[\"seek\"]} */\n  const seek = ({ topic, partition, offset }) => {\n    if (!topic) {\n      throw new KafkaJSNonRetriableError(`Invalid topic ${topic}`)\n    }\n\n    if (isNaN(partition)) {\n      throw new KafkaJSNonRetriableError(\n        `Invalid partition, expected a number received ${partition}`\n      )\n    }\n\n    let seekOffset\n    try {\n      seekOffset = Long.fromValue(offset)\n    } catch (_) {\n      throw new KafkaJSNonRetriableError(`Invalid offset, expected a long received ${offset}`)\n    }\n\n    if (seekOffset.lessThan(0) && !specialOffsets.includes(seekOffset.toString())) {\n      throw new KafkaJSNonRetriableError('Offset must not be a negative number')\n    }\n\n    if (!consumerGroup) {\n      throw new KafkaJSNonRetriableError(\n        'Consumer group was not initialized, consumer#run must be called first'\n      )\n    }\n\n    consumerGroup.seek({ topic, partition, offset: seekOffset.toString() })\n  }\n\n  /** @type {import(\"../../types\").Consumer[\"describeGroup\"]} */\n  const describeGroup = async () => {\n    const coordinator = await cluster.findGroupCoordinator({ groupId })\n    const retrier = createRetry(retry)\n    return retrier(async () => {\n      const { groups } = await coordinator.describeGroups({ groupIds: [groupId] })\n      return groups.find(group => group.groupId === groupId)\n    })\n  }\n\n  /**\n   * @type {import(\"../../types\").Consumer[\"pause\"]}\n   * @param topicPartitions\n   *   Example: [{ topic: 'topic-name', partitions: [1, 2] }]\n   */\n  const pause = (topicPartitions = []) => {\n    for (const topicPartition of topicPartitions) {\n      if (!topicPartition || !topicPartition.topic) {\n        throw new KafkaJSNonRetriableError(\n          `Invalid topic ${(topicPartition && topicPartition.topic) || topicPartition}`\n        )\n      } else if (\n        typeof topicPartition.partitions !== 'undefined' &&\n        (!Array.isArray(topicPartition.partitions) || topicPartition.partitions.some(isNaN))\n      ) {\n        throw new KafkaJSNonRetriableError(\n          `Array of valid partitions required to pause specific partitions instead of ${topicPartition.partitions}`\n        )\n      }\n    }\n\n    if (!consumerGroup) {\n      throw new KafkaJSNonRetriableError(\n        'Consumer group was not initialized, consumer#run must be called first'\n      )\n    }\n\n    consumerGroup.pause(topicPartitions)\n  }\n\n  /**\n   * Returns the list of topic partitions paused on this consumer\n   *\n   * @type {import(\"../../types\").Consumer[\"paused\"]}\n   */\n  const paused = () => {\n    if (!consumerGroup) {\n      return []\n    }\n\n    return consumerGroup.paused()\n  }\n\n  /**\n   * @type {import(\"../../types\").Consumer[\"resume\"]}\n   * @param topicPartitions\n   *  Example: [{ topic: 'topic-name', partitions: [1, 2] }]\n   */\n  const resume = (topicPartitions = []) => {\n    for (const topicPartition of topicPartitions) {\n      if (!topicPartition || !topicPartition.topic) {\n        throw new KafkaJSNonRetriableError(\n          `Invalid topic ${(topicPartition && topicPartition.topic) || topicPartition}`\n        )\n      } else if (\n        typeof topicPartition.partitions !== 'undefined' &&\n        (!Array.isArray(topicPartition.partitions) || topicPartition.partitions.some(isNaN))\n      ) {\n        throw new KafkaJSNonRetriableError(\n          `Array of valid partitions required to resume specific partitions instead of ${topicPartition.partitions}`\n        )\n      }\n    }\n\n    if (!consumerGroup) {\n      throw new KafkaJSNonRetriableError(\n        'Consumer group was not initialized, consumer#run must be called first'\n      )\n    }\n\n    consumerGroup.resume(topicPartitions)\n  }\n\n  /**\n   * @return {Object} logger\n   */\n  const getLogger = () => logger\n\n  return {\n    connect,\n    disconnect,\n    subscribe,\n    stop,\n    run,\n    commitOffsets,\n    seek,\n    describeGroup,\n    pause,\n    paused,\n    resume,\n    on,\n    events,\n    logger: getLogger,\n  }\n}\n", "const sleep = require('./sleep')\nconst { KafkaJSTimeout } = require('../errors')\n\nmodule.exports = (\n  fn,\n  { delay = 50, maxWait = 10000, timeoutMessage = 'Timeout', ignoreTimeout = false } = {}\n) => {\n  let timeoutId\n  let totalWait = 0\n  let fulfilled = false\n\n  const checkCondition = async (resolve, reject) => {\n    totalWait += delay\n    if (fulfilled) {\n      return\n    }\n\n    await sleep(delay)\n\n    try {\n      const result = await fn(totalWait)\n      if (result) {\n        fulfilled = true\n        clearTimeout(timeoutId)\n        return resolve(result)\n      }\n\n      checkCondition(resolve, reject)\n    } catch (e) {\n      fulfilled = true\n      clearTimeout(timeoutId)\n      reject(e)\n    }\n  }\n\n  return new Promise((resolve, reject) => {\n    checkCondition(resolve, reject)\n\n    if (ignoreTimeout) {\n      return\n    }\n\n    timeoutId = setTimeout(() => {\n      if (!fulfilled) {\n        fulfilled = true\n        return reject(new KafkaJSTimeout(timeoutMessage))\n      }\n    }, maxWait)\n  })\n}\n", "module.exports = async (array, groupFn) => {\n  const result = new Map()\n\n  for (const item of array) {\n    const group = await Promise.resolve(groupFn(item))\n    result.set(group, result.has(group) ? [...result.get(group), item] : [item])\n  }\n\n  return result\n}\n", "const swapObject = require('../utils/swapObject')\nconst networkEvents = require('../network/instrumentationEvents')\nconst InstrumentationEventType = require('../instrumentation/eventType')\nconst adminType = InstrumentationEventType('admin')\n\nconst events = {\n  CONNECT: adminType('connect'),\n  DISCONNECT: adminType('disconnect'),\n  REQUEST: adminType(networkEvents.NETWORK_REQUEST),\n  REQUEST_TIMEOUT: adminType(networkEvents.NETWORK_REQUEST_TIMEOUT),\n  REQUEST_QUEUE_SIZE: adminType(networkEvents.NETWORK_REQUEST_QUEUE_SIZE),\n}\n\nconst wrappedEvents = {\n  [events.REQUEST]: networkEvents.NETWORK_REQUEST,\n  [events.REQUEST_TIMEOUT]: networkEvents.NETWORK_REQUEST_TIMEOUT,\n  [events.REQUEST_QUEUE_SIZE]: networkEvents.NETWORK_REQUEST_QUEUE_SIZE,\n}\n\nconst reversedWrappedEvents = swapObject(wrappedEvents)\nconst unwrap = eventName => wrappedEvents[eventName] || eventName\nconst wrap = eventName => reversedWrappedEvents[eventName] || eventName\n\nmodule.exports = {\n  events,\n  wrap,\n  unwrap,\n}\n", "/**\n * @see https://github.com/apache/kafka/blob/a15387f34d142684859c2a57fcbef25edcdce25a/clients/src/main/java/org/apache/kafka/common/resource/ResourceType.java#L25-L31\n * @typedef {number} ACLResourceTypes\n *\n * Enum for ACL Resource Types\n * @readonly\n * @enum {ACLResourceTypes}\n */\n\nmodule.exports = {\n  /**\n   * Represents any ResourceType which this client cannot understand,\n   * perhaps because this client is too old.\n   */\n  UNKNOWN: 0,\n  /**\n   * In a filter, matches any ResourceType.\n   */\n  ANY: 1,\n  /**\n   * A Kafka topic.\n   * @see http://kafka.apache.org/documentation/#topicconfigs\n   */\n  TOPIC: 2,\n  /**\n   * A consumer group.\n   * @see http://kafka.apache.org/documentation/#consumerconfigs\n   */\n  GROUP: 3,\n  /**\n   * The cluster as a whole.\n   */\n  CLUSTER: 4,\n  /**\n   * A transactional ID.\n   */\n  TRANSACTIONAL_ID: 5,\n  /**\n   * A token ID.\n   */\n  DELEGATION_TOKEN: 6,\n}\n", "// From:\n// https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/common/acl/AclOperation.java#L44\n\n/**\n * @typedef {number} ACLOperationTypes\n *\n * Enum for ACL Operations Types\n * @readonly\n * @enum {ACLOperationTypes}\n */\nmodule.exports = {\n  /**\n   * Represents any AclOperation which this client cannot understand, perhaps because this\n   * client is too old.\n   */\n  UNKNOWN: 0,\n  /**\n   * In a filter, matches any AclOperation.\n   */\n  ANY: 1,\n  /**\n   * ALL operation.\n   */\n  ALL: 2,\n  /**\n   * READ operation.\n   */\n  READ: 3,\n  /**\n   * WRITE operation.\n   */\n  WRITE: 4,\n  /**\n   * CREATE operation.\n   */\n  CREATE: 5,\n  /**\n   * DELETE operation.\n   */\n  DELETE: 6,\n  /**\n   * ALTER operation.\n   */\n  ALTER: 7,\n  /**\n   * DESCRIBE operation.\n   */\n  DESCRIBE: 8,\n  /**\n   * CLUSTER_ACTION operation.\n   */\n  CLUSTER_ACTION: 9,\n  /**\n   * DESCRIBE_CONFIGS operation.\n   */\n  DESCRIBE_CONFIGS: 10,\n  /**\n   * ALTER_CONFIGS operation.\n   */\n  ALTER_CONFIGS: 11,\n  /**\n   * IDEMPOTENT_WRITE operation.\n   */\n  IDEMPOTENT_WRITE: 12,\n}\n", "// From:\n// https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/common/acl/AclPermissionType.java/#L31\n\n/**\n * @typedef {number} ACLPermissionTypes\n *\n * Enum for Permission Types\n * @readonly\n * @enum {ACLPermissionTypes}\n */\nmodule.exports = {\n  /**\n   * Represents any AclPermissionType which this client cannot understand,\n   * perhaps because this client is too old.\n   */\n  UNKNOWN: 0,\n  /**\n   * In a filter, matches any AclPermissionType.\n   */\n  ANY: 1,\n  /**\n   * Disallows access.\n   */\n  DENY: 2,\n  /**\n   * Grants access.\n   */\n  ALLOW: 3,\n}\n", "// From:\n// https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/common/resource/PatternType.java#L32\n\n/**\n * @typedef {number} ACLResourcePatternTypes\n *\n * Enum for ACL Resource Pattern Type\n * @readonly\n * @enum {ACLResourcePatternTypes}\n */\nmodule.exports = {\n  /**\n   * Represents any PatternType which this client cannot understand, perhaps because this client is too old.\n   */\n  UNKNOWN: 0,\n  /**\n   * In a filter, matches any resource pattern type.\n   */\n  ANY: 1,\n  /**\n   * In a filter, will perform pattern matching.\n   *\n   * e.g. Given a filter of {@code ResourcePatternFilter(TOPIC, \"payments.received\", MATCH)`}, the filter match\n   * any {@link ResourcePattern} that matches topic 'payments.received'. This might include:\n   * <ul>\n   *     <li>A Literal pattern with the same type and name, e.g. {@code ResourcePattern(TOPIC, \"payments.received\", LITERAL)}</li>\n   *     <li>A Wildcard pattern with the same type, e.g. {@code ResourcePattern(TOPIC, \"*\", LITERAL)}</li>\n   *     <li>A Prefixed pattern with the same type and where the name is a matching prefix, e.g. {@code ResourcePattern(TOPIC, \"payments.\", PREFIXED)}</li>\n   * </ul>\n   */\n  MATCH: 2,\n  /**\n   * A literal resource name.\n   *\n   * A literal name defines the full name of a resource, e.g. topic with name 'foo', or group with name 'bob'.\n   *\n   * The special wildcard character {@code *} can be used to represent a resource with any name.\n   */\n  LITERAL: 3,\n  /**\n   * A prefixed resource name.\n   *\n   * A prefixed name defines a prefix for a resource, e.g. topics with names that start with 'foo'.\n   */\n  PREFIXED: 4,\n}\n", "const createRetry = require('../retry')\nconst waitFor = require('../utils/waitFor')\nconst groupBy = require('../utils/groupBy')\nconst createConsumer = require('../consumer')\nconst InstrumentationEventEmitter = require('../instrumentation/emitter')\nconst { events, wrap: wrapEvent, unwrap: unwrapEvent } = require('./instrumentationEvents')\nconst { LEVELS } = require('../loggers')\nconst {\n  KafkaJSNonRetriableError,\n  KafkaJSDeleteGroupsError,\n  KafkaJSBrokerNotFound,\n  KafkaJSDeleteTopicRecordsError,\n  KafkaJSAggregateError,\n} = require('../errors')\nconst { staleMetadata } = require('../protocol/error')\nconst CONFIG_RESOURCE_TYPES = require('../protocol/configResourceTypes')\nconst ACL_RESOURCE_TYPES = require('../protocol/aclResourceTypes')\nconst ACL_OPERATION_TYPES = require('../protocol/aclOperationTypes')\nconst ACL_PERMISSION_TYPES = require('../protocol/aclPermissionTypes')\nconst RESOURCE_PATTERN_TYPES = require('../protocol/resourcePatternTypes')\nconst { EARLIEST_OFFSET, LATEST_OFFSET } = require('../constants')\n\nconst { CONNECT, DISCONNECT } = events\n\nconst NO_CONTROLLER_ID = -1\n\nconst { values, keys, entries } = Object\nconst eventNames = values(events)\nconst eventKeys = keys(events)\n  .map(key => `admin.events.${key}`)\n  .join(', ')\n\nconst retryOnLeaderNotAvailable = (fn, opts = {}) => {\n  const callback = async () => {\n    try {\n      return await fn()\n    } catch (e) {\n      if (e.type !== 'LEADER_NOT_AVAILABLE') {\n        throw e\n      }\n      return false\n    }\n  }\n\n  return waitFor(callback, opts)\n}\n\nconst isConsumerGroupRunning = description => ['Empty', 'Dead'].includes(description.state)\nconst findTopicPartitions = async (cluster, topic) => {\n  await cluster.addTargetTopic(topic)\n  await cluster.refreshMetadataIfNecessary()\n\n  return cluster\n    .findTopicPartitionMetadata(topic)\n    .map(({ partitionId }) => partitionId)\n    .sort()\n}\nconst indexByPartition = array =>\n  array.reduce(\n    (obj, { partition, ...props }) => Object.assign(obj, { [partition]: { ...props } }),\n    {}\n  )\n\n/**\n *\n * @param {Object} params\n * @param {import(\"../../types\").Logger} params.logger\n * @param {InstrumentationEventEmitter} [params.instrumentationEmitter]\n * @param {import('../../types').RetryOptions} params.retry\n * @param {import(\"../../types\").Cluster} params.cluster\n *\n * @returns {import(\"../../types\").Admin}\n */\nmodule.exports = ({\n  logger: rootLogger,\n  instrumentationEmitter: rootInstrumentationEmitter,\n  retry,\n  cluster,\n}) => {\n  const logger = rootLogger.namespace('Admin')\n  const instrumentationEmitter = rootInstrumentationEmitter || new InstrumentationEventEmitter()\n\n  /**\n   * @returns {Promise}\n   */\n  const connect = async () => {\n    await cluster.connect()\n    instrumentationEmitter.emit(CONNECT)\n  }\n\n  /**\n   * @return {Promise}\n   */\n  const disconnect = async () => {\n    await cluster.disconnect()\n    instrumentationEmitter.emit(DISCONNECT)\n  }\n\n  /**\n   * @return {Promise}\n   */\n  const listTopics = async () => {\n    const { topicMetadata } = await cluster.metadata()\n    const topics = topicMetadata.map(t => t.topic)\n    return topics\n  }\n\n  /**\n   * @param {Object} request\n   * @param {array} request.topics\n   * @param {boolean} [request.validateOnly=false]\n   * @param {number} [request.timeout=5000]\n   * @param {boolean} [request.waitForLeaders=true]\n   * @return {Promise}\n   */\n  const createTopics = async ({ topics, validateOnly, timeout, waitForLeaders = true }) => {\n    if (!topics || !Array.isArray(topics)) {\n      throw new KafkaJSNonRetriableError(`Invalid topics array ${topics}`)\n    }\n\n    if (topics.filter(({ topic }) => typeof topic !== 'string').length > 0) {\n      throw new KafkaJSNonRetriableError(\n        'Invalid topics array, the topic names have to be a valid string'\n      )\n    }\n\n    const topicNames = new Set(topics.map(({ topic }) => topic))\n    if (topicNames.size < topics.length) {\n      throw new KafkaJSNonRetriableError(\n        'Invalid topics array, it cannot have multiple entries for the same topic'\n      )\n    }\n\n    for (const { topic, configEntries } of topics) {\n      if (configEntries == null) {\n        continue\n      }\n\n      if (!Array.isArray(configEntries)) {\n        throw new KafkaJSNonRetriableError(\n          `Invalid configEntries for topic \"${topic}\", must be an array`\n        )\n      }\n\n      configEntries.forEach((entry, index) => {\n        if (typeof entry !== 'object' || entry == null) {\n          throw new KafkaJSNonRetriableError(\n            `Invalid configEntries for topic \"${topic}\". Entry ${index} must be an object`\n          )\n        }\n\n        for (const requiredProperty of ['name', 'value']) {\n          if (\n            !Object.prototype.hasOwnProperty.call(entry, requiredProperty) ||\n            typeof entry[requiredProperty] !== 'string'\n          ) {\n            throw new KafkaJSNonRetriableError(\n              `Invalid configEntries for topic \"${topic}\". Entry ${index} must have a valid \"${requiredProperty}\" property`\n            )\n          }\n        }\n      })\n    }\n\n    const retrier = createRetry(retry)\n\n    return retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await cluster.refreshMetadata()\n        const broker = await cluster.findControllerBroker()\n        await broker.createTopics({ topics, validateOnly, timeout })\n\n        if (waitForLeaders) {\n          const topicNamesArray = Array.from(topicNames.values())\n          await retryOnLeaderNotAvailable(async () => await broker.metadata(topicNamesArray), {\n            delay: 100,\n            maxWait: timeout,\n            timeoutMessage: 'Timed out while waiting for topic leaders',\n          })\n        }\n\n        return true\n      } catch (e) {\n        if (e.type === 'NOT_CONTROLLER') {\n          logger.warn('Could not create topics', { error: e.message, retryCount, retryTime })\n          throw e\n        }\n\n        if (e instanceof KafkaJSAggregateError) {\n          if (e.errors.every(error => error.type === 'TOPIC_ALREADY_EXISTS')) {\n            return false\n          }\n        }\n\n        bail(e)\n      }\n    })\n  }\n  /**\n   * @param {array} topicPartitions\n   * @param {boolean} [validateOnly=false]\n   * @param {number} [timeout=5000]\n   * @return {Promise<void>}\n   */\n  const createPartitions = async ({ topicPartitions, validateOnly, timeout }) => {\n    if (!topicPartitions || !Array.isArray(topicPartitions)) {\n      throw new KafkaJSNonRetriableError(`Invalid topic partitions array ${topicPartitions}`)\n    }\n    if (topicPartitions.length === 0) {\n      throw new KafkaJSNonRetriableError(`Empty topic partitions array`)\n    }\n\n    if (topicPartitions.filter(({ topic }) => typeof topic !== 'string').length > 0) {\n      throw new KafkaJSNonRetriableError(\n        'Invalid topic partitions array, the topic names have to be a valid string'\n      )\n    }\n\n    const topicNames = new Set(topicPartitions.map(({ topic }) => topic))\n    if (topicNames.size < topicPartitions.length) {\n      throw new KafkaJSNonRetriableError(\n        'Invalid topic partitions array, it cannot have multiple entries for the same topic'\n      )\n    }\n\n    const retrier = createRetry(retry)\n\n    return retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await cluster.refreshMetadata()\n        const broker = await cluster.findControllerBroker()\n        await broker.createPartitions({ topicPartitions, validateOnly, timeout })\n      } catch (e) {\n        if (e.type === 'NOT_CONTROLLER') {\n          logger.warn('Could not create topics', { error: e.message, retryCount, retryTime })\n          throw e\n        }\n\n        bail(e)\n      }\n    })\n  }\n\n  /**\n   * @param {string[]} topics\n   * @param {number} [timeout=5000]\n   * @return {Promise}\n   */\n  const deleteTopics = async ({ topics, timeout }) => {\n    if (!topics || !Array.isArray(topics)) {\n      throw new KafkaJSNonRetriableError(`Invalid topics array ${topics}`)\n    }\n\n    if (topics.filter(topic => typeof topic !== 'string').length > 0) {\n      throw new KafkaJSNonRetriableError('Invalid topics array, the names must be a valid string')\n    }\n\n    const retrier = createRetry(retry)\n\n    return retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await cluster.refreshMetadata()\n        const broker = await cluster.findControllerBroker()\n        await broker.deleteTopics({ topics, timeout })\n\n        // Remove deleted topics\n        for (const topic of topics) {\n          cluster.targetTopics.delete(topic)\n        }\n\n        await cluster.refreshMetadata()\n      } catch (e) {\n        if (['NOT_CONTROLLER', 'UNKNOWN_TOPIC_OR_PARTITION'].includes(e.type)) {\n          logger.warn('Could not delete topics', { error: e.message, retryCount, retryTime })\n          throw e\n        }\n\n        if (e.type === 'REQUEST_TIMED_OUT') {\n          logger.error(\n            'Could not delete topics, check if \"delete.topic.enable\" is set to \"true\" (the default value is \"false\") or increase the timeout',\n            {\n              error: e.message,\n              retryCount,\n              retryTime,\n            }\n          )\n        }\n\n        bail(e)\n      }\n    })\n  }\n\n  /**\n   * @param {string} topic\n   */\n\n  const fetchTopicOffsets = async topic => {\n    if (!topic || typeof topic !== 'string') {\n      throw new KafkaJSNonRetriableError(`Invalid topic ${topic}`)\n    }\n\n    const retrier = createRetry(retry)\n\n    return retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await cluster.addTargetTopic(topic)\n        await cluster.refreshMetadataIfNecessary()\n\n        const metadata = cluster.findTopicPartitionMetadata(topic)\n        const high = await cluster.fetchTopicsOffset([\n          {\n            topic,\n            fromBeginning: false,\n            partitions: metadata.map(p => ({ partition: p.partitionId })),\n          },\n        ])\n\n        const low = await cluster.fetchTopicsOffset([\n          {\n            topic,\n            fromBeginning: true,\n            partitions: metadata.map(p => ({ partition: p.partitionId })),\n          },\n        ])\n\n        const { partitions: highPartitions } = high.pop()\n        const { partitions: lowPartitions } = low.pop()\n        return highPartitions.map(({ partition, offset }) => ({\n          partition,\n          offset,\n          high: offset,\n          low: lowPartitions.find(({ partition: lowPartition }) => lowPartition === partition)\n            .offset,\n        }))\n      } catch (e) {\n        if (e.type === 'UNKNOWN_TOPIC_OR_PARTITION') {\n          await cluster.refreshMetadata()\n          throw e\n        }\n\n        bail(e)\n      }\n    })\n  }\n\n  /**\n   * @param {string} topic\n   * @param {number} [timestamp]\n   */\n\n  const fetchTopicOffsetsByTimestamp = async (topic, timestamp) => {\n    if (!topic || typeof topic !== 'string') {\n      throw new KafkaJSNonRetriableError(`Invalid topic ${topic}`)\n    }\n\n    const retrier = createRetry(retry)\n\n    return retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await cluster.addTargetTopic(topic)\n        await cluster.refreshMetadataIfNecessary()\n\n        const metadata = cluster.findTopicPartitionMetadata(topic)\n        const partitions = metadata.map(p => ({ partition: p.partitionId }))\n\n        const high = await cluster.fetchTopicsOffset([\n          {\n            topic,\n            fromBeginning: false,\n            partitions,\n          },\n        ])\n        const { partitions: highPartitions } = high.pop()\n\n        const offsets = await cluster.fetchTopicsOffset([\n          {\n            topic,\n            fromTimestamp: timestamp,\n            partitions,\n          },\n        ])\n        const { partitions: lowPartitions } = offsets.pop()\n\n        return lowPartitions.map(({ partition, offset }) => ({\n          partition,\n          offset:\n            parseInt(offset, 10) >= 0\n              ? offset\n              : highPartitions.find(({ partition: highPartition }) => highPartition === partition)\n                  .offset,\n        }))\n      } catch (e) {\n        if (e.type === 'UNKNOWN_TOPIC_OR_PARTITION') {\n          await cluster.refreshMetadata()\n          throw e\n        }\n\n        bail(e)\n      }\n    })\n  }\n\n  /**\n   * Fetch offsets for a topic or multiple topics\n   *\n   * Note: set either topic or topics but not both.\n   *\n   * @param {string} groupId\n   * @param {string[]} topics - list of topics to fetch offsets for, defaults to `[]` which fetches all topics for `groupId`.\n   * @param {boolean} [resolveOffsets=false]\n   * @return {Promise}\n   */\n  const fetchOffsets = async ({ groupId, topics, resolveOffsets = false }) => {\n    if (!groupId) {\n      throw new KafkaJSNonRetriableError(`Invalid groupId ${groupId}`)\n    }\n\n    if (!topics) {\n      topics = []\n    }\n\n    if (!Array.isArray(topics)) {\n      throw new KafkaJSNonRetriableError('Expected topics array to be set')\n    }\n\n    const coordinator = await cluster.findGroupCoordinator({ groupId })\n    const topicsToFetch = await Promise.all(\n      topics.map(async topic => {\n        const partitions = await findTopicPartitions(cluster, topic)\n        const partitionsToFetch = partitions.map(partition => ({ partition }))\n        return { topic, partitions: partitionsToFetch }\n      })\n    )\n    let { responses: consumerOffsets } = await coordinator.offsetFetch({\n      groupId,\n      topics: topicsToFetch,\n    })\n\n    if (resolveOffsets) {\n      consumerOffsets = await Promise.all(\n        consumerOffsets.map(async ({ topic, partitions }) => {\n          const indexedOffsets = indexByPartition(await fetchTopicOffsets(topic))\n          const recalculatedPartitions = partitions.map(({ offset, partition, ...props }) => {\n            let resolvedOffset = offset\n            if (Number(offset) === EARLIEST_OFFSET) {\n              resolvedOffset = indexedOffsets[partition].low\n            }\n            if (Number(offset) === LATEST_OFFSET) {\n              resolvedOffset = indexedOffsets[partition].high\n            }\n            return {\n              partition,\n              offset: resolvedOffset,\n              ...props,\n            }\n          })\n\n          await setOffsets({ groupId, topic, partitions: recalculatedPartitions })\n\n          return {\n            topic,\n            partitions: recalculatedPartitions,\n          }\n        })\n      )\n    }\n\n    return consumerOffsets.map(({ topic, partitions }) => {\n      const completePartitions = partitions.map(({ partition, offset, metadata }) => ({\n        partition,\n        offset,\n        metadata: metadata || null,\n      }))\n\n      return { topic, partitions: completePartitions }\n    })\n  }\n\n  /**\n   * @param {string} groupId\n   * @param {string} topic\n   * @param {boolean} [earliest=false]\n   * @return {Promise}\n   */\n  const resetOffsets = async ({ groupId, topic, earliest = false }) => {\n    if (!groupId) {\n      throw new KafkaJSNonRetriableError(`Invalid groupId ${groupId}`)\n    }\n\n    if (!topic) {\n      throw new KafkaJSNonRetriableError(`Invalid topic ${topic}`)\n    }\n\n    const partitions = await findTopicPartitions(cluster, topic)\n    const partitionsToSeek = partitions.map(partition => ({\n      partition,\n      offset: cluster.defaultOffset({ fromBeginning: earliest }),\n    }))\n\n    return setOffsets({ groupId, topic, partitions: partitionsToSeek })\n  }\n\n  /**\n   * @param {string} groupId\n   * @param {string} topic\n   * @param {Array<SeekEntry>} partitions\n   * @return {Promise}\n   *\n   * @typedef {Object} SeekEntry\n   * @property {number} partition\n   * @property {string} offset\n   */\n  const setOffsets = async ({ groupId, topic, partitions }) => {\n    if (!groupId) {\n      throw new KafkaJSNonRetriableError(`Invalid groupId ${groupId}`)\n    }\n\n    if (!topic) {\n      throw new KafkaJSNonRetriableError(`Invalid topic ${topic}`)\n    }\n\n    if (!partitions || partitions.length === 0) {\n      throw new KafkaJSNonRetriableError(`Invalid partitions`)\n    }\n\n    const consumer = createConsumer({\n      logger: rootLogger.namespace('Admin', LEVELS.NOTHING),\n      cluster,\n      groupId,\n    })\n\n    await consumer.subscribe({ topic, fromBeginning: true })\n    const description = await consumer.describeGroup()\n\n    if (!isConsumerGroupRunning(description)) {\n      throw new KafkaJSNonRetriableError(\n        `The consumer group must have no running instances, current state: ${description.state}`\n      )\n    }\n\n    return new Promise((resolve, reject) => {\n      consumer.on(consumer.events.FETCH, async () =>\n        consumer\n          .stop()\n          .then(resolve)\n          .catch(reject)\n      )\n\n      consumer\n        .run({\n          eachBatchAutoResolve: false,\n          eachBatch: async () => true,\n        })\n        .catch(reject)\n\n      // This consumer doesn't need to consume any data\n      consumer.pause([{ topic }])\n\n      for (const seekData of partitions) {\n        consumer.seek({ topic, ...seekData })\n      }\n    })\n  }\n\n  const isBrokerConfig = type =>\n    [CONFIG_RESOURCE_TYPES.BROKER, CONFIG_RESOURCE_TYPES.BROKER_LOGGER].includes(type)\n\n  /**\n   * Broker configs can only be returned by the target broker\n   *\n   * @see\n   * https://github.com/apache/kafka/blob/821c1ac6641845aeca96a43bc2b946ecec5cba4f/clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java#L3783\n   * https://github.com/apache/kafka/blob/821c1ac6641845aeca96a43bc2b946ecec5cba4f/clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java#L2027\n   *\n   * @param {Broker} defaultBroker. Broker used in case the configuration is not a broker config\n   */\n  const groupResourcesByBroker = ({ resources, defaultBroker }) =>\n    groupBy(resources, async ({ type, name: nodeId }) => {\n      return isBrokerConfig(type)\n        ? await cluster.findBroker({ nodeId: String(nodeId) })\n        : defaultBroker\n    })\n\n  /**\n   * @param {Array<ResourceConfigQuery>} resources\n   * @param {boolean} [includeSynonyms=false]\n   * @return {Promise}\n   *\n   * @typedef {Object} ResourceConfigQuery\n   * @property {ConfigResourceType} type\n   * @property {string} name\n   * @property {Array<String>} [configNames=[]]\n   */\n  const describeConfigs = async ({ resources, includeSynonyms }) => {\n    if (!resources || !Array.isArray(resources)) {\n      throw new KafkaJSNonRetriableError(`Invalid resources array ${resources}`)\n    }\n\n    if (resources.length === 0) {\n      throw new KafkaJSNonRetriableError('Resources array cannot be empty')\n    }\n\n    const validResourceTypes = Object.values(CONFIG_RESOURCE_TYPES)\n    const invalidType = resources.find(r => !validResourceTypes.includes(r.type))\n\n    if (invalidType) {\n      throw new KafkaJSNonRetriableError(\n        `Invalid resource type ${invalidType.type}: ${JSON.stringify(invalidType)}`\n      )\n    }\n\n    const invalidName = resources.find(r => !r.name || typeof r.name !== 'string')\n\n    if (invalidName) {\n      throw new KafkaJSNonRetriableError(\n        `Invalid resource name ${invalidName.name}: ${JSON.stringify(invalidName)}`\n      )\n    }\n\n    const invalidConfigs = resources.find(\n      r => !Array.isArray(r.configNames) && r.configNames != null\n    )\n\n    if (invalidConfigs) {\n      const { configNames } = invalidConfigs\n      throw new KafkaJSNonRetriableError(\n        `Invalid resource configNames ${configNames}: ${JSON.stringify(invalidConfigs)}`\n      )\n    }\n\n    const retrier = createRetry(retry)\n\n    return retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await cluster.refreshMetadata()\n        const controller = await cluster.findControllerBroker()\n        const resourcerByBroker = await groupResourcesByBroker({\n          resources,\n          defaultBroker: controller,\n        })\n\n        const describeConfigsAction = async broker => {\n          const targetBroker = broker || controller\n          return targetBroker.describeConfigs({\n            resources: resourcerByBroker.get(targetBroker),\n            includeSynonyms,\n          })\n        }\n\n        const brokers = Array.from(resourcerByBroker.keys())\n        const responses = await Promise.all(brokers.map(describeConfigsAction))\n        const responseResources = responses.reduce(\n          (result, { resources }) => [...result, ...resources],\n          []\n        )\n\n        return { resources: responseResources }\n      } catch (e) {\n        if (e.type === 'NOT_CONTROLLER') {\n          logger.warn('Could not describe configs', { error: e.message, retryCount, retryTime })\n          throw e\n        }\n\n        bail(e)\n      }\n    })\n  }\n\n  /**\n   * @param {Array<ResourceConfig>} resources\n   * @param {boolean} [validateOnly=false]\n   * @return {Promise}\n   *\n   * @typedef {Object} ResourceConfig\n   * @property {ConfigResourceType} type\n   * @property {string} name\n   * @property {Array<ResourceConfigEntry>} configEntries\n   *\n   * @typedef {Object} ResourceConfigEntry\n   * @property {string} name\n   * @property {string} value\n   */\n  const alterConfigs = async ({ resources, validateOnly }) => {\n    if (!resources || !Array.isArray(resources)) {\n      throw new KafkaJSNonRetriableError(`Invalid resources array ${resources}`)\n    }\n\n    if (resources.length === 0) {\n      throw new KafkaJSNonRetriableError('Resources array cannot be empty')\n    }\n\n    const validResourceTypes = Object.values(CONFIG_RESOURCE_TYPES)\n    const invalidType = resources.find(r => !validResourceTypes.includes(r.type))\n\n    if (invalidType) {\n      throw new KafkaJSNonRetriableError(\n        `Invalid resource type ${invalidType.type}: ${JSON.stringify(invalidType)}`\n      )\n    }\n\n    const invalidName = resources.find(r => !r.name || typeof r.name !== 'string')\n\n    if (invalidName) {\n      throw new KafkaJSNonRetriableError(\n        `Invalid resource name ${invalidName.name}: ${JSON.stringify(invalidName)}`\n      )\n    }\n\n    const invalidConfigs = resources.find(r => !Array.isArray(r.configEntries))\n\n    if (invalidConfigs) {\n      const { configEntries } = invalidConfigs\n      throw new KafkaJSNonRetriableError(\n        `Invalid resource configEntries ${configEntries}: ${JSON.stringify(invalidConfigs)}`\n      )\n    }\n\n    const invalidConfigValue = resources.find(r =>\n      r.configEntries.some(e => typeof e.name !== 'string' || typeof e.value !== 'string')\n    )\n\n    if (invalidConfigValue) {\n      throw new KafkaJSNonRetriableError(\n        `Invalid resource config value: ${JSON.stringify(invalidConfigValue)}`\n      )\n    }\n\n    const retrier = createRetry(retry)\n\n    return retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await cluster.refreshMetadata()\n        const controller = await cluster.findControllerBroker()\n        const resourcerByBroker = await groupResourcesByBroker({\n          resources,\n          defaultBroker: controller,\n        })\n\n        const alterConfigsAction = async broker => {\n          const targetBroker = broker || controller\n          return targetBroker.alterConfigs({\n            resources: resourcerByBroker.get(targetBroker),\n            validateOnly: !!validateOnly,\n          })\n        }\n\n        const brokers = Array.from(resourcerByBroker.keys())\n        const responses = await Promise.all(brokers.map(alterConfigsAction))\n        const responseResources = responses.reduce(\n          (result, { resources }) => [...result, ...resources],\n          []\n        )\n\n        return { resources: responseResources }\n      } catch (e) {\n        if (e.type === 'NOT_CONTROLLER') {\n          logger.warn('Could not alter configs', { error: e.message, retryCount, retryTime })\n          throw e\n        }\n\n        bail(e)\n      }\n    })\n  }\n\n  /**\n   * Fetch metadata for provided topics.\n   *\n   * If no topics are provided fetch metadata for all topics.\n   * @see https://kafka.apache.org/protocol#The_Messages_Metadata\n   *\n   * @param {Object} [options]\n   * @param {string[]} [options.topics]\n   * @return {Promise<TopicsMetadata>}\n   *\n   * @typedef {Object} TopicsMetadata\n   * @property {Array<TopicMetadata>} topics\n   *\n   * @typedef {Object} TopicMetadata\n   * @property {String} name\n   * @property {Array<PartitionMetadata>} partitions\n   *\n   * @typedef {Object} PartitionMetadata\n   * @property {number} partitionErrorCode Response error code\n   * @property {number} partitionId Topic partition id\n   * @property {number} leader  The id of the broker acting as leader for this partition.\n   * @property {Array<number>} replicas The set of all nodes that host this partition.\n   * @property {Array<number>} isr The set of nodes that are in sync with the leader for this partition.\n   */\n  const fetchTopicMetadata = async ({ topics = [] } = {}) => {\n    if (topics) {\n      topics.forEach(topic => {\n        if (!topic || typeof topic !== 'string') {\n          throw new KafkaJSNonRetriableError(`Invalid topic ${topic}`)\n        }\n      })\n    }\n\n    const metadata = await cluster.metadata({ topics })\n\n    return {\n      topics: metadata.topicMetadata.map(topicMetadata => ({\n        name: topicMetadata.topic,\n        partitions: topicMetadata.partitionMetadata,\n      })),\n    }\n  }\n\n  /**\n   * Describe cluster\n   *\n   * @return {Promise<ClusterMetadata>}\n   *\n   * @typedef {Object} ClusterMetadata\n   * @property {Array<Broker>} brokers\n   * @property {Number} controller Current controller id. Returns null if unknown.\n   * @property {String} clusterId\n   *\n   * @typedef {Object} Broker\n   * @property {Number} nodeId\n   * @property {String} host\n   * @property {Number} port\n   */\n  const describeCluster = async () => {\n    const { brokers: nodes, clusterId, controllerId } = await cluster.metadata({ topics: [] })\n    const brokers = nodes.map(({ nodeId, host, port }) => ({\n      nodeId,\n      host,\n      port,\n    }))\n    const controller =\n      controllerId == null || controllerId === NO_CONTROLLER_ID ? null : controllerId\n\n    return {\n      brokers,\n      controller,\n      clusterId,\n    }\n  }\n\n  /**\n   * List groups in a broker\n   *\n   * @return {Promise<ListGroups>}\n   *\n   * @typedef {Object} ListGroups\n   * @property {Array<ListGroup>} groups\n   *\n   * @typedef {Object} ListGroup\n   * @property {string} groupId\n   * @property {string} protocolType\n   */\n  const listGroups = async () => {\n    await cluster.refreshMetadata()\n    let groups = []\n    for (var nodeId in cluster.brokerPool.brokers) {\n      const broker = await cluster.findBroker({ nodeId })\n      const response = await broker.listGroups()\n      groups = groups.concat(response.groups)\n    }\n\n    return { groups }\n  }\n\n  /**\n   * Describe groups by group ids\n   * @param {Array<string>} groupIds\n   *\n   * @typedef {Object} GroupDescriptions\n   * @property {Array<GroupDescription>} groups\n   *\n   * @return {Promise<GroupDescriptions>}\n   */\n  const describeGroups = async groupIds => {\n    const coordinatorsForGroup = await Promise.all(\n      groupIds.map(async groupId => {\n        const coordinator = await cluster.findGroupCoordinator({ groupId })\n        return {\n          coordinator,\n          groupId,\n        }\n      })\n    )\n\n    const groupsByCoordinator = Object.values(\n      coordinatorsForGroup.reduce((coordinators, { coordinator, groupId }) => {\n        const group = coordinators[coordinator.nodeId]\n\n        if (group) {\n          coordinators[coordinator.nodeId] = {\n            ...group,\n            groupIds: [...group.groupIds, groupId],\n          }\n        } else {\n          coordinators[coordinator.nodeId] = { coordinator, groupIds: [groupId] }\n        }\n        return coordinators\n      }, {})\n    )\n\n    const responses = await Promise.all(\n      groupsByCoordinator.map(async ({ coordinator, groupIds }) => {\n        const retrier = createRetry(retry)\n        const { groups } = await retrier(() => coordinator.describeGroups({ groupIds }))\n        return groups\n      })\n    )\n\n    const groups = [].concat.apply([], responses)\n\n    return { groups }\n  }\n\n  /**\n   * Delete groups in a broker\n   *\n   * @param {string[]} [groupIds]\n   * @return {Promise<DeleteGroups>}\n   *\n   * @typedef {Array} DeleteGroups\n   * @property {string} groupId\n   * @property {number} errorCode\n   */\n  const deleteGroups = async groupIds => {\n    if (!groupIds || !Array.isArray(groupIds)) {\n      throw new KafkaJSNonRetriableError(`Invalid groupIds array ${groupIds}`)\n    }\n\n    const invalidGroupId = groupIds.some(g => typeof g !== 'string')\n\n    if (invalidGroupId) {\n      throw new KafkaJSNonRetriableError(`Invalid groupId name: ${JSON.stringify(invalidGroupId)}`)\n    }\n\n    const retrier = createRetry(retry)\n\n    let results = []\n\n    let clonedGroupIds = groupIds.slice()\n\n    return retrier(async (bail, retryCount, retryTime) => {\n      try {\n        if (clonedGroupIds.length === 0) return []\n\n        await cluster.refreshMetadata()\n\n        const brokersPerGroups = {}\n        const brokersPerNode = {}\n        for (const groupId of clonedGroupIds) {\n          const broker = await cluster.findGroupCoordinator({ groupId })\n          if (brokersPerGroups[broker.nodeId] === undefined) brokersPerGroups[broker.nodeId] = []\n          brokersPerGroups[broker.nodeId].push(groupId)\n          brokersPerNode[broker.nodeId] = broker\n        }\n\n        const res = await Promise.all(\n          Object.keys(brokersPerNode).map(\n            async nodeId => await brokersPerNode[nodeId].deleteGroups(brokersPerGroups[nodeId])\n          )\n        )\n\n        const errors = res\n          .flatMap(({ results }) =>\n            results.map(({ groupId, errorCode, error }) => {\n              return { groupId, errorCode, error }\n            })\n          )\n          .filter(({ errorCode }) => errorCode !== 0)\n\n        clonedGroupIds = errors.map(({ groupId }) => groupId)\n\n        if (errors.length > 0) throw new KafkaJSDeleteGroupsError('Error in DeleteGroups', errors)\n\n        results = res.flatMap(({ results }) => results)\n\n        return results\n      } catch (e) {\n        if (e.type === 'NOT_CONTROLLER' || e.type === 'COORDINATOR_NOT_AVAILABLE') {\n          logger.warn('Could not delete groups', { error: e.message, retryCount, retryTime })\n          throw e\n        }\n\n        bail(e)\n      }\n    })\n  }\n\n  /**\n   * Delete topic records up to the selected partition offsets\n   *\n   * @param {string} topic\n   * @param {Array<SeekEntry>} partitions\n   * @return {Promise}\n   *\n   * @typedef {Object} SeekEntry\n   * @property {number} partition\n   * @property {string} offset\n   */\n  const deleteTopicRecords = async ({ topic, partitions }) => {\n    if (!topic || typeof topic !== 'string') {\n      throw new KafkaJSNonRetriableError(`Invalid topic \"${topic}\"`)\n    }\n\n    if (!partitions || partitions.length === 0) {\n      throw new KafkaJSNonRetriableError(`Invalid partitions`)\n    }\n\n    const partitionsByBroker = cluster.findLeaderForPartitions(\n      topic,\n      partitions.map(p => p.partition)\n    )\n\n    const partitionsFound = values(partitionsByBroker).flat()\n    const topicOffsets = await fetchTopicOffsets(topic)\n\n    const leaderNotFoundErrors = []\n    partitions.forEach(({ partition, offset }) => {\n      // throw if no leader found for partition\n      if (!partitionsFound.includes(partition)) {\n        leaderNotFoundErrors.push({\n          partition,\n          offset,\n          error: new KafkaJSBrokerNotFound('Could not find the leader for the partition', {\n            retriable: false,\n          }),\n        })\n        return\n      }\n      const { low } = topicOffsets.find(p => p.partition === partition) || {\n        high: undefined,\n        low: undefined,\n      }\n      // warn in case of offset below low watermark\n      if (parseInt(offset) < parseInt(low) && parseInt(offset) !== -1) {\n        logger.warn(\n          'The requested offset is before the earliest offset maintained on the partition - no records will be deleted from this partition',\n          {\n            topic,\n            partition,\n            offset,\n          }\n        )\n      }\n    })\n\n    if (leaderNotFoundErrors.length > 0) {\n      throw new KafkaJSDeleteTopicRecordsError({ topic, partitions: leaderNotFoundErrors })\n    }\n\n    const seekEntriesByBroker = entries(partitionsByBroker).reduce(\n      (obj, [nodeId, nodePartitions]) => {\n        obj[nodeId] = {\n          topic,\n          partitions: partitions.filter(p => nodePartitions.includes(p.partition)),\n        }\n        return obj\n      },\n      {}\n    )\n\n    const retrier = createRetry(retry)\n    return retrier(async bail => {\n      try {\n        const partitionErrors = []\n\n        const brokerRequests = entries(seekEntriesByBroker).map(\n          ([nodeId, { topic, partitions }]) => async () => {\n            const broker = await cluster.findBroker({ nodeId })\n            await broker.deleteRecords({ topics: [{ topic, partitions }] })\n            // remove successful entry so it's ignored on retry\n            delete seekEntriesByBroker[nodeId]\n          }\n        )\n\n        await Promise.all(\n          brokerRequests.map(request =>\n            request().catch(e => {\n              if (e.name === 'KafkaJSDeleteTopicRecordsError') {\n                e.partitions.forEach(({ partition, offset, error }) => {\n                  partitionErrors.push({\n                    partition,\n                    offset,\n                    error,\n                  })\n                })\n              } else {\n                // then it's an unknown error, not from the broker response\n                throw e\n              }\n            })\n          )\n        )\n\n        if (partitionErrors.length > 0) {\n          throw new KafkaJSDeleteTopicRecordsError({\n            topic,\n            partitions: partitionErrors,\n          })\n        }\n      } catch (e) {\n        if (\n          e.retriable &&\n          e.partitions.some(\n            ({ error }) => staleMetadata(error) || error.name === 'KafkaJSMetadataNotLoaded'\n          )\n        ) {\n          await cluster.refreshMetadata()\n        }\n        throw e\n      }\n    })\n  }\n\n  /**\n   * @param {Array<ACLEntry>} acl\n   * @return {Promise<void>}\n   *\n   * @typedef {Object} ACLEntry\n   */\n  const createAcls = async ({ acl }) => {\n    if (!acl || !Array.isArray(acl)) {\n      throw new KafkaJSNonRetriableError(`Invalid ACL array ${acl}`)\n    }\n    if (acl.length === 0) {\n      throw new KafkaJSNonRetriableError('Empty ACL array')\n    }\n\n    // Validate principal\n    if (acl.some(({ principal }) => typeof principal !== 'string')) {\n      throw new KafkaJSNonRetriableError(\n        'Invalid ACL array, the principals have to be a valid string'\n      )\n    }\n\n    // Validate host\n    if (acl.some(({ host }) => typeof host !== 'string')) {\n      throw new KafkaJSNonRetriableError('Invalid ACL array, the hosts have to be a valid string')\n    }\n\n    // Validate resourceName\n    if (acl.some(({ resourceName }) => typeof resourceName !== 'string')) {\n      throw new KafkaJSNonRetriableError(\n        'Invalid ACL array, the resourceNames have to be a valid string'\n      )\n    }\n\n    let invalidType\n    // Validate operation\n    const validOperationTypes = Object.values(ACL_OPERATION_TYPES)\n    invalidType = acl.find(i => !validOperationTypes.includes(i.operation))\n\n    if (invalidType) {\n      throw new KafkaJSNonRetriableError(\n        `Invalid operation type ${invalidType.operation}: ${JSON.stringify(invalidType)}`\n      )\n    }\n\n    // Validate resourcePatternTypes\n    const validResourcePatternTypes = Object.values(RESOURCE_PATTERN_TYPES)\n    invalidType = acl.find(i => !validResourcePatternTypes.includes(i.resourcePatternType))\n\n    if (invalidType) {\n      throw new KafkaJSNonRetriableError(\n        `Invalid resource pattern type ${invalidType.resourcePatternType}: ${JSON.stringify(\n          invalidType\n        )}`\n      )\n    }\n\n    // Validate permissionTypes\n    const validPermissionTypes = Object.values(ACL_PERMISSION_TYPES)\n    invalidType = acl.find(i => !validPermissionTypes.includes(i.permissionType))\n\n    if (invalidType) {\n      throw new KafkaJSNonRetriableError(\n        `Invalid permission type ${invalidType.permissionType}: ${JSON.stringify(invalidType)}`\n      )\n    }\n\n    // Validate resourceTypes\n    const validResourceTypes = Object.values(ACL_RESOURCE_TYPES)\n    invalidType = acl.find(i => !validResourceTypes.includes(i.resourceType))\n\n    if (invalidType) {\n      throw new KafkaJSNonRetriableError(\n        `Invalid resource type ${invalidType.resourceType}: ${JSON.stringify(invalidType)}`\n      )\n    }\n\n    const retrier = createRetry(retry)\n\n    return retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await cluster.refreshMetadata()\n        const broker = await cluster.findControllerBroker()\n        await broker.createAcls({ acl })\n\n        return true\n      } catch (e) {\n        if (e.type === 'NOT_CONTROLLER') {\n          logger.warn('Could not create ACL', { error: e.message, retryCount, retryTime })\n          throw e\n        }\n\n        bail(e)\n      }\n    })\n  }\n\n  /**\n   * @param {ACLResourceTypes} resourceType The type of resource\n   * @param {string} resourceName The name of the resource\n   * @param {ACLResourcePatternTypes} resourcePatternType The resource pattern type filter\n   * @param {string} principal The principal name\n   * @param {string} host The hostname\n   * @param {ACLOperationTypes} operation The type of operation\n   * @param {ACLPermissionTypes} permissionType The type of permission\n   * @return {Promise<void>}\n   *\n   * @typedef {number} ACLResourceTypes\n   * @typedef {number} ACLResourcePatternTypes\n   * @typedef {number} ACLOperationTypes\n   * @typedef {number} ACLPermissionTypes\n   */\n  const describeAcls = async ({\n    resourceType,\n    resourceName,\n    resourcePatternType,\n    principal,\n    host,\n    operation,\n    permissionType,\n  }) => {\n    // Validate principal\n    if (typeof principal !== 'string' && typeof principal !== 'undefined') {\n      throw new KafkaJSNonRetriableError(\n        'Invalid principal, the principal have to be a valid string'\n      )\n    }\n\n    // Validate host\n    if (typeof host !== 'string' && typeof host !== 'undefined') {\n      throw new KafkaJSNonRetriableError('Invalid host, the host have to be a valid string')\n    }\n\n    // Validate resourceName\n    if (typeof resourceName !== 'string' && typeof resourceName !== 'undefined') {\n      throw new KafkaJSNonRetriableError(\n        'Invalid resourceName, the resourceName have to be a valid string'\n      )\n    }\n\n    // Validate operation\n    const validOperationTypes = Object.values(ACL_OPERATION_TYPES)\n    if (!validOperationTypes.includes(operation)) {\n      throw new KafkaJSNonRetriableError(`Invalid operation type ${operation}`)\n    }\n\n    // Validate resourcePatternType\n    const validResourcePatternTypes = Object.values(RESOURCE_PATTERN_TYPES)\n    if (!validResourcePatternTypes.includes(resourcePatternType)) {\n      throw new KafkaJSNonRetriableError(\n        `Invalid resource pattern filter type ${resourcePatternType}`\n      )\n    }\n\n    // Validate permissionType\n    const validPermissionTypes = Object.values(ACL_PERMISSION_TYPES)\n    if (!validPermissionTypes.includes(permissionType)) {\n      throw new KafkaJSNonRetriableError(`Invalid permission type ${permissionType}`)\n    }\n\n    // Validate resourceType\n    const validResourceTypes = Object.values(ACL_RESOURCE_TYPES)\n    if (!validResourceTypes.includes(resourceType)) {\n      throw new KafkaJSNonRetriableError(`Invalid resource type ${resourceType}`)\n    }\n\n    const retrier = createRetry(retry)\n\n    return retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await cluster.refreshMetadata()\n        const broker = await cluster.findControllerBroker()\n        const { resources } = await broker.describeAcls({\n          resourceType,\n          resourceName,\n          resourcePatternType,\n          principal,\n          host,\n          operation,\n          permissionType,\n        })\n        return { resources }\n      } catch (e) {\n        if (e.type === 'NOT_CONTROLLER') {\n          logger.warn('Could not describe ACL', { error: e.message, retryCount, retryTime })\n          throw e\n        }\n\n        bail(e)\n      }\n    })\n  }\n\n  /**\n   * @param {Array<ACLFilter>} filters\n   * @return {Promise<void>}\n   *\n   * @typedef {Object} ACLFilter\n   */\n  const deleteAcls = async ({ filters }) => {\n    if (!filters || !Array.isArray(filters)) {\n      throw new KafkaJSNonRetriableError(`Invalid ACL Filter array ${filters}`)\n    }\n\n    if (filters.length === 0) {\n      throw new KafkaJSNonRetriableError('Empty ACL Filter array')\n    }\n\n    // Validate principal\n    if (\n      filters.some(\n        ({ principal }) => typeof principal !== 'string' && typeof principal !== 'undefined'\n      )\n    ) {\n      throw new KafkaJSNonRetriableError(\n        'Invalid ACL Filter array, the principals have to be a valid string'\n      )\n    }\n\n    // Validate host\n    if (filters.some(({ host }) => typeof host !== 'string' && typeof host !== 'undefined')) {\n      throw new KafkaJSNonRetriableError(\n        'Invalid ACL Filter array, the hosts have to be a valid string'\n      )\n    }\n\n    // Validate resourceName\n    if (\n      filters.some(\n        ({ resourceName }) =>\n          typeof resourceName !== 'string' && typeof resourceName !== 'undefined'\n      )\n    ) {\n      throw new KafkaJSNonRetriableError(\n        'Invalid ACL Filter array, the resourceNames have to be a valid string'\n      )\n    }\n\n    let invalidType\n    // Validate operation\n    const validOperationTypes = Object.values(ACL_OPERATION_TYPES)\n    invalidType = filters.find(i => !validOperationTypes.includes(i.operation))\n\n    if (invalidType) {\n      throw new KafkaJSNonRetriableError(\n        `Invalid operation type ${invalidType.operation}: ${JSON.stringify(invalidType)}`\n      )\n    }\n\n    // Validate resourcePatternTypes\n    const validResourcePatternTypes = Object.values(RESOURCE_PATTERN_TYPES)\n    invalidType = filters.find(i => !validResourcePatternTypes.includes(i.resourcePatternType))\n\n    if (invalidType) {\n      throw new KafkaJSNonRetriableError(\n        `Invalid resource pattern type ${invalidType.resourcePatternType}: ${JSON.stringify(\n          invalidType\n        )}`\n      )\n    }\n\n    // Validate permissionTypes\n    const validPermissionTypes = Object.values(ACL_PERMISSION_TYPES)\n    invalidType = filters.find(i => !validPermissionTypes.includes(i.permissionType))\n\n    if (invalidType) {\n      throw new KafkaJSNonRetriableError(\n        `Invalid permission type ${invalidType.permissionType}: ${JSON.stringify(invalidType)}`\n      )\n    }\n\n    // Validate resourceTypes\n    const validResourceTypes = Object.values(ACL_RESOURCE_TYPES)\n    invalidType = filters.find(i => !validResourceTypes.includes(i.resourceType))\n\n    if (invalidType) {\n      throw new KafkaJSNonRetriableError(\n        `Invalid resource type ${invalidType.resourceType}: ${JSON.stringify(invalidType)}`\n      )\n    }\n\n    const retrier = createRetry(retry)\n\n    return retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await cluster.refreshMetadata()\n        const broker = await cluster.findControllerBroker()\n        const { filterResponses } = await broker.deleteAcls({ filters })\n        return { filterResponses }\n      } catch (e) {\n        if (e.type === 'NOT_CONTROLLER') {\n          logger.warn('Could not delete ACL', { error: e.message, retryCount, retryTime })\n          throw e\n        }\n\n        bail(e)\n      }\n    })\n  }\n\n  /**\n   * Alter the replicas partitions are assigned to for a topic\n   * @param {Object} request\n   * @param {import(\"../../types\").IPartitionReassignment[]} request.topics topics and the paritions to be reassigned\n   * @param {number} [request.timeout]\n   * @returns {Promise}\n   */\n  const alterPartitionReassignments = async ({ topics, timeout }) => {\n    if (!topics || !Array.isArray(topics)) {\n      throw new KafkaJSNonRetriableError(`Invalid topics array ${topics}`)\n    }\n\n    if (topics.filter(({ topic }) => typeof topic !== 'string').length > 0) {\n      throw new KafkaJSNonRetriableError(\n        'Invalid topics array, the topic names have to be a valid string'\n      )\n    }\n\n    const topicNames = new Set(topics.map(({ topic }) => topic))\n    if (topicNames.size < topics.length) {\n      throw new KafkaJSNonRetriableError(\n        'Invalid topics array, it cannot have multiple entries for the same topic'\n      )\n    }\n\n    for (const { topic, partitionAssignment } of topics) {\n      if (!partitionAssignment || !Array.isArray(partitionAssignment)) {\n        throw new KafkaJSNonRetriableError(\n          `Invalid partitions array: ${partitionAssignment} for topic: ${topic}`\n        )\n      }\n\n      for (const { partition, replicas } of partitionAssignment) {\n        if (\n          partition === null ||\n          partition === undefined ||\n          typeof partition !== 'number' ||\n          partition < 0\n        ) {\n          throw new KafkaJSNonRetriableError(\n            `Invalid partitions index: ${partition} for topic: ${topic}`\n          )\n        }\n\n        if (!replicas || !Array.isArray(replicas)) {\n          throw new KafkaJSNonRetriableError(\n            `Invalid replica assignment: ${replicas} for topic: ${topic} on partition: ${partition}`\n          )\n        }\n\n        if (replicas.filter(replica => typeof replica !== 'number' || replica < 0).length >= 1) {\n          throw new KafkaJSNonRetriableError(\n            `Invalid replica assignment: ${replicas} for topic: ${topic} on partition: ${partition}. Replicas must be a non negative number`\n          )\n        }\n      }\n    }\n\n    const retrier = createRetry(retry)\n\n    return retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await cluster.refreshMetadata()\n        const broker = await cluster.findControllerBroker()\n        await broker.alterPartitionReassignments({ topics, timeout })\n      } catch (e) {\n        if (e.type === 'NOT_CONTROLLER') {\n          logger.warn('Could not reassign partitions', { error: e.message, retryCount, retryTime })\n          throw e\n        }\n\n        bail(e)\n      }\n    })\n  }\n\n  /**\n   * List the partition reassignments in progress.\n   * If a partition is not going through a reassignment, its AddingReplicas and RemovingReplicas fields will simply be empty.\n   * If a partition doesn't exist, no response will be returned for it.\n   * @param {Object} request\n   * @param {import(\"../../types\").TopicPartitions[]} request.topics topics and the paritions to be returned, if this is null will return all the topics.\n   * @param {number} [request.timeout]\n   * @returns {Promise<import(\"../../types\").ListPartitionReassignmentsResponse>}\n   */\n  const listPartitionReassignments = async ({ topics = null, timeout }) => {\n    if (topics) {\n      if (!Array.isArray(topics)) {\n        throw new KafkaJSNonRetriableError(`Invalid topics array ${topics}`)\n      }\n\n      if (topics.filter(({ topic }) => typeof topic !== 'string').length > 0) {\n        throw new KafkaJSNonRetriableError(\n          'Invalid topics array, the topic names have to be a valid string'\n        )\n      }\n\n      const topicNames = new Set(topics.map(({ topic }) => topic))\n      if (topicNames.size < topics.length) {\n        throw new KafkaJSNonRetriableError(\n          'Invalid topics array, it cannot have multiple entries for the same topic'\n        )\n      }\n\n      for (const { topic, partitions } of topics) {\n        if (!partitions || !Array.isArray(partitions)) {\n          throw new KafkaJSNonRetriableError(\n            `Invalid partition array: ${partitions} for topic: ${topic}`\n          )\n        }\n\n        if (\n          partitions.filter(partition => typeof partition !== 'number' || partition < 0).length >= 1\n        ) {\n          throw new KafkaJSNonRetriableError(\n            `Invalid partition array: ${partitions} for topic: ${topic}. The partition indices have to be a valid number greater than 0.`\n          )\n        }\n      }\n    }\n\n    const retrier = createRetry(retry)\n\n    return retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await cluster.refreshMetadata()\n        const broker = await cluster.findControllerBroker()\n        const response = await broker.listPartitionReassignments({ topics, timeout })\n\n        return { topics: response.topics }\n      } catch (e) {\n        if (e.type === 'NOT_CONTROLLER') {\n          logger.warn('Could not reassign partitions', { error: e.message, retryCount, retryTime })\n          throw e\n        }\n\n        bail(e)\n      }\n    })\n  }\n\n  /** @type {import(\"../../types\").Admin[\"on\"]} */\n  const on = (eventName, listener) => {\n    if (!eventNames.includes(eventName)) {\n      throw new KafkaJSNonRetriableError(`Event name should be one of ${eventKeys}`)\n    }\n\n    return instrumentationEmitter.addListener(unwrapEvent(eventName), event => {\n      event.type = wrapEvent(event.type)\n      Promise.resolve(listener(event)).catch(e => {\n        logger.error(`Failed to execute listener: ${e.message}`, {\n          eventName,\n          stack: e.stack,\n        })\n      })\n    })\n  }\n\n  /**\n   * @return {Object} logger\n   */\n  const getLogger = () => logger\n\n  return {\n    connect,\n    disconnect,\n    listTopics,\n    createTopics,\n    deleteTopics,\n    createPartitions,\n    fetchTopicMetadata,\n    describeCluster,\n    events,\n    fetchOffsets,\n    fetchTopicOffsets,\n    fetchTopicOffsetsByTimestamp,\n    setOffsets,\n    resetOffsets,\n    describeConfigs,\n    alterConfigs,\n    on,\n    logger: getLogger,\n    listGroups,\n    describeGroups,\n    deleteGroups,\n    describeAcls,\n    deleteAcls,\n    createAcls,\n    deleteTopicRecords,\n    alterPartitionReassignments,\n    listPartitionReassignments,\n  }\n}\n", "const KEEP_ALIVE_DELAY = 60000 // in ms\n\n/**\n * @returns {import(\"../../types\").ISocketFactory}\n */\nmodule.exports = () => {\n  const net = require('net')\n  const tls = require('tls')\n\n  return ({ host, port, ssl, onConnect }) => {\n    const socket = ssl\n      ? tls.connect(\n          Object.assign({ host, port }, !net.isIP(host) ? { servername: host } : {}, ssl),\n          onConnect\n        )\n      : net.connect({ host, port }, onConnect)\n\n    socket.setKeepAlive(true, KEEP_ALIVE_DELAY)\n\n    return socket\n  }\n}\n", "module.exports = fn => {\n  let called = false\n\n  return (...args) => {\n    if (!called) {\n      called = true\n      return fn(...args)\n    }\n  }\n}\n", "const {\n  createLogger,\n  LEVELS: { INFO },\n} = require('./loggers')\n\nconst InstrumentationEventEmitter = require('./instrumentation/emitter')\nconst LoggerConsole = require('./loggers/console')\nconst Cluster = require('./cluster')\nconst createProducer = require('./producer')\nconst createConsumer = require('./consumer')\nconst createAdmin = require('./admin')\nconst ISOLATION_LEVEL = require('./protocol/isolationLevel')\nconst defaultSocketFactory = require('./network/socketFactory')\nconst once = require('./utils/once')\nconst websiteUrl = require('./utils/websiteUrl')\n\nconst PRIVATE = {\n  CREATE_CLUSTER: Symbol('private:Kafka:createCluster'),\n  CLUSTER_RETRY: Symbol('private:Kafka:clusterRetry'),\n  LOGGER: Symbol('private:Kafka:logger'),\n  OFFSETS: Symbol('private:Kafka:offsets'),\n}\n\nconst DEFAULT_METADATA_MAX_AGE = 300000\nconst warnOfDefaultPartitioner = once(logger => {\n  if (process.env.KAFKAJS_NO_PARTITIONER_WARNING == null) {\n    logger.warn(\n      `KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at ${websiteUrl(\n        'docs/migration-guide-v2.0.0',\n        'producer-new-default-partitioner'\n      )} for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\"`\n    )\n  }\n})\n\nmodule.exports = class Client {\n  /**\n   * @param {Object} options\n   * @param {Array<string>} options.brokers example: ['127.0.0.1:9092', '127.0.0.1:9094']\n   * @param {Object} options.ssl\n   * @param {Object} options.sasl\n   * @param {string} options.clientId\n   * @param {number} [options.connectionTimeout=1000] - in milliseconds\n   * @param {number} options.authenticationTimeout - in milliseconds\n   * @param {number} options.reauthenticationThreshold - in milliseconds\n   * @param {number} [options.requestTimeout=30000] - in milliseconds\n   * @param {boolean} [options.enforceRequestTimeout]\n   * @param {import(\"../types\").RetryOptions} [options.retry]\n   * @param {import(\"../types\").ISocketFactory} [options.socketFactory]\n   */\n  constructor({\n    brokers,\n    ssl,\n    sasl,\n    clientId,\n    connectionTimeout = 1000,\n    authenticationTimeout,\n    reauthenticationThreshold,\n    requestTimeout,\n    enforceRequestTimeout = true,\n    retry,\n    socketFactory = defaultSocketFactory(),\n    logLevel = INFO,\n    logCreator = LoggerConsole,\n  }) {\n    this[PRIVATE.OFFSETS] = new Map()\n    this[PRIVATE.LOGGER] = createLogger({ level: logLevel, logCreator })\n    this[PRIVATE.CLUSTER_RETRY] = retry\n    this[PRIVATE.CREATE_CLUSTER] = ({\n      metadataMaxAge,\n      allowAutoTopicCreation = true,\n      maxInFlightRequests = null,\n      instrumentationEmitter = null,\n      isolationLevel,\n    }) =>\n      new Cluster({\n        logger: this[PRIVATE.LOGGER],\n        retry: this[PRIVATE.CLUSTER_RETRY],\n        offsets: this[PRIVATE.OFFSETS],\n        socketFactory,\n        brokers,\n        ssl,\n        sasl,\n        clientId,\n        connectionTimeout,\n        authenticationTimeout,\n        reauthenticationThreshold,\n        requestTimeout,\n        enforceRequestTimeout,\n        metadataMaxAge,\n        instrumentationEmitter,\n        allowAutoTopicCreation,\n        maxInFlightRequests,\n        isolationLevel,\n      })\n  }\n\n  /**\n   * @public\n   */\n  producer({\n    createPartitioner,\n    retry,\n    metadataMaxAge = DEFAULT_METADATA_MAX_AGE,\n    allowAutoTopicCreation,\n    idempotent,\n    transactionalId,\n    transactionTimeout,\n    maxInFlightRequests,\n  } = {}) {\n    const instrumentationEmitter = new InstrumentationEventEmitter()\n    const cluster = this[PRIVATE.CREATE_CLUSTER]({\n      metadataMaxAge,\n      allowAutoTopicCreation,\n      maxInFlightRequests,\n      instrumentationEmitter,\n    })\n\n    if (createPartitioner == null) {\n      warnOfDefaultPartitioner(this[PRIVATE.LOGGER])\n    }\n\n    return createProducer({\n      retry: { ...this[PRIVATE.CLUSTER_RETRY], ...retry },\n      logger: this[PRIVATE.LOGGER],\n      cluster,\n      createPartitioner,\n      idempotent,\n      transactionalId,\n      transactionTimeout,\n      instrumentationEmitter,\n    })\n  }\n\n  /**\n   * @public\n   */\n  consumer({\n    groupId,\n    partitionAssigners,\n    metadataMaxAge = DEFAULT_METADATA_MAX_AGE,\n    sessionTimeout,\n    rebalanceTimeout,\n    heartbeatInterval,\n    maxBytesPerPartition,\n    minBytes,\n    maxBytes,\n    maxWaitTimeInMs,\n    retry = { retries: 5 },\n    allowAutoTopicCreation,\n    maxInFlightRequests,\n    readUncommitted = false,\n    rackId = '',\n  } = {}) {\n    const isolationLevel = readUncommitted\n      ? ISOLATION_LEVEL.READ_UNCOMMITTED\n      : ISOLATION_LEVEL.READ_COMMITTED\n\n    const instrumentationEmitter = new InstrumentationEventEmitter()\n    const cluster = this[PRIVATE.CREATE_CLUSTER]({\n      metadataMaxAge,\n      allowAutoTopicCreation,\n      maxInFlightRequests,\n      isolationLevel,\n      instrumentationEmitter,\n    })\n\n    return createConsumer({\n      retry: { ...this[PRIVATE.CLUSTER_RETRY], ...retry },\n      logger: this[PRIVATE.LOGGER],\n      cluster,\n      groupId,\n      partitionAssigners,\n      sessionTimeout,\n      rebalanceTimeout,\n      heartbeatInterval,\n      maxBytesPerPartition,\n      minBytes,\n      maxBytes,\n      maxWaitTimeInMs,\n      isolationLevel,\n      instrumentationEmitter,\n      rackId,\n      metadataMaxAge,\n    })\n  }\n\n  /**\n   * @public\n   */\n  admin({ retry } = {}) {\n    const instrumentationEmitter = new InstrumentationEventEmitter()\n    const cluster = this[PRIVATE.CREATE_CLUSTER]({\n      allowAutoTopicCreation: false,\n      instrumentationEmitter,\n    })\n\n    return createAdmin({\n      retry: { ...this[PRIVATE.CLUSTER_RETRY], ...retry },\n      logger: this[PRIVATE.LOGGER],\n      instrumentationEmitter,\n      cluster,\n    })\n  }\n\n  /**\n   * @public\n   */\n  logger() {\n    return this[PRIVATE.LOGGER]\n  }\n}\n", "const Kafka = require('./src')\nconst PartitionAssigners = require('./src/consumer/assigners')\nconst AssignerProtocol = require('./src/consumer/assignerProtocol')\nconst Partitioners = require('./src/producer/partitioners')\nconst Compression = require('./src/protocol/message/compression')\nconst ConfigResourceTypes = require('./src/protocol/configResourceTypes')\nconst ConfigSource = require('./src/protocol/configSource')\nconst AclResourceTypes = require('./src/protocol/aclResourceTypes')\nconst AclOperationTypes = require('./src/protocol/aclOperationTypes')\nconst AclPermissionTypes = require('./src/protocol/aclPermissionTypes')\nconst ResourcePatternTypes = require('./src/protocol/resourcePatternTypes')\nconst { isRebalancing, isKafkaJSError, ...errors } = require('./src/errors')\nconst { LEVELS } = require('./src/loggers')\n\nmodule.exports = {\n  Kafka,\n  PartitionAssigners,\n  AssignerProtocol,\n  Partitioners,\n  logLevel: LEVELS,\n  CompressionTypes: Compression.Types,\n  CompressionCodecs: Compression.Codecs,\n  ConfigResourceTypes,\n  AclResourceTypes,\n  AclOperationTypes,\n  AclPermissionTypes,\n  ResourcePatternTypes,\n  ConfigSource,\n  ...errors,\n}\n", "import { ReportType } from \"../types/reports\";\n\nexport const error = {\n  // generic errors\n  UNAUTHORIZED: \"User is not authorized to access this resource.\",\n  NO_KEY: \"Must provide key for table.\",\n  MISSING_DATA: \"Missing required data.\",\n  INVALID_DATA: \"Provided data is not valid.\",\n  NO_MATCHING_RECORD: \"No matching record found.\",\n  SERVER_ERROR: \"An unspecified server error occured.\",\n  // bucket errors\n  S3_OBJECT_CREATION_ERROR: \"Report could not be created due to an S3 error.\",\n  S3_OBJECT_UPDATE_ERROR: \"Report could not be updated due to an S3 error.\",\n  S3_OBJECT_GET_ERROR: \"Error while fetching report.\",\n  // dynamo errors\n  DYNAMO_CREATION_ERROR: \"Report could not be created due to a database error.\",\n  DYNAMO_UPDATE_ERROR: \"Report could not be updated due to a database error.\",\n  // template errors\n  NO_TEMPLATE_NAME: \"Must request template for download.\",\n  INVALID_TEMPLATE_NAME: \"Requested template does not exist or does not match.\",\n  NOT_IN_DATABASE: \"Record not found in database.\",\n  UNABLE_TO_COPY:\n    \"Unable to copy over report if todays date is in the same period and year as a previous report.\",\n  MISSING_FORM_TEMPLATE: \"Form Template not found in S3.\",\n  MISSING_FIELD_DATA: \"Field Data not found in S3.\",\n  NO_WORKPLANS_FOUND: \"No record of Work Plans found in database\",\n  // admin action errors\n  ALREADY_ARCHIVED: \"Cannot update archived report.\",\n  ALREADY_LOCKED: \"Cannot update locked report.\",\n  REPORT_INCOMPLETE: \"Cannot submit incomplete form.\",\n} as const;\n\nexport const buckets = {\n  FORM_TEMPLATE: \"formTemplates\",\n  FIELD_DATA: \"fieldData\",\n};\n\n// STATES\nexport enum States {\n  AL = \"Alabama\",\n  AK = \"Alaska\",\n  AS = \"American Samoa\",\n  AZ = \"Arizona\",\n  AR = \"Arkansas\",\n  CA = \"California\",\n  CO = \"Colorado\",\n  CT = \"Connecticut\",\n  DE = \"Delaware\",\n  DC = \"District of Columbia\",\n  FM = \"Federated States of Micronesia\",\n  FL = \"Florida\",\n  GA = \"Georgia\",\n  GU = \"Guam\",\n  HI = \"Hawaii\",\n  ID = \"Idaho\",\n  IL = \"Illinois\",\n  IN = \"Indiana\",\n  IA = \"Iowa\",\n  KS = \"Kansas\",\n  KY = \"Kentucky\",\n  LA = \"Louisiana\",\n  ME = \"Maine\",\n  MH = \"Marshall Islands\",\n  MD = \"Maryland\",\n  MA = \"Massachusetts\",\n  MI = \"Michigan\",\n  MN = \"Minnesota\",\n  MS = \"Mississippi\",\n  MO = \"Missouri\",\n  MT = \"Montana\",\n  NE = \"Nebraska\",\n  NV = \"Nevada\",\n  NH = \"New Hampshire\",\n  NJ = \"New Jersey\",\n  NM = \"New Mexico\",\n  NY = \"New York\",\n  NC = \"North Carolina\",\n  ND = \"North Dakota\",\n  MP = \"Northern Mariana Islands\",\n  OH = \"Ohio\",\n  OK = \"Oklahoma\",\n  OR = \"Oregon\",\n  PW = \"Palau\",\n  PA = \"Pennsylvania\",\n  PR = \"Puerto Rico\",\n  RI = \"Rhode Island\",\n  SC = \"South Carolina\",\n  SD = \"South Dakota\",\n  TN = \"Tennessee\",\n  TX = \"Texas\",\n  UT = \"Utah\",\n  VT = \"Vermont\",\n  VI = \"Virgin Islands\",\n  VA = \"Virginia\",\n  WA = \"Washington\",\n  WV = \"West Virginia\",\n  WI = \"Wisconsin\",\n  WY = \"Wyoming\",\n}\n\n// REPORTS\n\nexport const reportTables: { [key in ReportType]: string } = {\n  SAR: process.env.SarReportsTable!,\n  WP: process.env.WpReportsTable!,\n};\n\nexport const reportBuckets: { [key in ReportType]: string } = {\n  SAR: process.env.SAR_FORM_BUCKET!,\n  WP: process.env.WP_FORM_BUCKET!,\n};\n\nexport const reportNames: { [key in ReportType]: string } = {\n  SAR: \"SAR\",\n  WP: \"Work Plan\",\n};\n\nexport const tableTopics: { [key in ReportType]: string } = {\n  SAR: \"sar-reports\",\n  WP: \"wp-reports\",\n};\n\nexport const bucketTopics: { [key in ReportType]: string } = {\n  SAR: \"sar-form\",\n  WP: \"wp-form\",\n};\n\nexport const DEFAULT_TARGET_POPULATION_NAMES = [\n  \"Older adults\",\n  \"Individuals with physical disabilities (PD)\",\n  \"Individuals with intellectual and developmental disabilities (I/DD)\",\n  \"Individuals with mental health and substance use disorders (MH/SUD)\",\n  \"HCBS infrastructure/system-level development\",\n];\n", "/* eslint-disable no-console */\nimport { GetObjectCommand } from \"@aws-sdk/client-s3\";\nimport { unmarshall } from \"@aws-sdk/util-dynamodb\";\nimport { Kafka, Producer } from \"kafkajs\";\nimport { createClient } from \"../../storage/s3-lib\";\nimport { S3EventRecord } from \"../types\";\n\ntype KafkaPayload = {\n  key: string;\n  value: string;\n  partition: number;\n  headers: {\n    eventName: string;\n    eventTime?: string;\n    eventID?: string;\n  };\n};\ntype SourceTopicMapping = {\n  sourceName: string;\n  topicName: string;\n};\n\nlet kafka: Kafka;\nlet producer: Producer;\n\nclass KafkaSourceLib {\n  /*\n   * Event types:\n   * cmd \u2013 command; restful publish\n   * cdc \u2013 change data capture; record upsert/delete in data store\n   * sys \u2013 system event; send email, archive logs\n   * fct \u2013 fact; user activity, notifications, logs\n   *\n   * topicPrefix = \"[data_center].[system_of_record].[business_domain].[event_type]\";\n   * version = \"some version\";\n   * tables = [list of table mappings];\n   * buckets = [list of bucket mappings];\n   */\n\n  topicPrefix: string;\n  version: string | null;\n  tables: SourceTopicMapping[];\n  buckets: SourceTopicMapping[];\n  connected: boolean;\n  topicNamespace: string;\n  stage: string;\n  constructor(\n    topicPrefix: string,\n    version: string | null,\n    tables: SourceTopicMapping[],\n    buckets: SourceTopicMapping[]\n  ) {\n    if (!process.env.BOOTSTRAP_BROKER_STRING_TLS) {\n      throw new Error(\"Missing Broker Config. \");\n    }\n    // Setup vars\n    this.stage = process.env.stage ?? \"\";\n    this.topicNamespace = process.env.topicNamespace\n      ? process.env.topicNamespace\n      : \"\";\n    this.topicPrefix = topicPrefix;\n    this.version = version;\n    this.tables = tables;\n    this.buckets = buckets;\n\n    const brokerStrings = process.env.BOOTSTRAP_BROKER_STRING_TLS;\n    kafka = new Kafka({\n      clientId: `mfp-${this.stage}`,\n      brokers: brokerStrings!.split(\",\"),\n      retry: {\n        initialRetryTime: 300,\n        retries: 8,\n      },\n      ssl: {\n        rejectUnauthorized: false,\n      },\n    });\n\n    // Attach Events\n    producer = kafka.producer();\n    this.connected = false;\n    const signalTraps = [\"SIGTERM\", \"SIGINT\", \"SIGUSR2\", \"beforeExit\"];\n    signalTraps.map((type) => {\n      process.removeListener(type, producer.disconnect);\n    });\n    signalTraps.map((type) => {\n      process.once(type, producer.disconnect);\n    });\n  }\n\n  stringify(e: any, prettyPrint?: boolean) {\n    if (prettyPrint === true) return JSON.stringify(e, null, 2);\n    return JSON.stringify(e);\n  }\n\n  /**\n   * Checks if a streamArn is a valid topic. Returns undefined otherwise\n   * @param streamARN - DynamoDB streamARN\n   * @returns\n   */\n  determineDynamoTopicName(streamARN: string) {\n    for (const table of this.tables) {\n      if (streamARN.includes(`/${table.sourceName}/`))\n        return this.topic(table.topicName);\n    }\n    console.log(`Topic not found for table arn: ${streamARN}`);\n  }\n\n  /**\n   * Checks if a bucketArn is a valid topic. Returns undefined otherwise.\n   * @param bucketArn - ARN formatted like 'arn:aws:s3:::{stack}-{stage}-{bucket}' e.g. arn:aws:s3:::database-main-mcpar\n   * @returns A formatted topic name with \"-form\" specified\n   */\n  determineS3TopicName(bucketArn: string) {\n    for (const bucket of this.buckets) {\n      if (bucketArn.includes(bucket.sourceName)) {\n        return this.topic(bucket.topicName);\n      }\n    }\n    console.log(`Topic not found for bucket arn: ${bucketArn}`);\n  }\n\n  unmarshall(r: any) {\n    return unmarshall(r);\n  }\n\n  createDynamoPayload(record: any): KafkaPayload {\n    const dynamodb = record.dynamodb;\n    const { eventID, eventName } = record;\n    const dynamoRecord = {\n      NewImage: this.unmarshall(dynamodb.NewImage),\n      OldImage: this.unmarshall(dynamodb.OldImage ?? {}),\n      Keys: this.unmarshall(dynamodb.Keys),\n    };\n    return {\n      key: Object.values(dynamoRecord.Keys).join(\"#\"),\n      value: this.stringify(dynamoRecord),\n      partition: 0,\n      headers: { eventID: eventID, eventName: eventName },\n    };\n  }\n\n  async createS3Payload(record: S3EventRecord): Promise<KafkaPayload> {\n    const { eventName, eventTime } = record;\n    let entry = \"\";\n    if (!eventName.includes(\"ObjectRemoved\")) {\n      const client = createClient();\n      const response = await client.send(\n        new GetObjectCommand({\n          Bucket: record.s3.bucket.name,\n          Key: record.s3.object.key,\n        })\n      );\n      const responseString = await response.Body?.transformToString();\n      if (responseString) {\n        entry = responseString;\n      } else {\n        throw new Error(\n          `Failed to fetch S3 object with key: \"${record.s3.object.key}\"`\n        );\n      }\n    }\n\n    return {\n      key: record.s3.object.key,\n      value: entry,\n      partition: 0,\n      headers: { eventName, eventTime },\n    };\n  }\n\n  topic(t: string) {\n    if (this.version) {\n      return `${this.topicNamespace}${this.topicPrefix}.${t}.${this.version}`;\n    } else {\n      return `${this.topicNamespace}${this.topicPrefix}.${t}`;\n    }\n  }\n\n  async createOutboundEvents(records: any[]) {\n    let outboundEvents: { [key: string]: any } = {};\n    for (const record of records) {\n      let payload, topicName;\n      if (record[\"s3\"]) {\n        // Handle any S3 events\n        const s3Record = record as S3EventRecord;\n        const key: string = s3Record.s3.object.key;\n        topicName = this.determineS3TopicName(s3Record.s3.bucket.arn);\n\n        // Filter for only the response info\n        if (\n          !topicName ||\n          !key.startsWith(\"fieldData/\") ||\n          !key.includes(\".json\")\n        ) {\n          continue;\n        }\n        payload = await this.createS3Payload(record);\n      } else {\n        // DYNAMO\n        topicName = this.determineDynamoTopicName(\n          String(record.eventSourceARN.toString())\n        );\n        if (!topicName) continue;\n        payload = this.createDynamoPayload(record);\n      }\n\n      //initialize configuration object keyed to topic for quick lookup\n      if (!(outboundEvents[topicName] instanceof Object))\n        outboundEvents[topicName] = {\n          topic: topicName,\n          messages: [],\n        };\n\n      //add messages to messages array for corresponding topic\n      outboundEvents[topicName].messages.push(payload);\n    }\n    return outboundEvents;\n  }\n\n  async handler(event: any) {\n    if (process.env.BOOTSTRAP_BROKER_STRING_TLS === \"localstack\") {\n      return;\n    }\n\n    if (!this.connected) {\n      await producer.connect();\n      this.connected = true;\n    }\n\n    // Warmup events have no records.\n    if (!event.Records) {\n      console.log(\"No records to process. Exiting.\");\n      return;\n    }\n\n    // if dynamo\n    const outboundEvents = await this.createOutboundEvents(event.Records);\n\n    const topicMessages = Object.values(outboundEvents);\n    console.log(`Batch configuration: ${this.stringify(topicMessages, true)}`);\n\n    if (topicMessages.length > 0) await producer.sendBatch({ topicMessages });\n    console.log(`Successfully processed ${event.Records.length} records.`);\n  }\n}\n\nexport default KafkaSourceLib;\n", "import { S3Client, GetObjectCommandOutput } from \"@aws-sdk/client-s3\";\nimport { logger } from \"../utils/debugging/debug-lib\";\n\nconst localConfig = {\n  endpoint: process.env.S3_LOCAL_ENDPOINT,\n  region: \"localhost\",\n  forcePathStyle: true,\n  credentials: {\n    accessKeyId: \"S3RVER\", // pragma: allowlist secret\n    secretAccessKey: \"S3RVER\", // pragma: allowlist secret\n  },\n  logger,\n};\n\nconst awsConfig = {\n  region: \"us-east-1\",\n  logger,\n};\n\nconst getConfig = () => {\n  return process.env.S3_LOCAL_ENDPOINT ? localConfig : awsConfig;\n};\n\nexport const createClient = () => new S3Client(getConfig());\n\nexport const parseS3Response = async (response: GetObjectCommandOutput) => {\n  const stringBody = await response.Body?.transformToString();\n  if (!stringBody) {\n    logger.warn(`Empty response from S3`);\n    return undefined;\n  }\n  return JSON.parse(stringBody);\n};\n", "import util from \"util\";\nimport { Logger } from \"@smithy/types\";\n\ntype LogLevel = \"trace\" | \"debug\" | \"info\" | \"warn\" | \"error\";\ntype LogEntry = {\n  date: Date;\n  level: LogLevel;\n  string: string;\n};\n\nconst logs: LogEntry[] = [];\n\nconst buildLoggerForLevel = (level: LogLevel) => {\n  return function (...content: any[]) {\n    logs.push({\n      date: new Date(),\n      level: level,\n      string: util.format.apply(null, content),\n    });\n\n    /*\n     * If we have a function logging thousands of messages,\n     * better to take the console performance hit mid-operation\n     * than to let memory usage run away as well.\n     */\n    if (logs.length > 99) {\n      flush();\n    }\n  };\n};\n\n/*\n * Individual functions are exported to support handler-lib;\n * This integrates SDK client logging with lambda logging.\n */\nexport const trace = buildLoggerForLevel(\"trace\");\nexport const debug = buildLoggerForLevel(\"debug\");\nexport const info = buildLoggerForLevel(\"info\");\nexport const warn = buildLoggerForLevel(\"warn\");\nexport const error = buildLoggerForLevel(\"error\");\n\nexport function flush() {\n  while (logs.length > 0) {\n    const { date, level, string } = logs.shift()!;\n    // eslint-disable-next-line no-console\n    console[level](date, string);\n  }\n}\n\n/*\n * This is only called at the beginning of a lambda handler,\n * so the log buffer should be empty anyway. But it doesn't\n * hurt to make sure!\n */\nexport const init = flush;\n\n/**\n * A logger suitable for passing to any AWS client constructor.\n * Note that the `trace` log level is excluded.\n *\n * This logger accumulates log messages in an internal buffer,\n * eventually flushing them to the console.\n */\nexport const logger: Logger = { debug, info, warn, error };\n", "import {\n  bucketTopics,\n  reportBuckets,\n  reportTables,\n  tableTopics,\n} from \"../../../utils/constants/constants\";\nimport KafkaSourceLib from \"../../../utils/kafka/kafka-source-lib\";\n\nconst topicPrefix = \"aws.mdct.mfp\";\nconst version = \"v0\";\nconst tables = [\n  { sourceName: reportTables.WP, topicName: tableTopics.WP },\n  { sourceName: reportTables.SAR, topicName: tableTopics.SAR },\n];\nconst buckets = [\n  { sourceName: reportBuckets.WP, topicName: bucketTopics.WP },\n  { sourceName: reportBuckets.SAR, topicName: bucketTopics.SAR },\n];\n\nconst postKafkaData = new KafkaSourceLib(topicPrefix, version, tables, buckets);\n\nexports.handler = postKafkaData.handler.bind(postKafkaData);\n"],
./.cdk/cdk.out/asset.5ad9c2f99a748a497800bcef13ebc7ab4ed914a411cf082a6b54497ee5b4dc17/index.js.map:4: TODO: remove\n\n\n  get _type() {\n    return this.type;\n  }\n\n  _typeCheck(_value) {\n    return true;\n  }\n\n  clone(spec) {\n    if (this._mutate) {\n      if (spec) Object.assign(this.spec, spec);\n      return this;\n    } // if the nested value is a schema we can skip cloning, since\n    // they are already immutable\n\n\n    const next = Object.create(Object.getPrototypeOf(this)); // @ts-expect-error this is readonly\n\n    next.type = this.type;\n    next._typeError = this._typeError;\n    next._whitelistError = this._whitelistError;\n    next._blacklistError = this._blacklistError;\n    next._whitelist = this._whitelist.clone();\n    next._blacklist = this._blacklist.clone();\n    next.exclusiveTests = _extends({}, this.exclusiveTests); // @ts-expect-error this is readonly\n\n    next.deps = [...this.deps];\n    next.conditions = [...this.conditions];\n    next.tests = [...this.tests];\n    next.transforms = [...this.transforms];\n    next.spec = (0, _nanoclone.default)(_extends({}, this.spec, spec));\n    return next;\n  }\n\n  label(label) {\n    let next = this.clone();\n    next.spec.label = label;\n    return next;\n  }\n\n  meta(...args) {\n    if (args.length === 0) return this.spec.meta;\n    let next = this.clone();\n    next.spec.meta = Object.assign(next.spec.meta || {}, args[0]);\n    return next;\n  } // withContext<TContext extends AnyObject>(): BaseSchema<\n  //   TCast,\n  //   TContext,\n  //   TOutput\n  // > {\n  //   return this as any;\n  // }\n\n\n  withMutation(fn) {\n    let before = this._mutate;\n    this._mutate = true;\n    let result = fn(this);\n    this._mutate = before;\n    return result;\n  }\n\n  concat(schema) {\n    if (!schema || schema === this) return this;\n    if (schema.type !== this.type && this.type !== 'mixed') throw new TypeError(`You cannot \\`concat()\\` schema's of different types: ${this.type} and ${schema.type}`);\n    let base = this;\n    let combined = schema.clone();\n\n    const mergedSpec = _extends({}, base.spec, combined.spec); // if (combined.spec.nullable === UNSET)\n    //   mergedSpec.nullable = base.spec.nullable;\n    // if (combined.spec.presence === UNSET)\n    //   mergedSpec.presence = base.spec.presence;\n\n\n    combined.spec = mergedSpec;\n    combined._typeError || (combined._typeError = base._typeError);\n    combined._whitelistError || (combined._whitelistError = base._whitelistError);\n    combined._blacklistError || (combined._blacklistError = base._blacklistError); // manually merge the blacklist/whitelist (the other `schema` takes\n    // precedence in case of conflicts)\n\n    combined._whitelist = base._whitelist.merge(schema._whitelist, schema._blacklist);\n    combined._blacklist = base._blacklist.merge(schema._blacklist, schema._whitelist); // start with the current tests\n\n    combined.tests = base.tests;\n    combined.exclusiveTests = base.exclusiveTests; // manually add the new tests to ensure\n    // the deduping logic is consistent\n\n    combined.withMutation(next => {\n      schema.tests.forEach(fn => {\n        next.test(fn.OPTIONS);\n      });\n    });\n    combined.transforms = [...base.transforms, ...combined.transforms];\n    return combined;\n  }\n\n  isType(v) {\n    if (this.spec.nullable && v === null) return true;\n    return this._typeCheck(v);\n  }\n\n  resolve(options) {\n    let schema = this;\n\n    if (schema.conditions.length) {\n      let conditions = schema.conditions;\n      schema = schema.clone();\n      schema.conditions = [];\n      schema = conditions.reduce((schema, condition) => condition.resolve(schema, options), schema);\n      schema = schema.resolve(options);\n    }\n\n    return schema;\n  }\n  /**\n   *\n   * @param {*} value\n   * @param {Object} options\n   * @param {*=} options.parent\n   * @param {*=} options.context\n   */\n\n\n  cast(value, options = {}) {\n    let resolvedSchema = this.resolve(_extends({\n      value\n    }, options));\n\n    let result = resolvedSchema._cast(value, options);\n\n    if (value !== undefined && options.assert !== false && resolvedSchema.isType(result) !== true) {\n      let formattedValue = (0, _printValue.default)(value);\n      let formattedResult = (0, _printValue.default)(result);\n      throw new TypeError(`The value of ${options.path || 'field'} could not be cast to a value ` + `that satisfies the schema type: \"${resolvedSchema._type}\". \\n\\n` + `attempted value: ${formattedValue} \\n` + (formattedResult !== formattedValue ? `result of cast: ${formattedResult}` : ''));\n    }\n\n    return result;\n  }\n\n  _cast(rawValue, _options) {\n    let value = rawValue === undefined ? rawValue : this.transforms.reduce((value, fn) => fn.call(this, value, rawValue, this), rawValue);\n\n    if (value === undefined) {\n      value = this.getDefault();\n    }\n\n    return value;\n  }\n\n  _validate(_value, options = {}, cb) {\n    let {\n      sync,\n      path,\n      from = [],\n      originalValue = _value,\n      strict = this.spec.strict,\n      abortEarly = this.spec.abortEarly\n    } = options;\n    let value = _value;\n\n    if (!strict) {\n      // this._validating = true;\n      value = this._cast(value, _extends({\n        assert: false\n      }, options)); // this._validating = false;\n    } // value is cast, we can check if it meets type requirements\n\n\n    let args = {\n      value,\n      path,\n      options,\n      originalValue,\n      schema: this,\n      label: this.spec.label,\n      sync,\n      from\n    };\n    let initialTests = [];\n    if (this._typeError) initialTests.push(this._typeError);\n    let finalTests = [];\n    if (this._whitelistError) finalTests.push(this._whitelistError);\n    if (this._blacklistError) finalTests.push(this._blacklistError);\n    (0, _runTests.default)({\n      args,\n      value,\n      path,\n      sync,\n      tests: initialTests,\n      endEarly: abortEarly\n    }, err => {\n      if (err) return void cb(err, value);\n      (0, _runTests.default)({\n        tests: this.tests.concat(finalTests),\n        args,\n        path,\n        sync,\n        value,\n        endEarly: abortEarly\n      }, cb);\n    });\n  }\n\n  validate(value, options, maybeCb) {\n    let schema = this.resolve(_extends({}, options, {\n      value\n    })); // callback case is for nested validations\n\n    return typeof maybeCb === 'function' ? schema._validate(value, options, maybeCb) : new Promise((resolve, reject) => schema._validate(value, options, (err, value) => {\n      if (err) reject(err);else resolve(value);\n    }));\n  }\n\n  validateSync(value, options) {\n    let schema = this.resolve(_extends({}, options, {\n      value\n    }));\n    let result;\n\n    schema._validate(value, _extends({}, options, {\n      sync: true\n    }), (err, value) => {\n      if (err) throw err;\n      result = value;\n    });\n\n    return result;\n  }\n\n  isValid(value, options) {\n    return this.validate(value, options).then(() => true, err => {\n      if (_ValidationError.default.isError(err)) return false;\n      throw err;\n    });\n  }\n\n  isValidSync(value, options) {\n    try {\n      this.validateSync(value, options);\n      return true;\n    } catch (err) {\n      if (_ValidationError.default.isError(err)) return false;\n      throw err;\n    }\n  }\n\n  _getDefault() {\n    let defaultValue = this.spec.default;\n\n    if (defaultValue == null) {\n      return defaultValue;\n    }\n\n    return typeof defaultValue === 'function' ? defaultValue.call(this) : (0, _nanoclone.default)(defaultValue);\n  }\n\n  getDefault(options) {\n    let schema = this.resolve(options || {});\n    return schema._getDefault();\n  }\n\n  default(def) {\n    if (arguments.length === 0) {\n      return this._getDefault();\n    }\n\n    let next = this.clone({\n      default: def\n    });\n    return next;\n  }\n\n  strict(isStrict = true) {\n    let next = this.clone();\n    next.spec.strict = isStrict;\n    return next;\n  }\n\n  _isPresent(value) {\n    return value != null;\n  }\n\n  defined(message = _locale.mixed.defined) {\n    return this.test({\n      message,\n      name: 'defined',\n      exclusive: true,\n\n      test(value) {\n        return value !== undefined;\n      }\n\n    });\n  }\n\n  required(message = _locale.mixed.required) {\n    return this.clone({\n      presence: 'required'\n    }).withMutation(s => s.test({\n      message,\n      name: 'required',\n      exclusive: true,\n\n      test(value) {\n        return this.schema._isPresent(value);\n      }\n\n    }));\n  }\n\n  notRequired() {\n    let next = this.clone({\n      presence: 'optional'\n    });\n    next.tests = next.tests.filter(test => test.OPTIONS.name !== 'required');\n    return next;\n  }\n\n  nullable(isNullable = true) {\n    let next = this.clone({\n      nullable: isNullable !== false\n    });\n    return next;\n  }\n\n  transform(fn) {\n    let next = this.clone();\n    next.transforms.push(fn);\n    return next;\n  }\n  /**\n   * Adds a test function to the schema's queue of tests.\n   * tests can be exclusive or non-exclusive.\n   *\n   * - exclusive tests, will replace any existing tests of the same name.\n   * - non-exclusive: can be stacked\n   *\n   * If a non-exclusive test is added to a schema with an exclusive test of the same name\n   * the exclusive test is removed and further tests of the same name will be stacked.\n   *\n   * If an exclusive test is added to a schema with non-exclusive tests of the same name\n   * the previous tests are removed and further tests of the same name will replace each other.\n   */\n\n\n  test(...args) {\n    let opts;\n\n    if (args.length === 1) {\n      if (typeof args[0] === 'function') {\n        opts = {\n          test: args[0]\n        };\n      } else {\n        opts = args[0];\n      }\n    } else if (args.length === 2) {\n      opts = {\n        name: args[0],\n        test: args[1]\n      };\n    } else {\n      opts = {\n        name: args[0],\n        message: args[1],\n        test: args[2]\n      };\n    }\n\n    if (opts.message === undefined) opts.message = _locale.mixed.default;\n    if (typeof opts.test !== 'function') throw new TypeError('`test` is a required parameters');\n    let next = this.clone();\n    let validate = (0, _createValidation.default)(opts);\n    let isExclusive = opts.exclusive || opts.name && next.exclusiveTests[opts.name] === true;\n\n    if (opts.exclusive) {\n      if (!opts.name) throw new TypeError('Exclusive tests must provide a unique `name` identifying the test');\n    }\n\n    if (opts.name) next.exclusiveTests[opts.name] = !!opts.exclusive;\n    next.tests = next.tests.filter(fn => {\n      if (fn.OPTIONS.name === opts.name) {\n        if (isExclusive) return false;\n        if (fn.OPTIONS.test === validate.OPTIONS.test) return false;\n      }\n\n      return true;\n    });\n    next.tests.push(validate);\n    return next;\n  }\n\n  when(keys, options) {\n    if (!Array.isArray(keys) && typeof keys !== 'string') {\n      options = keys;\n      keys = '.';\n    }\n\n    let next = this.clone();\n    let deps = (0, _toArray.default)(keys).map(key => new _Reference.default(key));\n    deps.forEach(dep => {\n      // @ts-ignore\n      if (dep.isSibling) next.deps.push(dep.key);\n    });\n    next.conditions.push(new _Condition.default(deps, options));\n    return next;\n  }\n\n  typeError(message) {\n    let next = this.clone();\n    next._typeError = (0, _createValidation.default)({\n      message,\n      name: 'typeError',\n\n      test(value) {\n        if (value !== undefined && !this.schema.isType(value)) return this.createError({\n          params: {\n            type: this.schema._type\n          }\n        });\n        return true;\n      }\n\n    });\n    return next;\n  }\n\n  oneOf(enums, message = _locale.mixed.oneOf) {\n    let next = this.clone();\n    enums.forEach(val => {\n      next._whitelist.add(val);\n\n      next._blacklist.delete(val);\n    });\n    next._whitelistError = (0, _createValidation.default)({\n      message,\n      name: 'oneOf',\n\n      test(value) {\n        if (value === undefined) return true;\n        let valids = this.schema._whitelist;\n        let resolved = valids.resolveAll(this.resolve);\n        return resolved.includes(value) ? true : this.createError({\n          params: {\n            values: valids.toArray().join(', '),\n            resolved\n          }\n        });\n      }\n\n    });\n    return next;\n  }\n\n  notOneOf(enums, message = _locale.mixed.notOneOf) {\n    let next = this.clone();\n    enums.forEach(val => {\n      next._blacklist.add(val);\n\n      next._whitelist.delete(val);\n    });\n    next._blacklistError = (0, _createValidation.default)({\n      message,\n      name: 'notOneOf',\n\n      test(value) {\n        let invalids = this.schema._blacklist;\n        let resolved = invalids.resolveAll(this.resolve);\n        if (resolved.includes(value)) return this.createError({\n          params: {\n            values: invalids.toArray().join(', '),\n            resolved\n          }\n        });\n        return true;\n      }\n\n    });\n    return next;\n  }\n\n  strip(strip = true) {\n    let next = this.clone();\n    next.spec.strip = strip;\n    return next;\n  }\n\n  describe() {\n    const next = this.clone();\n    const {\n      label,\n      meta\n    } = next.spec;\n    const description = {\n      meta,\n      label,\n      type: next.type,\n      oneOf: next._whitelist.describe(),\n      notOneOf: next._blacklist.describe(),\n      tests: next.tests.map(fn => ({\n        name: fn.OPTIONS.name,\n        params: fn.OPTIONS.params\n      })).filter((n, idx, list) => list.findIndex(c => c.name === n.name) === idx)\n    };\n    return description;\n  }\n\n} // eslint-disable-next-line @typescript-eslint/no-unused-vars\n\n\nexports.default = BaseSchema;\n// @ts-expect-error\nBaseSchema.prototype.__isYupSchema__ = true;\n\nfor (const method of ['validate', 'validateSync']) BaseSchema.prototype[`${method}At`] = function (path, value, options = {}) {\n  const {\n    parent,\n    parentPath,\n    schema\n  } = (0, _reach.getIn)(this, path, value, options.context);\n  return schema[method](parent && parent[parentPath], _extends({}, options, {\n    parent,\n    path\n  }));\n};\n\nfor (const alias of ['equals', 'is']) BaseSchema.prototype[alias] = BaseSchema.prototype.oneOf;\n\nfor (const alias of ['not', 'nope']) BaseSchema.prototype[alias] = BaseSchema.prototype.notOneOf;\n\nBaseSchema.prototype.optional = BaseSchema.prototype.notRequired;", "\"use strict\";\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\nexports.create = create;\nexports.default = void 0;\n\nvar _schema = _interopRequireDefault(require(\"./schema\"));\n\nfunction _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { default: obj }; }\n\nconst Mixed = _schema.default;\nvar _default = Mixed;\nexports.default = _default;\n\nfunction create() {\n  return new Mixed();\n} // XXX: this is using the Base schema so that `addMethod(mixed)` works as a base class\n\n\ncreate.prototype = Mixed.prototype;", "\"use strict\";\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\nexports.default = void 0;\n\nconst isAbsent = value => value == null;\n\nvar _default = isAbsent;\nexports.default = _default;", "\"use strict\";\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\nexports.create = create;\nexports.default = void 0;\n\nvar _schema = _interopRequireDefault(require(\"./schema\"));\n\nvar _locale = require(\"./locale\");\n\nvar _isAbsent = _interopRequireDefault(require(\"./util/isAbsent\"));\n\nfunction _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { default: obj }; }\n\nfunction create() {\n  return new BooleanSchema();\n}\n\nclass BooleanSchema extends _schema.default {\n  constructor() {\n    super({\n      type: 'boolean'\n    });\n    this.withMutation(() => {\n      this.transform(function (value) {\n        if (!this.isType(value)) {\n          if (/^(true|1)$/i.test(String(value))) return true;\n          if (/^(false|0)$/i.test(String(value))) return false;\n        }\n\n        return value;\n      });\n    });\n  }\n\n  _typeCheck(v) {\n    if (v instanceof Boolean) v = v.valueOf();\n    return typeof v === 'boolean';\n  }\n\n  isTrue(message = _locale.boolean.isValue) {\n    return this.test({\n      message,\n      name: 'is-value',\n      exclusive: true,\n      params: {\n        value: 'true'\n      },\n\n      test(value) {\n        return (0, _isAbsent.default)(value) || value === true;\n      }\n\n    });\n  }\n\n  isFalse(message = _locale.boolean.isValue) {\n    return this.test({\n      message,\n      name: 'is-value',\n      exclusive: true,\n      params: {\n        value: 'false'\n      },\n\n      test(value) {\n        return (0, _isAbsent.default)(value) || value === false;\n      }\n\n    });\n  }\n\n}\n\nexports.default = BooleanSchema;\ncreate.prototype = BooleanSchema.prototype;", "\"use strict\";\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\nexports.create = create;\nexports.default = void 0;\n\nvar _locale = require(\"./locale\");\n\nvar _isAbsent = _interopRequireDefault(require(\"./util/isAbsent\"));\n\nvar _schema = _interopRequireDefault(require(\"./schema\"));\n\nfunction _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { default: obj }; }\n\n// eslint-disable-next-line\nlet rEmail = /^((([a-z]|\\d|[!#\\$%&'\\*\\+\\-\\/=\\?\\^_`{\\|}~]|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])+(\\.([a-z]|\\d|[!#\\$%&'\\*\\+\\-\\/=\\?\\^_`{\\|}~]|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])+)*)|((\\x22)((((\\x20|\\x09)*(\\x0d\\x0a))?(\\x20|\\x09)+)?(([\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x7f]|\\x21|[\\x23-\\x5b]|[\\x5d-\\x7e]|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])|(\\\\([\\x01-\\x09\\x0b\\x0c\\x0d-\\x7f]|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF]))))*(((\\x20|\\x09)*(\\x0d\\x0a))?(\\x20|\\x09)+)?(\\x22)))@((([a-z]|\\d|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])|(([a-z]|\\d|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])([a-z]|\\d|-|\\.|_|~|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])*([a-z]|\\d|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])))\\.)+(([a-z]|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])|(([a-z]|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])([a-z]|\\d|-|\\.|_|~|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])*([a-z]|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])))$/i; // eslint-disable-next-line\n\nlet rUrl = /^((https?|ftp):)?\\/\\/(((([a-z]|\\d|-|\\.|_|~|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])|(%[\\da-f]{2})|[!\\$&'\\(\\)\\*\\+,;=]|:)*@)?(((\\d|[1-9]\\d|1\\d\\d|2[0-4]\\d|25[0-5])\\.(\\d|[1-9]\\d|1\\d\\d|2[0-4]\\d|25[0-5])\\.(\\d|[1-9]\\d|1\\d\\d|2[0-4]\\d|25[0-5])\\.(\\d|[1-9]\\d|1\\d\\d|2[0-4]\\d|25[0-5]))|((([a-z]|\\d|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])|(([a-z]|\\d|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])([a-z]|\\d|-|\\.|_|~|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])*([a-z]|\\d|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])))\\.)+(([a-z]|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])|(([a-z]|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])([a-z]|\\d|-|\\.|_|~|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])*([a-z]|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])))\\.?)(:\\d*)?)(\\/((([a-z]|\\d|-|\\.|_|~|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])|(%[\\da-f]{2})|[!\\$&'\\(\\)\\*\\+,;=]|:|@)+(\\/(([a-z]|\\d|-|\\.|_|~|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])|(%[\\da-f]{2})|[!\\$&'\\(\\)\\*\\+,;=]|:|@)*)*)?)?(\\?((([a-z]|\\d|-|\\.|_|~|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])|(%[\\da-f]{2})|[!\\$&'\\(\\)\\*\\+,;=]|:|@)|[\\uE000-\\uF8FF]|\\/|\\?)*)?(\\#((([a-z]|\\d|-|\\.|_|~|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])|(%[\\da-f]{2})|[!\\$&'\\(\\)\\*\\+,;=]|:|@)|\\/|\\?)*)?$/i; // eslint-disable-next-line\n\nlet rUUID = /^(?:[0-9a-f]{8}-[0-9a-f]{4}-[1-5][0-9a-f]{3}-[89ab][0-9a-f]{3}-[0-9a-f]{12}|00000000-0000-0000-0000-000000000000)$/i;\n\nlet isTrimmed = value => (0, _isAbsent.default)(value) || value === value.trim();\n\nlet objStringTag = {}.toString();\n\nfunction create() {\n  return new StringSchema();\n}\n\nclass StringSchema extends _schema.default {\n  constructor() {\n    super({\n      type: 'string'\n    });\n    this.withMutation(() => {\n      this.transform(function (value) {\n        if (this.isType(value)) return value;\n        if (Array.isArray(value)) return value;\n        const strValue = value != null && value.toString ? value.toString() : value;\n        if (strValue === objStringTag) return value;\n        return strValue;\n      });\n    });\n  }\n\n  _typeCheck(value) {\n    if (value instanceof String) value = value.valueOf();\n    return typeof value === 'string';\n  }\n\n  _isPresent(value) {\n    return super._isPresent(value) && !!value.length;\n  }\n\n  length(length, message = _locale.string.length) {\n    return this.test({\n      message,\n      name: 'length',\n      exclusive: true,\n      params: {\n        length\n      },\n\n      test(value) {\n        return (0, _isAbsent.default)(value) || value.length === this.resolve(length);\n      }\n\n    });\n  }\n\n  min(min, message = _locale.string.min) {\n    return this.test({\n      message,\n      name: 'min',\n      exclusive: true,\n      params: {\n        min\n      },\n\n      test(value) {\n        return (0, _isAbsent.default)(value) || value.length >= this.resolve(min);\n      }\n\n    });\n  }\n\n  max(max, message = _locale.string.max) {\n    return this.test({\n      name: 'max',\n      exclusive: true,\n      message,\n      params: {\n        max\n      },\n\n      test(value) {\n        return (0, _isAbsent.default)(value) || value.length <= this.resolve(max);\n      }\n\n    });\n  }\n\n  matches(regex, options) {\n    let excludeEmptyString = false;\n    let message;\n    let name;\n\n    if (options) {\n      if (typeof options === 'object') {\n        ({\n          excludeEmptyString = false,\n          message,\n          name\n        } = options);\n      } else {\n        message = options;\n      }\n    }\n\n    return this.test({\n      name: name || 'matches',\n      message: message || _locale.string.matches,\n      params: {\n        regex\n      },\n      test: value => (0, _isAbsent.default)(value) || value === '' && excludeEmptyString || value.search(regex) !== -1\n    });\n  }\n\n  email(message = _locale.string.email) {\n    return this.matches(rEmail, {\n      name: 'email',\n      message,\n      excludeEmptyString: true\n    });\n  }\n\n  url(message = _locale.string.url) {\n    return this.matches(rUrl, {\n      name: 'url',\n      message,\n      excludeEmptyString: true\n    });\n  }\n\n  uuid(message = _locale.string.uuid) {\n    return this.matches(rUUID, {\n      name: 'uuid',\n      message,\n      excludeEmptyString: false\n    });\n  } //-- transforms --\n\n\n  ensure() {\n    return this.default('').transform(val => val === null ? '' : val);\n  }\n\n  trim(message = _locale.string.trim) {\n    return this.transform(val => val != null ? val.trim() : val).test({\n      message,\n      name: 'trim',\n      test: isTrimmed\n    });\n  }\n\n  lowercase(message = _locale.string.lowercase) {\n    return this.transform(value => !(0, _isAbsent.default)(value) ? value.toLowerCase() : value).test({\n      message,\n      name: 'string_case',\n      exclusive: true,\n      test: value => (0, _isAbsent.default)(value) || value === value.toLowerCase()\n    });\n  }\n\n  uppercase(message = _locale.string.uppercase) {\n    return this.transform(value => !(0, _isAbsent.default)(value) ? value.toUpperCase() : value).test({\n      message,\n      name: 'string_case',\n      exclusive: true,\n      test: value => (0, _isAbsent.default)(value) || value === value.toUpperCase()\n    });\n  }\n\n}\n\nexports.default = StringSchema;\ncreate.prototype = StringSchema.prototype; //\n// String Interfaces\n//", "\"use strict\";\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\nexports.create = create;\nexports.default = void 0;\n\nvar _locale = require(\"./locale\");\n\nvar _isAbsent = _interopRequireDefault(require(\"./util/isAbsent\"));\n\nvar _schema = _interopRequireDefault(require(\"./schema\"));\n\nfunction _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { default: obj }; }\n\nlet isNaN = value => value != +value;\n\nfunction create() {\n  return new NumberSchema();\n}\n\nclass NumberSchema extends _schema.default {\n  constructor() {\n    super({\n      type: 'number'\n    });\n    this.withMutation(() => {\n      this.transform(function (value) {\n        let parsed = value;\n\n        if (typeof parsed === 'string') {\n          parsed = parsed.replace(/\\s/g, '');\n          if (parsed === '') return NaN; // don't use parseFloat to avoid positives on alpha-numeric strings\n\n          parsed = +parsed;\n        }\n\n        if (this.isType(parsed)) return parsed;\n        return parseFloat(parsed);\n      });\n    });\n  }\n\n  _typeCheck(value) {\n    if (value instanceof Number) value = value.valueOf();\n    return typeof value === 'number' && !isNaN(value);\n  }\n\n  min(min, message = _locale.number.min) {\n    return this.test({\n      message,\n      name: 'min',\n      exclusive: true,\n      params: {\n        min\n      },\n\n      test(value) {\n        return (0, _isAbsent.default)(value) || value >= this.resolve(min);\n      }\n\n    });\n  }\n\n  max(max, message = _locale.number.max) {\n    return this.test({\n      message,\n      name: 'max',\n      exclusive: true,\n      params: {\n        max\n      },\n\n      test(value) {\n        return (0, _isAbsent.default)(value) || value <= this.resolve(max);\n      }\n\n    });\n  }\n\n  lessThan(less, message = _locale.number.lessThan) {\n    return this.test({\n      message,\n      name: 'max',\n      exclusive: true,\n      params: {\n        less\n      },\n\n      test(value) {\n        return (0, _isAbsent.default)(value) || value < this.resolve(less);\n      }\n\n    });\n  }\n\n  moreThan(more, message = _locale.number.moreThan) {\n    return this.test({\n      message,\n      name: 'min',\n      exclusive: true,\n      params: {\n        more\n      },\n\n      test(value) {\n        return (0, _isAbsent.default)(value) || value > this.resolve(more);\n      }\n\n    });\n  }\n\n  positive(msg = _locale.number.positive) {\n    return this.moreThan(0, msg);\n  }\n\n  negative(msg = _locale.number.negative) {\n    return this.lessThan(0, msg);\n  }\n\n  integer(message = _locale.number.integer) {\n    return this.test({\n      name: 'integer',\n      message,\n      test: val => (0, _isAbsent.default)(val) || Number.isInteger(val)\n    });\n  }\n\n  truncate() {\n    return this.transform(value => !(0, _isAbsent.default)(value) ? value | 0 : value);\n  }\n\n  round(method) {\n    var _method;\n\n    let avail = ['ceil', 'floor', 'round', 'trunc'];\n    method = ((_method = method) == null ? void 0 : _method.toLowerCase()) || 'round'; // this exists for symemtry with the new Math.trunc\n\n    if (method === 'trunc') return this.truncate();\n    if (avail.indexOf(method.toLowerCase()) === -1) throw new TypeError('Only valid options for round() are: ' + avail.join(', '));\n    return this.transform(value => !(0, _isAbsent.default)(value) ? Math[method](value) : value);\n  }\n\n}\n\nexports.default = NumberSchema;\ncreate.prototype = NumberSchema.prototype; //\n// Number Interfaces\n//", "\"use strict\";\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\nexports.default = parseIsoDate;\n\n/* eslint-disable */\n\n/**\n *\n * Date.parse with progressive enhancement for ISO 8601 <https://github.com/csnover/js-iso8601>\n * NON-CONFORMANT EDITION.\n * \u00A9 2011 Colin Snover <http://zetafleet.com>\n * Released under MIT license.\n */\n//              1 YYYY                 2 MM        3 DD              4 HH     5 mm        6 ss            7 msec         8 Z 9 \u00B1    10 tzHH    11 tzmm\nvar isoReg = /^(\\d{4}|[+\\-]\\d{6})(?:-?(\\d{2})(?:-?(\\d{2}))?)?(?:[ T]?(\\d{2}):?(\\d{2})(?::?(\\d{2})(?:[,\\.](\\d{1,}))?)?(?:(Z)|([+\\-])(\\d{2})(?::?(\\d{2}))?)?)?$/;\n\nfunction parseIsoDate(date) {\n  var numericKeys = [1, 4, 5, 6, 7, 10, 11],\n      minutesOffset = 0,\n      timestamp,\n      struct;\n\n  if (struct = isoReg.exec(date)) {\n    // avoid NaN timestamps caused by \u201Cundefined\u201D values being passed to Date.UTC\n    for (var i = 0, k; k = numericKeys[i]; ++i) struct[k] = +struct[k] || 0; // allow undefined days and months\n\n\n    struct[2] = (+struct[2] || 1) - 1;\n    struct[3] = +struct[3] || 1; // allow arbitrary sub-second precision beyond milliseconds\n\n    struct[7] = struct[7] ? String(struct[7]).substr(0, 3) : 0; // timestamps without timezone identifiers should be considered local time\n\n    if ((struct[8] === undefined || struct[8] === '') && (struct[9] === undefined || struct[9] === '')) timestamp = +new Date(struct[1], struct[2], struct[3], struct[4], struct[5], struct[6], struct[7]);else {\n      if (struct[8] !== 'Z' && struct[9] !== undefined) {\n        minutesOffset = struct[10] * 60 + struct[11];\n        if (struct[9] === '+') minutesOffset = 0 - minutesOffset;\n      }\n\n      timestamp = Date.UTC(struct[1], struct[2], struct[3], struct[4], struct[5] + minutesOffset, struct[6], struct[7]);\n    }\n  } else timestamp = Date.parse ? Date.parse(date) : NaN;\n\n  return timestamp;\n}", "\"use strict\";\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\nexports.create = create;\nexports.default = void 0;\n\nvar _isodate = _interopRequireDefault(require(\"./util/isodate\"));\n\nvar _locale = require(\"./locale\");\n\nvar _isAbsent = _interopRequireDefault(require(\"./util/isAbsent\"));\n\nvar _Reference = _interopRequireDefault(require(\"./Reference\"));\n\nvar _schema = _interopRequireDefault(require(\"./schema\"));\n\nfunction _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { default: obj }; }\n\n// @ts-ignore\nlet invalidDate = new Date('');\n\nlet isDate = obj => Object.prototype.toString.call(obj) === '[object Date]';\n\nfunction create() {\n  return new DateSchema();\n}\n\nclass DateSchema extends _schema.default {\n  constructor() {\n    super({\n      type: 'date'\n    });\n    this.withMutation(() => {\n      this.transform(function (value) {\n        if (this.isType(value)) return value;\n        value = (0, _isodate.default)(value); // 0 is a valid timestamp equivalent to 1970-01-01T00:00:00Z(unix epoch) or before.\n\n        return !isNaN(value) ? new Date(value) : invalidDate;\n      });\n    });\n  }\n\n  _typeCheck(v) {\n    return isDate(v) && !isNaN(v.getTime());\n  }\n\n  prepareParam(ref, name) {\n    let param;\n\n    if (!_Reference.default.isRef(ref)) {\n      let cast = this.cast(ref);\n      if (!this._typeCheck(cast)) throw new TypeError(`\\`${name}\\` must be a Date or a value that can be \\`cast()\\` to a Date`);\n      param = cast;\n    } else {\n      param = ref;\n    }\n\n    return param;\n  }\n\n  min(min, message = _locale.date.min) {\n    let limit = this.prepareParam(min, 'min');\n    return this.test({\n      message,\n      name: 'min',\n      exclusive: true,\n      params: {\n        min\n      },\n\n      test(value) {\n        return (0, _isAbsent.default)(value) || value >= this.resolve(limit);\n      }\n\n    });\n  }\n\n  max(max, message = _locale.date.max) {\n    let limit = this.prepareParam(max, 'max');\n    return this.test({\n      message,\n      name: 'max',\n      exclusive: true,\n      params: {\n        max\n      },\n\n      test(value) {\n        return (0, _isAbsent.default)(value) || value <= this.resolve(limit);\n      }\n\n    });\n  }\n\n}\n\nexports.default = DateSchema;\nDateSchema.INVALID_DATE = invalidDate;\ncreate.prototype = DateSchema.prototype;\ncreate.INVALID_DATE = invalidDate;", "/**\n * A specialized version of `_.reduce` for arrays without support for\n * iteratee shorthands.\n *\n * @private\n * @param {Array} [array] The array to iterate over.\n * @param {Function} iteratee The function invoked per iteration.\n * @param {*} [accumulator] The initial value.\n * @param {boolean} [initAccum] Specify using the first element of `array` as\n *  the initial value.\n * @returns {*} Returns the accumulated value.\n */\nfunction arrayReduce(array, iteratee, accumulator, initAccum) {\n  var index = -1,\n      length = array == null ? 0 : array.length;\n\n  if (initAccum && length) {\n    accumulator = array[++index];\n  }\n  while (++index < length) {\n    accumulator = iteratee(accumulator, array[index], index, array);\n  }\n  return accumulator;\n}\n\nmodule.exports = arrayReduce;\n", "/**\n * The base implementation of `_.propertyOf` without support for deep paths.\n *\n * @private\n * @param {Object} object The object to query.\n * @returns {Function} Returns the new accessor function.\n */\nfunction basePropertyOf(object) {\n  return function(key) {\n    return object == null ? undefined : object[key];\n  };\n}\n\nmodule.exports = basePropertyOf;\n", "var basePropertyOf = require('./_basePropertyOf');\n\n/** Used to map Latin Unicode letters to basic Latin letters. */\nvar deburredLetters = {\n  // Latin-1 Supplement block.\n  '\\xc0': 'A',  '\\xc1': 'A', '\\xc2': 'A', '\\xc3': 'A', '\\xc4': 'A', '\\xc5': 'A',\n  '\\xe0': 'a',  '\\xe1': 'a', '\\xe2': 'a', '\\xe3': 'a', '\\xe4': 'a', '\\xe5': 'a',\n  '\\xc7': 'C',  '\\xe7': 'c',\n  '\\xd0': 'D',  '\\xf0': 'd',\n  '\\xc8': 'E',  '\\xc9': 'E', '\\xca': 'E', '\\xcb': 'E',\n  '\\xe8': 'e',  '\\xe9': 'e', '\\xea': 'e', '\\xeb': 'e',\n  '\\xcc': 'I',  '\\xcd': 'I', '\\xce': 'I', '\\xcf': 'I',\n  '\\xec': 'i',  '\\xed': 'i', '\\xee': 'i', '\\xef': 'i',\n  '\\xd1': 'N',  '\\xf1': 'n',\n  '\\xd2': 'O',  '\\xd3': 'O', '\\xd4': 'O', '\\xd5': 'O', '\\xd6': 'O', '\\xd8': 'O',\n  '\\xf2': 'o',  '\\xf3': 'o', '\\xf4': 'o', '\\xf5': 'o', '\\xf6': 'o', '\\xf8': 'o',\n  '\\xd9': 'U',  '\\xda': 'U', '\\xdb': 'U', '\\xdc': 'U',\n  '\\xf9': 'u',  '\\xfa': 'u', '\\xfb': 'u', '\\xfc': 'u',\n  '\\xdd': 'Y',  '\\xfd': 'y', '\\xff': 'y',\n  '\\xc6': 'Ae', '\\xe6': 'ae',\n  '\\xde': 'Th', '\\xfe': 'th',\n  '\\xdf': 'ss',\n  // Latin Extended-A block.\n  '\\u0100': 'A',  '\\u0102': 'A', '\\u0104': 'A',\n  '\\u0101': 'a',  '\\u0103': 'a', '\\u0105': 'a',\n  '\\u0106': 'C',  '\\u0108': 'C', '\\u010a': 'C', '\\u010c': 'C',\n  '\\u0107': 'c',  '\\u0109': 'c', '\\u010b': 'c', '\\u010d': 'c',\n  '\\u010e': 'D',  '\\u0110': 'D', '\\u010f': 'd', '\\u0111': 'd',\n  '\\u0112': 'E',  '\\u0114': 'E', '\\u0116': 'E', '\\u0118': 'E', '\\u011a': 'E',\n  '\\u0113': 'e',  '\\u0115': 'e', '\\u0117': 'e', '\\u0119': 'e', '\\u011b': 'e',\n  '\\u011c': 'G',  '\\u011e': 'G', '\\u0120': 'G', '\\u0122': 'G',\n  '\\u011d': 'g',  '\\u011f': 'g', '\\u0121': 'g', '\\u0123': 'g',\n  '\\u0124': 'H',  '\\u0126': 'H', '\\u0125': 'h', '\\u0127': 'h',\n  '\\u0128': 'I',  '\\u012a': 'I', '\\u012c': 'I', '\\u012e': 'I', '\\u0130': 'I',\n  '\\u0129': 'i',  '\\u012b': 'i', '\\u012d': 'i', '\\u012f': 'i', '\\u0131': 'i',\n  '\\u0134': 'J',  '\\u0135': 'j',\n  '\\u0136': 'K',  '\\u0137': 'k', '\\u0138': 'k',\n  '\\u0139': 'L',  '\\u013b': 'L', '\\u013d': 'L', '\\u013f': 'L', '\\u0141': 'L',\n  '\\u013a': 'l',  '\\u013c': 'l', '\\u013e': 'l', '\\u0140': 'l', '\\u0142': 'l',\n  '\\u0143': 'N',  '\\u0145': 'N', '\\u0147': 'N', '\\u014a': 'N',\n  '\\u0144': 'n',  '\\u0146': 'n', '\\u0148': 'n', '\\u014b': 'n',\n  '\\u014c': 'O',  '\\u014e': 'O', '\\u0150': 'O',\n  '\\u014d': 'o',  '\\u014f': 'o', '\\u0151': 'o',\n  '\\u0154': 'R',  '\\u0156': 'R', '\\u0158': 'R',\n  '\\u0155': 'r',  '\\u0157': 'r', '\\u0159': 'r',\n  '\\u015a': 'S',  '\\u015c': 'S', '\\u015e': 'S', '\\u0160': 'S',\n  '\\u015b': 's',  '\\u015d': 's', '\\u015f': 's', '\\u0161': 's',\n  '\\u0162': 'T',  '\\u0164': 'T', '\\u0166': 'T',\n  '\\u0163': 't',  '\\u0165': 't', '\\u0167': 't',\n  '\\u0168': 'U',  '\\u016a': 'U', '\\u016c': 'U', '\\u016e': 'U', '\\u0170': 'U', '\\u0172': 'U',\n  '\\u0169': 'u',  '\\u016b': 'u', '\\u016d': 'u', '\\u016f': 'u', '\\u0171': 'u', '\\u0173': 'u',\n  '\\u0174': 'W',  '\\u0175': 'w',\n  '\\u0176': 'Y',  '\\u0177': 'y', '\\u0178': 'Y',\n  '\\u0179': 'Z',  '\\u017b': 'Z', '\\u017d': 'Z',\n  '\\u017a': 'z',  '\\u017c': 'z', '\\u017e': 'z',\n  '\\u0132': 'IJ', '\\u0133': 'ij',\n  '\\u0152': 'Oe', '\\u0153': 'oe',\n  '\\u0149': \"'n\", '\\u017f': 's'\n};\n\n/**\n * Used by `_.deburr` to convert Latin-1 Supplement and Latin Extended-A\n * letters to basic Latin letters.\n *\n * @private\n * @param {string} letter The matched letter to deburr.\n * @returns {string} Returns the deburred letter.\n */\nvar deburrLetter = basePropertyOf(deburredLetters);\n\nmodule.exports = deburrLetter;\n", "var deburrLetter = require('./_deburrLetter'),\n    toString = require('./toString');\n\n/** Used to match Latin Unicode letters (excluding mathematical operators). */\nvar reLatin = /[\\xc0-\\xd6\\xd8-\\xf6\\xf8-\\xff\\u0100-\\u017f]/g;\n\n/** Used to compose unicode character classes. */\nvar rsComboMarksRange = '\\\\u0300-\\\\u036f',\n    reComboHalfMarksRange = '\\\\ufe20-\\\\ufe2f',\n    rsComboSymbolsRange = '\\\\u20d0-\\\\u20ff',\n    rsComboRange = rsComboMarksRange + reComboHalfMarksRange + rsComboSymbolsRange;\n\n/** Used to compose unicode capture groups. */\nvar rsCombo = '[' + rsComboRange + ']';\n\n/**\n * Used to match [combining diacritical marks](https://en.wikipedia.org/wiki/Combining_Diacritical_Marks) and\n * [combining diacritical marks for symbols](https://en.wikipedia.org/wiki/Combining_Diacritical_Marks_for_Symbols).\n */\nvar reComboMark = RegExp(rsCombo, 'g');\n\n/**\n * Deburrs `string` by converting\n * [Latin-1 Supplement](https://en.wikipedia.org/wiki/Latin-1_Supplement_(Unicode_block)#Character_table)\n * and [Latin Extended-A](https://en.wikipedia.org/wiki/Latin_Extended-A)\n * letters to basic Latin letters and removing\n * [combining diacritical marks](https://en.wikipedia.org/wiki/Combining_Diacritical_Marks).\n *\n * @static\n * @memberOf _\n * @since 3.0.0\n * @category String\n * @param {string} [string=''] The string to deburr.\n * @returns {string} Returns the deburred string.\n * @example\n *\n * _.deburr('d\u00E9j\u00E0 vu');\n * // => 'deja vu'\n */\nfunction deburr(string) {\n  string = toString(string);\n  return string && string.replace(reLatin, deburrLetter).replace(reComboMark, '');\n}\n\nmodule.exports = deburr;\n", "/** Used to match words composed of alphanumeric characters. */\nvar reAsciiWord = /[^\\x00-\\x2f\\x3a-\\x40\\x5b-\\x60\\x7b-\\x7f]+/g;\n\n/**\n * Splits an ASCII `string` into an array of its words.\n *\n * @private\n * @param {string} The string to inspect.\n * @returns {Array} Returns the words of `string`.\n */\nfunction asciiWords(string) {\n  return string.match(reAsciiWord) || [];\n}\n\nmodule.exports = asciiWords;\n", "/** Used to detect strings that need a more robust regexp to match words. */\nvar reHasUnicodeWord = /[a-z][A-Z]|[A-Z]{2}[a-z]|[0-9][a-zA-Z]|[a-zA-Z][0-9]|[^a-zA-Z0-9 ]/;\n\n/**\n * Checks if `string` contains a word composed of Unicode symbols.\n *\n * @private\n * @param {string} string The string to inspect.\n * @returns {boolean} Returns `true` if a word is found, else `false`.\n */\nfunction hasUnicodeWord(string) {\n  return reHasUnicodeWord.test(string);\n}\n\nmodule.exports = hasUnicodeWord;\n", "/** Used to compose unicode character classes. */\nvar rsAstralRange = '\\\\ud800-\\\\udfff',\n    rsComboMarksRange = '\\\\u0300-\\\\u036f',\n    reComboHalfMarksRange = '\\\\ufe20-\\\\ufe2f',\n    rsComboSymbolsRange = '\\\\u20d0-\\\\u20ff',\n    rsComboRange = rsComboMarksRange + reComboHalfMarksRange + rsComboSymbolsRange,\n    rsDingbatRange = '\\\\u2700-\\\\u27bf',\n    rsLowerRange = 'a-z\\\\xdf-\\\\xf6\\\\xf8-\\\\xff',\n    rsMathOpRange = '\\\\xac\\\\xb1\\\\xd7\\\\xf7',\n    rsNonCharRange = '\\\\x00-\\\\x2f\\\\x3a-\\\\x40\\\\x5b-\\\\x60\\\\x7b-\\\\xbf',\n    rsPunctuationRange = '\\\\u2000-\\\\u206f',\n    rsSpaceRange = ' \\\\t\\\\x0b\\\\f\\\\xa0\\\\ufeff\\\\n\\\\r\\\\u2028\\\\u2029\\\\u1680\\\\u180e\\\\u2000\\\\u2001\\\\u2002\\\\u2003\\\\u2004\\\\u2005\\\\u2006\\\\u2007\\\\u2008\\\\u2009\\\\u200a\\\\u202f\\\\u205f\\\\u3000',\n    rsUpperRange = 'A-Z\\\\xc0-\\\\xd6\\\\xd8-\\\\xde',\n    rsVarRange = '\\\\ufe0e\\\\ufe0f',\n    rsBreakRange = rsMathOpRange + rsNonCharRange + rsPunctuationRange + rsSpaceRange;\n\n/** Used to compose unicode capture groups. */\nvar rsApos = \"['\\u2019]\",\n    rsBreak = '[' + rsBreakRange + ']',\n    rsCombo = '[' + rsComboRange + ']',\n    rsDigits = '\\\\d+',\n    rsDingbat = '[' + rsDingbatRange + ']',\n    rsLower = '[' + rsLowerRange + ']',\n    rsMisc = '[^' + rsAstralRange + rsBreakRange + rsDigits + rsDingbatRange + rsLowerRange + rsUpperRange + ']',\n    rsFitz = '\\\\ud83c[\\\\udffb-\\\\udfff]',\n    rsModifier = '(?:' + rsCombo + '|' + rsFitz + ')',\n    rsNonAstral = '[^' + rsAstralRange + ']',\n    rsRegional = '(?:\\\\ud83c[\\\\udde6-\\\\uddff]){2}',\n    rsSurrPair = '[\\\\ud800-\\\\udbff][\\\\udc00-\\\\udfff]',\n    rsUpper = '[' + rsUpperRange + ']',\n    rsZWJ = '\\\\u200d';\n\n/** Used to compose unicode regexes. */\nvar rsMiscLower = '(?:' + rsLower + '|' + rsMisc + ')',\n    rsMiscUpper = '(?:' + rsUpper + '|' + rsMisc + ')',\n    rsOptContrLower = '(?:' + rsApos + '(?:d|ll|m|re|s|t|ve))?',\n    rsOptContrUpper = '(?:' + rsApos + '(?:D|LL|M|RE|S|T|VE))?',\n    reOptMod = rsModifier + '?',\n    rsOptVar = '[' + rsVarRange + ']?',\n    rsOptJoin = '(?:' + rsZWJ + '(?:' + [rsNonAstral, rsRegional, rsSurrPair].join('|') + ')' + rsOptVar + reOptMod + ')*',\n    rsOrdLower = '\\\\d*(?:1st|2nd|3rd|(?![123])\\\\dth)(?=\\\\b|[A-Z_])',\n    rsOrdUpper = '\\\\d*(?:1ST|2ND|3RD|(?![123])\\\\dTH)(?=\\\\b|[a-z_])',\n    rsSeq = rsOptVar + reOptMod + rsOptJoin,\n    rsEmoji = '(?:' + [rsDingbat, rsRegional, rsSurrPair].join('|') + ')' + rsSeq;\n\n/** Used to match complex or compound words. */\nvar reUnicodeWord = RegExp([\n  rsUpper + '?' + rsLower + '+' + rsOptContrLower + '(?=' + [rsBreak, rsUpper, '$'].join('|') + ')',\n  rsMiscUpper + '+' + rsOptContrUpper + '(?=' + [rsBreak, rsUpper + rsMiscLower, '$'].join('|') + ')',\n  rsUpper + '?' + rsMiscLower + '+' + rsOptContrLower,\n  rsUpper + '+' + rsOptContrUpper,\n  rsOrdUpper,\n  rsOrdLower,\n  rsDigits,\n  rsEmoji\n].join('|'), 'g');\n\n/**\n * Splits a Unicode `string` into an array of its words.\n *\n * @private\n * @param {string} The string to inspect.\n * @returns {Array} Returns the words of `string`.\n */\nfunction unicodeWords(string) {\n  return string.match(reUnicodeWord) || [];\n}\n\nmodule.exports = unicodeWords;\n", "var asciiWords = require('./_asciiWords'),\n    hasUnicodeWord = require('./_hasUnicodeWord'),\n    toString = require('./toString'),\n    unicodeWords = require('./_unicodeWords');\n\n/**\n * Splits `string` into an array of its words.\n *\n * @static\n * @memberOf _\n * @since 3.0.0\n * @category String\n * @param {string} [string=''] The string to inspect.\n * @param {RegExp|string} [pattern] The pattern to match words.\n * @param- {Object} [guard] Enables use as an iteratee for methods like `_.map`.\n * @returns {Array} Returns the words of `string`.\n * @example\n *\n * _.words('fred, barney, & pebbles');\n * // => ['fred', 'barney', 'pebbles']\n *\n * _.words('fred, barney, & pebbles', /[^, ]+/g);\n * // => ['fred', 'barney', '&', 'pebbles']\n */\nfunction words(string, pattern, guard) {\n  string = toString(string);\n  pattern = guard ? undefined : pattern;\n\n  if (pattern === undefined) {\n    return hasUnicodeWord(string) ? unicodeWords(string) : asciiWords(string);\n  }\n  return string.match(pattern) || [];\n}\n\nmodule.exports = words;\n", "var arrayReduce = require('./_arrayReduce'),\n    deburr = require('./deburr'),\n    words = require('./words');\n\n/** Used to compose unicode capture groups. */\nvar rsApos = \"['\\u2019]\";\n\n/** Used to match apostrophes. */\nvar reApos = RegExp(rsApos, 'g');\n\n/**\n * Creates a function like `_.camelCase`.\n *\n * @private\n * @param {Function} callback The function to combine each word.\n * @returns {Function} Returns the new compounder function.\n */\nfunction createCompounder(callback) {\n  return function(string) {\n    return arrayReduce(words(deburr(string).replace(reApos, '')), callback, '');\n  };\n}\n\nmodule.exports = createCompounder;\n", "var createCompounder = require('./_createCompounder');\n\n/**\n * Converts `string` to\n * [snake case](https://en.wikipedia.org/wiki/Snake_case).\n *\n * @static\n * @memberOf _\n * @since 3.0.0\n * @category String\n * @param {string} [string=''] The string to convert.\n * @returns {string} Returns the snake cased string.\n * @example\n *\n * _.snakeCase('Foo Bar');\n * // => 'foo_bar'\n *\n * _.snakeCase('fooBar');\n * // => 'foo_bar'\n *\n * _.snakeCase('--FOO-BAR--');\n * // => 'foo_bar'\n */\nvar snakeCase = createCompounder(function(result, word, index) {\n  return result + (index ? '_' : '') + word.toLowerCase();\n});\n\nmodule.exports = snakeCase;\n", "/**\n * The base implementation of `_.slice` without an iteratee call guard.\n *\n * @private\n * @param {Array} array The array to slice.\n * @param {number} [start=0] The start position.\n * @param {number} [end=array.length] The end position.\n * @returns {Array} Returns the slice of `array`.\n */\nfunction baseSlice(array, start, end) {\n  var index = -1,\n      length = array.length;\n\n  if (start < 0) {\n    start = -start > length ? 0 : (length + start);\n  }\n  end = end > length ? length : end;\n  if (end < 0) {\n    end += length;\n  }\n  length = start > end ? 0 : ((end - start) >>> 0);\n  start >>>= 0;\n\n  var result = Array(length);\n  while (++index < length) {\n    result[index] = array[index + start];\n  }\n  return result;\n}\n\nmodule.exports = baseSlice;\n", "var baseSlice = require('./_baseSlice');\n\n/**\n * Casts `array` to a slice if it's needed.\n *\n * @private\n * @param {Array} array The array to inspect.\n * @param {number} start The start position.\n * @param {number} [end=array.length] The end position.\n * @returns {Array} Returns the cast slice.\n */\nfunction castSlice(array, start, end) {\n  var length = array.length;\n  end = end === undefined ? length : end;\n  return (!start && end >= length) ? array : baseSlice(array, start, end);\n}\n\nmodule.exports = castSlice;\n", "/** Used to compose unicode character classes. */\nvar rsAstralRange = '\\\\ud800-\\\\udfff',\n    rsComboMarksRange = '\\\\u0300-\\\\u036f',\n    reComboHalfMarksRange = '\\\\ufe20-\\\\ufe2f',\n    rsComboSymbolsRange = '\\\\u20d0-\\\\u20ff',\n    rsComboRange = rsComboMarksRange + reComboHalfMarksRange + rsComboSymbolsRange,\n    rsVarRange = '\\\\ufe0e\\\\ufe0f';\n\n/** Used to compose unicode capture groups. */\nvar rsZWJ = '\\\\u200d';\n\n/** Used to detect strings with [zero-width joiners or code points from the astral planes](http://eev.ee/blog/2015/09/12/dark-corners-of-unicode/). */\nvar reHasUnicode = RegExp('[' + rsZWJ + rsAstralRange  + rsComboRange + rsVarRange + ']');\n\n/**\n * Checks if `string` contains Unicode symbols.\n *\n * @private\n * @param {string} string The string to inspect.\n * @returns {boolean} Returns `true` if a symbol is found, else `false`.\n */\nfunction hasUnicode(string) {\n  return reHasUnicode.test(string);\n}\n\nmodule.exports = hasUnicode;\n", "/**\n * Converts an ASCII `string` to an array.\n *\n * @private\n * @param {string} string The string to convert.\n * @returns {Array} Returns the converted array.\n */\nfunction asciiToArray(string) {\n  return string.split('');\n}\n\nmodule.exports = asciiToArray;\n", "/** Used to compose unicode character classes. */\nvar rsAstralRange = '\\\\ud800-\\\\udfff',\n    rsComboMarksRange = '\\\\u0300-\\\\u036f',\n    reComboHalfMarksRange = '\\\\ufe20-\\\\ufe2f',\n    rsComboSymbolsRange = '\\\\u20d0-\\\\u20ff',\n    rsComboRange = rsComboMarksRange + reComboHalfMarksRange + rsComboSymbolsRange,\n    rsVarRange = '\\\\ufe0e\\\\ufe0f';\n\n/** Used to compose unicode capture groups. */\nvar rsAstral = '[' + rsAstralRange + ']',\n    rsCombo = '[' + rsComboRange + ']',\n    rsFitz = '\\\\ud83c[\\\\udffb-\\\\udfff]',\n    rsModifier = '(?:' + rsCombo + '|' + rsFitz + ')',\n    rsNonAstral = '[^' + rsAstralRange + ']',\n    rsRegional = '(?:\\\\ud83c[\\\\udde6-\\\\uddff]){2}',\n    rsSurrPair = '[\\\\ud800-\\\\udbff][\\\\udc00-\\\\udfff]',\n    rsZWJ = '\\\\u200d';\n\n/** Used to compose unicode regexes. */\nvar reOptMod = rsModifier + '?',\n    rsOptVar = '[' + rsVarRange + ']?',\n    rsOptJoin = '(?:' + rsZWJ + '(?:' + [rsNonAstral, rsRegional, rsSurrPair].join('|') + ')' + rsOptVar + reOptMod + ')*',\n    rsSeq = rsOptVar + reOptMod + rsOptJoin,\n    rsSymbol = '(?:' + [rsNonAstral + rsCombo + '?', rsCombo, rsRegional, rsSurrPair, rsAstral].join('|') + ')';\n\n/** Used to match [string symbols](https://mathiasbynens.be/notes/javascript-unicode). */\nvar reUnicode = RegExp(rsFitz + '(?=' + rsFitz + ')|' + rsSymbol + rsSeq, 'g');\n\n/**\n * Converts a Unicode `string` to an array.\n *\n * @private\n * @param {string} string The string to convert.\n * @returns {Array} Returns the converted array.\n */\nfunction unicodeToArray(string) {\n  return string.match(reUnicode) || [];\n}\n\nmodule.exports = unicodeToArray;\n", "var asciiToArray = require('./_asciiToArray'),\n    hasUnicode = require('./_hasUnicode'),\n    unicodeToArray = require('./_unicodeToArray');\n\n/**\n * Converts `string` to an array.\n *\n * @private\n * @param {string} string The string to convert.\n * @returns {Array} Returns the converted array.\n */\nfunction stringToArray(string) {\n  return hasUnicode(string)\n    ? unicodeToArray(string)\n    : asciiToArray(string);\n}\n\nmodule.exports = stringToArray;\n", "var castSlice = require('./_castSlice'),\n    hasUnicode = require('./_hasUnicode'),\n    stringToArray = require('./_stringToArray'),\n    toString = require('./toString');\n\n/**\n * Creates a function like `_.lowerFirst`.\n *\n * @private\n * @param {string} methodName The name of the `String` case method to use.\n * @returns {Function} Returns the new case function.\n */\nfunction createCaseFirst(methodName) {\n  return function(string) {\n    string = toString(string);\n\n    var strSymbols = hasUnicode(string)\n      ? stringToArray(string)\n      : undefined;\n\n    var chr = strSymbols\n      ? strSymbols[0]\n      : string.charAt(0);\n\n    var trailing = strSymbols\n      ? castSlice(strSymbols, 1).join('')\n      : string.slice(1);\n\n    return chr[methodName]() + trailing;\n  };\n}\n\nmodule.exports = createCaseFirst;\n", "var createCaseFirst = require('./_createCaseFirst');\n\n/**\n * Converts the first character of `string` to upper case.\n *\n * @static\n * @memberOf _\n * @since 4.0.0\n * @category String\n * @param {string} [string=''] The string to convert.\n * @returns {string} Returns the converted string.\n * @example\n *\n * _.upperFirst('fred');\n * // => 'Fred'\n *\n * _.upperFirst('FRED');\n * // => 'FRED'\n */\nvar upperFirst = createCaseFirst('toUpperCase');\n\nmodule.exports = upperFirst;\n", "var toString = require('./toString'),\n    upperFirst = require('./upperFirst');\n\n/**\n * Converts the first character of `string` to upper case and the remaining\n * to lower case.\n *\n * @static\n * @memberOf _\n * @since 3.0.0\n * @category String\n * @param {string} [string=''] The string to capitalize.\n * @returns {string} Returns the capitalized string.\n * @example\n *\n * _.capitalize('FRED');\n * // => 'Fred'\n */\nfunction capitalize(string) {\n  return upperFirst(toString(string).toLowerCase());\n}\n\nmodule.exports = capitalize;\n", "var capitalize = require('./capitalize'),\n    createCompounder = require('./_createCompounder');\n\n/**\n * Converts `string` to [camel case](https://en.wikipedia.org/wiki/CamelCase).\n *\n * @static\n * @memberOf _\n * @since 3.0.0\n * @category String\n * @param {string} [string=''] The string to convert.\n * @returns {string} Returns the camel cased string.\n * @example\n *\n * _.camelCase('Foo Bar');\n * // => 'fooBar'\n *\n * _.camelCase('--foo-bar--');\n * // => 'fooBar'\n *\n * _.camelCase('__FOO_BAR__');\n * // => 'fooBar'\n */\nvar camelCase = createCompounder(function(result, word, index) {\n  word = word.toLowerCase();\n  return result + (index ? capitalize(word) : word);\n});\n\nmodule.exports = camelCase;\n", "var baseAssignValue = require('./_baseAssignValue'),\n    baseForOwn = require('./_baseForOwn'),\n    baseIteratee = require('./_baseIteratee');\n\n/**\n * The opposite of `_.mapValues`; this method creates an object with the\n * same values as `object` and keys generated by running each own enumerable\n * string keyed property of `object` thru `iteratee`. The iteratee is invoked\n * with three arguments: (value, key, object).\n *\n * @static\n * @memberOf _\n * @since 3.8.0\n * @category Object\n * @param {Object} object The object to iterate over.\n * @param {Function} [iteratee=_.identity] The function invoked per iteration.\n * @returns {Object} Returns the new mapped object.\n * @see _.mapValues\n * @example\n *\n * _.mapKeys({ 'a': 1, 'b': 2 }, function(value, key) {\n *   return key + value;\n * });\n * // => { 'a1': 1, 'b2': 2 }\n */\nfunction mapKeys(object, iteratee) {\n  var result = {};\n  iteratee = baseIteratee(iteratee, 3);\n\n  baseForOwn(object, function(value, key, object) {\n    baseAssignValue(result, iteratee(value, key, object), value);\n  });\n  return result;\n}\n\nmodule.exports = mapKeys;\n", "\n/**\n * Topological sorting function\n *\n * @param {Array} edges\n * @returns {Array}\n */\n\nmodule.exports = function(edges) {\n  return toposort(uniqueNodes(edges), edges)\n}\n\nmodule.exports.array = toposort\n\nfunction toposort(nodes, edges) {\n  var cursor = nodes.length\n    , sorted = new Array(cursor)\n    , visited = {}\n    , i = cursor\n    // Better data structures make algorithm much faster.\n    , outgoingEdges = makeOutgoingEdges(edges)\n    , nodesHash = makeNodesHash(nodes)\n\n  // check for unknown nodes\n  edges.forEach(function(edge) {\n    if (!nodesHash.has(edge[0]) || !nodesHash.has(edge[1])) {\n      throw new Error('Unknown node. There is an unknown node in the supplied edges.')\n    }\n  })\n\n  while (i--) {\n    if (!visited[i]) visit(nodes[i], i, new Set())\n  }\n\n  return sorted\n\n  function visit(node, i, predecessors) {\n    if(predecessors.has(node)) {\n      var nodeRep\n      try {\n        nodeRep = \", node was:\" + JSON.stringify(node)\n      } catch(e) {\n        nodeRep = \"\"\n      }\n      throw new Error('Cyclic dependency' + nodeRep)\n    }\n\n    if (!nodesHash.has(node)) {\n      throw new Error('Found unknown node. Make sure to provided all involved nodes. Unknown node: '+JSON.stringify(node))\n    }\n\n    if (visited[i]) return;\n    visited[i] = true\n\n    var outgoing = outgoingEdges.get(node) || new Set()\n    outgoing = Array.from(outgoing)\n\n    if (i = outgoing.length) {\n      predecessors.add(node)\n      do {\n        var child = outgoing[--i]\n        visit(child, nodesHash.get(child), predecessors)\n      } while (i)\n      predecessors.delete(node)\n    }\n\n    sorted[--cursor] = node\n  }\n}\n\nfunction uniqueNodes(arr){\n  var res = new Set()\n  for (var i = 0, len = arr.length; i < len; i++) {\n    var edge = arr[i]\n    res.add(edge[0])\n    res.add(edge[1])\n  }\n  return Array.from(res)\n}\n\nfunction makeOutgoingEdges(arr){\n  var edges = new Map()\n  for (var i = 0, len = arr.length; i < len; i++) {\n    var edge = arr[i]\n    if (!edges.has(edge[0])) edges.set(edge[0], new Set())\n    if (!edges.has(edge[1])) edges.set(edge[1], new Set())\n    edges.get(edge[0]).add(edge[1])\n  }\n  return edges\n}\n\nfunction makeNodesHash(arr){\n  var res = new Map()\n  for (var i = 0, len = arr.length; i < len; i++) {\n    res.set(arr[i], i)\n  }\n  return res\n}\n", "\"use strict\";\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\nexports.default = sortFields;\n\nvar _has = _interopRequireDefault(require(\"lodash/has\"));\n\nvar _toposort = _interopRequireDefault(require(\"toposort\"));\n\nvar _propertyExpr = require(\"property-expr\");\n\nvar _Reference = _interopRequireDefault(require(\"../Reference\"));\n\nvar _isSchema = _interopRequireDefault(require(\"./isSchema\"));\n\nfunction _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { default: obj }; }\n\n// @ts-expect-error\nfunction sortFields(fields, excludedEdges = []) {\n  let edges = [];\n  let nodes = new Set();\n  let excludes = new Set(excludedEdges.map(([a, b]) => `${a}-${b}`));\n\n  function addNode(depPath, key) {\n    let node = (0, _propertyExpr.split)(depPath)[0];\n    nodes.add(node);\n    if (!excludes.has(`${key}-${node}`)) edges.push([key, node]);\n  }\n\n  for (const key in fields) if ((0, _has.default)(fields, key)) {\n    let value = fields[key];\n    nodes.add(key);\n    if (_Reference.default.isRef(value) && value.isSibling) addNode(value.path, key);else if ((0, _isSchema.default)(value) && 'deps' in value) value.deps.forEach(path => addNode(path, key));\n  }\n\n  return _toposort.default.array(Array.from(nodes), edges).reverse();\n}", "\"use strict\";\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\nexports.default = sortByKeyOrder;\n\nfunction findIndex(arr, err) {\n  let idx = Infinity;\n  arr.some((key, ii) => {\n    var _err$path;\n\n    if (((_err$path = err.path) == null ? void 0 : _err$path.indexOf(key)) !== -1) {\n      idx = ii;\n      return true;\n    }\n  });\n  return idx;\n}\n\nfunction sortByKeyOrder(keys) {\n  return (a, b) => {\n    return findIndex(keys, a) - findIndex(keys, b);\n  };\n}", "\"use strict\";\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\nexports.create = create;\nexports.default = void 0;\n\nvar _has = _interopRequireDefault(require(\"lodash/has\"));\n\nvar _snakeCase = _interopRequireDefault(require(\"lodash/snakeCase\"));\n\nvar _camelCase = _interopRequireDefault(require(\"lodash/camelCase\"));\n\nvar _mapKeys = _interopRequireDefault(require(\"lodash/mapKeys\"));\n\nvar _mapValues = _interopRequireDefault(require(\"lodash/mapValues\"));\n\nvar _propertyExpr = require(\"property-expr\");\n\nvar _locale = require(\"./locale\");\n\nvar _sortFields = _interopRequireDefault(require(\"./util/sortFields\"));\n\nvar _sortByKeyOrder = _interopRequireDefault(require(\"./util/sortByKeyOrder\"));\n\nvar _runTests = _interopRequireDefault(require(\"./util/runTests\"));\n\nvar _ValidationError = _interopRequireDefault(require(\"./ValidationError\"));\n\nvar _schema = _interopRequireDefault(require(\"./schema\"));\n\nfunction _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { default: obj }; }\n\nfunction _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nlet isObject = obj => Object.prototype.toString.call(obj) === '[object Object]';\n\nfunction unknown(ctx, value) {\n  let known = Object.keys(ctx.fields);\n  return Object.keys(value).filter(key => known.indexOf(key) === -1);\n}\n\nconst defaultSort = (0, _sortByKeyOrder.default)([]);\n\nclass ObjectSchema extends _schema.default {\n  constructor(spec) {\n    super({\n      type: 'object'\n    });\n    this.fields = Object.create(null);\n    this._sortErrors = defaultSort;\n    this._nodes = [];\n    this._excludedEdges = [];\n    this.withMutation(() => {\n      this.transform(function coerce(value) {\n        if (typeof value === 'string') {\n          try {\n            value = JSON.parse(value);\n          } catch (err) {\n            value = null;\n          }\n        }\n\n        if (this.isType(value)) return value;\n        return null;\n      });\n\n      if (spec) {\n        this.shape(spec);\n      }\n    });\n  }\n\n  _typeCheck(value) {\n    return isObject(value) || typeof value === 'function';\n  }\n\n  _cast(_value, options = {}) {\n    var _options$stripUnknown;\n\n    let value = super._cast(_value, options); //should ignore nulls here\n\n\n    if (value === undefined) return this.getDefault();\n    if (!this._typeCheck(value)) return value;\n    let fields = this.fields;\n    let strip = (_options$stripUnknown = options.stripUnknown) != null ? _options$stripUnknown : this.spec.noUnknown;\n\n    let props = this._nodes.concat(Object.keys(value).filter(v => this._nodes.indexOf(v) === -1));\n\n    let intermediateValue = {}; // is filled during the transform below\n\n    let innerOptions = _extends({}, options, {\n      parent: intermediateValue,\n      __validating: options.__validating || false\n    });\n\n    let isChanged = false;\n\n    for (const prop of props) {\n      let field = fields[prop];\n      let exists = (0, _has.default)(value, prop);\n\n      if (field) {\n        let fieldValue;\n        let inputValue = value[prop]; // safe to mutate since this is fired in sequence\n\n        innerOptions.path = (options.path ? `${options.path}.` : '') + prop; // innerOptions.value = value[prop];\n\n        field = field.resolve({\n          value: inputValue,\n          context: options.context,\n          parent: intermediateValue\n        });\n        let fieldSpec = 'spec' in field ? field.spec : undefined;\n        let strict = fieldSpec == null ? void 0 : fieldSpec.strict;\n\n        if (fieldSpec == null ? void 0 : fieldSpec.strip) {\n          isChanged = isChanged || prop in value;\n          continue;\n        }\n\n        fieldValue = !options.__validating || !strict ? // TODO: use _cast, this is double resolving\n        field.cast(value[prop], innerOptions) : value[prop];\n\n        if (fieldValue !== undefined) {\n          intermediateValue[prop] = fieldValue;\n        }\n      } else if (exists && !strip) {\n        intermediateValue[prop] = value[prop];\n      }\n\n      if (intermediateValue[prop] !== value[prop]) {\n        isChanged = true;\n      }\n    }\n\n    return isChanged ? intermediateValue : value;\n  }\n\n  _validate(_value, opts = {}, callback) {\n    let errors = [];\n    let {\n      sync,\n      from = [],\n      originalValue = _value,\n      abortEarly = this.spec.abortEarly,\n      recursive = this.spec.recursive\n    } = opts;\n    from = [{\n      schema: this,\n      value: originalValue\n    }, ...from]; // this flag is needed for handling `strict` correctly in the context of\n    // validation vs just casting. e.g strict() on a field is only used when validating\n\n    opts.__validating = true;\n    opts.originalValue = originalValue;\n    opts.from = from;\n\n    super._validate(_value, opts, (err, value) => {\n      if (err) {\n        if (!_ValidationError.default.isError(err) || abortEarly) {\n          return void callback(err, value);\n        }\n\n        errors.push(err);\n      }\n\n      if (!recursive || !isObject(value)) {\n        callback(errors[0] || null, value);\n        return;\n      }\n\n      originalValue = originalValue || value;\n\n      let tests = this._nodes.map(key => (_, cb) => {\n        let path = key.indexOf('.') === -1 ? (opts.path ? `${opts.path}.` : '') + key : `${opts.path || ''}[\"${key}\"]`;\n        let field = this.fields[key];\n\n        if (field && 'validate' in field) {\n          field.validate(value[key], _extends({}, opts, {\n            // @ts-ignore\n            path,\n            from,\n            // inner fields are always strict:\n            // 1. this isn't strict so the casting will also have cast inner values\n            // 2. this is strict in which case the nested values weren't cast either\n            strict: true,\n            parent: value,\n            originalValue: originalValue[key]\n          }), cb);\n          return;\n        }\n\n        cb(null);\n      });\n\n      (0, _runTests.default)({\n        sync,\n        tests,\n        value,\n        errors,\n        endEarly: abortEarly,\n        sort: this._sortErrors,\n        path: opts.path\n      }, callback);\n    });\n  }\n\n  clone(spec) {\n    const next = super.clone(spec);\n    next.fields = _extends({}, this.fields);\n    next._nodes = this._nodes;\n    next._excludedEdges = this._excludedEdges;\n    next._sortErrors = this._sortErrors;\n    return next;\n  }\n\n  concat(schema) {\n    let next = super.concat(schema);\n    let nextFields = next.fields;\n\n    for (let [field, schemaOrRef] of Object.entries(this.fields)) {\n      const target = nextFields[field];\n\n      if (target === undefined) {\n        nextFields[field] = schemaOrRef;\n      } else if (target instanceof _schema.default && schemaOrRef instanceof _schema.default) {\n        nextFields[field] = schemaOrRef.concat(target);\n      }\n    }\n\n    return next.withMutation(() => next.shape(nextFields, this._excludedEdges));\n  }\n\n  getDefaultFromShape() {\n    let dft = {};\n\n    this._nodes.forEach(key => {\n      const field = this.fields[key];\n      dft[key] = 'default' in field ? field.getDefault() : undefined;\n    });\n\n    return dft;\n  }\n\n  _getDefault() {\n    if ('default' in this.spec) {\n      return super._getDefault();\n    } // if there is no default set invent one\n\n\n    if (!this._nodes.length) {\n      return undefined;\n    }\n\n    return this.getDefaultFromShape();\n  }\n\n  shape(additions, excludes = []) {\n    let next = this.clone();\n    let fields = Object.assign(next.fields, additions);\n    next.fields = fields;\n    next._sortErrors = (0, _sortByKeyOrder.default)(Object.keys(fields));\n\n    if (excludes.length) {\n      // this is a convenience for when users only supply a single pair\n      if (!Array.isArray(excludes[0])) excludes = [excludes];\n      next._excludedEdges = [...next._excludedEdges, ...excludes];\n    }\n\n    next._nodes = (0, _sortFields.default)(fields, next._excludedEdges);\n    return next;\n  }\n\n  pick(keys) {\n    const picked = {};\n\n    for (const key of keys) {\n      if (this.fields[key]) picked[key] = this.fields[key];\n    }\n\n    return this.clone().withMutation(next => {\n      next.fields = {};\n      return next.shape(picked);\n    });\n  }\n\n  omit(keys) {\n    const next = this.clone();\n    const fields = next.fields;\n    next.fields = {};\n\n    for (const key of keys) {\n      delete fields[key];\n    }\n\n    return next.withMutation(() => next.shape(fields));\n  }\n\n  from(from, to, alias) {\n    let fromGetter = (0, _propertyExpr.getter)(from, true);\n    return this.transform(obj => {\n      if (obj == null) return obj;\n      let newObj = obj;\n\n      if ((0, _has.default)(obj, from)) {\n        newObj = _extends({}, obj);\n        if (!alias) delete newObj[from];\n        newObj[to] = fromGetter(obj);\n      }\n\n      return newObj;\n    });\n  }\n\n  noUnknown(noAllow = true, message = _locale.object.noUnknown) {\n    if (typeof noAllow === 'string') {\n      message = noAllow;\n      noAllow = true;\n    }\n\n    let next = this.test({\n      name: 'noUnknown',\n      exclusive: true,\n      message: message,\n\n      test(value) {\n        if (value == null) return true;\n        const unknownKeys = unknown(this.schema, value);\n        return !noAllow || unknownKeys.length === 0 || this.createError({\n          params: {\n            unknown: unknownKeys.join(', ')\n          }\n        });\n      }\n\n    });\n    next.spec.noUnknown = noAllow;\n    return next;\n  }\n\n  unknown(allow = true, message = _locale.object.noUnknown) {\n    return this.noUnknown(!allow, message);\n  }\n\n  transformKeys(fn) {\n    return this.transform(obj => obj && (0, _mapKeys.default)(obj, (_, key) => fn(key)));\n  }\n\n  camelCase() {\n    return this.transformKeys(_camelCase.default);\n  }\n\n  snakeCase() {\n    return this.transformKeys(_snakeCase.default);\n  }\n\n  constantCase() {\n    return this.transformKeys(key => (0, _snakeCase.default)(key).toUpperCase());\n  }\n\n  describe() {\n    let base = super.describe();\n    base.fields = (0, _mapValues.default)(this.fields, value => value.describe());\n    return base;\n  }\n\n}\n\nexports.default = ObjectSchema;\n\nfunction create(spec) {\n  return new ObjectSchema(spec);\n}\n\ncreate.prototype = ObjectSchema.prototype;", "\"use strict\";\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\nexports.create = create;\nexports.default = void 0;\n\nvar _isAbsent = _interopRequireDefault(require(\"./util/isAbsent\"));\n\nvar _isSchema = _interopRequireDefault(require(\"./util/isSchema\"));\n\nvar _printValue = _interopRequireDefault(require(\"./util/printValue\"));\n\nvar _locale = require(\"./locale\");\n\nvar _runTests = _interopRequireDefault(require(\"./util/runTests\"));\n\nvar _ValidationError = _interopRequireDefault(require(\"./ValidationError\"));\n\nvar _schema = _interopRequireDefault(require(\"./schema\"));\n\nfunction _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { default: obj }; }\n\nfunction _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction create(type) {\n  return new ArraySchema(type);\n}\n\nclass ArraySchema extends _schema.default {\n  constructor(type) {\n    super({\n      type: 'array'\n    }); // `undefined` specifically means uninitialized, as opposed to\n    // \"no subtype\"\n\n    this.innerType = void 0;\n    this.innerType = type;\n    this.withMutation(() => {\n      this.transform(function (values) {\n        if (typeof values === 'string') try {\n          values = JSON.parse(values);\n        } catch (err) {\n          values = null;\n        }\n        return this.isType(values) ? values : null;\n      });\n    });\n  }\n\n  _typeCheck(v) {\n    return Array.isArray(v);\n  }\n\n  get _subType() {\n    return this.innerType;\n  }\n\n  _cast(_value, _opts) {\n    const value = super._cast(_value, _opts); //should ignore nulls here\n\n\n    if (!this._typeCheck(value) || !this.innerType) return value;\n    let isChanged = false;\n    const castArray = value.map((v, idx) => {\n      const castElement = this.innerType.cast(v, _extends({}, _opts, {\n        path: `${_opts.path || ''}[${idx}]`\n      }));\n\n      if (castElement !== v) {\n        isChanged = true;\n      }\n\n      return castElement;\n    });\n    return isChanged ? castArray : value;\n  }\n\n  _validate(_value, options = {}, callback) {\n    var _options$abortEarly, _options$recursive;\n\n    let errors = [];\n    let sync = options.sync;\n    let path = options.path;\n    let innerType = this.innerType;\n    let endEarly = (_options$abortEarly = options.abortEarly) != null ? _options$abortEarly : this.spec.abortEarly;\n    let recursive = (_options$recursive = options.recursive) != null ? _options$recursive : this.spec.recursive;\n    let originalValue = options.originalValue != null ? options.originalValue : _value;\n\n    super._validate(_value, options, (err, value) => {\n      if (err) {\n        if (!_ValidationError.default.isError(err) || endEarly) {\n          return void callback(err, value);\n        }\n\n        errors.push(err);\n      }\n\n      if (!recursive || !innerType || !this._typeCheck(value)) {\n        callback(errors[0] || null, value);\n        return;\n      }\n\n      originalValue = originalValue || value; // #950 Ensure that sparse array empty slots are validated\n\n      let tests = new Array(value.length);\n\n      for (let idx = 0; idx < value.length; idx++) {\n        let item = value[idx];\n        let path = `${options.path || ''}[${idx}]`; // object._validate note for isStrict explanation\n\n        let innerOptions = _extends({}, options, {\n          path,\n          strict: true,\n          parent: value,\n          index: idx,\n          originalValue: originalValue[idx]\n        });\n\n        tests[idx] = (_, cb) => innerType.validate(item, innerOptions, cb);\n      }\n\n      (0, _runTests.default)({\n        sync,\n        path,\n        value,\n        errors,\n        endEarly,\n        tests\n      }, callback);\n    });\n  }\n\n  clone(spec) {\n    const next = super.clone(spec);\n    next.innerType = this.innerType;\n    return next;\n  }\n\n  concat(schema) {\n    let next = super.concat(schema);\n    next.innerType = this.innerType;\n    if (schema.innerType) next.innerType = next.innerType ? // @ts-expect-error Lazy doesn't have concat()\n    next.innerType.concat(schema.innerType) : schema.innerType;\n    return next;\n  }\n\n  of(schema) {\n    // FIXME: this should return a new instance of array without the default to be\n    let next = this.clone();\n    if (!(0, _isSchema.default)(schema)) throw new TypeError('`array.of()` sub-schema must be a valid yup schema not: ' + (0, _printValue.default)(schema)); // FIXME(ts):\n\n    next.innerType = schema;\n    return next;\n  }\n\n  length(length, message = _locale.array.length) {\n    return this.test({\n      message,\n      name: 'length',\n      exclusive: true,\n      params: {\n        length\n      },\n\n      test(value) {\n        return (0, _isAbsent.default)(value) || value.length === this.resolve(length);\n      }\n\n    });\n  }\n\n  min(min, message) {\n    message = message || _locale.array.min;\n    return this.test({\n      message,\n      name: 'min',\n      exclusive: true,\n      params: {\n        min\n      },\n\n      // FIXME(ts): Array<typeof T>\n      test(value) {\n        return (0, _isAbsent.default)(value) || value.length >= this.resolve(min);\n      }\n\n    });\n  }\n\n  max(max, message) {\n    message = message || _locale.array.max;\n    return this.test({\n      message,\n      name: 'max',\n      exclusive: true,\n      params: {\n        max\n      },\n\n      test(value) {\n        return (0, _isAbsent.default)(value) || value.length <= this.resolve(max);\n      }\n\n    });\n  }\n\n  ensure() {\n    return this.default(() => []).transform((val, original) => {\n      // We don't want to return `null` for nullable schema\n      if (this._typeCheck(val)) return val;\n      return original == null ? [] : [].concat(original);\n    });\n  }\n\n  compact(rejector) {\n    let reject = !rejector ? v => !!v : (v, i, a) => !rejector(v, i, a);\n    return this.transform(values => values != null ? values.filter(reject) : values);\n  }\n\n  describe() {\n    let base = super.describe();\n    if (this.innerType) base.innerType = this.innerType.describe();\n    return base;\n  }\n\n  nullable(isNullable = true) {\n    return super.nullable(isNullable);\n  }\n\n  defined() {\n    return super.defined();\n  }\n\n  required(msg) {\n    return super.required(msg);\n  }\n\n}\n\nexports.default = ArraySchema;\ncreate.prototype = ArraySchema.prototype; //\n// Interfaces\n//", "\"use strict\";\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\nexports.create = create;\nexports.default = void 0;\n\nvar _isSchema = _interopRequireDefault(require(\"./util/isSchema\"));\n\nfunction _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { default: obj }; }\n\nfunction create(builder) {\n  return new Lazy(builder);\n}\n\nclass Lazy {\n  constructor(builder) {\n    this.type = 'lazy';\n    this.__isYupSchema__ = true;\n    this.__inputType = void 0;\n    this.__outputType = void 0;\n\n    this._resolve = (value, options = {}) => {\n      let schema = this.builder(value, options);\n      if (!(0, _isSchema.default)(schema)) throw new TypeError('lazy() functions must return a valid schema');\n      return schema.resolve(options);\n    };\n\n    this.builder = builder;\n  }\n\n  resolve(options) {\n    return this._resolve(options.value, options);\n  }\n\n  cast(value, options) {\n    return this._resolve(value, options).cast(value, options);\n  }\n\n  validate(value, options, maybeCb) {\n    // @ts-expect-error missing public callback on type\n    return this._resolve(value, options).validate(value, options, maybeCb);\n  }\n\n  validateSync(value, options) {\n    return this._resolve(value, options).validateSync(value, options);\n  }\n\n  validateAt(path, value, options) {\n    return this._resolve(value, options).validateAt(path, value, options);\n  }\n\n  validateSyncAt(path, value, options) {\n    return this._resolve(value, options).validateSyncAt(path, value, options);\n  }\n\n  describe() {\n    return null;\n  }\n\n  isValid(value, options) {\n    return this._resolve(value, options).isValid(value, options);\n  }\n\n  isValidSync(value, options) {\n    return this._resolve(value, options).isValidSync(value, options);\n  }\n\n}\n\nvar _default = Lazy;\nexports.default = _default;", "\"use strict\";\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\nexports.default = setLocale;\n\nvar _locale = _interopRequireDefault(require(\"./locale\"));\n\nfunction _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { default: obj }; }\n\nfunction setLocale(custom) {\n  Object.keys(custom).forEach(type => {\n    // @ts-ignore\n    Object.keys(custom[type]).forEach(method => {\n      // @ts-ignore\n      _locale.default[type][method] = custom[type][method];\n    });\n  });\n}", "\"use strict\";\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\nObject.defineProperty(exports, \"ArraySchema\", {\n  enumerable: true,\n  get: function () {\n    return _array.default;\n  }\n});\nObject.defineProperty(exports, \"BaseSchema\", {\n  enumerable: true,\n  get: function () {\n    return _schema.default;\n  }\n});\nObject.defineProperty(exports, \"BooleanSchema\", {\n  enumerable: true,\n  get: function () {\n    return _boolean.default;\n  }\n});\nObject.defineProperty(exports, \"DateSchema\", {\n  enumerable: true,\n  get: function () {\n    return _date.default;\n  }\n});\nObject.defineProperty(exports, \"MixedSchema\", {\n  enumerable: true,\n  get: function () {\n    return _mixed.default;\n  }\n});\nObject.defineProperty(exports, \"NumberSchema\", {\n  enumerable: true,\n  get: function () {\n    return _number.default;\n  }\n});\nObject.defineProperty(exports, \"ObjectSchema\", {\n  enumerable: true,\n  get: function () {\n    return _object.default;\n  }\n});\nObject.defineProperty(exports, \"StringSchema\", {\n  enumerable: true,\n  get: function () {\n    return _string.default;\n  }\n});\nObject.defineProperty(exports, \"ValidationError\", {\n  enumerable: true,\n  get: function () {\n    return _ValidationError.default;\n  }\n});\nexports.addMethod = addMethod;\nObject.defineProperty(exports, \"array\", {\n  enumerable: true,\n  get: function () {\n    return _array.create;\n  }\n});\nObject.defineProperty(exports, \"bool\", {\n  enumerable: true,\n  get: function () {\n    return _boolean.create;\n  }\n});\nObject.defineProperty(exports, \"boolean\", {\n  enumerable: true,\n  get: function () {\n    return _boolean.create;\n  }\n});\nObject.defineProperty(exports, \"date\", {\n  enumerable: true,\n  get: function () {\n    return _date.create;\n  }\n});\nObject.defineProperty(exports, \"isSchema\", {\n  enumerable: true,\n  get: function () {\n    return _isSchema.default;\n  }\n});\nObject.defineProperty(exports, \"lazy\", {\n  enumerable: true,\n  get: function () {\n    return _Lazy.create;\n  }\n});\nObject.defineProperty(exports, \"mixed\", {\n  enumerable: true,\n  get: function () {\n    return _mixed.create;\n  }\n});\nObject.defineProperty(exports, \"number\", {\n  enumerable: true,\n  get: function () {\n    return _number.create;\n  }\n});\nObject.defineProperty(exports, \"object\", {\n  enumerable: true,\n  get: function () {\n    return _object.create;\n  }\n});\nObject.defineProperty(exports, \"reach\", {\n  enumerable: true,\n  get: function () {\n    return _reach.default;\n  }\n});\nObject.defineProperty(exports, \"ref\", {\n  enumerable: true,\n  get: function () {\n    return _Reference.create;\n  }\n});\nObject.defineProperty(exports, \"setLocale\", {\n  enumerable: true,\n  get: function () {\n    return _setLocale.default;\n  }\n});\nObject.defineProperty(exports, \"string\", {\n  enumerable: true,\n  get: function () {\n    return _string.create;\n  }\n});\n\nvar _mixed = _interopRequireWildcard(require(\"./mixed\"));\n\nvar _boolean = _interopRequireWildcard(require(\"./boolean\"));\n\nvar _string = _interopRequireWildcard(require(\"./string\"));\n\nvar _number = _interopRequireWildcard(require(\"./number\"));\n\nvar _date = _interopRequireWildcard(require(\"./date\"));\n\nvar _object = _interopRequireWildcard(require(\"./object\"));\n\nvar _array = _interopRequireWildcard(require(\"./array\"));\n\nvar _Reference = require(\"./Reference\");\n\nvar _Lazy = require(\"./Lazy\");\n\nvar _ValidationError = _interopRequireDefault(require(\"./ValidationError\"));\n\nvar _reach = _interopRequireDefault(require(\"./util/reach\"));\n\nvar _isSchema = _interopRequireDefault(require(\"./util/isSchema\"));\n\nvar _setLocale = _interopRequireDefault(require(\"./setLocale\"));\n\nvar _schema = _interopRequireDefault(require(\"./schema\"));\n\nfunction _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { default: obj }; }\n\nfunction _getRequireWildcardCache(nodeInterop) { if (typeof WeakMap !== \"function\") return null; var cacheBabelInterop = new WeakMap(); var cacheNodeInterop = new WeakMap(); return (_getRequireWildcardCache = function (nodeInterop) { return nodeInterop ? cacheNodeInterop : cacheBabelInterop; })(nodeInterop); }\n\nfunction _interopRequireWildcard(obj, nodeInterop) { if (!nodeInterop && obj && obj.__esModule) { return obj; } if (obj === null || typeof obj !== \"object\" && typeof obj !== \"function\") { return { default: obj }; } var cache = _getRequireWildcardCache(nodeInterop); if (cache && cache.has(obj)) { return cache.get(obj); } var newObj = {}; var hasPropertyDescriptor = Object.defineProperty && Object.getOwnPropertyDescriptor; for (var key in obj) { if (key !== \"default\" && Object.prototype.hasOwnProperty.call(obj, key)) { var desc = hasPropertyDescriptor ? Object.getOwnPropertyDescriptor(obj, key) : null; if (desc && (desc.get || desc.set)) { Object.defineProperty(newObj, key, desc); } else { newObj[key] = obj[key]; } } } newObj.default = obj; if (cache) { cache.set(obj, newObj); } return newObj; }\n\nfunction addMethod(schemaType, name, fn) {\n  if (!schemaType || !(0, _isSchema.default)(schemaType.prototype)) throw new TypeError('You must provide a yup schema constructor function');\n  if (typeof name !== 'string') throw new TypeError('A Method name must be provided');\n  if (typeof fn !== 'function') throw new TypeError('Method function must be provided');\n  schemaType.prototype[name] = fn;\n}", "import handler from \"../handler-lib\";\nimport KSUID from \"ksuid\";\n// utils\nimport { error } from \"../../utils/constants/constants\";\nimport { hasPermissions } from \"../../utils/auth/authorization\";\nimport { parseSpecificReportParameters } from \"../../utils/auth/parameters\";\nimport { calculateCompletionStatus } from \"../../utils/validation/completionStatus\";\nimport {\n  getReportFieldData,\n  getReportFormTemplate,\n  getReportMetadata,\n  putReportFieldData,\n  putReportMetadata,\n} from \"../../storage/reports\";\n// types\nimport {\n  ReportMetadataShape,\n  ReportStatus,\n  UserRoles,\n} from \"../../utils/types\";\nimport {\n  badRequest,\n  conflict,\n  forbidden,\n  internalServerError,\n  notFound,\n  ok,\n} from \"../../utils/responses/response-lib\";\n\n/**\n * Locked reports can be released by admins.\n *\n * When reports are released:\n *\n * 1) Report metadata is set to `locked=false`\n * 2) previousVersions has current metadata fieldDataId appended\n * 3) Report table row is updated with new fieldDataId\n * 4) User can now edit report.\n *\n */\nexport const releaseReport = handler(async (event) => {\n  const { allParamsValid, reportType, state, id } =\n    parseSpecificReportParameters(event);\n  if (!allParamsValid) {\n    return badRequest(error.NO_KEY);\n  }\n\n  // Return a 403 status if the user is not an admin.\n  if (!hasPermissions(event, [UserRoles.ADMIN, UserRoles.APPROVER])) {\n    return forbidden(error.UNAUTHORIZED);\n  }\n\n  const metadata = await getReportMetadata(reportType, state, id);\n  if (!metadata) {\n    return notFound(error.NO_MATCHING_RECORD);\n  }\n\n  const isLocked = metadata.locked;\n\n  // Report is not locked.\n  if (!isLocked) {\n    return ok(metadata);\n  }\n\n  const isArchived = metadata.archived;\n\n  if (isArchived) {\n    return conflict(error.ALREADY_ARCHIVED);\n  }\n\n  const newFieldDataId = KSUID.randomSync().string;\n\n  const previousRevisions = Array.isArray(metadata.previousRevisions)\n    ? metadata.previousRevisions.concat([metadata.fieldDataId])\n    : [metadata.fieldDataId];\n\n  const fieldData = await getReportFieldData(metadata);\n  if (!fieldData) {\n    return notFound(error.NO_MATCHING_RECORD);\n  }\n\n  const formTemplate = await getReportFormTemplate(metadata);\n  if (!formTemplate) {\n    return notFound(error.NO_MATCHING_RECORD);\n  }\n\n  const updatedFieldData = {\n    ...fieldData,\n    generalInformation_resubmissionInformation: \"\",\n  };\n\n  const newReportMetadata: ReportMetadataShape = {\n    ...metadata,\n    fieldDataId: newFieldDataId,\n    locked: false,\n    previousRevisions,\n    status: ReportStatus.IN_REVISION,\n    completionStatus: await calculateCompletionStatus(\n      updatedFieldData,\n      formTemplate\n    ),\n  };\n\n  try {\n    await putReportMetadata(newReportMetadata);\n  } catch {\n    return internalServerError(error.DYNAMO_UPDATE_ERROR);\n  }\n  // Copy the original field data to a new location.\n  try {\n    await putReportFieldData(newReportMetadata, updatedFieldData);\n  } catch {\n    return internalServerError(error.S3_OBJECT_CREATION_ERROR);\n  }\n\n  return ok(newReportMetadata);\n});\n", "import util from \"util\";\nimport { Logger } from \"@smithy/types\";\n\ntype LogLevel = \"trace\" | \"debug\" | \"info\" | \"warn\" | \"error\";\ntype LogEntry = {\n  date: Date;\n  level: LogLevel;\n  string: string;\n};\n\nconst logs: LogEntry[] = [];\n\nconst buildLoggerForLevel = (level: LogLevel) => {\n  return function (...content: any[]) {\n    logs.push({\n      date: new Date(),\n      level: level,\n      string: util.format.apply(null, content),\n    });\n\n    /*\n     * If we have a function logging thousands of messages,\n     * better to take the console performance hit mid-operation\n     * than to let memory usage run away as well.\n     */\n    if (logs.length > 99) {\n      flush();\n    }\n  };\n};\n\n/*\n * Individual functions are exported to support handler-lib;\n * This integrates SDK client logging with lambda logging.\n */\nexport const trace = buildLoggerForLevel(\"trace\");\nexport const debug = buildLoggerForLevel(\"debug\");\nexport const info = buildLoggerForLevel(\"info\");\nexport const warn = buildLoggerForLevel(\"warn\");\nexport const error = buildLoggerForLevel(\"error\");\n\nexport function flush() {\n  while (logs.length > 0) {\n    const { date, level, string } = logs.shift()!;\n    // eslint-disable-next-line no-console\n    console[level](date, string);\n  }\n}\n\n/*\n * This is only called at the beginning of a lambda handler,\n * so the log buffer should be empty anyway. But it doesn't\n * hurt to make sure!\n */\nexport const init = flush;\n\n/**\n * A logger suitable for passing to any AWS client constructor.\n * Note that the `trace` log level is excluded.\n *\n * This logger accumulates log messages in an internal buffer,\n * eventually flushing them to the console.\n */\nexport const logger: Logger = { debug, info, warn, error };\n", "import { SSMClient, GetParameterCommand } from \"@aws-sdk/client-ssm\";\nimport jwt_decode from \"jwt-decode\";\nimport { CognitoJwtVerifier } from \"aws-jwt-verify\";\n// types\nimport { APIGatewayProxyEvent, UserRoles } from \"../types\";\nimport { logger } from \"../debugging/debug-lib\";\n\ninterface DecodedToken {\n  \"custom:cms_roles\": UserRoles;\n  \"custom:cms_state\": string | undefined;\n}\n\nconst loadCognitoValues = async () => {\n  if (\n    process.env.COGNITO_USER_POOL_ID &&\n    process.env.COGNITO_USER_POOL_CLIENT_ID\n  ) {\n    return {\n      userPoolId: process.env.COGNITO_USER_POOL_ID,\n      userPoolClientId: process.env.COGNITO_USER_POOL_CLIENT_ID,\n    };\n  } else {\n    const ssmClient = new SSMClient({ logger });\n    const stage = process.env.stage!;\n    const getParam = async (identifier: string) => {\n      const command = new GetParameterCommand({\n        Name: `/${stage}/ui-auth/${identifier}`,\n      });\n      const result = await ssmClient.send(command);\n      return result.Parameter?.Value;\n    };\n    const userPoolId = await getParam(\"cognito_user_pool_id\");\n    const userPoolClientId = await getParam(\"cognito_user_pool_client_id\");\n    if (userPoolId && userPoolClientId) {\n      process.env[\"COGNITO_USER_POOL_ID\"] = userPoolId;\n      process.env[\"COGNITO_USER_POOL_CLIENT_ID\"] = userPoolClientId;\n      return { userPoolId, userPoolClientId };\n    } else {\n      throw new Error(\"cannot load cognito values\");\n    }\n  }\n};\n\nexport const isAuthenticated = async (event: APIGatewayProxyEvent) => {\n  const isLocalStack = event.requestContext.accountId === \"000000000000\";\n  if (isLocalStack) {\n    return true;\n  }\n\n  const cognitoValues = await loadCognitoValues();\n\n  // Verifier that expects valid access tokens:\n  const verifier = CognitoJwtVerifier.create({\n    userPoolId: cognitoValues.userPoolId,\n    tokenUse: \"id\",\n    clientId: cognitoValues.userPoolClientId,\n  });\n\n  try {\n    await verifier.verify(event?.headers?.[\"x-api-key\"]!);\n    return true;\n  } catch {\n    return false;\n  }\n};\n\nexport const hasPermissions = (\n  event: APIGatewayProxyEvent,\n  allowedRoles: UserRoles[],\n  state?: string\n) => {\n  let isAllowed = false;\n  // decode the idToken\n  if (event?.headers?.[\"x-api-key\"]) {\n    const decoded = jwt_decode(event.headers[\"x-api-key\"]) as DecodedToken;\n    const idmUserRoles = decoded[\"custom:cms_roles\"];\n    const idmUserState = decoded[\"custom:cms_state\"];\n    const mfpUserRole = idmUserRoles\n      ?.split(\",\")\n      .find((role) => role.includes(\"mdctmfp\")) as UserRoles;\n\n    isAllowed =\n      allowedRoles.includes(mfpUserRole) &&\n      (!state || idmUserState?.includes(state))!;\n  }\n\n  return isAllowed;\n};\n\nexport const isAuthorizedToFetchState = (\n  event: APIGatewayProxyEvent,\n  state: string\n) => {\n  // If this is a state user for the matching state, authorize them.\n  if (hasPermissions(event, [UserRoles.STATE_USER], state)) {\n    return true;\n  }\n\n  const nonStateUserRoles = Object.values(UserRoles).filter(\n    (role) => role !== UserRoles.STATE_USER\n  );\n\n  // If they are any other user type, they don't need to belong to this state.\n  return hasPermissions(event, nonStateUserRoles);\n};\n", "// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n// SPDX-License-Identifier: Apache-2.0\n/**\n * Base Error for all other errors in this file\n */\nexport class JwtBaseError extends Error {\n}\n/**\n * An error that is raised because an actual value does not match with the expected value\n */\nexport class FailedAssertionError extends JwtBaseError {\n    constructor(msg, actual, expected) {\n        super(msg);\n        this.failedAssertion = {\n            actual,\n            expected,\n        };\n    }\n}\n/**\n * JWT errors\n */\nexport class JwtParseError extends JwtBaseError {\n    constructor(msg, error) {\n        const message = error != null ? `${msg}: ${error}` : msg;\n        super(message);\n    }\n}\nexport class ParameterValidationError extends JwtBaseError {\n}\nexport class JwtInvalidSignatureError extends JwtBaseError {\n}\nexport class JwtInvalidSignatureAlgorithmError extends FailedAssertionError {\n}\nexport class JwtInvalidClaimError extends FailedAssertionError {\n    withRawJwt({ header, payload }) {\n        this.rawJwt = {\n            header,\n            payload,\n        };\n        return this;\n    }\n}\nexport class JwtInvalidIssuerError extends JwtInvalidClaimError {\n}\nexport class JwtInvalidAudienceError extends JwtInvalidClaimError {\n}\nexport class JwtInvalidScopeError extends JwtInvalidClaimError {\n}\nexport class JwtExpiredError extends JwtInvalidClaimError {\n}\nexport class JwtNotBeforeError extends JwtInvalidClaimError {\n}\n/**\n * Amazon Cognito specific erros\n */\nexport class CognitoJwtInvalidGroupError extends JwtInvalidClaimError {\n}\nexport class CognitoJwtInvalidTokenUseError extends JwtInvalidClaimError {\n}\nexport class CognitoJwtInvalidClientIdError extends JwtInvalidClaimError {\n}\n/**\n * ASN.1 errors\n */\nexport class Asn1DecodingError extends JwtBaseError {\n}\n/**\n * JWK errors\n */\nexport class JwksValidationError extends JwtBaseError {\n}\nexport class JwkValidationError extends JwtBaseError {\n}\nexport class JwtWithoutValidKidError extends JwtBaseError {\n}\nexport class KidNotFoundInJwksError extends JwtBaseError {\n}\nexport class WaitPeriodNotYetEndedJwkError extends JwtBaseError {\n}\nexport class JwksNotAvailableInCacheError extends JwtBaseError {\n}\nexport class JwkInvalidUseError extends FailedAssertionError {\n}\nexport class JwkInvalidKtyError extends FailedAssertionError {\n}\n/**\n * HTTPS fetch errors\n */\nexport class FetchError extends JwtBaseError {\n    constructor(uri, msg) {\n        super(`Failed to fetch ${uri}: ${msg}`);\n    }\n}\nexport class NonRetryableFetchError extends FetchError {\n}\n/**\n * Web compatibility errors\n */\nexport class NotSupportedError extends JwtBaseError {\n}\n", "// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n// SPDX-License-Identifier: Apache-2.0\n//\n// Node.js implementations for the node-web-compatibility layer\nimport { createPublicKey, createVerify } from \"crypto\";\nimport { constructPublicKeyInDerFormat } from \"./asn1.js\";\nimport { fetchJson } from \"./https-node.js\";\n/**\n * Enum to map supported JWT signature algorithms with OpenSSL message digest algorithm names\n */\nvar JwtSignatureAlgorithms;\n(function (JwtSignatureAlgorithms) {\n    JwtSignatureAlgorithms[\"RS256\"] = \"RSA-SHA256\";\n    JwtSignatureAlgorithms[\"RS384\"] = \"RSA-SHA384\";\n    JwtSignatureAlgorithms[\"RS512\"] = \"RSA-SHA512\";\n})(JwtSignatureAlgorithms || (JwtSignatureAlgorithms = {}));\nexport const nodeWebCompat = {\n    fetchJson,\n    transformJwkToKeyObjectSync: (jwk) => createPublicKey({\n        key: constructPublicKeyInDerFormat(Buffer.from(jwk.n, \"base64\"), Buffer.from(jwk.e, \"base64\")),\n        format: \"der\",\n        type: \"spki\",\n    }),\n    transformJwkToKeyObjectAsync: async (jwk) => createPublicKey({\n        key: constructPublicKeyInDerFormat(Buffer.from(jwk.n, \"base64\"), Buffer.from(jwk.e, \"base64\")),\n        format: \"der\",\n        type: \"spki\",\n    }),\n    parseB64UrlString: (b64) => Buffer.from(b64, \"base64\").toString(\"utf8\"),\n    verifySignatureSync: ({ alg, keyObject, jwsSigningInput, signature }) => \n    // eslint-disable-next-line security/detect-object-injection\n    createVerify(JwtSignatureAlgorithms[alg])\n        .update(jwsSigningInput)\n        .verify(keyObject, signature, \"base64\"),\n    verifySignatureAsync: async ({ alg, keyObject, jwsSigningInput, signature, }) => \n    // eslint-disable-next-line security/detect-object-injection\n    createVerify(JwtSignatureAlgorithms[alg])\n        .update(jwsSigningInput)\n        .verify(keyObject, signature, \"base64\"),\n    defaultFetchTimeouts: {\n        socketIdle: 500,\n        response: 1500,\n    },\n    setTimeoutUnref: (...args) => setTimeout(...args).unref(),\n};\n", "// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n// SPDX-License-Identifier: Apache-2.0\n//\n// Utility to encode RSA public keys (a pair of modulus (n) and exponent (e)) into DER-encoding, per ASN.1 specification.\nimport { Asn1DecodingError } from \"./error.js\";\n/** Enum with possible values for supported ASN.1 classes */\nvar Asn1Class;\n(function (Asn1Class) {\n    Asn1Class[Asn1Class[\"Universal\"] = 0] = \"Universal\";\n})(Asn1Class || (Asn1Class = {}));\n/** Enum with possible values for supported ASN.1 encodings */\nvar Asn1Encoding;\n(function (Asn1Encoding) {\n    Asn1Encoding[Asn1Encoding[\"Primitive\"] = 0] = \"Primitive\";\n    Asn1Encoding[Asn1Encoding[\"Constructed\"] = 1] = \"Constructed\";\n})(Asn1Encoding || (Asn1Encoding = {}));\n/** Enum with possible values for supported ASN.1 tags */\nvar Asn1Tag;\n(function (Asn1Tag) {\n    Asn1Tag[Asn1Tag[\"BitString\"] = 3] = \"BitString\";\n    Asn1Tag[Asn1Tag[\"ObjectIdentifier\"] = 6] = \"ObjectIdentifier\";\n    Asn1Tag[Asn1Tag[\"Sequence\"] = 16] = \"Sequence\";\n    Asn1Tag[Asn1Tag[\"Null\"] = 5] = \"Null\";\n    Asn1Tag[Asn1Tag[\"Integer\"] = 2] = \"Integer\";\n})(Asn1Tag || (Asn1Tag = {}));\n/**\n * Encode an ASN.1 identifier per ASN.1 spec (DER-encoding)\n * See https://www.itu.int/ITU-T/studygroups/com17/languages/X.690-0207.pdf chapter 8.1.2\n *\n * @param identifier - The ASN.1 identifier\n * @returns The buffer\n */\nfunction encodeIdentifier(identifier) {\n    const identifierAsNumber = (identifier.class << 7) |\n        (identifier.primitiveOrConstructed << 5) |\n        identifier.tag;\n    return Buffer.from([identifierAsNumber]);\n}\n/**\n * Encode the length of an ASN.1 type per ASN.1 spec (DER-encoding)\n * See https://www.itu.int/ITU-T/studygroups/com17/languages/X.690-0207.pdf chapter 8.1.3\n *\n * @param length - The length of the ASN.1 type\n * @returns The buffer\n */\nfunction encodeLength(length) {\n    if (length < 128) {\n        return Buffer.from([length]);\n    }\n    const integers = [];\n    while (length > 0) {\n        integers.push(length % 256);\n        length = length >> 8;\n    }\n    integers.reverse();\n    return Buffer.from([128 | integers.length, ...integers]);\n}\n/**\n * Encode a buffer (that represent an integer) as integer per ASN.1 spec (DER-encoding)\n * See https://www.itu.int/ITU-T/studygroups/com17/languages/X.690-0207.pdf chapter 8.3\n *\n * @param buffer - The buffer that represent an integer to encode\n * @returns The buffer\n */\nfunction encodeBufferAsInteger(buffer) {\n    return Buffer.concat([\n        encodeIdentifier({\n            class: Asn1Class.Universal,\n            primitiveOrConstructed: Asn1Encoding.Primitive,\n            tag: Asn1Tag.Integer,\n        }),\n        encodeLength(buffer.length),\n        buffer,\n    ]);\n}\n/**\n * Encode an object identifier (a string such as \"1.2.840.113549.1.1.1\") per ASN.1 spec (DER-encoding)\n * See https://www.itu.int/ITU-T/studygroups/com17/languages/X.690-0207.pdf chapter 8.19\n *\n * @param oid - The object identifier to encode\n * @returns The buffer\n */\nfunction encodeObjectIdentifier(oid) {\n    const oidComponents = oid.split(\".\").map((i) => parseInt(i));\n    const firstSubidentifier = oidComponents[0] * 40 + oidComponents[1];\n    const subsequentSubidentifiers = oidComponents\n        .slice(2)\n        .reduce((expanded, component) => {\n        const bytes = [];\n        do {\n            bytes.push(component % 128);\n            component = component >> 7;\n        } while (component);\n        return expanded.concat(bytes.map((b, index) => (index ? b + 128 : b)).reverse());\n    }, []);\n    const oidBuffer = Buffer.from([\n        firstSubidentifier,\n        ...subsequentSubidentifiers,\n    ]);\n    return Buffer.concat([\n        encodeIdentifier({\n            class: Asn1Class.Universal,\n            primitiveOrConstructed: Asn1Encoding.Primitive,\n            tag: Asn1Tag.ObjectIdentifier,\n        }),\n        encodeLength(oidBuffer.length),\n        oidBuffer,\n    ]);\n}\n/**\n * Encode a buffer as bit string per ASN.1 spec (DER-encoding)\n * See https://www.itu.int/ITU-T/studygroups/com17/languages/X.690-0207.pdf chapter 8.6\n *\n * @param buffer - The buffer to encode\n * @returns The buffer\n */\nfunction encodeBufferAsBitString(buffer) {\n    const bitString = Buffer.concat([Buffer.from([0]), buffer]);\n    return Buffer.concat([\n        encodeIdentifier({\n            class: Asn1Class.Universal,\n            primitiveOrConstructed: Asn1Encoding.Primitive,\n            tag: Asn1Tag.BitString,\n        }),\n        encodeLength(bitString.length),\n        bitString,\n    ]);\n}\n/**\n * Encode a sequence of DER-encoded items per ASN.1 spec (DER-encoding)\n * See https://www.itu.int/ITU-T/studygroups/com17/languages/X.690-0207.pdf chapter 8.9\n *\n * @param sequenceItems - The sequence of DER-encoded items\n * @returns The buffer\n */\nfunction encodeSequence(sequenceItems) {\n    const concatenated = Buffer.concat(sequenceItems);\n    return Buffer.concat([\n        encodeIdentifier({\n            class: Asn1Class.Universal,\n            primitiveOrConstructed: Asn1Encoding.Constructed,\n            tag: Asn1Tag.Sequence,\n        }),\n        encodeLength(concatenated.length),\n        concatenated,\n    ]);\n}\n/**\n * Encode null per ASN.1 spec (DER-encoding)\n * See https://www.itu.int/ITU-T/studygroups/com17/languages/X.690-0207.pdf chapter 8.8\n *\n * @returns The buffer\n */\nfunction encodeNull() {\n    return Buffer.concat([\n        encodeIdentifier({\n            class: Asn1Class.Universal,\n            primitiveOrConstructed: Asn1Encoding.Primitive,\n            tag: Asn1Tag.Null,\n        }),\n        encodeLength(0),\n    ]);\n}\n/**\n * RSA encryption object identifier constant\n *\n * From: https://tools.ietf.org/html/rfc8017\n *\n * pkcs-1    OBJECT IDENTIFIER ::= {\n *     iso(1) member-body(2) us(840) rsadsi(113549) pkcs(1) 1\n * }\n *\n * -- When rsaEncryption is used in an AlgorithmIdentifier,\n * -- the parameters MUST be present and MUST be NULL.\n * --\n * rsaEncryption    OBJECT IDENTIFIER ::= { pkcs-1 1 }\n *\n * See also: http://www.oid-info.com/get/1.2.840.113549.1.1.1\n */\nconst ALGORITHM_RSA_ENCRYPTION = encodeSequence([\n    encodeObjectIdentifier(\"1.2.840.113549.1.1.1\"),\n    encodeNull(), // parameters\n]);\n/**\n * Transform an RSA public key, which is a pair of modulus (n) and exponent (e),\n *  into a buffer per ASN.1 spec (DER-encoding)\n *\n * @param n - The modulus of the public key as buffer\n * @param e - The exponent of the public key as buffer\n * @returns The buffer, which is the public key encoded per ASN.1 spec (DER-encoding)\n */\nexport function constructPublicKeyInDerFormat(n, e) {\n    return encodeSequence([\n        ALGORITHM_RSA_ENCRYPTION,\n        encodeBufferAsBitString(encodeSequence([encodeBufferAsInteger(n), encodeBufferAsInteger(e)])),\n    ]);\n}\n/**\n * Decode an ASN.1 identifier (a number) into its parts: class, primitiveOrConstructed, tag\n * See https://www.itu.int/ITU-T/studygroups/com17/languages/X.690-0207.pdf chapter 8.1.2\n *\n * @param identifier - The identifier\n * @returns An object with properties class, primitiveOrConstructed, tag\n */\nfunction decodeIdentifier(identifier) {\n    if (identifier >> 3 === 0b11111) {\n        throw new Asn1DecodingError(\"Decoding of identifier with tag > 30 not implemented\");\n    }\n    return {\n        class: identifier >> 6,\n        primitiveOrConstructed: (identifier >> 5) & 0b001,\n        tag: identifier & 0b11111, // bit 1-5\n    };\n}\n/**\n * Decode an ASN.1 block of length value combinations,\n * and return the length and byte range of the first length value combination.\n * See https://www.itu.int/ITU-T/studygroups/com17/languages/X.690-0207.pdf chapter 8.1.3 - 8.1.5\n *\n * @param blockOfLengthValues - The ASN.1 length value\n * @returns The length and byte range of the first included length value\n */\nfunction decodeLengthValue(blockOfLengthValues) {\n    if (!(blockOfLengthValues[0] & 0b10000000)) {\n        return {\n            length: blockOfLengthValues[0],\n            firstByteOffset: 1,\n            lastByteOffset: 1 + blockOfLengthValues[0],\n        };\n    }\n    const nrLengthOctets = blockOfLengthValues[0] & 0b01111111;\n    const length = Buffer.from(blockOfLengthValues.subarray(1, 1 + 1 + nrLengthOctets)).readUIntBE(0, nrLengthOctets);\n    return {\n        length,\n        firstByteOffset: 1 + nrLengthOctets,\n        lastByteOffset: 1 + nrLengthOctets + length,\n    };\n}\n/**\n * Decode an ASN.1 sequence into its constituent parts, each part being an identifier-length-value triplet\n * See https://www.itu.int/ITU-T/studygroups/com17/languages/X.690-0207.pdf chapter 8.9\n *\n * @param sequenceValue - The ASN.1 sequence value\n * @returns Array of identifier-length-value triplets\n */\nfunction decodeSequence(sequence) {\n    const { tag } = decodeIdentifier(sequence[0]);\n    if (tag !== Asn1Tag.Sequence) {\n        throw new Asn1DecodingError(`Expected a sequence to decode, but got tag ${tag}`);\n    }\n    const { firstByteOffset, lastByteOffset } = decodeLengthValue(sequence.subarray(1));\n    const sequenceValue = sequence.subarray(1 + firstByteOffset, 1 + 1 + lastByteOffset);\n    const parts = [];\n    let offset = 0;\n    while (offset < sequenceValue.length) {\n        // Silence false postive: accessing an octet in a Buffer at a particular index\n        // is to be done with index operator: [index]\n        // eslint-disable-next-line security/detect-object-injection\n        const identifier = decodeIdentifier(sequenceValue[offset]);\n        const next = decodeLengthValue(sequenceValue.subarray(offset + 1));\n        const value = sequenceValue.subarray(offset + 1 + next.firstByteOffset, offset + 1 + next.lastByteOffset);\n        parts.push({ identifier, length: next.length, value });\n        offset += 1 + next.lastByteOffset;\n    }\n    return parts;\n}\n/**\n * Decode an ASN.1 sequence that is wrapped in a bit string\n * (Which is the way RSA public keys are encoded in ASN.1 DER-encoding)\n * See https://www.itu.int/ITU-T/studygroups/com17/languages/X.690-0207.pdf chapter 8.6 and 8.9\n *\n * @param bitStringValue - The ASN.1 bit string value\n * @returns Array of identifier-length-value triplets\n */\nfunction decodeBitStringWrappedSequenceValue(bitStringValue) {\n    const wrappedSequence = bitStringValue.subarray(1);\n    return decodeSequence(wrappedSequence);\n}\n/**\n * Decode an ASN.1 DER-encoded public key, into its modulus (n) and exponent (e)\n *\n * @param publicKey - The ASN.1 DER-encoded public key\n * @returns Object with modulus (n) and exponent (e)\n */\nexport function deconstructPublicKeyInDerFormat(publicKey) {\n    const [, pubkeyinfo] = decodeSequence(publicKey);\n    const [n, e] = decodeBitStringWrappedSequenceValue(pubkeyinfo.value);\n    return { n: n.value, e: e.value };\n}\n", "// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n// SPDX-License-Identifier: Apache-2.0\n//\n// NodeJS implementation for fetching JSON documents over HTTPS\nimport { request } from \"https\";\nimport { validateHttpsJsonResponse } from \"./https-common.js\";\nimport { pipeline } from \"stream\";\nimport { TextDecoder } from \"util\";\nimport { safeJsonParse } from \"./safe-json-parse.js\";\nimport { FetchError, NonRetryableFetchError } from \"./error.js\";\n/**\n * Execute a HTTPS request\n * @param uri - The URI\n * @param requestOptions - The RequestOptions to use\n * @param data - Data to send to the URI (e.g. POST data)\n * @returns - The response as parsed JSON\n */\nexport async function fetchJson(uri, requestOptions, data) {\n    let responseTimeout;\n    return new Promise((resolve, reject) => {\n        const req = request(uri, {\n            method: \"GET\",\n            ...requestOptions,\n        }, (response) => {\n            // Capture response data\n            // @types/node is incomplete so cast to any\n            // eslint-disable-next-line @typescript-eslint/no-explicit-any\n            pipeline([\n                response,\n                getJsonDestination(uri, response.statusCode, response.headers),\n            ], done);\n        });\n        if (requestOptions?.responseTimeout) {\n            responseTimeout = setTimeout(() => done(new FetchError(uri, `Response time-out (after ${requestOptions.responseTimeout} ms.)`)), requestOptions.responseTimeout);\n            responseTimeout.unref(); // Don't block Node from exiting\n        }\n        function done(...args) {\n            if (responseTimeout)\n                clearTimeout(responseTimeout);\n            if (args[0] == null) {\n                resolve(args[1]);\n                return;\n            }\n            // In case of errors, let the Agent (if any) know to abandon the socket\n            // This is probably best, because the socket may have become stale\n            /* istanbul ignore next */\n            req.socket?.emit(\"agentRemove\");\n            // Turn error into FetchError so the URI is nicely captured in the message\n            let error = args[0];\n            if (!(error instanceof FetchError)) {\n                error = new FetchError(uri, error.message);\n            }\n            req.destroy();\n            reject(error);\n        }\n        // Handle errors while sending request\n        req.on(\"error\", done);\n        // Signal end of request (include optional data)\n        req.end(data);\n    });\n}\n/**\n * Ensures the HTTPS response contains valid JSON\n *\n * @param uri - The URI you were requesting\n * @param statusCode - The response status code to your HTTPS request\n * @param headers - The response headers to your HTTPS request\n *\n * @returns - Async function that can be used as destination in a stream.pipeline, it will return the JSON, if valid, or throw an error otherwise\n */\nfunction getJsonDestination(uri, statusCode, headers) {\n    return async (responseIterable) => {\n        validateHttpsJsonResponse(uri, statusCode, headers[\"content-type\"]);\n        const collected = [];\n        for await (const chunk of responseIterable) {\n            collected.push(chunk);\n        }\n        try {\n            return safeJsonParse(new TextDecoder(\"utf8\", { fatal: true, ignoreBOM: true }).decode(Buffer.concat(collected)));\n        }\n        catch (err) {\n            throw new NonRetryableFetchError(uri, err);\n        }\n    };\n}\n", "// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n// SPDX-License-Identifier: Apache-2.0\n//\n// Lower-level HTTPS functionalities, common for Node.js and Web\nimport { FetchError, NonRetryableFetchError } from \"./error.js\";\n/**\n * Sanity check a HTTPS response where we expect to get JSON data back\n *\n * @param uri the uri that was being requested\n * @param statusCode the HTTP status code, should be 200\n * @param contentType the value of the \"Content-Type\" header in the response, should start with \"application/json\"\n * @returns void - throws an error if the status code or content type aren't as expected\n */\nexport function validateHttpsJsonResponse(uri, statusCode, contentType) {\n    if (statusCode === 429) {\n        throw new FetchError(uri, \"Too many requests\");\n    }\n    else if (statusCode !== 200) {\n        throw new NonRetryableFetchError(uri, `Status code is ${statusCode}, expected 200`);\n    }\n    if (!contentType ||\n        !contentType.toLowerCase().startsWith(\"application/json\")) {\n        throw new NonRetryableFetchError(uri, `Content-type is \"${contentType}\", expected \"application/json\"`);\n    }\n}\n", "// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n// SPDX-License-Identifier: Apache-2.0\n//\n// Utility to parse JSON safely\n/**\n * Check if a piece of JSON is a JSON object, and not e.g. a mere string or null\n *\n * @param j - the JSON\n */\nexport function isJsonObject(j) {\n    // It is not enough to check that `typeof j === \"object\"`\n    // because in JS `typeof null` is also \"object\", and so is `typeof []`.\n    // So we need to check that j is an object, and not null, and not an array\n    return typeof j === \"object\" && !Array.isArray(j) && j !== null;\n}\n/**\n * Parse a string as JSON, while removing __proto__ and constructor, so JS prototype pollution is prevented\n *\n * @param s - the string to JSON parse\n */\nexport function safeJsonParse(s) {\n    return JSON.parse(s, (_, value) => {\n        if (typeof value === \"object\" && !Array.isArray(value) && value !== null) {\n            delete value.__proto__;\n            delete value.constructor;\n        }\n        return value;\n    });\n}\n", "// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n// SPDX-License-Identifier: Apache-2.0\n//\n// Utilities for fetching the JWKS URI, to get the public keys with which to verify JWTs\nimport { NonRetryableFetchError } from \"./error.js\";\nimport { nodeWebCompat } from \"#node-web-compat\";\n/**\n * Execute a HTTPS request\n * @param uri - The URI\n * @param requestOptions - The RequestOptions to use (depending on the runtime context, either Node.js RequestOptions or Web Fetch init)\n * @param data - Data to send to the URI (e.g. POST data)\n * @returns - The response as parsed JSON\n */\nexport const fetchJson = nodeWebCompat.fetchJson;\n/**\n * HTTPS Fetcher for URIs with JSON body\n *\n * @param defaultRequestOptions - The default RequestOptions to use on individual HTTPS requests\n */\nexport class SimpleJsonFetcher {\n    constructor(props) {\n        this.defaultRequestOptions = {\n            timeout: nodeWebCompat.defaultFetchTimeouts.socketIdle,\n            responseTimeout: nodeWebCompat.defaultFetchTimeouts.response,\n            ...props?.defaultRequestOptions,\n        };\n    }\n    /**\n     * Execute a HTTPS request (with 1 immediate retry in case of errors)\n     * @param uri - The URI\n     * @param requestOptions - The RequestOptions to use\n     * @param data - Data to send to the URI (e.g. POST data)\n     * @returns - The response as parsed JSON\n     */\n    async fetch(uri, requestOptions, data) {\n        requestOptions = { ...this.defaultRequestOptions, ...requestOptions };\n        try {\n            return await fetchJson(uri, requestOptions, data);\n        }\n        catch (err) {\n            if (err instanceof NonRetryableFetchError) {\n                throw err;\n            }\n            // Retry once, immediately\n            return fetchJson(uri, requestOptions, data);\n        }\n    }\n}\n", "// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n// SPDX-License-Identifier: Apache-2.0\n//\n// Utilities to assert that supplied values match with expected values\nimport { FailedAssertionError } from \"./error.js\";\n/**\n * Assert value is a non-empty string and equal to the expected value,\n * or throw an error otherwise\n *\n * @param name - Name for the value being checked\n * @param actual - The value to check\n * @param expected - The expected value\n * @param errorConstructor - Constructor for the concrete error to be thrown\n */\nexport function assertStringEquals(name, actual, expected, errorConstructor = FailedAssertionError) {\n    if (!actual) {\n        throw new errorConstructor(`Missing ${name}. Expected: ${expected}`, actual, expected);\n    }\n    if (typeof actual !== \"string\") {\n        throw new errorConstructor(`${name} is not of type string`, actual, expected);\n    }\n    if (expected !== actual) {\n        throw new errorConstructor(`${name} not allowed: ${actual}. Expected: ${expected}`, actual, expected);\n    }\n}\n/**\n * Assert value is a non-empty string and is indeed one of the expected values,\n * or throw an error otherwise\n *\n * @param name - Name for the value being checked\n * @param actual - The value to check\n * @param expected - The array of expected values. For your convenience you can provide\n * @param errorConstructor - Constructor for the concrete error to be thrown\n * a string here as well, which will mean an array with just that string\n */\nexport function assertStringArrayContainsString(name, actual, expected, errorConstructor = FailedAssertionError) {\n    if (!actual) {\n        throw new errorConstructor(`Missing ${name}. ${expectationMessage(expected)}`, actual, expected);\n    }\n    if (typeof actual !== \"string\") {\n        throw new errorConstructor(`${name} is not of type string`, actual, expected);\n    }\n    return assertStringArraysOverlap(name, actual, expected, errorConstructor);\n}\n/**\n * Assert value is an array of strings, where at least one of the strings is indeed one of the expected values,\n * or throw an error otherwise\n *\n * @param name - Name for the value being checked\n * @param actual - The value to check, must be an array of strings, or a single string (which will be treated\n * as an array with just that string)\n * @param expected - The array of expected values. For your convenience you can provide\n * a string here as well, which will mean an array with just that string\n * @param errorConstructor - Constructor for the concrete error to be thrown\n */\nexport function assertStringArraysOverlap(name, actual, expected, errorConstructor = FailedAssertionError) {\n    if (!actual) {\n        throw new errorConstructor(`Missing ${name}. ${expectationMessage(expected)}`, actual, expected);\n    }\n    const expectedAsSet = new Set(Array.isArray(expected) ? expected : [expected]);\n    if (typeof actual === \"string\") {\n        actual = [actual];\n    }\n    if (!Array.isArray(actual)) {\n        throw new errorConstructor(`${name} is not an array`, actual, expected);\n    }\n    const overlaps = actual.some((actualItem) => {\n        if (typeof actualItem !== \"string\") {\n            throw new errorConstructor(`${name} includes elements that are not of type string`, actual, expected);\n        }\n        return expectedAsSet.has(actualItem);\n    });\n    if (!overlaps) {\n        throw new errorConstructor(`${name} not allowed: ${actual.join(\", \")}. ${expectationMessage(expected)}`, actual, expected);\n    }\n}\n/**\n * Get a nicely readable message regarding an expectation\n *\n * @param expected - The expected value.\n */\nfunction expectationMessage(expected) {\n    if (Array.isArray(expected)) {\n        if (expected.length > 1) {\n            return `Expected one of: ${expected.join(\", \")}`;\n        }\n        return `Expected: ${expected[0]}`;\n    }\n    return `Expected: ${expected}`;\n}\n/**\n * Assert value is not a promise, or throw an error otherwise\n *\n * @param actual - The value to check\n * @param errorFactory - Function that returns the error to be thrown\n */\nexport function assertIsNotPromise(actual, errorFactory) {\n    if (actual && typeof actual.then === \"function\") {\n        throw errorFactory();\n    }\n}\n", "// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n// SPDX-License-Identifier: Apache-2.0\nimport { SimpleJsonFetcher, fetchJson } from \"./https.js\";\nimport { isJsonObject } from \"./safe-json-parse.js\";\nimport { JwkValidationError, JwksNotAvailableInCacheError, JwksValidationError, KidNotFoundInJwksError, WaitPeriodNotYetEndedJwkError, JwtWithoutValidKidError, JwkInvalidUseError, JwkInvalidKtyError, } from \"./error.js\";\nimport { nodeWebCompat } from \"#node-web-compat\";\nimport { assertStringEquals } from \"./assert.js\";\nconst optionalJwkFieldNames = [\n    \"use\",\n    \"alg\",\n    \"kid\",\n    \"n\",\n    \"e\", // https://datatracker.ietf.org/doc/html/rfc7518#section-6.3.1.2\n];\nconst mandatoryJwkFieldNames = [\n    \"kty\", // https://datatracker.ietf.org/doc/html/rfc7517#section-4.1\n];\nexport function findJwkInJwks(jwks, kid) {\n    return jwks.keys.find((jwk) => jwk.kid != null && jwk.kid === kid);\n}\nexport async function fetchJwks(jwksUri) {\n    const jwks = await fetchJson(jwksUri);\n    assertIsJwks(jwks);\n    return jwks;\n}\nexport async function fetchJwk(jwksUri, decomposedJwt) {\n    if (!decomposedJwt.header.kid) {\n        throw new JwtWithoutValidKidError(\"JWT header does not have valid kid claim\");\n    }\n    const jwks = await fetchJwks(jwksUri);\n    const jwk = findJwkInJwks(jwks, decomposedJwt.header.kid);\n    if (!jwk) {\n        throw new KidNotFoundInJwksError(`JWK for kid \"${decomposedJwt.header.kid}\" not found in the JWKS`);\n    }\n    return jwk;\n}\nexport function assertIsJwks(jwks) {\n    if (!jwks) {\n        throw new JwksValidationError(\"JWKS empty\");\n    }\n    if (!isJsonObject(jwks)) {\n        throw new JwksValidationError(\"JWKS should be an object\");\n    }\n    if (!Object.keys(jwks).includes(\"keys\")) {\n        throw new JwksValidationError(\"JWKS does not include keys\");\n    }\n    if (!Array.isArray(jwks.keys)) {\n        throw new JwksValidationError(\"JWKS keys should be an array\");\n    }\n    for (const jwk of jwks.keys) {\n        assertIsJwk(jwk);\n    }\n}\nexport function assertIsRsaSignatureJwk(jwk) {\n    // Check JWK use\n    assertStringEquals(\"JWK use\", jwk.use, \"sig\", JwkInvalidUseError);\n    // Check JWK kty\n    assertStringEquals(\"JWK kty\", jwk.kty, \"RSA\", JwkInvalidKtyError);\n    // Check modulus (n) has a value\n    if (!jwk.n)\n        throw new JwkValidationError(\"Missing modulus (n)\");\n    // Check exponent (e) has a value\n    if (!jwk.e)\n        throw new JwkValidationError(\"Missing exponent (e)\");\n}\nexport function assertIsJwk(jwk) {\n    if (!jwk) {\n        throw new JwkValidationError(\"JWK empty\");\n    }\n    if (!isJsonObject(jwk)) {\n        throw new JwkValidationError(\"JWK should be an object\");\n    }\n    for (const field of mandatoryJwkFieldNames) {\n        // disable eslint rule because `field` is trusted\n        // eslint-disable-next-line security/detect-object-injection\n        if (typeof jwk[field] !== \"string\") {\n            throw new JwkValidationError(`JWK ${field} should be a string`);\n        }\n    }\n    for (const field of optionalJwkFieldNames) {\n        // disable eslint rule because `field` is trusted\n        // eslint-disable-next-line security/detect-object-injection\n        if (field in jwk && typeof jwk[field] !== \"string\") {\n            throw new JwkValidationError(`JWK ${field} should be a string`);\n        }\n    }\n}\nexport function isJwks(jwks) {\n    try {\n        assertIsJwks(jwks);\n        return true;\n    }\n    catch {\n        return false;\n    }\n}\nexport function isJwk(jwk) {\n    try {\n        assertIsJwk(jwk);\n        return true;\n    }\n    catch {\n        return false;\n    }\n}\nexport class SimplePenaltyBox {\n    constructor(props) {\n        this.waitingUris = new Map();\n        this.waitSeconds = props?.waitSeconds ?? 10;\n    }\n    async wait(jwksUri) {\n        // SimplePenaltyBox does not actually wait but bluntly throws an error\n        // Any waiting and retries are expected to be done upstream (e.g. in the browser / app)\n        if (this.waitingUris.has(jwksUri)) {\n            throw new WaitPeriodNotYetEndedJwkError(\"Not allowed to fetch JWKS yet, still waiting for back off period to end\");\n        }\n    }\n    release(jwksUri) {\n        const i = this.waitingUris.get(jwksUri);\n        if (i) {\n            clearTimeout(i);\n            this.waitingUris.delete(jwksUri);\n        }\n    }\n    registerFailedAttempt(jwksUri) {\n        const i = nodeWebCompat.setTimeoutUnref(() => {\n            this.waitingUris.delete(jwksUri);\n        }, this.waitSeconds * 1000);\n        this.waitingUris.set(jwksUri, i);\n    }\n    registerSuccessfulAttempt(jwksUri) {\n        this.release(jwksUri);\n    }\n}\nexport class SimpleJwksCache {\n    constructor(props) {\n        this.jwksCache = new Map();\n        this.fetchingJwks = new Map();\n        this.penaltyBox = props?.penaltyBox ?? new SimplePenaltyBox();\n        this.fetcher = props?.fetcher ?? new SimpleJsonFetcher();\n    }\n    addJwks(jwksUri, jwks) {\n        this.jwksCache.set(jwksUri, jwks);\n    }\n    async getJwks(jwksUri) {\n        const existingFetch = this.fetchingJwks.get(jwksUri);\n        if (existingFetch) {\n            return existingFetch;\n        }\n        const jwksPromise = this.fetcher.fetch(jwksUri).then((res) => {\n            assertIsJwks(res);\n            return res;\n        });\n        this.fetchingJwks.set(jwksUri, jwksPromise);\n        let jwks;\n        try {\n            jwks = await jwksPromise;\n        }\n        finally {\n            this.fetchingJwks.delete(jwksUri);\n        }\n        this.jwksCache.set(jwksUri, jwks);\n        return jwks;\n    }\n    getCachedJwk(jwksUri, decomposedJwt) {\n        if (typeof decomposedJwt.header.kid !== \"string\") {\n            throw new JwtWithoutValidKidError(\"JWT header does not have valid kid claim\");\n        }\n        if (!this.jwksCache.has(jwksUri)) {\n            throw new JwksNotAvailableInCacheError(`JWKS for uri ${jwksUri} not yet available in cache`);\n        }\n        const jwk = findJwkInJwks(this.jwksCache.get(jwksUri), decomposedJwt.header.kid);\n        if (!jwk) {\n            throw new KidNotFoundInJwksError(`JWK for kid ${decomposedJwt.header.kid} not found in the JWKS`);\n        }\n        return jwk;\n    }\n    async getJwk(jwksUri, decomposedJwt) {\n        if (typeof decomposedJwt.header.kid !== \"string\") {\n            throw new JwtWithoutValidKidError(\"JWT header does not have valid kid claim\");\n        }\n        // Try to get JWK from cache:\n        const cachedJwks = this.jwksCache.get(jwksUri);\n        if (cachedJwks) {\n            const cachedJwk = findJwkInJwks(cachedJwks, decomposedJwt.header.kid);\n            if (cachedJwk) {\n                return cachedJwk;\n            }\n        }\n        // Await any wait period that is currently in effect\n        // This prevents us from flooding the JWKS URI with requests\n        await this.penaltyBox.wait(jwksUri, decomposedJwt.header.kid);\n        // Fetch the JWKS and (try to) locate the JWK\n        const jwks = await this.getJwks(jwksUri);\n        const jwk = findJwkInJwks(jwks, decomposedJwt.header.kid);\n        // If the JWK could not be located, someone might be messing around with us\n        // Register the failed attempt with the penaltyBox, so it can enforce a wait period\n        // before trying again next time (instead of flooding the JWKS URI with requests)\n        if (!jwk) {\n            this.penaltyBox.registerFailedAttempt(jwksUri, decomposedJwt.header.kid);\n            throw new KidNotFoundInJwksError(`JWK for kid \"${decomposedJwt.header.kid}\" not found in the JWKS`);\n        }\n        else {\n            this.penaltyBox.registerSuccessfulAttempt(jwksUri, decomposedJwt.header.kid);\n        }\n        return jwk;\n    }\n}\n", "// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n// SPDX-License-Identifier: Apache-2.0\nexport const supportedSignatureAlgorithms = [\n    \"RS256\",\n    \"RS384\",\n    \"RS512\",\n];\n", "// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n// SPDX-License-Identifier: Apache-2.0\nimport { assertStringArrayContainsString, assertStringArraysOverlap, } from \"./assert.js\";\nimport { safeJsonParse, isJsonObject } from \"./safe-json-parse.js\";\nimport { JwtExpiredError, JwtNotBeforeError, JwtInvalidIssuerError, JwtInvalidAudienceError, JwtInvalidScopeError, JwtParseError, ParameterValidationError, } from \"./error.js\";\nimport { nodeWebCompat } from \"#node-web-compat\";\n/**\n * Assert that the argument is a valid JWT header object.\n * Throws an error in case it is not.\n *\n * @param header\n * @returns void\n */\nfunction assertJwtHeader(header) {\n    if (!isJsonObject(header)) {\n        throw new JwtParseError(\"JWT header is not an object\");\n    }\n    if (header.alg !== undefined && typeof header.alg !== \"string\") {\n        throw new JwtParseError(\"JWT header alg claim is not a string\");\n    }\n    if (header.kid !== undefined && typeof header.kid !== \"string\") {\n        throw new JwtParseError(\"JWT header kid claim is not a string\");\n    }\n}\n/**\n * Assert that the argument is a valid JWT payload object.\n * Throws an error in case it is not.\n *\n * @param payload\n * @returns void\n */\nfunction assertJwtPayload(payload) {\n    if (!isJsonObject(payload)) {\n        throw new JwtParseError(\"JWT payload is not an object\");\n    }\n    if (payload.exp !== undefined && !Number.isFinite(payload.exp)) {\n        throw new JwtParseError(\"JWT payload exp claim is not a number\");\n    }\n    if (payload.iss !== undefined && typeof payload.iss !== \"string\") {\n        throw new JwtParseError(\"JWT payload iss claim is not a string\");\n    }\n    if (payload.aud !== undefined &&\n        typeof payload.aud !== \"string\" &&\n        (!Array.isArray(payload.aud) ||\n            payload.aud.some((aud) => typeof aud !== \"string\"))) {\n        throw new JwtParseError(\"JWT payload aud claim is not a string or array of strings\");\n    }\n    if (payload.nbf !== undefined && !Number.isFinite(payload.nbf)) {\n        throw new JwtParseError(\"JWT payload nbf claim is not a number\");\n    }\n    if (payload.iat !== undefined && !Number.isFinite(payload.iat)) {\n        throw new JwtParseError(\"JWT payload iat claim is not a number\");\n    }\n    if (payload.scope !== undefined && typeof payload.scope !== \"string\") {\n        throw new JwtParseError(\"JWT payload scope claim is not a string\");\n    }\n    if (payload.jti !== undefined && typeof payload.jti !== \"string\") {\n        throw new JwtParseError(\"JWT payload jti claim is not a string\");\n    }\n}\n/**\n * Sanity check, decompose and JSON parse a JWT string into its constituent parts:\n * - header object\n * - payload object\n * - signature string\n *\n * @param jwt The JWT (as string)\n * @returns the decomposed JWT\n */\nexport function decomposeJwt(jwt) {\n    // Sanity checks on JWT\n    if (!jwt) {\n        throw new JwtParseError(\"Empty JWT\");\n    }\n    if (typeof jwt !== \"string\") {\n        throw new JwtParseError(\"JWT is not a string\");\n    }\n    if (!jwt.match(/^[A-Za-z0-9_-]+\\.[A-Za-z0-9_-]+\\.[A-Za-z0-9_-]+$/)) {\n        throw new JwtParseError(\"JWT string does not consist of exactly 3 parts (header, payload, signature)\");\n    }\n    const [headerB64, payloadB64, signatureB64] = jwt.split(\".\");\n    // B64 decode header and payload\n    const [headerString, payloadString] = [headerB64, payloadB64].map(nodeWebCompat.parseB64UrlString);\n    // Parse header\n    let header;\n    try {\n        header = safeJsonParse(headerString);\n    }\n    catch (err) {\n        throw new JwtParseError(\"Invalid JWT. Header is not a valid JSON object\", err);\n    }\n    assertJwtHeader(header);\n    // parse payload\n    let payload;\n    try {\n        payload = safeJsonParse(payloadString);\n    }\n    catch (err) {\n        throw new JwtParseError(\"Invalid JWT. Payload is not a valid JSON object\", err);\n    }\n    assertJwtPayload(payload);\n    return {\n        header,\n        headerB64,\n        payload,\n        payloadB64,\n        signatureB64,\n    };\n}\n/**\n * Validate JWT payload fields. Throws an error in case there's any validation issue.\n *\n * @param payload The (JSON parsed) JWT payload\n * @param options The options to use during validation\n * @returns void\n */\nexport function validateJwtFields(payload, options) {\n    // Check expiry\n    if (payload.exp !== undefined) {\n        if (payload.exp + (options.graceSeconds ?? 0) < Date.now() / 1000) {\n            throw new JwtExpiredError(`Token expired at ${new Date(payload.exp * 1000).toISOString()}`, payload.exp);\n        }\n    }\n    // Check not before\n    if (payload.nbf !== undefined) {\n        if (payload.nbf - (options.graceSeconds ?? 0) > Date.now() / 1000) {\n            throw new JwtNotBeforeError(`Token can't be used before ${new Date(payload.nbf * 1000).toISOString()}`, payload.nbf);\n        }\n    }\n    // Check JWT issuer\n    if (options.issuer !== null) {\n        if (options.issuer === undefined) {\n            throw new ParameterValidationError(\"issuer must be provided or set to null explicitly\");\n        }\n        assertStringArrayContainsString(\"Issuer\", payload.iss, options.issuer, JwtInvalidIssuerError);\n    }\n    // Check audience\n    if (options.audience !== null) {\n        if (options.audience === undefined) {\n            throw new ParameterValidationError(\"audience must be provided or set to null explicitly\");\n        }\n        assertStringArraysOverlap(\"Audience\", payload.aud, options.audience, JwtInvalidAudienceError);\n    }\n    // Check scope\n    if (options.scope != null) {\n        assertStringArraysOverlap(\"Scope\", payload.scope?.split(\" \"), options.scope, JwtInvalidScopeError);\n    }\n}\n", "// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n// SPDX-License-Identifier: Apache-2.0\nimport { SimpleJwksCache, isJwk, isJwks, fetchJwk, assertIsRsaSignatureJwk, findJwkInJwks, } from \"./jwk.js\";\nimport { assertIsNotPromise, assertStringArrayContainsString, assertStringEquals, } from \"./assert.js\";\nimport { supportedSignatureAlgorithms, } from \"./jwt-model.js\";\nimport { decomposeJwt, validateJwtFields } from \"./jwt.js\";\nimport { JwtInvalidClaimError, JwtInvalidIssuerError, JwtInvalidSignatureAlgorithmError, JwtInvalidSignatureError, KidNotFoundInJwksError, ParameterValidationError, } from \"./error.js\";\nimport { nodeWebCompat } from \"#node-web-compat\";\n/**\n * Sanity check the JWT header and the selected JWK\n *\n * @param header: the JWT header (decoded and JSON parsed)\n * @param jwk: the JWK\n */\nfunction validateJwtHeaderAndJwk(header, jwk) {\n    // Check that the JWK is in fact a JWK for RSA signatures\n    assertIsRsaSignatureJwk(jwk);\n    // Check that JWT signature algorithm matches JWK\n    if (jwk.alg) {\n        assertStringEquals(\"JWT signature algorithm\", header.alg, jwk.alg, JwtInvalidSignatureAlgorithmError);\n    }\n    // Check JWT signature algorithm is one of the supported signature algorithms\n    assertStringArrayContainsString(\"JWT signature algorithm\", header.alg, supportedSignatureAlgorithms, JwtInvalidSignatureAlgorithmError);\n}\n/**\n * Verify a JWT asynchronously (thus allowing for the JWKS to be fetched from the JWKS URI)\n *\n * @param jwt The JWT\n * @param jwksUri The JWKS URI, where the JWKS can be fetched from\n * @param options Verification options\n * @returns Promise that resolves to the payload of the JWT\u2013\u2013if the JWT is valid, otherwise the promise rejects\n */\nexport async function verifyJwt(jwt, jwksUri, options) {\n    return verifyDecomposedJwt(decomposeJwt(jwt), jwksUri, options);\n}\n/**\n * Verify (asynchronously) a JWT that is already decomposed (by function `decomposeJwt`)\n *\n * @param decomposedJwt The decomposed JWT\n * @param jwksUri The JWKS URI, where the JWKS can be fetched from\n * @param options Verification options\n * @param jwkFetcher A function that can execute the fetch of the JWKS from the JWKS URI\n * @param transformJwkToKeyObjectFn A function that can transform a JWK into a crypto native key object\n * @returns Promise that resolves to the payload of the JWT\u2013\u2013if the JWT is valid, otherwise the promise rejects\n */\nasync function verifyDecomposedJwt(decomposedJwt, jwksUri, options, jwkFetcher = fetchJwk, transformJwkToKeyObjectFn = nodeWebCompat.transformJwkToKeyObjectAsync) {\n    const { header, headerB64, payload, payloadB64, signatureB64 } = decomposedJwt;\n    const jwk = await jwkFetcher(jwksUri, decomposedJwt);\n    validateJwtHeaderAndJwk(decomposedJwt.header, jwk);\n    // Transform the JWK to native key format, that can be used with verifySignature\n    const keyObject = await transformJwkToKeyObjectFn(jwk, header.alg, payload.iss);\n    // Verify the JWT signature\n    const valid = await nodeWebCompat.verifySignatureAsync({\n        jwsSigningInput: `${headerB64}.${payloadB64}`,\n        signature: signatureB64,\n        alg: header.alg,\n        keyObject,\n    });\n    if (!valid) {\n        throw new JwtInvalidSignatureError(\"Invalid signature\");\n    }\n    try {\n        validateJwtFields(payload, options);\n        if (options.customJwtCheck) {\n            await options.customJwtCheck({ header, payload, jwk });\n        }\n    }\n    catch (err) {\n        if (options.includeRawJwtInErrors && err instanceof JwtInvalidClaimError) {\n            throw err.withRawJwt(decomposedJwt);\n        }\n        throw err;\n    }\n    return payload;\n}\n/**\n * Verify a JWT synchronously, using a JWKS or JWK that has already been fetched\n *\n * @param jwt The JWT\n * @param jwkOrJwks The JWKS that includes the right JWK (indexed by kid). Alternatively, provide the right JWK directly\n * @param options Verification options\n * @param transformJwkToKeyObjectFn A function that can transform a JWK into a crypto native key object\n * @returns The (JSON parsed) payload of the JWT\u2013\u2013if the JWT is valid, otherwise an error is thrown\n */\nexport function verifyJwtSync(jwt, jwkOrJwks, options, transformJwkToKeyObjectFn = nodeWebCompat.transformJwkToKeyObjectSync) {\n    return verifyDecomposedJwtSync(decomposeJwt(jwt), jwkOrJwks, options, transformJwkToKeyObjectFn);\n}\n/**\n * Verify (synchronously) a JWT that is already decomposed (by function `decomposeJwt`)\n *\n * @param decomposedJwt The decomposed JWT\n * @param jwkOrJwks The JWKS that includes the right JWK (indexed by kid). Alternatively, provide the right JWK directly\n * @param options Verification options\n * @param transformJwkToKeyObjectFn A function that can transform a JWK into a crypto native key object\n * @returns The (JSON parsed) payload of the JWT\u2013\u2013if the JWT is valid, otherwise an error is thrown\n */\nfunction verifyDecomposedJwtSync(decomposedJwt, jwkOrJwks, options, transformJwkToKeyObjectFn) {\n    const { header, headerB64, payload, payloadB64, signatureB64 } = decomposedJwt;\n    let jwk;\n    if (isJwk(jwkOrJwks)) {\n        jwk = jwkOrJwks;\n    }\n    else if (isJwks(jwkOrJwks)) {\n        const locatedJwk = header.kid\n            ? findJwkInJwks(jwkOrJwks, header.kid)\n            : undefined;\n        if (!locatedJwk) {\n            throw new KidNotFoundInJwksError(`JWK for kid ${header.kid} not found in the JWKS`);\n        }\n        jwk = locatedJwk;\n    }\n    else {\n        throw new ParameterValidationError([\n            `Expected a valid JWK or JWKS (parsed as JavaScript object), but received: ${jwkOrJwks}.`,\n            \"If you're passing a JWKS URI, use the async verify() method instead, it will download and parse the JWKS for you\",\n        ].join());\n    }\n    validateJwtHeaderAndJwk(decomposedJwt.header, jwk);\n    // Transform the JWK to native key format, that can be used with verifySignature\n    const keyObject = transformJwkToKeyObjectFn(jwk, header.alg, payload.iss);\n    // Verify the JWT signature (JWS)\n    const valid = nodeWebCompat.verifySignatureSync({\n        jwsSigningInput: `${headerB64}.${payloadB64}`,\n        signature: signatureB64,\n        alg: header.alg,\n        keyObject,\n    });\n    if (!valid) {\n        throw new JwtInvalidSignatureError(\"Invalid signature\");\n    }\n    try {\n        validateJwtFields(payload, options);\n        if (options.customJwtCheck) {\n            const res = options.customJwtCheck({ header, payload, jwk });\n            assertIsNotPromise(res, () => new ParameterValidationError(\"Custom JWT checks must be synchronous but a promise was returned\"));\n        }\n    }\n    catch (err) {\n        if (options.includeRawJwtInErrors && err instanceof JwtInvalidClaimError) {\n            throw err.withRawJwt(decomposedJwt);\n        }\n        throw err;\n    }\n    return payload;\n}\n/**\n * Abstract class representing a verifier for JWTs signed with RSA (e.g. RS256, RS384, RS512)\n *\n * A class is used, because there is state:\n * - The JWKS is fetched (downloaded) from the JWKS URI and cached in memory\n * - Verification properties at verifier level, are used as default options for individual verify calls\n *\n * When instantiating this class, relevant type parameters should be provided, for your concrete case:\n * @param StillToProvide The verification options that you want callers of verify to provide on individual verify calls\n * @param SpecificVerifyProperties The verification options that you'll use\n * @param IssuerConfig The issuer config that you'll use (config options are used as default verification options)\n * @param MultiIssuer Verify multiple issuers (true) or just a single one (false)\n */\nexport class JwtRsaVerifierBase {\n    constructor(verifyProperties, jwksCache = new SimpleJwksCache()) {\n        this.jwksCache = jwksCache;\n        this.issuersConfig = new Map();\n        this.publicKeyCache = new KeyObjectCache();\n        if (Array.isArray(verifyProperties)) {\n            if (!verifyProperties.length) {\n                throw new ParameterValidationError(\"Provide at least one issuer configuration\");\n            }\n            for (const prop of verifyProperties) {\n                if (this.issuersConfig.has(prop.issuer)) {\n                    throw new ParameterValidationError(`issuer ${prop.issuer} supplied multiple times`);\n                }\n                this.issuersConfig.set(prop.issuer, this.withJwksUri(prop));\n            }\n        }\n        else {\n            this.issuersConfig.set(verifyProperties.issuer, this.withJwksUri(verifyProperties));\n        }\n    }\n    get expectedIssuers() {\n        return Array.from(this.issuersConfig.keys());\n    }\n    getIssuerConfig(issuer) {\n        if (!issuer) {\n            if (this.issuersConfig.size !== 1) {\n                throw new ParameterValidationError(\"issuer must be provided\");\n            }\n            issuer = this.issuersConfig.keys().next().value;\n        }\n        const config = this.issuersConfig.get(issuer);\n        if (!config) {\n            throw new ParameterValidationError(`issuer not configured: ${issuer}`);\n        }\n        return config;\n    }\n    /**\n     * This method loads a JWKS that you provide, into the JWKS cache, so that it is\n     * available for JWT verification. Use this method to speed up the first JWT verification\n     * (when the JWKS would otherwise have to be downloaded from the JWKS uri), or to provide the JWKS\n     * in case the JwtVerifier does not have internet access to download the JWKS\n     *\n     * @param jwksThe JWKS\n     * @param issuer The issuer for which you want to cache the JWKS\n     *  Supply this field, if you instantiated the JwtVerifier with multiple issuers\n     * @returns void\n     */\n    cacheJwks(...[jwks, issuer]) {\n        const issuerConfig = this.getIssuerConfig(issuer);\n        this.jwksCache.addJwks(issuerConfig.jwksUri, jwks);\n        this.publicKeyCache.clearCache(issuerConfig.issuer);\n    }\n    /**\n     * Hydrate the JWKS cache for (all of) the configured issuer(s).\n     * This will fetch and cache the latest and greatest JWKS for concerned issuer(s).\n     *\n     * @param issuer The issuer to fetch the JWKS for\n     * @returns void\n     */\n    async hydrate() {\n        const jwksFetches = this.expectedIssuers\n            .map((issuer) => this.getIssuerConfig(issuer).jwksUri)\n            .map((jwksUri) => this.jwksCache.getJwks(jwksUri));\n        await Promise.all(jwksFetches);\n    }\n    /**\n     * Verify (synchronously) a JWT that is signed using RS256 / RS384 / RS512.\n     *\n     * @param jwt The JWT, as string\n     * @param props Verification properties\n     * @returns The payload of the JWT\u2013\u2013if the JWT is valid, otherwise an error is thrown\n     */\n    verifySync(...[jwt, properties]) {\n        const { decomposedJwt, jwksUri, verifyProperties } = this.getVerifyParameters(jwt, properties);\n        return this.verifyDecomposedJwtSync(decomposedJwt, jwksUri, verifyProperties);\n    }\n    /**\n     * Verify (synchronously) an already decomposed JWT, that is signed using RS256 / RS384 / RS512.\n     *\n     * @param decomposedJwt The decomposed Jwt\n     * @param jwk The JWK to verify the JWTs signature with\n     * @param verifyProperties The properties to use for verification\n     * @returns The payload of the JWT\u2013\u2013if the JWT is valid, otherwise an error is thrown\n     */\n    verifyDecomposedJwtSync(decomposedJwt, jwksUri, verifyProperties) {\n        const jwk = this.jwksCache.getCachedJwk(jwksUri, decomposedJwt);\n        return verifyDecomposedJwtSync(decomposedJwt, jwk, verifyProperties, this.publicKeyCache.transformJwkToKeyObjectSync.bind(this.publicKeyCache));\n    }\n    /**\n     * Verify (asynchronously) a JWT that is signed using RS256 / RS384 / RS512.\n     * This call is asynchronous, and the JWKS will be fetched from the JWKS uri,\n     * in case it is not yet available in the cache.\n     *\n     * @param jwt The JWT, as string\n     * @param props Verification properties\n     * @returns Promise that resolves to the payload of the JWT\u2013\u2013if the JWT is valid, otherwise the promise rejects\n     */\n    async verify(...[jwt, properties]) {\n        const { decomposedJwt, jwksUri, verifyProperties } = this.getVerifyParameters(jwt, properties);\n        return this.verifyDecomposedJwt(decomposedJwt, jwksUri, verifyProperties);\n    }\n    /**\n     * Verify (asynchronously) an already decomposed JWT, that is signed using RS256 / RS384 / RS512.\n     *\n     * @param decomposedJwt The decomposed Jwt\n     * @param jwk The JWK to verify the JWTs signature with\n     * @param verifyProperties The properties to use for verification\n     * @returns The payload of the JWT\u2013\u2013if the JWT is valid, otherwise an error is thrown\n     */\n    verifyDecomposedJwt(decomposedJwt, jwksUri, verifyProperties) {\n        return verifyDecomposedJwt(decomposedJwt, jwksUri, verifyProperties, this.jwksCache.getJwk.bind(this.jwksCache), this.publicKeyCache.transformJwkToKeyObjectAsync.bind(this.publicKeyCache));\n    }\n    /**\n     * Get the verification parameters to use, by merging the issuer configuration,\n     * with the overriding properties that are now provided\n     *\n     * @param jwt: the JWT that is going to be verified\n     * @param verifyProperties: the overriding properties, that override the issuer configuration\n     * @returns The merged verification parameters\n     */\n    getVerifyParameters(jwt, verifyProperties) {\n        const decomposedJwt = decomposeJwt(jwt);\n        assertStringArrayContainsString(\"Issuer\", decomposedJwt.payload.iss, this.expectedIssuers, JwtInvalidIssuerError);\n        const issuerConfig = this.getIssuerConfig(decomposedJwt.payload.iss);\n        return {\n            decomposedJwt,\n            jwksUri: issuerConfig.jwksUri,\n            verifyProperties: {\n                ...issuerConfig,\n                ...verifyProperties,\n            },\n        };\n    }\n    /**\n     * Get issuer config with JWKS URI, by adding a default JWKS URI if needed\n     *\n     * @param config: the issuer config.\n     * @returns The config with JWKS URI\n     */\n    withJwksUri(config) {\n        if (config.jwksUri) {\n            return config;\n        }\n        const issuerUri = new URL(config.issuer).pathname.replace(/\\/$/, \"\");\n        return {\n            jwksUri: new URL(`${issuerUri}/.well-known/jwks.json`, config.issuer)\n                .href,\n            ...config,\n        };\n    }\n}\n/**\n * Class representing a verifier for JWTs signed with RSA (e.g. RS256 / RS384 / RS512)\n */\nexport class JwtRsaVerifier extends JwtRsaVerifierBase {\n    // eslint-disable-next-line @typescript-eslint/explicit-module-boundary-types\n    static create(verifyProperties, additionalProperties) {\n        return new this(verifyProperties, additionalProperties?.jwksCache);\n    }\n}\n/**\n * Class representing a cache of RSA public keys in native key object format\n *\n * Because it takes a bit of compute time to turn a JWK into native key object format,\n * we want to cache this computation.\n */\nexport class KeyObjectCache {\n    constructor(transformJwkToKeyObjectSyncFn = nodeWebCompat.transformJwkToKeyObjectSync, transformJwkToKeyObjectAsyncFn = nodeWebCompat.transformJwkToKeyObjectAsync) {\n        this.transformJwkToKeyObjectSyncFn = transformJwkToKeyObjectSyncFn;\n        this.transformJwkToKeyObjectAsyncFn = transformJwkToKeyObjectAsyncFn;\n        this.publicKeys = new Map();\n    }\n    /**\n     * Transform the JWK into an RSA public key in native key object format.\n     * If the transformed JWK is already in the cache, it is returned from the cache instead.\n     *\n     * @param jwk: the JWK\n     * @param jwtHeaderAlg: the alg from the JWT header (used if absent on JWK)\n     * @param issuer: the issuer that uses the JWK for signing JWTs (used for caching the transformation)\n     * @returns the RSA public key in native key object format\n     */\n    transformJwkToKeyObjectSync(jwk, jwtHeaderAlg, issuer) {\n        const alg = jwk.alg ?? jwtHeaderAlg;\n        if (!issuer || !jwk.kid || !alg) {\n            return this.transformJwkToKeyObjectSyncFn(jwk, alg, issuer);\n        }\n        const fromCache = this.publicKeys.get(issuer)?.get(jwk.kid)?.get(alg);\n        if (fromCache)\n            return fromCache;\n        const publicKey = this.transformJwkToKeyObjectSyncFn(jwk, alg, issuer);\n        this.putKeyObjectInCache(issuer, jwk.kid, alg, publicKey);\n        return publicKey;\n    }\n    /**\n     * Transform the JWK into an RSA public key in native key object format (async).\n     * If the transformed JWK is already in the cache, it is returned from the cache instead.\n     *\n     * @param jwk: the JWK\n     * @param jwtHeaderAlg: the alg from the JWT header (used if absent on JWK)\n     * @param issuer: the issuer that uses the JWK for signing JWTs (used for caching the transformation)\n     * @returns the RSA public key in native key object format\n     */\n    async transformJwkToKeyObjectAsync(jwk, jwtHeaderAlg, issuer) {\n        const alg = jwk.alg ?? jwtHeaderAlg;\n        if (!issuer || !jwk.kid || !alg) {\n            return this.transformJwkToKeyObjectAsyncFn(jwk, alg, issuer);\n        }\n        const fromCache = this.publicKeys.get(issuer)?.get(jwk.kid)?.get(alg);\n        if (fromCache)\n            return fromCache;\n        const publicKey = await this.transformJwkToKeyObjectAsyncFn(jwk, alg, issuer);\n        this.putKeyObjectInCache(issuer, jwk.kid, alg, publicKey);\n        return publicKey;\n    }\n    putKeyObjectInCache(issuer, kid, alg, publicKey) {\n        const cachedIssuer = this.publicKeys.get(issuer);\n        const cachedIssuerKid = cachedIssuer?.get(kid);\n        if (cachedIssuerKid) {\n            cachedIssuerKid.set(alg, publicKey);\n        }\n        else if (cachedIssuer) {\n            cachedIssuer.set(kid, new Map([[alg, publicKey]]));\n        }\n        else {\n            this.publicKeys.set(issuer, new Map([[kid, new Map([[alg, publicKey]])]]));\n        }\n    }\n    clearCache(issuer) {\n        this.publicKeys.delete(issuer);\n    }\n}\n", "// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n// SPDX-License-Identifier: Apache-2.0\nimport { CognitoJwtInvalidClientIdError, CognitoJwtInvalidGroupError, CognitoJwtInvalidTokenUseError, JwtInvalidClaimError, ParameterValidationError, } from \"./error.js\";\nimport { JwtRsaVerifierBase } from \"./jwt-rsa.js\";\nimport { assertStringArrayContainsString, assertStringEquals, assertStringArraysOverlap, } from \"./assert.js\";\n/**\n * Validate claims of a decoded Cognito JWT.\n * This function throws an error in case there's any validation issue.\n *\n * @param payload - The JSON parsed payload of the Cognito JWT\n * @param options - Validation options\n * @param options.groups - The cognito groups, of which at least one must be present in the JWT's cognito:groups claim\n * @param options.tokenUse - The required token use of the JWT: \"id\" or \"access\"\n * @param options.clientId - The required clientId of the JWT. May be an array of string, of which at least one must match\n * @returns void\n */\nexport function validateCognitoJwtFields(payload, options) {\n    // Check groups\n    if (options.groups != null) {\n        assertStringArraysOverlap(\"Cognito group\", payload[\"cognito:groups\"], options.groups, CognitoJwtInvalidGroupError);\n    }\n    // Check token use\n    assertStringArrayContainsString(\"Token use\", payload.token_use, [\"id\", \"access\"], CognitoJwtInvalidTokenUseError);\n    if (options.tokenUse !== null) {\n        if (options.tokenUse === undefined) {\n            throw new ParameterValidationError(\"tokenUse must be provided or set to null explicitly\");\n        }\n        assertStringEquals(\"Token use\", payload.token_use, options.tokenUse, CognitoJwtInvalidTokenUseError);\n    }\n    // Check clientId aka audience\n    if (options.clientId !== null) {\n        if (options.clientId === undefined) {\n            throw new ParameterValidationError(\"clientId must be provided or set to null explicitly\");\n        }\n        if (payload.token_use === \"id\") {\n            assertStringArrayContainsString('Client ID (\"audience\")', payload.aud, options.clientId, CognitoJwtInvalidClientIdError);\n        }\n        else {\n            assertStringArrayContainsString(\"Client ID\", payload.client_id, options.clientId, CognitoJwtInvalidClientIdError);\n        }\n    }\n}\n/**\n * Class representing a verifier for JWTs signed by Amazon Cognito\n */\nexport class CognitoJwtVerifier extends JwtRsaVerifierBase {\n    constructor(props, jwksCache) {\n        const issuerConfig = Array.isArray(props)\n            ? props.map((p) => ({\n                ...p,\n                ...CognitoJwtVerifier.parseUserPoolId(p.userPoolId),\n                audience: null, // checked instead by validateCognitoJwtFields\n            }))\n            : {\n                ...props,\n                ...CognitoJwtVerifier.parseUserPoolId(props.userPoolId),\n                audience: null, // checked instead by validateCognitoJwtFields\n            };\n        super(issuerConfig, jwksCache);\n    }\n    /**\n     * Parse a User Pool ID, to extract the issuer and JWKS URI\n     *\n     * @param userPoolId The User Pool ID\n     * @returns The issuer and JWKS URI for the User Pool\n     */\n    static parseUserPoolId(userPoolId) {\n        // Disable safe regexp check as userPoolId is provided by developer, i.e. is not user input\n        // eslint-disable-next-line security/detect-unsafe-regex\n        const match = userPoolId.match(/^(?<region>(\\w+-)?\\w+-\\w+-\\d)+_\\w+$/);\n        if (!match) {\n            throw new ParameterValidationError(`Invalid Cognito User Pool ID: ${userPoolId}`);\n        }\n        const region = match.groups.region;\n        const issuer = `https://cognito-idp.${region}.amazonaws.com/${userPoolId}`;\n        return {\n            issuer,\n            jwksUri: `${issuer}/.well-known/jwks.json`,\n        };\n    }\n    // eslint-disable-next-line @typescript-eslint/explicit-module-boundary-types\n    static create(verifyProperties, additionalProperties) {\n        return new this(verifyProperties, additionalProperties?.jwksCache);\n    }\n    /**\n     * Verify (synchronously) a JWT that is signed by Amazon Cognito.\n     *\n     * @param jwt The JWT, as string\n     * @param props Verification properties\n     * @returns The payload of the JWT\u2013\u2013if the JWT is valid, otherwise an error is thrown\n     */\n    verifySync(...[jwt, properties]) {\n        const { decomposedJwt, jwksUri, verifyProperties } = this.getVerifyParameters(jwt, properties);\n        this.verifyDecomposedJwtSync(decomposedJwt, jwksUri, verifyProperties);\n        try {\n            validateCognitoJwtFields(decomposedJwt.payload, verifyProperties);\n        }\n        catch (err) {\n            if (verifyProperties.includeRawJwtInErrors &&\n                err instanceof JwtInvalidClaimError) {\n                throw err.withRawJwt(decomposedJwt);\n            }\n            throw err;\n        }\n        return decomposedJwt.payload;\n    }\n    /**\n     * Verify (asynchronously) a JWT that is signed by Amazon Cognito.\n     * This call is asynchronous, and the JWKS will be fetched from the JWKS uri,\n     * in case it is not yet available in the cache.\n     *\n     * @param jwt The JWT, as string\n     * @param props Verification properties\n     * @returns Promise that resolves to the payload of the JWT\u2013\u2013if the JWT is valid, otherwise the promise rejects\n     */\n    async verify(...[jwt, properties]) {\n        const { decomposedJwt, jwksUri, verifyProperties } = this.getVerifyParameters(jwt, properties);\n        await this.verifyDecomposedJwt(decomposedJwt, jwksUri, verifyProperties);\n        try {\n            validateCognitoJwtFields(decomposedJwt.payload, verifyProperties);\n        }\n        catch (err) {\n            if (verifyProperties.includeRawJwtInErrors &&\n                err instanceof JwtInvalidClaimError) {\n                throw err.withRawJwt(decomposedJwt);\n            }\n            throw err;\n        }\n        return decomposedJwt.payload;\n    }\n    /**\n     * This method loads a JWKS that you provide, into the JWKS cache, so that it is\n     * available for JWT verification. Use this method to speed up the first JWT verification\n     * (when the JWKS would otherwise have to be downloaded from the JWKS uri), or to provide the JWKS\n     * in case the JwtVerifier does not have internet access to download the JWKS\n     *\n     * @param jwks The JWKS\n     * @param userPoolId The userPoolId for which you want to cache the JWKS\n     *  Supply this field, if you instantiated the CognitoJwtVerifier with multiple userPoolIds\n     * @returns void\n     */\n    cacheJwks(...[jwks, userPoolId]) {\n        let issuer;\n        if (userPoolId !== undefined) {\n            issuer = CognitoJwtVerifier.parseUserPoolId(userPoolId).issuer;\n        }\n        else if (this.expectedIssuers.length > 1) {\n            throw new ParameterValidationError(\"userPoolId must be provided\");\n        }\n        const issuerConfig = this.getIssuerConfig(issuer);\n        super.cacheJwks(jwks, issuerConfig.issuer);\n    }\n}\n", "// GLOBAL\n\nexport interface AnyObject {\n  [key: string]: any;\n}\n\nexport interface CompletionData {\n  [key: string]: boolean | CompletionData;\n}\n\n/**\n * Abridged copy of the type used by `aws-lambda@1.0.7` (from `@types/aws-lambda@8.10.88`)\n * We only this package for these types, and we use only a subset of the\n * properties. Since `aws-lambda` depends on `aws-sdk` (that is, SDK v2),\n * we can save ourselves a big dependency with this small redundancy.\n */\n\nexport interface APIGatewayProxyEventPathParameters {\n  [name: string]: string | undefined;\n}\n\nexport interface APIGatewayProxyEvent {\n  body: string | null;\n  headers: Record<string, string | undefined>;\n  multiValueHeaders: Record<string, string | undefined>;\n  httpMethod: string;\n  isBase64Encoded: boolean;\n  path: string;\n  pathParameters: Record<string, string | undefined> | null;\n  queryStringParameters: Record<string, string | undefined> | null;\n  multiValueQueryStringParameters: Record<string, string | undefined> | null;\n  stageVariables: Record<string, string | undefined> | null;\n  /** The context is complicated, and we don't (as of 2023) use it at all. */\n  requestContext: any;\n  resource: string;\n}\n\n// ALERTS\n\nexport enum AlertTypes {\n  ERROR = \"error\",\n  INFO = \"info\",\n  SUCCESS = \"success\",\n  WARNING = \"warning\",\n}\n\n// TIME\n\nexport interface DateShape {\n  year: number;\n  month: number;\n  day: number;\n}\n\nexport interface TimeShape {\n  hour: number;\n  minute: number;\n  second: number;\n}\n\n// OTHER\n\nexport interface CustomHtmlElement {\n  type: string;\n  content: string | any;\n  as?: string;\n  props?: AnyObject;\n}\n\nexport interface ErrorVerbiage {\n  title: string;\n  description: string | CustomHtmlElement[];\n}\n\nconst states = [\n  \"AL\",\n  \"AK\",\n  \"AS\", // American Samoa\n  \"AZ\",\n  \"AR\",\n  \"CA\",\n  \"CO\",\n  \"CT\",\n  \"DE\",\n  \"DC\",\n  \"FL\",\n  \"FM\", // Federated States of Micronesia\n  \"GA\",\n  \"GU\", // Guam\n  \"HI\",\n  \"ID\",\n  \"IL\",\n  \"IN\",\n  \"IA\",\n  \"KS\",\n  \"KY\",\n  \"LA\",\n  \"ME\",\n  \"MH\", // Marshall Islands\n  \"MD\",\n  \"MA\",\n  \"MI\",\n  \"MN\",\n  \"MS\",\n  \"MO\",\n  \"MP\", // Northern Mariana Islands\n  \"MT\",\n  \"NE\",\n  \"NV\",\n  \"NH\",\n  \"NJ\",\n  \"NM\",\n  \"NY\",\n  \"NC\",\n  \"ND\",\n  \"OH\",\n  \"OK\",\n  \"OR\",\n  \"PA\",\n  \"PR\",\n  \"PW\", // Palau\n  \"RI\",\n  \"SC\",\n  \"SD\",\n  \"TN\",\n  \"TX\",\n  \"UT\",\n  \"VT\",\n  \"VA\",\n  \"VI\", // Virgin Islands\n  \"WA\",\n  \"WV\",\n  \"WI\",\n  \"WY\",\n] as const;\nexport type State = typeof states[number];\n\nexport const isState = (state: unknown): state is State => {\n  return states.includes(state as State);\n};\n\nexport interface FormTemplateVersion {\n  md5Hash: string;\n  versionNumber: number;\n  id: string;\n  lastAltered: string;\n  reportType: string;\n}\n\nexport const enum TemplateKeys {\n  WP = \"templates/MFP-Work-Plan-Help-File.pdf\",\n  SAR = \"templates/MFP-Semi-Annual-Rprt-Help-File.pdf\",\n}\n\n/**\n * S3Create event\n * https://docs.aws.amazon.com/AmazonS3/latest/dev/notification-content-structure.html\n */\n\nexport interface S3EventRecordGlacierRestoreEventData {\n  lifecycleRestorationExpiryTime: string;\n  lifecycleRestoreStorageClass: string;\n}\nexport interface S3EventRecordGlacierEventData {\n  restoreEventData: S3EventRecordGlacierRestoreEventData;\n}\n\nexport interface S3EventRecord {\n  eventVersion: string;\n  eventSource: string;\n  awsRegion: string;\n  eventTime: string;\n  eventName: string;\n  userIdentity: {\n    principalId: string;\n  };\n  requestParameters: {\n    sourceIPAddress: string;\n  };\n  responseElements: {\n    \"x-amz-request-id\": string;\n    \"x-amz-id-2\": string;\n  };\n  s3: {\n    s3SchemaVersion: string;\n    configurationId: string;\n    bucket: {\n      name: string;\n      ownerIdentity: {\n        principalId: string;\n      };\n      arn: string;\n    };\n    object: {\n      key: string;\n      size: number;\n      eTag: string;\n      versionId?: string | undefined;\n      sequencer: string;\n    };\n  };\n  glacierEventData?: S3EventRecordGlacierEventData | undefined;\n}\n\n/**\n * Use this type to create a type guard for filtering arrays of objects\n * by the presence of certain attributes.\n *\n * @example\n * interface Foo {\n *    bar: string;\n *    baz?: string;\n *    buzz?: string;\n *    bizz?: string;\n * }\n * type RequireBaz = SomeRequired<Foo, 'baz'>\n * const array: Foo[] = [\n *  { bar: 'always here' },\n *  { bar: 'always here', baz: 'sometimes here' }\n * ]\n * array.filter((f): f is RequireBaz => typeof f.baz !== 'undefined' )\n * // `array`'s type now shows bar and baz as required.\n * array.map((f) => return f.baz)\n */\nexport type SomeRequired<T, K extends keyof T> = Required<Pick<T, K>> &\n  Omit<T, K>;\n\n/**\n * Instructs Typescript to complain if it detects that this function may be reachable.\n * Useful for the default branch of a switch statement that verifiably covers every case.\n */\nexport const assertExhaustive = (_: never): void => {};\n", "import { FormJson } from \"./formFields\";\nimport { AnyObject, CustomHtmlElement } from \"./other\";\n\n// REPORT STRUCTURE\n\nexport interface ReportJson {\n  id?: string;\n  type: ReportType;\n  name: string;\n  basePath: string;\n  routes: ReportRoute[];\n  validationSchema?: AnyObject;\n  /**\n   * The validationJson property is populated at the moment any form template\n   * is stored in S3 for the first time. It will be populated from that moment on.\n   */\n  validationJson?: AnyObject;\n}\n\nexport type ReportRoute = ReportRouteWithForm | ReportRouteWithoutForm;\n\nexport interface ReportRouteBase {\n  name: string;\n  path: string;\n  pageType?: string;\n  conditionallyRender?: string;\n}\n\nexport type ReportRouteWithForm =\n  | StandardReportPageShape\n  | DrawerReportPageShape\n  | ModalDrawerReportPageShape\n  | ModalOverlayReportPageShape\n  | OverlayModalPageShape\n  | EntityDetailsOverlayShape\n  | DynamicModalOverlayReportPageShape;\n\nexport interface ReportPageShapeBase extends ReportRouteBase {\n  children?: never;\n  verbiage: ReportPageVerbiage;\n}\n\nexport interface StandardReportPageShape extends ReportPageShapeBase {\n  form: FormJson;\n  dashboard?: never;\n  modalForm?: never;\n  drawerForm?: never;\n  entityType?: never;\n  entitySteps?: never;\n  template?: never;\n  initiatives?: never;\n  objectiveCards?: never;\n}\n\nexport interface DrawerReportPageShape extends ReportPageShapeBase {\n  entityType: string;\n  verbiage: DrawerReportPageVerbiage;\n  drawerForm: FormJson;\n  modalForm?: never;\n  form?: never;\n  entitySteps?: never;\n  dashboard?: never;\n  template?: never;\n  initiatives?: never;\n  objectiveCards?: never;\n}\n\nexport interface ModalDrawerReportPageShape extends ReportPageShapeBase {\n  entityType: string;\n  verbiage: ModalDrawerReportPageVerbiage;\n  modalForm: FormJson;\n  drawerForm: FormJson;\n  form?: never;\n  entitySteps?: never;\n  dashboard?: never;\n  template?: never;\n  initiatives?: never;\n  objectiveCards?: never;\n}\n\nexport interface ModalOverlayReportPageShape extends ReportPageShapeBase {\n  initiativeId: string | undefined;\n  entityType: string;\n  entityInfo?: string[];\n  verbiage: ModalOverlayReportPageVerbiage;\n  modalForm: FormJson;\n  drawerForm?: never;\n  form?: never;\n  dashboard: EntityDetailsDashboardOverlayShape;\n  entitySteps?: (EntityDetailsOverlayShape | OverlayModalPageShape)[];\n  template?: never;\n  initiatives?: never;\n  objectiveCards?: never;\n}\n\nexport interface DynamicModalOverlayReportPageShape\n  extends ReportPageShapeBase {\n  entityType: string;\n  entityInfo: string[];\n  verbiage: ModalOverlayReportPageVerbiage;\n  drawerForm?: never;\n  modalForm?: never;\n  form?: never;\n  entitySteps?: never;\n  dashboard?: never;\n  initiatives: {\n    initiativeId: string;\n    name: string;\n    topic: string;\n    dashboard: FormJson;\n    entitySteps: (EntityDetailsOverlayShape | OverlayModalPageShape)[];\n  }[];\n  objectiveCards?: never;\n  /** Only used during form template transformation; will be absent after transformation */\n  template?: AnyObject;\n}\n\nexport interface OverlayModalPageShape extends ReportPageShapeBase {\n  entityType: string;\n  stepName: string;\n  hint: string;\n  verbiage: ModalOverlayReportPageVerbiage;\n  modalForm: FormJson;\n  drawerForm?: FormJson;\n  form?: never;\n  entitySteps?: never;\n  dashboard?: never;\n  template?: never;\n  initiatives?: never;\n  objectiveCards?: {\n    modalForm?: FormJson;\n  }[];\n}\n\nexport interface EntityDetailsOverlayShape extends ReportPageShapeBase {\n  stepName: string;\n  hint: string;\n  form: FormJson;\n  verbiage: EntityOverlayPageVerbiage;\n  entityType?: never;\n  dashboard?: never;\n  modalForm?: never;\n  drawerForm?: never;\n  entitySteps?: never;\n  template?: never;\n  initiatives?: never;\n  objectiveCards?: {\n    modalForm?: FormJson;\n  }[];\n}\n\nexport interface EntityDetailsDashboardOverlayShape\n  extends ReportPageShapeBase {\n  dashboard?: never;\n  template?: never;\n  initiatives?: never;\n  objectiveCards?: never;\n}\n\nexport interface ReportRouteWithoutForm extends ReportRouteBase {\n  children?: ReportRoute[];\n  pageType?: string;\n  entityType?: never;\n  verbiage?: never;\n  modalForm?: never;\n  drawerForm?: never;\n  form?: never;\n  entitySteps?: never;\n  dashboard?: never;\n  template?: never;\n  initiatives?: never;\n  objectiveCards?: never;\n}\n\nexport interface ReportPageVerbiage {\n  intro: {\n    section: string;\n    subsection?: string;\n    hint?: string;\n    info?: string | CustomHtmlElement[];\n  };\n  closeOutWarning?: AnyObject;\n  closeOutModal?: AnyObject;\n}\n\nexport interface DrawerReportPageVerbiage extends ReportPageVerbiage {\n  dashboardTitle: string;\n  countEntitiesInTitle?: boolean;\n  drawerTitle: string;\n  drawerInfo?: CustomHtmlElement[];\n  missingEntityMessage?: CustomHtmlElement[];\n}\n\nexport interface ModalDrawerReportPageVerbiage\n  extends DrawerReportPageVerbiage {\n  addEntityButtonText: string;\n  editEntityButtonText: string;\n  readOnlyEntityButtonText: string;\n  addEditModalAddTitle: string;\n  addEditModalEditTitle: string;\n  addEditModalMessage: string;\n  deleteEntityButtonAltText: string;\n  deleteModalTitle: string;\n  deleteModalConfirmButtonText: string;\n  deleteModalWarning: string;\n  entityUnfinishedMessage: string;\n  enterEntityDetailsButtonText: string;\n  readOnlyEntityDetailsButtonText: string;\n  editEntityDetailsButtonText: string;\n}\n\nexport interface ModalOverlayReportPageVerbiage\n  extends EntityOverlayPageVerbiage {\n  addEntityButtonText: string;\n  dashboardTitle: string;\n  countEntitiesInTitle: boolean;\n  tableHeader: string;\n  addEditModalHint: string;\n  emptyDashboardText: string;\n}\n\nexport interface EntityOverlayPageVerbiage extends ReportPageVerbiage {\n  closeOutWarning?: {\n    title?: string;\n    description?: string;\n  };\n  closeOutModal?: {\n    closeOutModalButtonText?: string;\n    closeOutModalTitle?: string;\n    closeOutModalBodyText?: string;\n    closeOutModalConfirmButtonText?: string;\n  };\n}\n\nexport enum ReportType {\n  WP = \"WP\",\n  SAR = \"SAR\",\n}\n/**\n * Check if unknown value is a report type\n *\n * @param reportType possible report type value\n * @returns type assertion for value\n */\nexport function isReportType(reportType: unknown): reportType is ReportType {\n  return Object.values(ReportType).includes(reportType as ReportType);\n}\n", "/**\n * The response for a successful request.\n * Should include a body for GET, PUT, or POST.\n * Need not include a body for DELETE\n */\nexport const ok = (body?: Object) => new HttpResponse(StatusCodes.Ok, body);\n\n/**\n * The response for a successful POST or PUT request,\n * which resulted in the creation of a new resource.\n */\nexport const created = (body: Object) =>\n  new HttpResponse(StatusCodes.Created, body);\n\n/**\n * The response for a successful request which returns no content.\n */\nexport const noContent = (body?: Object) =>\n  new HttpResponse(StatusCodes.NoContent, body);\n\n/**\n * The response for a failed request, due to client-side issues.\n * Typically indicates a missing parameter or malformed body.\n */\nexport const badRequest = (body?: Object) =>\n  new HttpResponse(StatusCodes.BadRequest, body);\n\n/**\n * The response for a client without any authorization.\n * Typically indicates an issue with the request's headers or token.\n *\n * Note: The usual name for HTTP 401 is \"Unauthorized\", but that's misleading.\n * Authentication is for identity; authorization is for permissions.\n */\nexport const unauthenticated = (body?: Object) =>\n  new HttpResponse(StatusCodes.Unauthenticated, body);\n\n/**\n * The response for a client without sufficient permissions.\n * This is specific to the requested operation.\n * For example, a regular user requesting an admin-only endpoint.\n */\nexport const forbidden = (body?: Object) =>\n  new HttpResponse(StatusCodes.Forbidden, body);\n\n/**\n * The response for a request that assumes the existence of a missing resource.\n * For example, attempting to submit a report that isn't in the database.\n */\nexport const notFound = (body?: Object) =>\n  new HttpResponse(StatusCodes.NotFound, body);\n\n/**\n * The response for a request that assumes the server is in a different state.\n * For example, attempting to submit a report that's already submitted.\n */\nexport const conflict = (body?: Object) =>\n  new HttpResponse(StatusCodes.Conflict, body);\n\n/**\n * The response for a request that errored out on the server side.\n * Typically indicates there is nothing the client can do to resolve the issue.\n */\nexport const internalServerError = (body?: Object) =>\n  new HttpResponse(StatusCodes.InternalServerError, body);\n\n/**\n * Note: Production code shouldn't need to reference this directly.\n * Use a helper method instead.\n *\n * This enum is listed mainly for the purpose of unit testing.\n */\nexport enum StatusCodes {\n  Ok = 200,\n  Created = 201,\n  NoContent = 204,\n  BadRequest = 400,\n  Unauthenticated = 401,\n  Forbidden = 403,\n  NotFound = 404,\n  Conflict = 409,\n  InternalServerError = 500,\n}\n\n/**\n * Note: Production code shouldn't need to reference this directly.\n * Use a helper method instead.\n */\nexport class HttpResponse {\n  readonly statusCode: number;\n  readonly body: string | undefined;\n  readonly headers = {\n    \"Access-Control-Allow-Origin\": \"*\",\n    \"Access-Control-Allow-Credentials\": true,\n  };\n  constructor(statusCode: number, body?: Object | undefined) {\n    this.statusCode = statusCode;\n    if (body !== undefined) {\n      this.body = JSON.stringify(body);\n    }\n  }\n}\n", "import { ReportType } from \"../types/reports\";\n\nexport const error = {\n  // generic errors\n  UNAUTHORIZED: \"User is not authorized to access this resource.\",\n  NO_KEY: \"Must provide key for table.\",\n  MISSING_DATA: \"Missing required data.\",\n  INVALID_DATA: \"Provided data is not valid.\",\n  NO_MATCHING_RECORD: \"No matching record found.\",\n  SERVER_ERROR: \"An unspecified server error occured.\",\n  // bucket errors\n  S3_OBJECT_CREATION_ERROR: \"Report could not be created due to an S3 error.\",\n  S3_OBJECT_UPDATE_ERROR: \"Report could not be updated due to an S3 error.\",\n  S3_OBJECT_GET_ERROR: \"Error while fetching report.\",\n  // dynamo errors\n  DYNAMO_CREATION_ERROR: \"Report could not be created due to a database error.\",\n  DYNAMO_UPDATE_ERROR: \"Report could not be updated due to a database error.\",\n  // template errors\n  NO_TEMPLATE_NAME: \"Must request template for download.\",\n  INVALID_TEMPLATE_NAME: \"Requested template does not exist or does not match.\",\n  NOT_IN_DATABASE: \"Record not found in database.\",\n  UNABLE_TO_COPY:\n    \"Unable to copy over report if todays date is in the same period and year as a previous report.\",\n  MISSING_FORM_TEMPLATE: \"Form Template not found in S3.\",\n  MISSING_FIELD_DATA: \"Field Data not found in S3.\",\n  NO_WORKPLANS_FOUND: \"No record of Work Plans found in database\",\n  // admin action errors\n  ALREADY_ARCHIVED: \"Cannot update archived report.\",\n  ALREADY_LOCKED: \"Cannot update locked report.\",\n  REPORT_INCOMPLETE: \"Cannot submit incomplete form.\",\n} as const;\n\nexport const buckets = {\n  FORM_TEMPLATE: \"formTemplates\",\n  FIELD_DATA: \"fieldData\",\n};\n\n// STATES\nexport enum States {\n  AL = \"Alabama\",\n  AK = \"Alaska\",\n  AS = \"American Samoa\",\n  AZ = \"Arizona\",\n  AR = \"Arkansas\",\n  CA = \"California\",\n  CO = \"Colorado\",\n  CT = \"Connecticut\",\n  DE = \"Delaware\",\n  DC = \"District of Columbia\",\n  FM = \"Federated States of Micronesia\",\n  FL = \"Florida\",\n  GA = \"Georgia\",\n  GU = \"Guam\",\n  HI = \"Hawaii\",\n  ID = \"Idaho\",\n  IL = \"Illinois\",\n  IN = \"Indiana\",\n  IA = \"Iowa\",\n  KS = \"Kansas\",\n  KY = \"Kentucky\",\n  LA = \"Louisiana\",\n  ME = \"Maine\",\n  MH = \"Marshall Islands\",\n  MD = \"Maryland\",\n  MA = \"Massachusetts\",\n  MI = \"Michigan\",\n  MN = \"Minnesota\",\n  MS = \"Mississippi\",\n  MO = \"Missouri\",\n  MT = \"Montana\",\n  NE = \"Nebraska\",\n  NV = \"Nevada\",\n  NH = \"New Hampshire\",\n  NJ = \"New Jersey\",\n  NM = \"New Mexico\",\n  NY = \"New York\",\n  NC = \"North Carolina\",\n  ND = \"North Dakota\",\n  MP = \"Northern Mariana Islands\",\n  OH = \"Ohio\",\n  OK = \"Oklahoma\",\n  OR = \"Oregon\",\n  PW = \"Palau\",\n  PA = \"Pennsylvania\",\n  PR = \"Puerto Rico\",\n  RI = \"Rhode Island\",\n  SC = \"South Carolina\",\n  SD = \"South Dakota\",\n  TN = \"Tennessee\",\n  TX = \"Texas\",\n  UT = \"Utah\",\n  VT = \"Vermont\",\n  VI = \"Virgin Islands\",\n  VA = \"Virginia\",\n  WA = \"Washington\",\n  WV = \"West Virginia\",\n  WI = \"Wisconsin\",\n  WY = \"Wyoming\",\n}\n\n// REPORTS\n\nexport const reportTables: { [key in ReportType]: string } = {\n  SAR: process.env.SarReportsTable!,\n  WP: process.env.WpReportsTable!,\n};\n\nexport const reportBuckets: { [key in ReportType]: string } = {\n  SAR: process.env.SAR_FORM_BUCKET!,\n  WP: process.env.WP_FORM_BUCKET!,\n};\n\nexport const reportNames: { [key in ReportType]: string } = {\n  SAR: \"SAR\",\n  WP: \"Work Plan\",\n};\n\nexport const tableTopics: { [key in ReportType]: string } = {\n  SAR: \"sar-reports\",\n  WP: \"wp-reports\",\n};\n\nexport const bucketTopics: { [key in ReportType]: string } = {\n  SAR: \"sar-form\",\n  WP: \"wp-form\",\n};\n\nexport const DEFAULT_TARGET_POPULATION_NAMES = [\n  \"Older adults\",\n  \"Individuals with physical disabilities (PD)\",\n  \"Individuals with intellectual and developmental disabilities (I/DD)\",\n  \"Individuals with mental health and substance use disorders (MH/SUD)\",\n  \"HCBS infrastructure/system-level development\",\n];\n", "/*! @license DOMPurify 3.2.4 | (c) Cure53 and other contributors | Released under the Apache license 2.0 and Mozilla Public License 2.0 | github.com/cure53/DOMPurify/blob/3.2.4/LICENSE */\n\nconst {\n  entries,\n  setPrototypeOf,\n  isFrozen,\n  getPrototypeOf,\n  getOwnPropertyDescriptor\n} = Object;\nlet {\n  freeze,\n  seal,\n  create\n} = Object; // eslint-disable-line import/no-mutable-exports\nlet {\n  apply,\n  construct\n} = typeof Reflect !== 'undefined' && Reflect;\nif (!freeze) {\n  freeze = function freeze(x) {\n    return x;\n  };\n}\nif (!seal) {\n  seal = function seal(x) {\n    return x;\n  };\n}\nif (!apply) {\n  apply = function apply(fun, thisValue, args) {\n    return fun.apply(thisValue, args);\n  };\n}\nif (!construct) {\n  construct = function construct(Func, args) {\n    return new Func(...args);\n  };\n}\nconst arrayForEach = unapply(Array.prototype.forEach);\nconst arrayLastIndexOf = unapply(Array.prototype.lastIndexOf);\nconst arrayPop = unapply(Array.prototype.pop);\nconst arrayPush = unapply(Array.prototype.push);\nconst arraySplice = unapply(Array.prototype.splice);\nconst stringToLowerCase = unapply(String.prototype.toLowerCase);\nconst stringToString = unapply(String.prototype.toString);\nconst stringMatch = unapply(String.prototype.match);\nconst stringReplace = unapply(String.prototype.replace);\nconst stringIndexOf = unapply(String.prototype.indexOf);\nconst stringTrim = unapply(String.prototype.trim);\nconst objectHasOwnProperty = unapply(Object.prototype.hasOwnProperty);\nconst regExpTest = unapply(RegExp.prototype.test);\nconst typeErrorCreate = unconstruct(TypeError);\n/**\n * Creates a new function that calls the given function with a specified thisArg and arguments.\n *\n * @param func - The function to be wrapped and called.\n * @returns A new function that calls the given function with a specified thisArg and arguments.\n */\nfunction unapply(func) {\n  return function (thisArg) {\n    for (var _len = arguments.length, args = new Array(_len > 1 ? _len - 1 : 0), _key = 1; _key < _len; _key++) {\n      args[_key - 1] = arguments[_key];\n    }\n    return apply(func, thisArg, args);\n  };\n}\n/**\n * Creates a new function that constructs an instance of the given constructor function with the provided arguments.\n *\n * @param func - The constructor function to be wrapped and called.\n * @returns A new function that constructs an instance of the given constructor function with the provided arguments.\n */\nfunction unconstruct(func) {\n  return function () {\n    for (var _len2 = arguments.length, args = new Array(_len2), _key2 = 0; _key2 < _len2; _key2++) {\n      args[_key2] = arguments[_key2];\n    }\n    return construct(func, args);\n  };\n}\n/**\n * Add properties to a lookup table\n *\n * @param set - The set to which elements will be added.\n * @param array - The array containing elements to be added to the set.\n * @param transformCaseFunc - An optional function to transform the case of each element before adding to the set.\n * @returns The modified set with added elements.\n */\nfunction addToSet(set, array) {\n  let transformCaseFunc = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : stringToLowerCase;\n  if (setPrototypeOf) {\n    // Make 'in' and truthy checks like Boolean(set.constructor)\n    // independent of any properties defined on Object.prototype.\n    // Prevent prototype setters from intercepting set as a this value.\n    setPrototypeOf(set, null);\n  }\n  let l = array.length;\n  while (l--) {\n    let element = array[l];\n    if (typeof element === 'string') {\n      const lcElement = transformCaseFunc(element);\n      if (lcElement !== element) {\n        // Config presets (e.g. tags.js, attrs.js) are immutable.\n        if (!isFrozen(array)) {\n          array[l] = lcElement;\n        }\n        element = lcElement;\n      }\n    }\n    set[element] = true;\n  }\n  return set;\n}\n/**\n * Clean up an array to harden against CSPP\n *\n * @param array - The array to be cleaned.\n * @returns The cleaned version of the array\n */\nfunction cleanArray(array) {\n  for (let index = 0; index < array.length; index++) {\n    const isPropertyExist = objectHasOwnProperty(array, index);\n    if (!isPropertyExist) {\n      array[index] = null;\n    }\n  }\n  return array;\n}\n/**\n * Shallow clone an object\n *\n * @param object - The object to be cloned.\n * @returns A new object that copies the original.\n */\nfunction clone(object) {\n  const newObject = create(null);\n  for (const [property, value] of entries(object)) {\n    const isPropertyExist = objectHasOwnProperty(object, property);\n    if (isPropertyExist) {\n      if (Array.isArray(value)) {\n        newObject[property] = cleanArray(value);\n      } else if (value && typeof value === 'object' && value.constructor === Object) {\n        newObject[property] = clone(value);\n      } else {\n        newObject[property] = value;\n      }\n    }\n  }\n  return newObject;\n}\n/**\n * This method automatically checks if the prop is function or getter and behaves accordingly.\n *\n * @param object - The object to look up the getter function in its prototype chain.\n * @param prop - The property name for which to find the getter function.\n * @returns The getter function found in the prototype chain or a fallback function.\n */\nfunction lookupGetter(object, prop) {\n  while (object !== null) {\n    const desc = getOwnPropertyDescriptor(object, prop);\n    if (desc) {\n      if (desc.get) {\n        return unapply(desc.get);\n      }\n      if (typeof desc.value === 'function') {\n        return unapply(desc.value);\n      }\n    }\n    object = getPrototypeOf(object);\n  }\n  function fallbackValue() {\n    return null;\n  }\n  return fallbackValue;\n}\n\nconst html$1 = freeze(['a', 'abbr', 'acronym', 'address', 'area', 'article', 'aside', 'audio', 'b', 'bdi', 'bdo', 'big', 'blink', 'blockquote', 'body', 'br', 'button', 'canvas', 'caption', 'center', 'cite', 'code', 'col', 'colgroup', 'content', 'data', 'datalist', 'dd', 'decorator', 'del', 'details', 'dfn', 'dialog', 'dir', 'div', 'dl', 'dt', 'element', 'em', 'fieldset', 'figcaption', 'figure', 'font', 'footer', 'form', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'head', 'header', 'hgroup', 'hr', 'html', 'i', 'img', 'input', 'ins', 'kbd', 'label', 'legend', 'li', 'main', 'map', 'mark', 'marquee', 'menu', 'menuitem', 'meter', 'nav', 'nobr', 'ol', 'optgroup', 'option', 'output', 'p', 'picture', 'pre', 'progress', 'q', 'rp', 'rt', 'ruby', 's', 'samp', 'section', 'select', 'shadow', 'small', 'source', 'spacer', 'span', 'strike', 'strong', 'style', 'sub', 'summary', 'sup', 'table', 'tbody', 'td', 'template', 'textarea', 'tfoot', 'th', 'thead', 'time', 'tr', 'track', 'tt', 'u', 'ul', 'var', 'video', 'wbr']);\nconst svg$1 = freeze(['svg', 'a', 'altglyph', 'altglyphdef', 'altglyphitem', 'animatecolor', 'animatemotion', 'animatetransform', 'circle', 'clippath', 'defs', 'desc', 'ellipse', 'filter', 'font', 'g', 'glyph', 'glyphref', 'hkern', 'image', 'line', 'lineargradient', 'marker', 'mask', 'metadata', 'mpath', 'path', 'pattern', 'polygon', 'polyline', 'radialgradient', 'rect', 'stop', 'style', 'switch', 'symbol', 'text', 'textpath', 'title', 'tref', 'tspan', 'view', 'vkern']);\nconst svgFilters = freeze(['feBlend', 'feColorMatrix', 'feComponentTransfer', 'feComposite', 'feConvolveMatrix', 'feDiffuseLighting', 'feDisplacementMap', 'feDistantLight', 'feDropShadow', 'feFlood', 'feFuncA', 'feFuncB', 'feFuncG', 'feFuncR', 'feGaussianBlur', 'feImage', 'feMerge', 'feMergeNode', 'feMorphology', 'feOffset', 'fePointLight', 'feSpecularLighting', 'feSpotLight', 'feTile', 'feTurbulence']);\n// List of SVG elements that are disallowed by default.\n// We still need to know them so that we can do namespace\n// checks properly in case one wants to add them to\n// allow-list.\nconst svgDisallowed = freeze(['animate', 'color-profile', 'cursor', 'discard', 'font-face', 'font-face-format', 'font-face-name', 'font-face-src', 'font-face-uri', 'foreignobject', 'hatch', 'hatchpath', 'mesh', 'meshgradient', 'meshpatch', 'meshrow', 'missing-glyph', 'script', 'set', 'solidcolor', 'unknown', 'use']);\nconst mathMl$1 = freeze(['math', 'menclose', 'merror', 'mfenced', 'mfrac', 'mglyph', 'mi', 'mlabeledtr', 'mmultiscripts', 'mn', 'mo', 'mover', 'mpadded', 'mphantom', 'mroot', 'mrow', 'ms', 'mspace', 'msqrt', 'mstyle', 'msub', 'msup', 'msubsup', 'mtable', 'mtd', 'mtext', 'mtr', 'munder', 'munderover', 'mprescripts']);\n// Similarly to SVG, we want to know all MathML elements,\n// even those that we disallow by default.\nconst mathMlDisallowed = freeze(['maction', 'maligngroup', 'malignmark', 'mlongdiv', 'mscarries', 'mscarry', 'msgroup', 'mstack', 'msline', 'msrow', 'semantics', 'annotation', 'annotation-xml', 'mprescripts', 'none']);\nconst text = freeze(['#text']);\n\nconst html = freeze(['accept', 'action', 'align', 'alt', 'autocapitalize', 'autocomplete', 'autopictureinpicture', 'autoplay', 'background', 'bgcolor', 'border', 'capture', 'cellpadding', 'cellspacing', 'checked', 'cite', 'class', 'clear', 'color', 'cols', 'colspan', 'controls', 'controlslist', 'coords', 'crossorigin', 'datetime', 'decoding', 'default', 'dir', 'disabled', 'disablepictureinpicture', 'disableremoteplayback', 'download', 'draggable', 'enctype', 'enterkeyhint', 'face', 'for', 'headers', 'height', 'hidden', 'high', 'href', 'hreflang', 'id', 'inputmode', 'integrity', 'ismap', 'kind', 'label', 'lang', 'list', 'loading', 'loop', 'low', 'max', 'maxlength', 'media', 'method', 'min', 'minlength', 'multiple', 'muted', 'name', 'nonce', 'noshade', 'novalidate', 'nowrap', 'open', 'optimum', 'pattern', 'placeholder', 'playsinline', 'popover', 'popovertarget', 'popovertargetaction', 'poster', 'preload', 'pubdate', 'radiogroup', 'readonly', 'rel', 'required', 'rev', 'reversed', 'role', 'rows', 'rowspan', 'spellcheck', 'scope', 'selected', 'shape', 'size', 'sizes', 'span', 'srclang', 'start', 'src', 'srcset', 'step', 'style', 'summary', 'tabindex', 'title', 'translate', 'type', 'usemap', 'valign', 'value', 'width', 'wrap', 'xmlns', 'slot']);\nconst svg = freeze(['accent-height', 'accumulate', 'additive', 'alignment-baseline', 'amplitude', 'ascent', 'attributename', 'attributetype', 'azimuth', 'basefrequency', 'baseline-shift', 'begin', 'bias', 'by', 'class', 'clip', 'clippathunits', 'clip-path', 'clip-rule', 'color', 'color-interpolation', 'color-interpolation-filters', 'color-profile', 'color-rendering', 'cx', 'cy', 'd', 'dx', 'dy', 'diffuseconstant', 'direction', 'display', 'divisor', 'dur', 'edgemode', 'elevation', 'end', 'exponent', 'fill', 'fill-opacity', 'fill-rule', 'filter', 'filterunits', 'flood-color', 'flood-opacity', 'font-family', 'font-size', 'font-size-adjust', 'font-stretch', 'font-style', 'font-variant', 'font-weight', 'fx', 'fy', 'g1', 'g2', 'glyph-name', 'glyphref', 'gradientunits', 'gradienttransform', 'height', 'href', 'id', 'image-rendering', 'in', 'in2', 'intercept', 'k', 'k1', 'k2', 'k3', 'k4', 'kerning', 'keypoints', 'keysplines', 'keytimes', 'lang', 'lengthadjust', 'letter-spacing', 'kernelmatrix', 'kernelunitlength', 'lighting-color', 'local', 'marker-end', 'marker-mid', 'marker-start', 'markerheight', 'markerunits', 'markerwidth', 'maskcontentunits', 'maskunits', 'max', 'mask', 'media', 'method', 'mode', 'min', 'name', 'numoctaves', 'offset', 'operator', 'opacity', 'order', 'orient', 'orientation', 'origin', 'overflow', 'paint-order', 'path', 'pathlength', 'patterncontentunits', 'patterntransform', 'patternunits', 'points', 'preservealpha', 'preserveaspectratio', 'primitiveunits', 'r', 'rx', 'ry', 'radius', 'refx', 'refy', 'repeatcount', 'repeatdur', 'restart', 'result', 'rotate', 'scale', 'seed', 'shape-rendering', 'slope', 'specularconstant', 'specularexponent', 'spreadmethod', 'startoffset', 'stddeviation', 'stitchtiles', 'stop-color', 'stop-opacity', 'stroke-dasharray', 'stroke-dashoffset', 'stroke-linecap', 'stroke-linejoin', 'stroke-miterlimit', 'stroke-opacity', 'stroke', 'stroke-width', 'style', 'surfacescale', 'systemlanguage', 'tabindex', 'tablevalues', 'targetx', 'targety', 'transform', 'transform-origin', 'text-anchor', 'text-decoration', 'text-rendering', 'textlength', 'type', 'u1', 'u2', 'unicode', 'values', 'viewbox', 'visibility', 'version', 'vert-adv-y', 'vert-origin-x', 'vert-origin-y', 'width', 'word-spacing', 'wrap', 'writing-mode', 'xchannelselector', 'ychannelselector', 'x', 'x1', 'x2', 'xmlns', 'y', 'y1', 'y2', 'z', 'zoomandpan']);\nconst mathMl = freeze(['accent', 'accentunder', 'align', 'bevelled', 'close', 'columnsalign', 'columnlines', 'columnspan', 'denomalign', 'depth', 'dir', 'display', 'displaystyle', 'encoding', 'fence', 'frame', 'height', 'href', 'id', 'largeop', 'length', 'linethickness', 'lspace', 'lquote', 'mathbackground', 'mathcolor', 'mathsize', 'mathvariant', 'maxsize', 'minsize', 'movablelimits', 'notation', 'numalign', 'open', 'rowalign', 'rowlines', 'rowspacing', 'rowspan', 'rspace', 'rquote', 'scriptlevel', 'scriptminsize', 'scriptsizemultiplier', 'selection', 'separator', 'separators', 'stretchy', 'subscriptshift', 'supscriptshift', 'symmetric', 'voffset', 'width', 'xmlns']);\nconst xml = freeze(['xlink:href', 'xml:id', 'xlink:title', 'xml:space', 'xmlns:xlink']);\n\n// eslint-disable-next-line unicorn/better-regex\nconst MUSTACHE_EXPR = seal(/\\{\\{[\\w\\W]*|[\\w\\W]*\\}\\}/gm); // Specify template detection regex for SAFE_FOR_TEMPLATES mode\nconst ERB_EXPR = seal(/<%[\\w\\W]*|[\\w\\W]*%>/gm);\nconst TMPLIT_EXPR = seal(/\\$\\{[\\w\\W]*/gm); // eslint-disable-line unicorn/better-regex\nconst DATA_ATTR = seal(/^data-[\\-\\w.\\u00B7-\\uFFFF]+$/); // eslint-disable-line no-useless-escape\nconst ARIA_ATTR = seal(/^aria-[\\-\\w]+$/); // eslint-disable-line no-useless-escape\nconst IS_ALLOWED_URI = seal(/^(?:(?:(?:f|ht)tps?|mailto|tel|callto|sms|cid|xmpp):|[^a-z]|[a-z+.\\-]+(?:[^a-z+.\\-:]|$))/i // eslint-disable-line no-useless-escape\n);\nconst IS_SCRIPT_OR_DATA = seal(/^(?:\\w+script|data):/i);\nconst ATTR_WHITESPACE = seal(/[\\u0000-\\u0020\\u00A0\\u1680\\u180E\\u2000-\\u2029\\u205F\\u3000]/g // eslint-disable-line no-control-regex\n);\nconst DOCTYPE_NAME = seal(/^html$/i);\nconst CUSTOM_ELEMENT = seal(/^[a-z][.\\w]*(-[.\\w]+)+$/i);\n\nvar EXPRESSIONS = /*#__PURE__*/Object.freeze({\n  __proto__: null,\n  ARIA_ATTR: ARIA_ATTR,\n  ATTR_WHITESPACE: ATTR_WHITESPACE,\n  CUSTOM_ELEMENT: CUSTOM_ELEMENT,\n  DATA_ATTR: DATA_ATTR,\n  DOCTYPE_NAME: DOCTYPE_NAME,\n  ERB_EXPR: ERB_EXPR,\n  IS_ALLOWED_URI: IS_ALLOWED_URI,\n  IS_SCRIPT_OR_DATA: IS_SCRIPT_OR_DATA,\n  MUSTACHE_EXPR: MUSTACHE_EXPR,\n  TMPLIT_EXPR: TMPLIT_EXPR\n});\n\n/* eslint-disable @typescript-eslint/indent */\n// https://developer.mozilla.org/en-US/docs/Web/API/Node/nodeType\nconst NODE_TYPE = {\n  element: 1,\n  attribute: 2,\n  text: 3,\n  cdataSection: 4,\n  entityReference: 5,\n  // Deprecated\n  entityNode: 6,\n  // Deprecated\n  progressingInstruction: 7,\n  comment: 8,\n  document: 9,\n  documentType: 10,\n  documentFragment: 11,\n  notation: 12 // Deprecated\n};\nconst getGlobal = function getGlobal() {\n  return typeof window === 'undefined' ? null : window;\n};\n/**\n * Creates a no-op policy for internal use only.\n * Don't export this function outside this module!\n * @param trustedTypes The policy factory.\n * @param purifyHostElement The Script element used to load DOMPurify (to determine policy name suffix).\n * @return The policy created (or null, if Trusted Types\n * are not supported or creating the policy failed).\n */\nconst _createTrustedTypesPolicy = function _createTrustedTypesPolicy(trustedTypes, purifyHostElement) {\n  if (typeof trustedTypes !== 'object' || typeof trustedTypes.createPolicy !== 'function') {\n    return null;\n  }\n  // Allow the callers to control the unique policy name\n  // by adding a data-tt-policy-suffix to the script element with the DOMPurify.\n  // Policy creation with duplicate names throws in Trusted Types.\n  let suffix = null;\n  const ATTR_NAME = 'data-tt-policy-suffix';\n  if (purifyHostElement && purifyHostElement.hasAttribute(ATTR_NAME)) {\n    suffix = purifyHostElement.getAttribute(ATTR_NAME);\n  }\n  const policyName = 'dompurify' + (suffix ? '#' + suffix : '');\n  try {\n    return trustedTypes.createPolicy(policyName, {\n      createHTML(html) {\n        return html;\n      },\n      createScriptURL(scriptUrl) {\n        return scriptUrl;\n      }\n    });\n  } catch (_) {\n    // Policy creation failed (most likely another DOMPurify script has\n    // already run). Skip creating the policy, as this will only cause errors\n    // if TT are enforced.\n    console.warn('TrustedTypes policy ' + policyName + ' could not be created.');\n    return null;\n  }\n};\nconst _createHooksMap = function _createHooksMap() {\n  return {\n    afterSanitizeAttributes: [],\n    afterSanitizeElements: [],\n    afterSanitizeShadowDOM: [],\n    beforeSanitizeAttributes: [],\n    beforeSanitizeElements: [],\n    beforeSanitizeShadowDOM: [],\n    uponSanitizeAttribute: [],\n    uponSanitizeElement: [],\n    uponSanitizeShadowNode: []\n  };\n};\nfunction createDOMPurify() {\n  let window = arguments.length > 0 && arguments[0] !== undefined ? arguments[0] : getGlobal();\n  const DOMPurify = root => createDOMPurify(root);\n  DOMPurify.version = '3.2.4';\n  DOMPurify.removed = [];\n  if (!window || !window.document || window.document.nodeType !== NODE_TYPE.document || !window.Element) {\n    // Not running in a browser, provide a factory function\n    // so that you can pass your own Window\n    DOMPurify.isSupported = false;\n    return DOMPurify;\n  }\n  let {\n    document\n  } = window;\n  const originalDocument = document;\n  const currentScript = originalDocument.currentScript;\n  const {\n    DocumentFragment,\n    HTMLTemplateElement,\n    Node,\n    Element,\n    NodeFilter,\n    NamedNodeMap = window.NamedNodeMap || window.MozNamedAttrMap,\n    HTMLFormElement,\n    DOMParser,\n    trustedTypes\n  } = window;\n  const ElementPrototype = Element.prototype;\n  const cloneNode = lookupGetter(ElementPrototype, 'cloneNode');\n  const remove = lookupGetter(ElementPrototype, 'remove');\n  const getNextSibling = lookupGetter(ElementPrototype, 'nextSibling');\n  const getChildNodes = lookupGetter(ElementPrototype, 'childNodes');\n  const getParentNode = lookupGetter(ElementPrototype, 'parentNode');\n  // As per issue #47, the web-components registry is inherited by a\n  // new document created via createHTMLDocument. As per the spec\n  // (http://w3c.github.io/webcomponents/spec/custom/#creating-and-passing-registries)\n  // a new empty registry is used when creating a template contents owner\n  // document, so we use that as our parent document to ensure nothing\n  // is inherited.\n  if (typeof HTMLTemplateElement === 'function') {\n    const template = document.createElement('template');\n    if (template.content && template.content.ownerDocument) {\n      document = template.content.ownerDocument;\n    }\n  }\n  let trustedTypesPolicy;\n  let emptyHTML = '';\n  const {\n    implementation,\n    createNodeIterator,\n    createDocumentFragment,\n    getElementsByTagName\n  } = document;\n  const {\n    importNode\n  } = originalDocument;\n  let hooks = _createHooksMap();\n  /**\n   * Expose whether this browser supports running the full DOMPurify.\n   */\n  DOMPurify.isSupported = typeof entries === 'function' && typeof getParentNode === 'function' && implementation && implementation.createHTMLDocument !== undefined;\n  const {\n    MUSTACHE_EXPR,\n    ERB_EXPR,\n    TMPLIT_EXPR,\n    DATA_ATTR,\n    ARIA_ATTR,\n    IS_SCRIPT_OR_DATA,\n    ATTR_WHITESPACE,\n    CUSTOM_ELEMENT\n  } = EXPRESSIONS;\n  let {\n    IS_ALLOWED_URI: IS_ALLOWED_URI$1\n  } = EXPRESSIONS;\n  /**\n   * We consider the elements and attributes below to be safe. Ideally\n   * don't add any new ones but feel free to remove unwanted ones.\n   */\n  /* allowed element names */\n  let ALLOWED_TAGS = null;\n  const DEFAULT_ALLOWED_TAGS = addToSet({}, [...html$1, ...svg$1, ...svgFilters, ...mathMl$1, ...text]);\n  /* Allowed attribute names */\n  let ALLOWED_ATTR = null;\n  const DEFAULT_ALLOWED_ATTR = addToSet({}, [...html, ...svg, ...mathMl, ...xml]);\n  /*\n   * Configure how DOMPurify should handle custom elements and their attributes as well as customized built-in elements.\n   * @property {RegExp|Function|null} tagNameCheck one of [null, regexPattern, predicate]. Default: `null` (disallow any custom elements)\n   * @property {RegExp|Function|null} attributeNameCheck one of [null, regexPattern, predicate]. Default: `null` (disallow any attributes not on the allow list)\n   * @property {boolean} allowCustomizedBuiltInElements allow custom elements derived from built-ins if they pass CUSTOM_ELEMENT_HANDLING.tagNameCheck. Default: `false`.\n   */\n  let CUSTOM_ELEMENT_HANDLING = Object.seal(create(null, {\n    tagNameCheck: {\n      writable: true,\n      configurable: false,\n      enumerable: true,\n      value: null\n    },\n    attributeNameCheck: {\n      writable: true,\n      configurable: false,\n      enumerable: true,\n      value: null\n    },\n    allowCustomizedBuiltInElements: {\n      writable: true,\n      configurable: false,\n      enumerable: true,\n      value: false\n    }\n  }));\n  /* Explicitly forbidden tags (overrides ALLOWED_TAGS/ADD_TAGS) */\n  let FORBID_TAGS = null;\n  /* Explicitly forbidden attributes (overrides ALLOWED_ATTR/ADD_ATTR) */\n  let FORBID_ATTR = null;\n  /* Decide if ARIA attributes are okay */\n  let ALLOW_ARIA_ATTR = true;\n  /* Decide if custom data attributes are okay */\n  let ALLOW_DATA_ATTR = true;\n  /* Decide if unknown protocols are okay */\n  let ALLOW_UNKNOWN_PROTOCOLS = false;\n  /* Decide if self-closing tags in attributes are allowed.\n   * Usually removed due to a mXSS issue in jQuery 3.0 */\n  let ALLOW_SELF_CLOSE_IN_ATTR = true;\n  /* Output should be safe for common template engines.\n   * This means, DOMPurify removes data attributes, mustaches and ERB\n   */\n  let SAFE_FOR_TEMPLATES = false;\n  /* Output should be safe even for XML used within HTML and alike.\n   * This means, DOMPurify removes comments when containing risky content.\n   */\n  let SAFE_FOR_XML = true;\n  /* Decide if document with <html>... should be returned */\n  let WHOLE_DOCUMENT = false;\n  /* Track whether config is already set on this instance of DOMPurify. */\n  let SET_CONFIG = false;\n  /* Decide if all elements (e.g. style, script) must be children of\n   * document.body. By default, browsers might move them to document.head */\n  let FORCE_BODY = false;\n  /* Decide if a DOM `HTMLBodyElement` should be returned, instead of a html\n   * string (or a TrustedHTML object if Trusted Types are supported).\n   * If `WHOLE_DOCUMENT` is enabled a `HTMLHtmlElement` will be returned instead\n   */\n  let RETURN_DOM = false;\n  /* Decide if a DOM `DocumentFragment` should be returned, instead of a html\n   * string  (or a TrustedHTML object if Trusted Types are supported) */\n  let RETURN_DOM_FRAGMENT = false;\n  /* Try to return a Trusted Type object instead of a string, return a string in\n   * case Trusted Types are not supported  */\n  let RETURN_TRUSTED_TYPE = false;\n  /* Output should be free from DOM clobbering attacks?\n   * This sanitizes markups named with colliding, clobberable built-in DOM APIs.\n   */\n  let SANITIZE_DOM = true;\n  /* Achieve full DOM Clobbering protection by isolating the namespace of named\n   * properties and JS variables, mitigating attacks that abuse the HTML/DOM spec rules.\n   *\n   * HTML/DOM spec rules that enable DOM Clobbering:\n   *   - Named Access on Window (\u00A77.3.3)\n   *   - DOM Tree Accessors (\u00A73.1.5)\n   *   - Form Element Parent-Child Relations (\u00A74.10.3)\n   *   - Iframe srcdoc / Nested WindowProxies (\u00A74.8.5)\n   *   - HTMLCollection (\u00A74.2.10.2)\n   *\n   * Namespace isolation is implemented by prefixing `id` and `name` attributes\n   * with a constant string, i.e., `user-content-`\n   */\n  let SANITIZE_NAMED_PROPS = false;\n  const SANITIZE_NAMED_PROPS_PREFIX = 'user-content-';\n  /* Keep element content when removing element? */\n  let KEEP_CONTENT = true;\n  /* If a `Node` is passed to sanitize(), then performs sanitization in-place instead\n   * of importing it into a new Document and returning a sanitized copy */\n  let IN_PLACE = false;\n  /* Allow usage of profiles like html, svg and mathMl */\n  let USE_PROFILES = {};\n  /* Tags to ignore content of when KEEP_CONTENT is true */\n  let FORBID_CONTENTS = null;\n  const DEFAULT_FORBID_CONTENTS = addToSet({}, ['annotation-xml', 'audio', 'colgroup', 'desc', 'foreignobject', 'head', 'iframe', 'math', 'mi', 'mn', 'mo', 'ms', 'mtext', 'noembed', 'noframes', 'noscript', 'plaintext', 'script', 'style', 'svg', 'template', 'thead', 'title', 'video', 'xmp']);\n  /* Tags that are safe for data: URIs */\n  let DATA_URI_TAGS = null;\n  const DEFAULT_DATA_URI_TAGS = addToSet({}, ['audio', 'video', 'img', 'source', 'image', 'track']);\n  /* Attributes safe for values like \"javascript:\" */\n  let URI_SAFE_ATTRIBUTES = null;\n  const DEFAULT_URI_SAFE_ATTRIBUTES = addToSet({}, ['alt', 'class', 'for', 'id', 'label', 'name', 'pattern', 'placeholder', 'role', 'summary', 'title', 'value', 'style', 'xmlns']);\n  const MATHML_NAMESPACE = 'http://www.w3.org/1998/Math/MathML';\n  const SVG_NAMESPACE = 'http://www.w3.org/2000/svg';\n  const HTML_NAMESPACE = 'http://www.w3.org/1999/xhtml';\n  /* Document namespace */\n  let NAMESPACE = HTML_NAMESPACE;\n  let IS_EMPTY_INPUT = false;\n  /* Allowed XHTML+XML namespaces */\n  let ALLOWED_NAMESPACES = null;\n  const DEFAULT_ALLOWED_NAMESPACES = addToSet({}, [MATHML_NAMESPACE, SVG_NAMESPACE, HTML_NAMESPACE], stringToString);\n  let MATHML_TEXT_INTEGRATION_POINTS = addToSet({}, ['mi', 'mo', 'mn', 'ms', 'mtext']);\n  let HTML_INTEGRATION_POINTS = addToSet({}, ['annotation-xml']);\n  // Certain elements are allowed in both SVG and HTML\n  // namespace. We need to specify them explicitly\n  // so that they don't get erroneously deleted from\n  // HTML namespace.\n  const COMMON_SVG_AND_HTML_ELEMENTS = addToSet({}, ['title', 'style', 'font', 'a', 'script']);\n  /* Parsing of strict XHTML documents */\n  let PARSER_MEDIA_TYPE = null;\n  const SUPPORTED_PARSER_MEDIA_TYPES = ['application/xhtml+xml', 'text/html'];\n  const DEFAULT_PARSER_MEDIA_TYPE = 'text/html';\n  let transformCaseFunc = null;\n  /* Keep a reference to config to pass to hooks */\n  let CONFIG = null;\n  /* Ideally, do not touch anything below this line */\n  /* ______________________________________________ */\n  const formElement = document.createElement('form');\n  const isRegexOrFunction = function isRegexOrFunction(testValue) {\n    return testValue instanceof RegExp || testValue instanceof Function;\n  };\n  /**\n   * _parseConfig\n   *\n   * @param cfg optional config literal\n   */\n  // eslint-disable-next-line complexity\n  const _parseConfig = function _parseConfig() {\n    let cfg = arguments.length > 0 && arguments[0] !== undefined ? arguments[0] : {};\n    if (CONFIG && CONFIG === cfg) {\n      return;\n    }\n    /* Shield configuration object from tampering */\n    if (!cfg || typeof cfg !== 'object') {\n      cfg = {};\n    }\n    /* Shield configuration object from prototype pollution */\n    cfg = clone(cfg);\n    PARSER_MEDIA_TYPE =\n    // eslint-disable-next-line unicorn/prefer-includes\n    SUPPORTED_PARSER_MEDIA_TYPES.indexOf(cfg.PARSER_MEDIA_TYPE) === -1 ? DEFAULT_PARSER_MEDIA_TYPE : cfg.PARSER_MEDIA_TYPE;\n    // HTML tags and attributes are not case-sensitive, converting to lowercase. Keeping XHTML as is.\n    transformCaseFunc = PARSER_MEDIA_TYPE === 'application/xhtml+xml' ? stringToString : stringToLowerCase;\n    /* Set configuration parameters */\n    ALLOWED_TAGS = objectHasOwnProperty(cfg, 'ALLOWED_TAGS') ? addToSet({}, cfg.ALLOWED_TAGS, transformCaseFunc) : DEFAULT_ALLOWED_TAGS;\n    ALLOWED_ATTR = objectHasOwnProperty(cfg, 'ALLOWED_ATTR') ? addToSet({}, cfg.ALLOWED_ATTR, transformCaseFunc) : DEFAULT_ALLOWED_ATTR;\n    ALLOWED_NAMESPACES = objectHasOwnProperty(cfg, 'ALLOWED_NAMESPACES') ? addToSet({}, cfg.ALLOWED_NAMESPACES, stringToString) : DEFAULT_ALLOWED_NAMESPACES;\n    URI_SAFE_ATTRIBUTES = objectHasOwnProperty(cfg, 'ADD_URI_SAFE_ATTR') ? addToSet(clone(DEFAULT_URI_SAFE_ATTRIBUTES), cfg.ADD_URI_SAFE_ATTR, transformCaseFunc) : DEFAULT_URI_SAFE_ATTRIBUTES;\n    DATA_URI_TAGS = objectHasOwnProperty(cfg, 'ADD_DATA_URI_TAGS') ? addToSet(clone(DEFAULT_DATA_URI_TAGS), cfg.ADD_DATA_URI_TAGS, transformCaseFunc) : DEFAULT_DATA_URI_TAGS;\n    FORBID_CONTENTS = objectHasOwnProperty(cfg, 'FORBID_CONTENTS') ? addToSet({}, cfg.FORBID_CONTENTS, transformCaseFunc) : DEFAULT_FORBID_CONTENTS;\n    FORBID_TAGS = objectHasOwnProperty(cfg, 'FORBID_TAGS') ? addToSet({}, cfg.FORBID_TAGS, transformCaseFunc) : {};\n    FORBID_ATTR = objectHasOwnProperty(cfg, 'FORBID_ATTR') ? addToSet({}, cfg.FORBID_ATTR, transformCaseFunc) : {};\n    USE_PROFILES = objectHasOwnProperty(cfg, 'USE_PROFILES') ? cfg.USE_PROFILES : false;\n    ALLOW_ARIA_ATTR = cfg.ALLOW_ARIA_ATTR !== false; // Default true\n    ALLOW_DATA_ATTR = cfg.ALLOW_DATA_ATTR !== false; // Default true\n    ALLOW_UNKNOWN_PROTOCOLS = cfg.ALLOW_UNKNOWN_PROTOCOLS || false; // Default false\n    ALLOW_SELF_CLOSE_IN_ATTR = cfg.ALLOW_SELF_CLOSE_IN_ATTR !== false; // Default true\n    SAFE_FOR_TEMPLATES = cfg.SAFE_FOR_TEMPLATES || false; // Default false\n    SAFE_FOR_XML = cfg.SAFE_FOR_XML !== false; // Default true\n    WHOLE_DOCUMENT = cfg.WHOLE_DOCUMENT || false; // Default false\n    RETURN_DOM = cfg.RETURN_DOM || false; // Default false\n    RETURN_DOM_FRAGMENT = cfg.RETURN_DOM_FRAGMENT || false; // Default false\n    RETURN_TRUSTED_TYPE = cfg.RETURN_TRUSTED_TYPE || false; // Default false\n    FORCE_BODY = cfg.FORCE_BODY || false; // Default false\n    SANITIZE_DOM = cfg.SANITIZE_DOM !== false; // Default true\n    SANITIZE_NAMED_PROPS = cfg.SANITIZE_NAMED_PROPS || false; // Default false\n    KEEP_CONTENT = cfg.KEEP_CONTENT !== false; // Default true\n    IN_PLACE = cfg.IN_PLACE || false; // Default false\n    IS_ALLOWED_URI$1 = cfg.ALLOWED_URI_REGEXP || IS_ALLOWED_URI;\n    NAMESPACE = cfg.NAMESPACE || HTML_NAMESPACE;\n    MATHML_TEXT_INTEGRATION_POINTS = cfg.MATHML_TEXT_INTEGRATION_POINTS || MATHML_TEXT_INTEGRATION_POINTS;\n    HTML_INTEGRATION_POINTS = cfg.HTML_INTEGRATION_POINTS || HTML_INTEGRATION_POINTS;\n    CUSTOM_ELEMENT_HANDLING = cfg.CUSTOM_ELEMENT_HANDLING || {};\n    if (cfg.CUSTOM_ELEMENT_HANDLING && isRegexOrFunction(cfg.CUSTOM_ELEMENT_HANDLING.tagNameCheck)) {\n      CUSTOM_ELEMENT_HANDLING.tagNameCheck = cfg.CUSTOM_ELEMENT_HANDLING.tagNameCheck;\n    }\n    if (cfg.CUSTOM_ELEMENT_HANDLING && isRegexOrFunction(cfg.CUSTOM_ELEMENT_HANDLING.attributeNameCheck)) {\n      CUSTOM_ELEMENT_HANDLING.attributeNameCheck = cfg.CUSTOM_ELEMENT_HANDLING.attributeNameCheck;\n    }\n    if (cfg.CUSTOM_ELEMENT_HANDLING && typeof cfg.CUSTOM_ELEMENT_HANDLING.allowCustomizedBuiltInElements === 'boolean') {\n      CUSTOM_ELEMENT_HANDLING.allowCustomizedBuiltInElements = cfg.CUSTOM_ELEMENT_HANDLING.allowCustomizedBuiltInElements;\n    }\n    if (SAFE_FOR_TEMPLATES) {\n      ALLOW_DATA_ATTR = false;\n    }\n    if (RETURN_DOM_FRAGMENT) {\n      RETURN_DOM = true;\n    }\n    /* Parse profile info */\n    if (USE_PROFILES) {\n      ALLOWED_TAGS = addToSet({}, text);\n      ALLOWED_ATTR = [];\n      if (USE_PROFILES.html === true) {\n        addToSet(ALLOWED_TAGS, html$1);\n        addToSet(ALLOWED_ATTR, html);\n      }\n      if (USE_PROFILES.svg === true) {\n        addToSet(ALLOWED_TAGS, svg$1);\n        addToSet(ALLOWED_ATTR, svg);\n        addToSet(ALLOWED_ATTR, xml);\n      }\n      if (USE_PROFILES.svgFilters === true) {\n        addToSet(ALLOWED_TAGS, svgFilters);\n        addToSet(ALLOWED_ATTR, svg);\n        addToSet(ALLOWED_ATTR, xml);\n      }\n      if (USE_PROFILES.mathMl === true) {\n        addToSet(ALLOWED_TAGS, mathMl$1);\n        addToSet(ALLOWED_ATTR, mathMl);\n        addToSet(ALLOWED_ATTR, xml);\n      }\n    }\n    /* Merge configuration parameters */\n    if (cfg.ADD_TAGS) {\n      if (ALLOWED_TAGS === DEFAULT_ALLOWED_TAGS) {\n        ALLOWED_TAGS = clone(ALLOWED_TAGS);\n      }\n      addToSet(ALLOWED_TAGS, cfg.ADD_TAGS, transformCaseFunc);\n    }\n    if (cfg.ADD_ATTR) {\n      if (ALLOWED_ATTR === DEFAULT_ALLOWED_ATTR) {\n        ALLOWED_ATTR = clone(ALLOWED_ATTR);\n      }\n      addToSet(ALLOWED_ATTR, cfg.ADD_ATTR, transformCaseFunc);\n    }\n    if (cfg.ADD_URI_SAFE_ATTR) {\n      addToSet(URI_SAFE_ATTRIBUTES, cfg.ADD_URI_SAFE_ATTR, transformCaseFunc);\n    }\n    if (cfg.FORBID_CONTENTS) {\n      if (FORBID_CONTENTS === DEFAULT_FORBID_CONTENTS) {\n        FORBID_CONTENTS = clone(FORBID_CONTENTS);\n      }\n      addToSet(FORBID_CONTENTS, cfg.FORBID_CONTENTS, transformCaseFunc);\n    }\n    /* Add #text in case KEEP_CONTENT is set to true */\n    if (KEEP_CONTENT) {\n      ALLOWED_TAGS['#text'] = true;\n    }\n    /* Add html, head and body to ALLOWED_TAGS in case WHOLE_DOCUMENT is true */\n    if (WHOLE_DOCUMENT) {\n      addToSet(ALLOWED_TAGS, ['html', 'head', 'body']);\n    }\n    /* Add tbody to ALLOWED_TAGS in case tables are permitted, see #286, #365 */\n    if (ALLOWED_TAGS.table) {\n      addToSet(ALLOWED_TAGS, ['tbody']);\n      delete FORBID_TAGS.tbody;\n    }\n    if (cfg.TRUSTED_TYPES_POLICY) {\n      if (typeof cfg.TRUSTED_TYPES_POLICY.createHTML !== 'function') {\n        throw typeErrorCreate('TRUSTED_TYPES_POLICY configuration option must provide a \"createHTML\" hook.');\n      }\n      if (typeof cfg.TRUSTED_TYPES_POLICY.createScriptURL !== 'function') {\n        throw typeErrorCreate('TRUSTED_TYPES_POLICY configuration option must provide a \"createScriptURL\" hook.');\n      }\n      // Overwrite existing TrustedTypes policy.\n      trustedTypesPolicy = cfg.TRUSTED_TYPES_POLICY;\n      // Sign local variables required by `sanitize`.\n      emptyHTML = trustedTypesPolicy.createHTML('');\n    } else {\n      // Uninitialized policy, attempt to initialize the internal dompurify policy.\n      if (trustedTypesPolicy === undefined) {\n        trustedTypesPolicy = _createTrustedTypesPolicy(trustedTypes, currentScript);\n      }\n      // If creating the internal policy succeeded sign internal variables.\n      if (trustedTypesPolicy !== null && typeof emptyHTML === 'string') {\n        emptyHTML = trustedTypesPolicy.createHTML('');\n      }\n    }\n    // Prevent further manipulation of configuration.\n    // Not available in IE8, Safari 5, etc.\n    if (freeze) {\n      freeze(cfg);\n    }\n    CONFIG = cfg;\n  };\n  /* Keep track of all possible SVG and MathML tags\n   * so that we can perform the namespace checks\n   * correctly. */\n  const ALL_SVG_TAGS = addToSet({}, [...svg$1, ...svgFilters, ...svgDisallowed]);\n  const ALL_MATHML_TAGS = addToSet({}, [...mathMl$1, ...mathMlDisallowed]);\n  /**\n   * @param element a DOM element whose namespace is being checked\n   * @returns Return false if the element has a\n   *  namespace that a spec-compliant parser would never\n   *  return. Return true otherwise.\n   */\n  const _checkValidNamespace = function _checkValidNamespace(element) {\n    let parent = getParentNode(element);\n    // In JSDOM, if we're inside shadow DOM, then parentNode\n    // can be null. We just simulate parent in this case.\n    if (!parent || !parent.tagName) {\n      parent = {\n        namespaceURI: NAMESPACE,\n        tagName: 'template'\n      };\n    }\n    const tagName = stringToLowerCase(element.tagName);\n    const parentTagName = stringToLowerCase(parent.tagName);\n    if (!ALLOWED_NAMESPACES[element.namespaceURI]) {\n      return false;\n    }\n    if (element.namespaceURI === SVG_NAMESPACE) {\n      // The only way to switch from HTML namespace to SVG\n      // is via <svg>. If it happens via any other tag, then\n      // it should be killed.\n      if (parent.namespaceURI === HTML_NAMESPACE) {\n        return tagName === 'svg';\n      }\n      // The only way to switch from MathML to SVG is via`\n      // svg if parent is either <annotation-xml> or MathML\n      // text integration points.\n      if (parent.namespaceURI === MATHML_NAMESPACE) {\n        return tagName === 'svg' && (parentTagName === 'annotation-xml' || MATHML_TEXT_INTEGRATION_POINTS[parentTagName]);\n      }\n      // We only allow elements that are defined in SVG\n      // spec. All others are disallowed in SVG namespace.\n      return Boolean(ALL_SVG_TAGS[tagName]);\n    }\n    if (element.namespaceURI === MATHML_NAMESPACE) {\n      // The only way to switch from HTML namespace to MathML\n      // is via <math>. If it happens via any other tag, then\n      // it should be killed.\n      if (parent.namespaceURI === HTML_NAMESPACE) {\n        return tagName === 'math';\n      }\n      // The only way to switch from SVG to MathML is via\n      // <math> and HTML integration points\n      if (parent.namespaceURI === SVG_NAMESPACE) {\n        return tagName === 'math' && HTML_INTEGRATION_POINTS[parentTagName];\n      }\n      // We only allow elements that are defined in MathML\n      // spec. All others are disallowed in MathML namespace.\n      return Boolean(ALL_MATHML_TAGS[tagName]);\n    }\n    if (element.namespaceURI === HTML_NAMESPACE) {\n      // The only way to switch from SVG to HTML is via\n      // HTML integration points, and from MathML to HTML\n      // is via MathML text integration points\n      if (parent.namespaceURI === SVG_NAMESPACE && !HTML_INTEGRATION_POINTS[parentTagName]) {\n        return false;\n      }\n      if (parent.namespaceURI === MATHML_NAMESPACE && !MATHML_TEXT_INTEGRATION_POINTS[parentTagName]) {\n        return false;\n      }\n      // We disallow tags that are specific for MathML\n      // or SVG and should never appear in HTML namespace\n      return !ALL_MATHML_TAGS[tagName] && (COMMON_SVG_AND_HTML_ELEMENTS[tagName] || !ALL_SVG_TAGS[tagName]);\n    }\n    // For XHTML and XML documents that support custom namespaces\n    if (PARSER_MEDIA_TYPE === 'application/xhtml+xml' && ALLOWED_NAMESPACES[element.namespaceURI]) {\n      return true;\n    }\n    // The code should never reach this place (this means\n    // that the element somehow got namespace that is not\n    // HTML, SVG, MathML or allowed via ALLOWED_NAMESPACES).\n    // Return false just in case.\n    return false;\n  };\n  /**\n   * _forceRemove\n   *\n   * @param node a DOM node\n   */\n  const _forceRemove = function _forceRemove(node) {\n    arrayPush(DOMPurify.removed, {\n      element: node\n    });\n    try {\n      // eslint-disable-next-line unicorn/prefer-dom-node-remove\n      getParentNode(node).removeChild(node);\n    } catch (_) {\n      remove(node);\n    }\n  };\n  /**\n   * _removeAttribute\n   *\n   * @param name an Attribute name\n   * @param element a DOM node\n   */\n  const _removeAttribute = function _removeAttribute(name, element) {\n    try {\n      arrayPush(DOMPurify.removed, {\n        attribute: element.getAttributeNode(name),\n        from: element\n      });\n    } catch (_) {\n      arrayPush(DOMPurify.removed, {\n        attribute: null,\n        from: element\n      });\n    }\n    element.removeAttribute(name);\n    // We void attribute values for unremovable \"is\" attributes\n    if (name === 'is') {\n      if (RETURN_DOM || RETURN_DOM_FRAGMENT) {\n        try {\n          _forceRemove(element);\n        } catch (_) {}\n      } else {\n        try {\n          element.setAttribute(name, '');\n        } catch (_) {}\n      }\n    }\n  };\n  /**\n   * _initDocument\n   *\n   * @param dirty - a string of dirty markup\n   * @return a DOM, filled with the dirty markup\n   */\n  const _initDocument = function _initDocument(dirty) {\n    /* Create a HTML document */\n    let doc = null;\n    let leadingWhitespace = null;\n    if (FORCE_BODY) {\n      dirty = '<remove></remove>' + dirty;\n    } else {\n      /* If FORCE_BODY isn't used, leading whitespace needs to be preserved manually */\n      const matches = stringMatch(dirty, /^[\\r\\n\\t ]+/);\n      leadingWhitespace = matches && matches[0];\n    }\n    if (PARSER_MEDIA_TYPE === 'application/xhtml+xml' && NAMESPACE === HTML_NAMESPACE) {\n      // Root of XHTML doc must contain xmlns declaration (see https://www.w3.org/TR/xhtml1/normative.html#strict)\n      dirty = '<html xmlns=\"http://www.w3.org/1999/xhtml\"><head></head><body>' + dirty + '</body></html>';\n    }\n    const dirtyPayload = trustedTypesPolicy ? trustedTypesPolicy.createHTML(dirty) : dirty;\n    /*\n     * Use the DOMParser API by default, fallback later if needs be\n     * DOMParser not work for svg when has multiple root element.\n     */\n    if (NAMESPACE === HTML_NAMESPACE) {\n      try {\n        doc = new DOMParser().parseFromString(dirtyPayload, PARSER_MEDIA_TYPE);\n      } catch (_) {}\n    }\n    /* Use createHTMLDocument in case DOMParser is not available */\n    if (!doc || !doc.documentElement) {\n      doc = implementation.createDocument(NAMESPACE, 'template', null);\n      try {\n        doc.documentElement.innerHTML = IS_EMPTY_INPUT ? emptyHTML : dirtyPayload;\n      } catch (_) {\n        // Syntax error if dirtyPayload is invalid xml\n      }\n    }\n    const body = doc.body || doc.documentElement;\n    if (dirty && leadingWhitespace) {\n      body.insertBefore(document.createTextNode(leadingWhitespace), body.childNodes[0] || null);\n    }\n    /* Work on whole document or just its body */\n    if (NAMESPACE === HTML_NAMESPACE) {\n      return getElementsByTagName.call(doc, WHOLE_DOCUMENT ? 'html' : 'body')[0];\n    }\n    return WHOLE_DOCUMENT ? doc.documentElement : body;\n  };\n  /**\n   * Creates a NodeIterator object that you can use to traverse filtered lists of nodes or elements in a document.\n   *\n   * @param root The root element or node to start traversing on.\n   * @return The created NodeIterator\n   */\n  const _createNodeIterator = function _createNodeIterator(root) {\n    return createNodeIterator.call(root.ownerDocument || root, root,\n    // eslint-disable-next-line no-bitwise\n    NodeFilter.SHOW_ELEMENT | NodeFilter.SHOW_COMMENT | NodeFilter.SHOW_TEXT | NodeFilter.SHOW_PROCESSING_INSTRUCTION | NodeFilter.SHOW_CDATA_SECTION, null);\n  };\n  /**\n   * _isClobbered\n   *\n   * @param element element to check for clobbering attacks\n   * @return true if clobbered, false if safe\n   */\n  const _isClobbered = function _isClobbered(element) {\n    return element instanceof HTMLFormElement && (typeof element.nodeName !== 'string' || typeof element.textContent !== 'string' || typeof element.removeChild !== 'function' || !(element.attributes instanceof NamedNodeMap) || typeof element.removeAttribute !== 'function' || typeof element.setAttribute !== 'function' || typeof element.namespaceURI !== 'string' || typeof element.insertBefore !== 'function' || typeof element.hasChildNodes !== 'function');\n  };\n  /**\n   * Checks whether the given object is a DOM node.\n   *\n   * @param value object to check whether it's a DOM node\n   * @return true is object is a DOM node\n   */\n  const _isNode = function _isNode(value) {\n    return typeof Node === 'function' && value instanceof Node;\n  };\n  function _executeHooks(hooks, currentNode, data) {\n    arrayForEach(hooks, hook => {\n      hook.call(DOMPurify, currentNode, data, CONFIG);\n    });\n  }\n  /**\n   * _sanitizeElements\n   *\n   * @protect nodeName\n   * @protect textContent\n   * @protect removeChild\n   * @param currentNode to check for permission to exist\n   * @return true if node was killed, false if left alive\n   */\n  const _sanitizeElements = function _sanitizeElements(currentNode) {\n    let content = null;\n    /* Execute a hook if present */\n    _executeHooks(hooks.beforeSanitizeElements, currentNode, null);\n    /* Check if element is clobbered or can clobber */\n    if (_isClobbered(currentNode)) {\n      _forceRemove(currentNode);\n      return true;\n    }\n    /* Now let's check the element's type and name */\n    const tagName = transformCaseFunc(currentNode.nodeName);\n    /* Execute a hook if present */\n    _executeHooks(hooks.uponSanitizeElement, currentNode, {\n      tagName,\n      allowedTags: ALLOWED_TAGS\n    });\n    /* Detect mXSS attempts abusing namespace confusion */\n    if (currentNode.hasChildNodes() && !_isNode(currentNode.firstElementChild) && regExpTest(/<[/\\w]/g, currentNode.innerHTML) && regExpTest(/<[/\\w]/g, currentNode.textContent)) {\n      _forceRemove(currentNode);\n      return true;\n    }\n    /* Remove any occurrence of processing instructions */\n    if (currentNode.nodeType === NODE_TYPE.progressingInstruction) {\n      _forceRemove(currentNode);\n      return true;\n    }\n    /* Remove any kind of possibly harmful comments */\n    if (SAFE_FOR_XML && currentNode.nodeType === NODE_TYPE.comment && regExpTest(/<[/\\w]/g, currentNode.data)) {\n      _forceRemove(currentNode);\n      return true;\n    }\n    /* Remove element if anything forbids its presence */\n    if (!ALLOWED_TAGS[tagName] || FORBID_TAGS[tagName]) {\n      /* Check if we have a custom element to handle */\n      if (!FORBID_TAGS[tagName] && _isBasicCustomElement(tagName)) {\n        if (CUSTOM_ELEMENT_HANDLING.tagNameCheck instanceof RegExp && regExpTest(CUSTOM_ELEMENT_HANDLING.tagNameCheck, tagName)) {\n          return false;\n        }\n        if (CUSTOM_ELEMENT_HANDLING.tagNameCheck instanceof Function && CUSTOM_ELEMENT_HANDLING.tagNameCheck(tagName)) {\n          return false;\n        }\n      }\n      /* Keep content except for bad-listed elements */\n      if (KEEP_CONTENT && !FORBID_CONTENTS[tagName]) {\n        const parentNode = getParentNode(currentNode) || currentNode.parentNode;\n        const childNodes = getChildNodes(currentNode) || currentNode.childNodes;\n        if (childNodes && parentNode) {\n          const childCount = childNodes.length;\n          for (let i = childCount - 1; i >= 0; --i) {\n            const childClone = cloneNode(childNodes[i], true);\n            childClone.__removalCount = (currentNode.__removalCount || 0) + 1;\n            parentNode.insertBefore(childClone, getNextSibling(currentNode));\n          }\n        }\n      }\n      _forceRemove(currentNode);\n      return true;\n    }\n    /* Check whether element has a valid namespace */\n    if (currentNode instanceof Element && !_checkValidNamespace(currentNode)) {\n      _forceRemove(currentNode);\n      return true;\n    }\n    /* Make sure that older browsers don't get fallback-tag mXSS */\n    if ((tagName === 'noscript' || tagName === 'noembed' || tagName === 'noframes') && regExpTest(/<\\/no(script|embed|frames)/i, currentNode.innerHTML)) {\n      _forceRemove(currentNode);\n      return true;\n    }\n    /* Sanitize element content to be template-safe */\n    if (SAFE_FOR_TEMPLATES && currentNode.nodeType === NODE_TYPE.text) {\n      /* Get the element's text content */\n      content = currentNode.textContent;\n      arrayForEach([MUSTACHE_EXPR, ERB_EXPR, TMPLIT_EXPR], expr => {\n        content = stringReplace(content, expr, ' ');\n      });\n      if (currentNode.textContent !== content) {\n        arrayPush(DOMPurify.removed, {\n          element: currentNode.cloneNode()\n        });\n        currentNode.textContent = content;\n      }\n    }\n    /* Execute a hook if present */\n    _executeHooks(hooks.afterSanitizeElements, currentNode, null);\n    return false;\n  };\n  /**\n   * _isValidAttribute\n   *\n   * @param lcTag Lowercase tag name of containing element.\n   * @param lcName Lowercase attribute name.\n   * @param value Attribute value.\n   * @return Returns true if `value` is valid, otherwise false.\n   */\n  // eslint-disable-next-line complexity\n  const _isValidAttribute = function _isValidAttribute(lcTag, lcName, value) {\n    /* Make sure attribute cannot clobber */\n    if (SANITIZE_DOM && (lcName === 'id' || lcName === 'name') && (value in document || value in formElement)) {\n      return false;\n    }\n    /* Allow valid data-* attributes: At least one character after \"-\"\n        (https://html.spec.whatwg.org/multipage/dom.html#embedding-custom-non-visible-data-with-the-data-*-attributes)\n        XML-compatible (https://html.spec.whatwg.org/multipage/infrastructure.html#xml-compatible and http://www.w3.org/TR/xml/#d0e804)\n        We don't need to check the value; it's always URI safe. */\n    if (ALLOW_DATA_ATTR && !FORBID_ATTR[lcName] && regExpTest(DATA_ATTR, lcName)) ; else if (ALLOW_ARIA_ATTR && regExpTest(ARIA_ATTR, lcName)) ; else if (!ALLOWED_ATTR[lcName] || FORBID_ATTR[lcName]) {\n      if (\n      // First condition does a very basic check if a) it's basically a valid custom element tagname AND\n      // b) if the tagName passes whatever the user has configured for CUSTOM_ELEMENT_HANDLING.tagNameCheck\n      // and c) if the attribute name passes whatever the user has configured for CUSTOM_ELEMENT_HANDLING.attributeNameCheck\n      _isBasicCustomElement(lcTag) && (CUSTOM_ELEMENT_HANDLING.tagNameCheck instanceof RegExp && regExpTest(CUSTOM_ELEMENT_HANDLING.tagNameCheck, lcTag) || CUSTOM_ELEMENT_HANDLING.tagNameCheck instanceof Function && CUSTOM_ELEMENT_HANDLING.tagNameCheck(lcTag)) && (CUSTOM_ELEMENT_HANDLING.attributeNameCheck instanceof RegExp && regExpTest(CUSTOM_ELEMENT_HANDLING.attributeNameCheck, lcName) || CUSTOM_ELEMENT_HANDLING.attributeNameCheck instanceof Function && CUSTOM_ELEMENT_HANDLING.attributeNameCheck(lcName)) ||\n      // Alternative, second condition checks if it's an `is`-attribute, AND\n      // the value passes whatever the user has configured for CUSTOM_ELEMENT_HANDLING.tagNameCheck\n      lcName === 'is' && CUSTOM_ELEMENT_HANDLING.allowCustomizedBuiltInElements && (CUSTOM_ELEMENT_HANDLING.tagNameCheck instanceof RegExp && regExpTest(CUSTOM_ELEMENT_HANDLING.tagNameCheck, value) || CUSTOM_ELEMENT_HANDLING.tagNameCheck instanceof Function && CUSTOM_ELEMENT_HANDLING.tagNameCheck(value))) ; else {\n        return false;\n      }\n      /* Check value is safe. First, is attr inert? If so, is safe */\n    } else if (URI_SAFE_ATTRIBUTES[lcName]) ; else if (regExpTest(IS_ALLOWED_URI$1, stringReplace(value, ATTR_WHITESPACE, ''))) ; else if ((lcName === 'src' || lcName === 'xlink:href' || lcName === 'href') && lcTag !== 'script' && stringIndexOf(value, 'data:') === 0 && DATA_URI_TAGS[lcTag]) ; else if (ALLOW_UNKNOWN_PROTOCOLS && !regExpTest(IS_SCRIPT_OR_DATA, stringReplace(value, ATTR_WHITESPACE, ''))) ; else if (value) {\n      return false;\n    } else ;\n    return true;\n  };\n  /**\n   * _isBasicCustomElement\n   * checks if at least one dash is included in tagName, and it's not the first char\n   * for more sophisticated checking see https://github.com/sindresorhus/validate-element-name\n   *\n   * @param tagName name of the tag of the node to sanitize\n   * @returns Returns true if the tag name meets the basic criteria for a custom element, otherwise false.\n   */\n  const _isBasicCustomElement = function _isBasicCustomElement(tagName) {\n    return tagName !== 'annotation-xml' && stringMatch(tagName, CUSTOM_ELEMENT);\n  };\n  /**\n   * _sanitizeAttributes\n   *\n   * @protect attributes\n   * @protect nodeName\n   * @protect removeAttribute\n   * @protect setAttribute\n   *\n   * @param currentNode to sanitize\n   */\n  const _sanitizeAttributes = function _sanitizeAttributes(currentNode) {\n    /* Execute a hook if present */\n    _executeHooks(hooks.beforeSanitizeAttributes, currentNode, null);\n    const {\n      attributes\n    } = currentNode;\n    /* Check if we have attributes; if not we might have a text node */\n    if (!attributes || _isClobbered(currentNode)) {\n      return;\n    }\n    const hookEvent = {\n      attrName: '',\n      attrValue: '',\n      keepAttr: true,\n      allowedAttributes: ALLOWED_ATTR,\n      forceKeepAttr: undefined\n    };\n    let l = attributes.length;\n    /* Go backwards over all attributes; safely remove bad ones */\n    while (l--) {\n      const attr = attributes[l];\n      const {\n        name,\n        namespaceURI,\n        value: attrValue\n      } = attr;\n      const lcName = transformCaseFunc(name);\n      let value = name === 'value' ? attrValue : stringTrim(attrValue);\n      /* Execute a hook if present */\n      hookEvent.attrName = lcName;\n      hookEvent.attrValue = value;\n      hookEvent.keepAttr = true;\n      hookEvent.forceKeepAttr = undefined; // Allows developers to see this is a property they can set\n      _executeHooks(hooks.uponSanitizeAttribute, currentNode, hookEvent);\n      value = hookEvent.attrValue;\n      /* Full DOM Clobbering protection via namespace isolation,\n       * Prefix id and name attributes with `user-content-`\n       */\n      if (SANITIZE_NAMED_PROPS && (lcName === 'id' || lcName === 'name')) {\n        // Remove the attribute with this value\n        _removeAttribute(name, currentNode);\n        // Prefix the value and later re-create the attribute with the sanitized value\n        value = SANITIZE_NAMED_PROPS_PREFIX + value;\n      }\n      /* Work around a security issue with comments inside attributes */\n      if (SAFE_FOR_XML && regExpTest(/((--!?|])>)|<\\/(style|title)/i, value)) {\n        _removeAttribute(name, currentNode);\n        continue;\n      }\n      /* Did the hooks approve of the attribute? */\n      if (hookEvent.forceKeepAttr) {\n        continue;\n      }\n      /* Remove attribute */\n      _removeAttribute(name, currentNode);\n      /* Did the hooks approve of the attribute? */\n      if (!hookEvent.keepAttr) {\n        continue;\n      }\n      /* Work around a security issue in jQuery 3.0 */\n      if (!ALLOW_SELF_CLOSE_IN_ATTR && regExpTest(/\\/>/i, value)) {\n        _removeAttribute(name, currentNode);\n        continue;\n      }\n      /* Sanitize attribute content to be template-safe */\n      if (SAFE_FOR_TEMPLATES) {\n        arrayForEach([MUSTACHE_EXPR, ERB_EXPR, TMPLIT_EXPR], expr => {\n          value = stringReplace(value, expr, ' ');\n        });\n      }\n      /* Is `value` valid for this attribute? */\n      const lcTag = transformCaseFunc(currentNode.nodeName);\n      if (!_isValidAttribute(lcTag, lcName, value)) {\n        continue;\n      }\n      /* Handle attributes that require Trusted Types */\n      if (trustedTypesPolicy && typeof trustedTypes === 'object' && typeof trustedTypes.getAttributeType === 'function') {\n        if (namespaceURI) ; else {\n          switch (trustedTypes.getAttributeType(lcTag, lcName)) {\n            case 'TrustedHTML':\n              {\n                value = trustedTypesPolicy.createHTML(value);\n                break;\n              }\n            case 'TrustedScriptURL':\n              {\n                value = trustedTypesPolicy.createScriptURL(value);\n                break;\n              }\n          }\n        }\n      }\n      /* Handle invalid data-* attribute set by try-catching it */\n      try {\n        if (namespaceURI) {\n          currentNode.setAttributeNS(namespaceURI, name, value);\n        } else {\n          /* Fallback to setAttribute() for browser-unrecognized namespaces e.g. \"x-schema\". */\n          currentNode.setAttribute(name, value);\n        }\n        if (_isClobbered(currentNode)) {\n          _forceRemove(currentNode);\n        } else {\n          arrayPop(DOMPurify.removed);\n        }\n      } catch (_) {}\n    }\n    /* Execute a hook if present */\n    _executeHooks(hooks.afterSanitizeAttributes, currentNode, null);\n  };\n  /**\n   * _sanitizeShadowDOM\n   *\n   * @param fragment to iterate over recursively\n   */\n  const _sanitizeShadowDOM = function _sanitizeShadowDOM(fragment) {\n    let shadowNode = null;\n    const shadowIterator = _createNodeIterator(fragment);\n    /* Execute a hook if present */\n    _executeHooks(hooks.beforeSanitizeShadowDOM, fragment, null);\n    while (shadowNode = shadowIterator.nextNode()) {\n      /* Execute a hook if present */\n      _executeHooks(hooks.uponSanitizeShadowNode, shadowNode, null);\n      /* Sanitize tags and elements */\n      _sanitizeElements(shadowNode);\n      /* Check attributes next */\n      _sanitizeAttributes(shadowNode);\n      /* Deep shadow DOM detected */\n      if (shadowNode.content instanceof DocumentFragment) {\n        _sanitizeShadowDOM(shadowNode.content);\n      }\n    }\n    /* Execute a hook if present */\n    _executeHooks(hooks.afterSanitizeShadowDOM, fragment, null);\n  };\n  // eslint-disable-next-line complexity\n  DOMPurify.sanitize = function (dirty) {\n    let cfg = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : {};\n    let body = null;\n    let importedNode = null;\n    let currentNode = null;\n    let returnNode = null;\n    /* Make sure we have a string to sanitize.\n      DO NOT return early, as this will return the wrong type if\n      the user has requested a DOM object rather than a string */\n    IS_EMPTY_INPUT = !dirty;\n    if (IS_EMPTY_INPUT) {\n      dirty = '<!-->';\n    }\n    /* Stringify, in case dirty is an object */\n    if (typeof dirty !== 'string' && !_isNode(dirty)) {\n      if (typeof dirty.toString === 'function') {\n        dirty = dirty.toString();\n        if (typeof dirty !== 'string') {\n          throw typeErrorCreate('dirty is not a string, aborting');\n        }\n      } else {\n        throw typeErrorCreate('toString is not a function');\n      }\n    }\n    /* Return dirty HTML if DOMPurify cannot run */\n    if (!DOMPurify.isSupported) {\n      return dirty;\n    }\n    /* Assign config vars */\n    if (!SET_CONFIG) {\n      _parseConfig(cfg);\n    }\n    /* Clean up removed elements */\n    DOMPurify.removed = [];\n    /* Check if dirty is correctly typed for IN_PLACE */\n    if (typeof dirty === 'string') {\n      IN_PLACE = false;\n    }\n    if (IN_PLACE) {\n      /* Do some early pre-sanitization to avoid unsafe root nodes */\n      if (dirty.nodeName) {\n        const tagName = transformCaseFunc(dirty.nodeName);\n        if (!ALLOWED_TAGS[tagName] || FORBID_TAGS[tagName]) {\n          throw typeErrorCreate('root node is forbidden and cannot be sanitized in-place');\n        }\n      }\n    } else if (dirty instanceof Node) {\n      /* If dirty is a DOM element, append to an empty document to avoid\n         elements being stripped by the parser */\n      body = _initDocument('<!---->');\n      importedNode = body.ownerDocument.importNode(dirty, true);\n      if (importedNode.nodeType === NODE_TYPE.element && importedNode.nodeName === 'BODY') {\n        /* Node is already a body, use as is */\n        body = importedNode;\n      } else if (importedNode.nodeName === 'HTML') {\n        body = importedNode;\n      } else {\n        // eslint-disable-next-line unicorn/prefer-dom-node-append\n        body.appendChild(importedNode);\n      }\n    } else {\n      /* Exit directly if we have nothing to do */\n      if (!RETURN_DOM && !SAFE_FOR_TEMPLATES && !WHOLE_DOCUMENT &&\n      // eslint-disable-next-line unicorn/prefer-includes\n      dirty.indexOf('<') === -1) {\n        return trustedTypesPolicy && RETURN_TRUSTED_TYPE ? trustedTypesPolicy.createHTML(dirty) : dirty;\n      }\n      /* Initialize the document to work on */\n      body = _initDocument(dirty);\n      /* Check we have a DOM node from the data */\n      if (!body) {\n        return RETURN_DOM ? null : RETURN_TRUSTED_TYPE ? emptyHTML : '';\n      }\n    }\n    /* Remove first element node (ours) if FORCE_BODY is set */\n    if (body && FORCE_BODY) {\n      _forceRemove(body.firstChild);\n    }\n    /* Get node iterator */\n    const nodeIterator = _createNodeIterator(IN_PLACE ? dirty : body);\n    /* Now start iterating over the created document */\n    while (currentNode = nodeIterator.nextNode()) {\n      /* Sanitize tags and elements */\n      _sanitizeElements(currentNode);\n      /* Check attributes next */\n      _sanitizeAttributes(currentNode);\n      /* Shadow DOM detected, sanitize it */\n      if (currentNode.content instanceof DocumentFragment) {\n        _sanitizeShadowDOM(currentNode.content);\n      }\n    }\n    /* If we sanitized `dirty` in-place, return it. */\n    if (IN_PLACE) {\n      return dirty;\n    }\n    /* Return sanitized string or DOM */\n    if (RETURN_DOM) {\n      if (RETURN_DOM_FRAGMENT) {\n        returnNode = createDocumentFragment.call(body.ownerDocument);\n        while (body.firstChild) {\n          // eslint-disable-next-line unicorn/prefer-dom-node-append\n          returnNode.appendChild(body.firstChild);\n        }\n      } else {\n        returnNode = body;\n      }\n      if (ALLOWED_ATTR.shadowroot || ALLOWED_ATTR.shadowrootmode) {\n        /*\n          AdoptNode() is not used because internal state is not reset\n          (e.g. the past names map of a HTMLFormElement), this is safe\n          in theory but we would rather not risk another attack vector.\n          The state that is cloned by importNode() is explicitly defined\n          by the specs.\n        */\n        returnNode = importNode.call(originalDocument, returnNode, true);\n      }\n      return returnNode;\n    }\n    let serializedHTML = WHOLE_DOCUMENT ? body.outerHTML : body.innerHTML;\n    /* Serialize doctype if allowed */\n    if (WHOLE_DOCUMENT && ALLOWED_TAGS['!doctype'] && body.ownerDocument && body.ownerDocument.doctype && body.ownerDocument.doctype.name && regExpTest(DOCTYPE_NAME, body.ownerDocument.doctype.name)) {\n      serializedHTML = '<!DOCTYPE ' + body.ownerDocument.doctype.name + '>\\n' + serializedHTML;\n    }\n    /* Sanitize final string template-safe */\n    if (SAFE_FOR_TEMPLATES) {\n      arrayForEach([MUSTACHE_EXPR, ERB_EXPR, TMPLIT_EXPR], expr => {\n        serializedHTML = stringReplace(serializedHTML, expr, ' ');\n      });\n    }\n    return trustedTypesPolicy && RETURN_TRUSTED_TYPE ? trustedTypesPolicy.createHTML(serializedHTML) : serializedHTML;\n  };\n  DOMPurify.setConfig = function () {\n    let cfg = arguments.length > 0 && arguments[0] !== undefined ? arguments[0] : {};\n    _parseConfig(cfg);\n    SET_CONFIG = true;\n  };\n  DOMPurify.clearConfig = function () {\n    CONFIG = null;\n    SET_CONFIG = false;\n  };\n  DOMPurify.isValidAttribute = function (tag, attr, value) {\n    /* Initialize shared config vars if necessary. */\n    if (!CONFIG) {\n      _parseConfig({});\n    }\n    const lcTag = transformCaseFunc(tag);\n    const lcName = transformCaseFunc(attr);\n    return _isValidAttribute(lcTag, lcName, value);\n  };\n  DOMPurify.addHook = function (entryPoint, hookFunction) {\n    if (typeof hookFunction !== 'function') {\n      return;\n    }\n    arrayPush(hooks[entryPoint], hookFunction);\n  };\n  DOMPurify.removeHook = function (entryPoint, hookFunction) {\n    if (hookFunction !== undefined) {\n      const index = arrayLastIndexOf(hooks[entryPoint], hookFunction);\n      return index === -1 ? undefined : arraySplice(hooks[entryPoint], index, 1)[0];\n    }\n    return arrayPop(hooks[entryPoint]);\n  };\n  DOMPurify.removeHooks = function (entryPoint) {\n    hooks[entryPoint] = [];\n  };\n  DOMPurify.removeAllHooks = function () {\n    hooks = _createHooksMap();\n  };\n  return DOMPurify;\n}\nvar purify = createDOMPurify();\n\nexport { purify as default };\n//# sourceMappingURL=purify.es.mjs.map\n", "import createDOMPurify from \"dompurify\";\nimport { JSDOM } from \"jsdom\";\n\nconst windowEmulator: any = new JSDOM(\"\").window;\nconst DOMPurify = createDOMPurify(windowEmulator);\n\n/*\n * DOMPurify prevents all XSS attacks by default. With these settings, it also\n * prevents \"deception\" attacks. If an attacker could put <div style=\"...\">\n * into the site's admin banner, they could make give the banner any appearance,\n * overlaid anywhere on the page. For example, a fake \"session expired\" modal\n * with a malicious link. Thus, this very strict DOMPurify config.\n */\nDOMPurify.setConfig({\n  // Only these tags will be allowed through\n  ALLOWED_TAGS: [\"ul\", \"ol\", \"li\", \"a\", \"#text\"],\n  // On those tags, only these attributes are allowed\n  ALLOWED_ATTR: [\"href\", \"alt\"],\n  // If a tag is removed, so will all its child elements & text\n  KEEP_CONTENT: false,\n});\n\n// sanitize string\nexport const sanitizeString = (string: string) => {\n  if (DOMPurify.isSupported) {\n    return DOMPurify.sanitize(string);\n  }\n};\n\n// iterates over array items, sanitizing items recursively\nexport const sanitizeArray = (array: unknown[]): unknown[] =>\n  array.map((entry: unknown) => sanitizeEntry(entry));\n\n// iterates over object key-value pairs, sanitizing values recursively\nexport const sanitizeObject = (object: { [key: string]: unknown }) => {\n  if (object) {\n    const entries = Object.entries(object);\n    const sanitizedEntries = entries.map((entry: [string, unknown]) => {\n      const [key, value] = entry;\n      return [key, sanitizeEntry(value)];\n    });\n    return Object.fromEntries(sanitizedEntries);\n  }\n};\n\nconst sanitizerMap: any = {\n  string: sanitizeString,\n  array: sanitizeArray,\n  object: sanitizeObject,\n};\n\n// return sanitized entry, or if safe type, return entry\nconst sanitizeEntry = (entry: unknown) => {\n  const entryType = Array.isArray(entry) ? \"array\" : typeof entry;\n  const sanitizer = sanitizerMap[entryType];\n  return sanitizer?.(entry) || entry;\n};\n", "// utils\nimport * as logger from \"../utils/debugging/debug-lib\";\nimport { isAuthenticated } from \"../utils/auth/authorization\";\nimport {\n  HttpResponse,\n  internalServerError,\n  unauthenticated,\n} from \"../utils/responses/response-lib\";\nimport { error } from \"../utils/constants/constants\";\nimport { sanitizeObject } from \"../utils/sanitize/sanitize\";\n// types\nimport { APIGatewayProxyEvent } from \"../utils/types\";\n\ntype LambdaFunction = (\n  event: APIGatewayProxyEvent, // eslint-disable-line no-unused-vars\n  context: any // eslint-disable-line no-unused-vars\n) => Promise<HttpResponse>;\n\nexport default function handler(lambda: LambdaFunction) {\n  return async function (event: APIGatewayProxyEvent, context: any) {\n    // Start debugger\n    logger.init();\n    logger.debug(\"API event: %O\", {\n      body: event.body,\n      pathParameters: event.pathParameters,\n      queryStringParameters: event.queryStringParameters,\n    });\n    if (await isAuthenticated(event)) {\n      try {\n        if (event.body) {\n          const newEventBody = sanitizeObject(JSON.parse(event.body));\n          event.body = JSON.stringify(newEventBody);\n        }\n        return await lambda(event, context);\n      } catch (error: any) {\n        logger.error(\"Error: %O\", error);\n\n        const body = { error: error.message };\n        return internalServerError(body);\n      } finally {\n        logger.flush();\n      }\n    } else {\n      return unauthenticated(error.UNAUTHORIZED);\n    }\n  };\n}\n", "import { APIGatewayProxyEvent, isReportType, isState } from \"../types\";\nimport { logger } from \"../debugging/debug-lib\";\n\nexport const parseSpecificReportParameters = (event: APIGatewayProxyEvent) => {\n  const { reportType, state, id } = event.pathParameters ?? {};\n  if (!isReportType(reportType)) {\n    logger.warn(\"Invalid report type in path\");\n    return { allParamsValid: false as const };\n  }\n  if (!isState(state)) {\n    logger.warn(\"Invalid state in path\");\n    return { allParamsValid: false as const };\n  }\n  if (!id) {\n    logger.warn(\"Invalid report ID in path\");\n    return { allParamsValid: false as const };\n  }\n  return { allParamsValid: true as const, reportType, state, id };\n};\n\nexport const parseStateReportParameters = (event: APIGatewayProxyEvent) => {\n  const { reportType, state } = event.pathParameters ?? {};\n  if (!isReportType(reportType)) {\n    logger.warn(\"Invalid report type in path\");\n    return { allParamsValid: false as const };\n  }\n  if (!isState(state)) {\n    logger.warn(\"Invalid state in path\");\n    return { allParamsValid: false as const };\n  }\n  return { allParamsValid: true as const, reportType, state };\n};\n", "import { object } from \"yup\";\nimport { error } from \"../constants/constants\";\n// types\nimport { AnyObject } from \"../types\";\n// utils\nimport { nested, endDate } from \"./completionSchemas\";\nimport { completionSchemaMap as schemaMap } from \"./completionSchemaMap\";\n\n// map field validation types to validation schema\nexport const mapValidationTypesToSchema = (fieldValidationTypes: AnyObject) => {\n  let validationSchema: AnyObject = {};\n  // for each field to be validated,\n  Object.entries(fieldValidationTypes).forEach(\n    (fieldValidationType: [string, string | AnyObject]) => {\n      const [key, fieldValidation] = fieldValidationType;\n\n      // if standard validation type, set corresponding schema from map\n      if (typeof fieldValidation === \"string\") {\n        const correspondingSchema = schemaMap[fieldValidation];\n        if (correspondingSchema) {\n          validationSchema[key] = correspondingSchema;\n        }\n      }\n      // else if nested validation type, make and set nested schema\n      else if (fieldValidation.nested) {\n        validationSchema[key] = makeNestedFieldSchema(fieldValidation);\n        // else if not nested, make and set other dependent field types\n      } else if (fieldValidation.type === \"endDate\") {\n        validationSchema[key] = makeEndDateFieldSchema(fieldValidation);\n      }\n    }\n  );\n  return validationSchema;\n};\n\n// return created endDate schema\nexport const makeEndDateFieldSchema = (fieldValidationObject: AnyObject) => {\n  const { dependentFieldName } = fieldValidationObject;\n  return endDate(dependentFieldName);\n};\n\n// return created nested field schema\nexport const makeNestedFieldSchema = (fieldValidationObject: AnyObject) => {\n  const { type, parentFieldName, parentOptionId } = fieldValidationObject;\n  if (fieldValidationObject.type === \"endDate\") {\n    return nested(\n      () => makeEndDateFieldSchema(fieldValidationObject),\n      parentFieldName,\n      parentOptionId\n    );\n  } else {\n    const fieldValidationSchema = schemaMap[type];\n    return nested(() => fieldValidationSchema, parentFieldName, parentOptionId);\n  }\n};\n\nexport const validateFieldData = async (\n  validationJson: AnyObject,\n  unvalidatedFieldData: AnyObject\n) => {\n  let validatedFieldData: AnyObject | undefined = undefined;\n  // filter field validation to just what's needed for the passed fields\n  const filteredFieldDataValidationJson = filterValidationSchema(\n    validationJson,\n    unvalidatedFieldData\n  );\n  // transform field validation instructions to yup validation schema\n  const fieldDataValidationSchema = object().shape(\n    mapValidationTypesToSchema(filteredFieldDataValidationJson)\n  );\n  if (fieldDataValidationSchema) {\n    validatedFieldData = await validateData(\n      fieldDataValidationSchema,\n      unvalidatedFieldData\n    );\n  }\n  return validatedFieldData;\n};\n\n// filter field validation to just what's needed for the passed fields\nexport const filterValidationSchema = (\n  validationObject: AnyObject,\n  data: AnyObject\n): AnyObject => {\n  const validationEntries = Object.entries(validationObject);\n  const dataKeys = Object.keys(data);\n  const filteredEntries = validationEntries.filter(\n    (entry: [string, string | AnyObject]) => {\n      const [entryKey] = entry;\n      return dataKeys.includes(entryKey);\n    }\n  );\n  return Object.fromEntries(filteredEntries);\n};\n\nexport const validateData = async (\n  validationSchema: AnyObject,\n  data: AnyObject,\n  options?: AnyObject\n) => {\n  try {\n    // returns valid data to be passed through API\n    return await validationSchema.validate(data, {\n      stripUnknown: true,\n      ...options,\n    });\n  } catch {\n    throw new Error(error.INVALID_DATA);\n  }\n};\n", "import {\n  array,\n  boolean,\n  mixed,\n  object,\n  string,\n  number as yupNumber,\n} from \"yup\";\nimport { Choice } from \"../types\";\n\nexport const error = {\n  REQUIRED_GENERIC: \"A response is required\",\n  REQUIRED_CHECKBOX: \"Select at least one response\",\n  INVALID_GENERIC: \"Response must be valid\",\n  INVALID_EMAIL: \"Response must be a valid email address\",\n  INVALID_URL: \"Response must be a valid hyperlink/URL\",\n  INVALID_DATE: \"Response must be a valid date\",\n  INVALID_END_DATE: \"End date can't be before start date\",\n  NUMBER_LESS_THAN_ZERO: \"Response must be greater than or equal to zero\",\n  INVALID_NUMBER: \"Response must be a valid number\",\n  INVALID_NUMBER_OR_NA: 'Response must be a valid number or \"N/A\"',\n  INVALID_RATIO: \"Response must be a valid ratio\",\n};\n\n// TEXT - Helpers\nconst isWhitespaceString = (value?: string) => value?.trim().length === 0;\n\n// TEXT\nconst textSchema = () =>\n  string()\n    .typeError(error.INVALID_GENERIC)\n    .test({\n      test: (value) => !isWhitespaceString(value),\n      message: error.REQUIRED_GENERIC,\n    });\n\nexport const text = () => textSchema().required();\nexport const textOptional = () => textSchema().notRequired().nullable();\n\n// NUMBER - Helpers\nconst validNAValues = [\"N/A\", \"Data not available\"];\n\n/** This regex must be at least as permissive as the one in ui-src */\nconst validNumberRegex = /^\\.$|[0-9]/;\n\nconst validIntegerRegex = /^[0-9\\s,]+$/;\n\n// NUMBER - Number or Valid Strings\nconst numberSchema = () =>\n  string()\n    .test({\n      message: error.INVALID_NUMBER_OR_NA,\n      test: (value) => {\n        if (value) {\n          const isValidStringValue = validNAValues.includes(value);\n          const isValidNumberValue = validNumberRegex.test(value);\n          return isValidStringValue || isValidNumberValue;\n        } else return true;\n      },\n    })\n    .test({\n      test: (value) => !isWhitespaceString(value),\n      message: error.REQUIRED_GENERIC,\n    })\n    .test({\n      test: (value) => {\n        if (validNumberRegex.test(value!)) {\n          return parseFloat(value!) >= 0;\n        } else return true;\n      },\n      message: error.NUMBER_LESS_THAN_ZERO,\n    });\n\nconst valueCleaningNumberSchema = (value: string, charsToReplace: RegExp) => {\n  return yupNumber().transform((_value) => {\n    return Number(value.replace(charsToReplace, \"\"));\n  });\n};\n\nexport const number = () => numberSchema().required();\nexport const numberOptional = () => numberSchema().notRequired().nullable();\n\n// Integer or Valid Strings\nexport const validIntegerSchema = () =>\n  string().test({\n    message: error.INVALID_NUMBER_OR_NA,\n    test: (value) => {\n      if (value) {\n        const isValidStringValue = validNAValues.includes(value);\n        const isValidIntegerValue = validIntegerRegex.test(value);\n        return isValidStringValue || isValidIntegerValue;\n      } else return true;\n    },\n  });\n\nexport const validInteger = () =>\n  validIntegerSchema()\n    .required(error.REQUIRED_GENERIC)\n    .test({\n      test: (value) => !isWhitespaceString(value),\n      message: error.REQUIRED_GENERIC,\n    });\n\nexport const validIntegerOptional = () =>\n  validIntegerSchema().notRequired().nullable();\n\n// Number - Ratio\nexport const ratio = () =>\n  mixed()\n    .test({\n      message: error.REQUIRED_GENERIC,\n      test: (val) => val != \"\",\n    })\n    .required(error.REQUIRED_GENERIC)\n    .test({\n      message: error.INVALID_RATIO,\n      test: (val) => {\n        const replaceCharsRegex = /[,.:]/g;\n        const ratio = val?.split(\":\");\n\n        // Double check and make sure that a ratio contains numbers on both sides\n        if (\n          !ratio ||\n          ratio.length != 2 ||\n          ratio[0].trim().length == 0 ||\n          ratio[1].trim().length == 0\n        ) {\n          return false;\n        }\n\n        // Check if the left side of the ratio is a valid number\n        const firstTest = valueCleaningNumberSchema(\n          ratio[0],\n          replaceCharsRegex\n        ).isValidSync(val);\n\n        // Check if the right side of the ratio is a valid number\n        const secondTest = valueCleaningNumberSchema(\n          ratio[1],\n          replaceCharsRegex\n        ).isValidSync(val);\n\n        // If both sides are valid numbers, return true!\n        return firstTest && secondTest;\n      },\n    });\n\n// EMAIL\n\nexport const email = () => textSchema().email(error.INVALID_EMAIL).required();\nexport const emailOptional = () =>\n  textSchema().email(error.INVALID_EMAIL).notRequired().nullable();\n\n// URL\nexport const url = () => textSchema().url(error.INVALID_URL).required();\nexport const urlOptional = () =>\n  textSchema().url(error.INVALID_URL).notRequired().nullable();\n\n// DATE\nconst dateSchema = () =>\n  string()\n    .matches(dateFormatRegex, error.INVALID_DATE)\n    .test({\n      message: error.REQUIRED_GENERIC,\n      test: (value) => !isWhitespaceString(value),\n    });\n\nexport const date = () => dateSchema().required(error.REQUIRED_GENERIC);\nexport const dateOptional = () => dateSchema().notRequired().nullable();\n\nexport const endDate = (startDateField: string) =>\n  date()\n    .typeError(error.INVALID_DATE)\n    .test({\n      message: error.INVALID_END_DATE,\n      test: (endDateString, context) => {\n        return isEndDateAfterStartDate(\n          context.parent[startDateField],\n          endDateString as string\n        );\n      },\n    });\n\nexport const isEndDateAfterStartDate = (\n  startDateString: string,\n  endDateString: string\n) => {\n  const startDate = new Date(startDateString);\n  const endDate = new Date(endDateString!);\n  return endDate >= startDate;\n};\n\n// DROPDOWN\nexport const dropdown = () =>\n  object({ label: textSchema(), value: textSchema() }).required(\n    error.REQUIRED_GENERIC\n  );\n\n// CHECKBOX\nexport const checkboxSchema = () =>\n  array()\n    .of(object({ key: text(), value: text() }))\n    .required(error.REQUIRED_GENERIC);\nexport const checkbox = () =>\n  checkboxSchema()\n    .min(1, error.REQUIRED_GENERIC)\n    .required(error.REQUIRED_GENERIC);\nexport const checkboxOptional = () =>\n  checkboxSchema().min(0, error.REQUIRED_GENERIC).notRequired().nullable();\nexport const checkboxSingle = () => boolean();\n\n// RADIO\nexport const radioSchema = () =>\n  array()\n    .of(object({ key: textSchema(), value: textSchema() }))\n    .min(0);\n\nexport const radio = () =>\n  radioSchema().min(1, error.REQUIRED_GENERIC).required();\nexport const radioOptional = () => radioSchema().notRequired().nullable();\n\n// DYNAMIC\nexport const dynamic = () =>\n  array()\n    .min(1)\n    .of(\n      object().shape({\n        id: textSchema(),\n        name: textSchema(),\n      })\n    )\n    .required(error.REQUIRED_GENERIC);\nexport const dynamicOptional = () => dynamic().notRequired();\n\n// NESTED\nexport const nested = (\n  fieldSchema: Function,\n  parentFieldName: string,\n  parentOptionId: string\n) => {\n  const fieldTypeMap = {\n    array: array(),\n    string: string(),\n    date: dateSchema(),\n    object: object(),\n  };\n  const fieldType: keyof typeof fieldTypeMap = fieldSchema().type;\n  const baseSchema: any = fieldTypeMap[fieldType];\n  return baseSchema.when(parentFieldName, {\n    is: (value: Choice[]) =>\n      // look for parentOptionId in checked choices\n      value?.find((option: Choice) => option.key.endsWith(parentOptionId)),\n    then: () => fieldSchema(), // returns standard field schema (required)\n    otherwise: () => baseSchema, // returns not-required Yup base schema\n  });\n};\n\n// REGEX\nexport const dateFormatRegex =\n  /^((0[1-9]|1[0-2])\\/(0[1-9]|1\\d|2\\d|3[01])\\/(19|20)\\d{2})|((0[1-9]|1[0-2])(0[1-9]|1\\d|2\\d|3[01])(19|20)\\d{2})$/;\n", "import * as schema from \"./completionSchemas\";\n\nexport const completionSchemaMap: any = {\n  text: schema.text(),\n  textOptional: schema.textOptional(),\n  number: schema.number(),\n  numberOptional: schema.numberOptional(),\n  ratio: schema.ratio(),\n  email: schema.email(),\n  emailOptional: schema.emailOptional(),\n  url: schema.url(),\n  urlOptional: schema.urlOptional(),\n  date: schema.date(),\n  dateOptional: schema.dateOptional(),\n  dropdown: schema.dropdown(),\n  checkbox: schema.checkbox(),\n  checkboxOptional: schema.checkboxOptional(),\n  checkboxSingle: schema.checkboxSingle(),\n  radio: schema.radio(),\n  radioOptional: schema.radioOptional(),\n  dynamic: schema.dynamic(),\n  dynamicOptional: schema.dynamicOptional(),\n  validInteger: schema.validInteger(),\n  validIntegerOptional: schema.validIntegerOptional(),\n};\n", "// types\nimport { DEFAULT_TARGET_POPULATION_NAMES } from \"../constants/constants\";\nimport {\n  AnyObject,\n  CompletionData,\n  FormJson,\n  FieldChoice,\n  Choice,\n  FormField,\n  ReportRoute,\n} from \"../types\";\n// utils\nimport { validateFieldData } from \"./completionValidation\";\n\nexport const isComplete = (completionStatus: CompletionData): boolean => {\n  const flatten = (obj: AnyObject, out: AnyObject) => {\n    Object.keys(obj).forEach((key) => {\n      if (typeof obj[key] == \"object\") {\n        out = flatten(obj[key], out);\n      } else {\n        out[key] = obj[key];\n      }\n    });\n    return out;\n  };\n\n  const flattenedStatus = flatten(completionStatus, {});\n\n  for (const status in flattenedStatus) {\n    if (flattenedStatus[status] === false) {\n      return false;\n    }\n  }\n  return true;\n};\n\n// Entry point for calculating completion status\nexport const calculateCompletionStatus = async (\n  fieldData: AnyObject,\n  formTemplate: AnyObject\n) => {\n  // Parent Dictionary for holding all route completion status\n\n  const validationJson = formTemplate.validationJson;\n\n  const areFieldsValid = async (\n    fieldsToBeValidated: Record<string, string>\n  ) => {\n    let areAllFieldsValid = false;\n    try {\n      // all fields successfully validated if validatedFields is not undefined\n      areAllFieldsValid =\n        (await validateFieldData(validationJson, fieldsToBeValidated)) !==\n        undefined;\n    } catch {\n      // Silently ignore error, will result in false\n    }\n    return areAllFieldsValid;\n  };\n\n  const calculateFormCompletion = async (\n    nestedFormTemplate: FormJson,\n    dataForObject: AnyObject = fieldData\n  ) => {\n    // Build an object of k:v for fields to validate\n    let fieldsToBeValidated: Record<string, string> = {};\n    // Repeat fields can't be validated at same time, so holding their completion status here\n    let repeatersValid = true; //default to true in case of no repeat fields\n\n    const getNestedFields = (\n      fieldChoices: FieldChoice[],\n      selectedChoices: Choice[]\n    ) => {\n      let selectedChoicesIds = selectedChoices\n        .map((choice: Choice) => choice.key)\n        .map((choiceId: string) => choiceId?.split(\"-\").pop());\n      let selectedChoicesWithChildren = fieldChoices?.filter(\n        (fieldChoice: FieldChoice) =>\n          selectedChoicesIds.includes(fieldChoice.id) && fieldChoice.children\n      );\n      let fieldIds: string[] = [];\n      selectedChoicesWithChildren?.forEach((selectedChoice: FieldChoice) => {\n        selectedChoice.children?.forEach((childChoice: FormField) => {\n          fieldIds.push(childChoice.id);\n          if (childChoice.props?.choices && dataForObject?.[childChoice.id]) {\n            let childFields = getNestedFields(\n              childChoice.props?.choices,\n              dataForObject[childChoice.id]\n            );\n            fieldIds.push(...childFields);\n          }\n        });\n      });\n      return fieldIds;\n    };\n    // Iterate over all fields in form\n    for (var formField of nestedFormTemplate?.fields || []) {\n      // Key: Form Field ID, Value: Report Data for field\n      if (Array.isArray(dataForObject[formField.id])) {\n        let nestedFields: string[] = getNestedFields(\n          formField.props?.choices,\n          dataForObject[formField.id]\n        );\n        nestedFields?.forEach((nestedField: string) => {\n          fieldsToBeValidated[nestedField] = dataForObject[nestedField]\n            ? dataForObject[nestedField]\n            : null;\n        });\n      }\n\n      fieldsToBeValidated[formField.id] = dataForObject[formField.id]\n        ? dataForObject[formField.id]\n        : null;\n    }\n    // Validate all fields en masse, passing flag that uses required validation schema\n    return repeatersValid && (await areFieldsValid(fieldsToBeValidated));\n  };\n\n  const isDefaultPopulationApplicable = (targetPopulations: AnyObject[]) => {\n    const filteredPopulations = targetPopulations?.filter((population) => {\n      const isDefault = DEFAULT_TARGET_POPULATION_NAMES.includes(\n        population.transitionBenchmarks_targetPopulationName\n      );\n\n      const isApplicable =\n        population?.transitionBenchmarks_applicableToMfpDemonstration?.[0]\n          ?.value === \"Yes\";\n      return isDefault && isApplicable;\n    });\n    return filteredPopulations.length >= 1;\n  };\n\n  const calculateEntityCompletion = async (\n    nestedFormTemplates: FormJson[],\n    entityType: string\n  ) => {\n    let atLeastOneTargetPopApplicable = false;\n    //value for holding combined result\n    var areAllFormsComplete = true;\n    for (var nestedFormTemplate of nestedFormTemplates) {\n      if (fieldData[entityType] && fieldData[entityType].length > 0) {\n        // if target population, at least one must be applicable to be complete\n        if (\n          entityType === \"targetPopulations\" &&\n          nestedFormTemplate?.id === \"tb-drawer\"\n        ) {\n          atLeastOneTargetPopApplicable = isDefaultPopulationApplicable(\n            fieldData[entityType]\n          );\n        }\n        // iterate over each entity (eg transition benchmark)\n        for (var dataForEntity of fieldData[entityType]) {\n          // get completion status for entity, using the correct form template\n          const isEntityComplete = await calculateFormCompletion(\n            nestedFormTemplate,\n            dataForEntity\n          );\n          // update combined result\n          areAllFormsComplete &&= isEntityComplete;\n        }\n      } else {\n        //Entity not present in report data, so check to see if it is required and update combined result\n        areAllFormsComplete &&=\n          formTemplate.entities && !formTemplate.entities[entityType]?.required;\n      }\n    }\n    if (entityType === \"targetPopulations\" && !atLeastOneTargetPopApplicable) {\n      return false;\n    }\n    return areAllFormsComplete;\n  };\n\n  const calculateEntityWithStepsCompletion = async (\n    stepFormTemplates: any[],\n    entityType: string\n  ) => {\n    if (!fieldData[entityType] || fieldData[entityType].length <= 0)\n      return false;\n\n    var areAllFormsComplete = true;\n    for (let i = 0; i < stepFormTemplates.length; i++) {\n      let stepForm = stepFormTemplates[i];\n      for (var entityFields of fieldData[entityType]) {\n        //modal overlay pages should have an array of key stepType in fieldData, automatic false if it doesn't exist or array is empty\n        if (\n          stepForm.pageType === \"overlayModal\" &&\n          (!entityFields[stepForm.stepType] ||\n            entityFields[stepForm.stepType].length <= 0)\n        ) {\n          areAllFormsComplete &&= false;\n        } else if (stepForm.stepType === \"closeOutInformation\") {\n          //skip over closeOut at the moment until we can make WP copies\n        } else {\n          //detemine which fieldData to match to the stepForm\n          const entityFieldsList = entityFields[stepForm.stepType]\n            ? entityFields[stepForm.stepType]\n            : [entityFields];\n          //loop through all children that belong to that entity and validate the values\n          for (var stepFields of entityFieldsList) {\n            if (stepForm?.objectiveCards) {\n              for (let card of stepForm.objectiveCards) {\n                if (card?.modalForm) {\n                  const nestedFormTemplate = card.modalForm;\n\n                  if (nestedFormTemplate?.objectiveId !== stepFields?.id) {\n                    continue;\n                  }\n                  const isEntityComplete = await calculateFormCompletion(\n                    nestedFormTemplate,\n                    stepFields\n                  );\n                  areAllFormsComplete &&= isEntityComplete;\n                }\n              }\n            } else {\n              const nestedFormTemplate = stepForm.form\n                ? stepForm.form\n                : stepForm.modalForm;\n\n              //WP uses modaloverlay so it doesn't have an initiativeId, only SAR does\n              if (\n                nestedFormTemplate?.initiativeId !== stepFields?.id &&\n                formTemplate.type === \"SAR\"\n              ) {\n                continue;\n              }\n              const isEntityComplete = await calculateFormCompletion(\n                nestedFormTemplate,\n                stepFields\n              );\n              areAllFormsComplete &&= isEntityComplete;\n            }\n          }\n        }\n      }\n    }\n\n    return areAllFormsComplete;\n  };\n\n  const calculateDynamicModalOverlayCompletion = async (\n    initiatives: any[],\n    entityType: string\n  ) => {\n    let areAllFormsComplete = true;\n\n    for (let initiative of initiatives) {\n      const isComplete = await calculateEntityWithStepsCompletion(\n        initiative.entitySteps,\n        entityType\n      );\n      if (!isComplete) {\n        areAllFormsComplete = false;\n        break;\n      }\n    }\n    return areAllFormsComplete;\n  };\n\n  const calculateRouteCompletion = async (route: ReportRoute) => {\n    let routeCompletion;\n    // Determine which type of page we are calculating status for\n    switch (route.pageType) {\n      case \"standard\":\n        if (!route.form) break;\n        // Standard forms use simple validation\n        routeCompletion = {\n          [route.path]: await calculateFormCompletion(route.form),\n        };\n        break;\n      case \"drawer\":\n        if (!route.drawerForm) break;\n        routeCompletion = {\n          [route.path]: await calculateEntityCompletion(\n            [route.drawerForm],\n            route.entityType\n          ),\n        };\n        break;\n      case \"modalDrawer\":\n        if (!route.drawerForm || !route.modalForm) break;\n        routeCompletion = {\n          [route.path]: await calculateEntityCompletion(\n            [route.drawerForm, route.modalForm],\n            route.entityType\n          ),\n        };\n        break;\n      case \"modalOverlay\":\n        if (!route.modalForm) break;\n        if (route.entitySteps) {\n          routeCompletion = {\n            [route.path]: await calculateEntityWithStepsCompletion(\n              route.entitySteps as [],\n              route.entityType\n            ),\n          };\n        } else {\n          routeCompletion = {\n            [route.path]: await calculateEntityCompletion(\n              [route.modalForm],\n              route.entityType\n            ),\n          };\n        }\n        break;\n      case \"dynamicModalOverlay\":\n        if (!route.initiatives) break;\n        routeCompletion = {\n          [route.path]: await calculateDynamicModalOverlayCompletion(\n            route.initiatives as [],\n            route.entityType\n          ),\n        };\n        break;\n      case \"reviewSubmit\":\n        // Don't evaluate the review and submit page\n        break;\n      default:\n        if (!route.children) break;\n        // Default behavior indicates that we are not on a form to be evaluated, which implies we have child routes to evaluate\n        routeCompletion = {\n          [route.path]: await calculateRoutesCompletion(route.children),\n        };\n        break;\n    }\n    return routeCompletion;\n  };\n\n  const calculateRoutesCompletion = async (routes: ReportRoute[]) => {\n    var completionDict: CompletionData = {};\n    // Iterate over each route\n    for (var route of routes || []) {\n      // Determine the status of each child in the route\n      const routeCompletionDict = await calculateRouteCompletion(route);\n      // Add completion status to parent dictionary\n      completionDict = { ...completionDict, ...routeCompletionDict };\n    }\n    return completionDict;\n  };\n\n  return await calculateRoutesCompletion(formTemplate.routes);\n};\n", "import { GetObjectCommand, PutObjectCommand } from \"@aws-sdk/client-s3\";\nimport {\n  GetCommand,\n  paginateQuery,\n  PutCommand,\n  QueryCommand,\n} from \"@aws-sdk/lib-dynamodb\";\nimport {\n  FormTemplateVersion,\n  ReportFieldData,\n  ReportJson,\n  ReportMetadataShape,\n  ReportType,\n  State,\n} from \"../utils/types\";\nimport {\n  createClient as createDynamoClient,\n  collectPageItems,\n} from \"./dynamodb-lib\";\nimport { createClient as createS3Client, parseS3Response } from \"./s3-lib\";\nimport { reportBuckets, reportTables } from \"../utils/constants/constants\";\n\nconst dynamoClient = createDynamoClient();\nconst s3Client = createS3Client();\n\nconst formTemplateVersionTable = process.env.FormTemplateVersionsTable!;\n\n/* METADATA (dynamo) */\n\nexport const putReportMetadata = async (metadata: ReportMetadataShape) => {\n  await dynamoClient.send(\n    new PutCommand({\n      TableName: reportTables[metadata.reportType],\n      Item: metadata,\n    })\n  );\n};\n\nexport const queryReportMetadatasForState = async (\n  reportType: ReportType,\n  state: State\n) => {\n  const table = reportTables[reportType];\n  const responsePages = paginateQuery(\n    { client: dynamoClient },\n    {\n      TableName: table,\n      KeyConditionExpression: \"#state = :state\",\n      ExpressionAttributeNames: { \"#state\": \"state\" },\n      ExpressionAttributeValues: { \":state\": state },\n    }\n  );\n  const metadatas = await collectPageItems(responsePages);\n  return metadatas as ReportMetadataShape[];\n};\n\nexport const getReportMetadata = async (\n  reportType: ReportType,\n  state: State,\n  id: string\n) => {\n  const table = reportTables[reportType];\n  const response = await dynamoClient.send(\n    new GetCommand({\n      TableName: table,\n      Key: { state, id },\n    })\n  );\n  return response.Item as ReportMetadataShape | undefined;\n};\n\n/* FIELD DATA (s3) */\n\nexport const putReportFieldData = async (\n  {\n    reportType,\n    state,\n    fieldDataId,\n  }: Pick<ReportMetadataShape, \"reportType\" | \"state\" | \"fieldDataId\">,\n  fieldData: ReportFieldData\n) => {\n  const bucket = reportBuckets[reportType];\n  await s3Client.send(\n    new PutObjectCommand({\n      Bucket: bucket,\n      ContentType: \"application/json\",\n      Key: `fieldData/${state}/${fieldDataId}.json`,\n      Body: JSON.stringify(fieldData),\n    })\n  );\n};\n\nexport const getReportFieldData = async ({\n  reportType,\n  state,\n  fieldDataId,\n}: Pick<ReportMetadataShape, \"reportType\" | \"state\" | \"fieldDataId\">) => {\n  const bucket = reportBuckets[reportType];\n  const response = await s3Client.send(\n    new GetObjectCommand({\n      Bucket: bucket,\n      Key: `fieldData/${state}/${fieldDataId}.json`,\n    })\n  );\n  const fieldData = await parseS3Response(response);\n  return fieldData as ReportFieldData | undefined;\n};\n\n/* FORM TEMPLATES (s3) */\n\nexport const putReportFormTemplate = async (\n  {\n    reportType,\n    formTemplateId,\n  }: Pick<ReportMetadataShape, \"reportType\" | \"formTemplateId\">,\n  formTemplate: ReportJson\n) => {\n  const bucket = reportBuckets[reportType];\n  await s3Client.send(\n    new PutObjectCommand({\n      Bucket: bucket,\n      ContentType: \"application/json\",\n      Key: `formTemplates/${formTemplateId}.json`,\n      Body: JSON.stringify(formTemplate),\n    })\n  );\n};\n\nexport const getReportFormTemplate = async ({\n  reportType,\n  formTemplateId,\n}: Pick<ReportMetadataShape, \"reportType\" | \"formTemplateId\">) => {\n  const bucket = reportBuckets[reportType];\n  const response = await s3Client.send(\n    new GetObjectCommand({\n      Bucket: bucket,\n      Key: `formTemplates/${formTemplateId}.json`,\n    })\n  );\n  const fieldData = await parseS3Response(response);\n  return fieldData as ReportJson | undefined;\n};\n\n/* FORM TEMPLATE VERSIONS (dynamo) */\n\nexport const putFormTemplateVersion = async (\n  formTemplateVersion: FormTemplateVersion\n) => {\n  await dynamoClient.send(\n    new PutCommand({\n      TableName: formTemplateVersionTable,\n      Item: formTemplateVersion,\n    })\n  );\n};\n\nexport const queryFormTemplateVersionByHash = async (\n  reportType: ReportType,\n  md5Hash: string\n) => {\n  const response = await dynamoClient.send(\n    new QueryCommand({\n      TableName: formTemplateVersionTable,\n      IndexName: \"HashIndex\",\n      KeyConditionExpression: \"reportType = :reportType AND md5Hash = :md5Hash\",\n      ExpressionAttributeValues: {\n        \":reportType\": reportType,\n        \":md5Hash\": md5Hash,\n      },\n      Limit: 1,\n    })\n  );\n  return response.Items?.[0] as FormTemplateVersion | undefined;\n};\n\nexport const queryLatestFormTemplateVersionNumber = async (\n  reportType: ReportType\n) => {\n  const response = await dynamoClient.send(\n    new QueryCommand({\n      TableName: formTemplateVersionTable,\n      KeyConditionExpression: \"reportType = :reportType\",\n      ExpressionAttributeValues: { \":reportType\": reportType },\n      Limit: 1,\n      ScanIndexForward: false, // false -> backwards -> highest version first\n    })\n  );\n  const latestFormTemplate = response.Items?.[0] as\n    | FormTemplateVersion\n    | undefined;\n  return latestFormTemplate?.versionNumber ?? 0;\n};\n", "import {\n  DynamoDBClient,\n  QueryCommandOutput,\n  ScanCommandOutput,\n} from \"@aws-sdk/client-dynamodb\";\nimport { DynamoDBDocumentClient, Paginator } from \"@aws-sdk/lib-dynamodb\";\n// utils\nimport { logger } from \"../utils/debugging/debug-lib\";\n\nconst localConfig = {\n  endpoint: process.env.DYNAMODB_URL,\n  region: \"localhost\",\n  credentials: {\n    accessKeyId: \"LOCALFAKEKEY\", // pragma: allowlist secret\n    secretAccessKey: \"LOCALFAKESECRET\", // pragma: allowlist secret\n  },\n  logger,\n};\n\nconst awsConfig = {\n  region: \"us-east-1\",\n  logger,\n};\n\nconst getConfig = () => {\n  return process.env.DYNAMODB_URL ? localConfig : awsConfig;\n};\n\nexport const createClient = () => {\n  return DynamoDBDocumentClient.from(new DynamoDBClient(getConfig()));\n};\n\nexport const collectPageItems = async <\n  T extends QueryCommandOutput | ScanCommandOutput\n>(\n  paginator: Paginator<T>\n) => {\n  let items: Record<string, any>[] = [];\n  for await (let page of paginator) {\n    items = items.concat(page.Items ?? []);\n  }\n  return items;\n};\n", "import { S3Client, GetObjectCommandOutput } from \"@aws-sdk/client-s3\";\nimport { logger } from \"../utils/debugging/debug-lib\";\n\nconst localConfig = {\n  endpoint: process.env.S3_LOCAL_ENDPOINT,\n  region: \"localhost\",\n  forcePathStyle: true,\n  credentials: {\n    accessKeyId: \"S3RVER\", // pragma: allowlist secret\n    secretAccessKey: \"S3RVER\", // pragma: allowlist secret\n  },\n  logger,\n};\n\nconst awsConfig = {\n  region: \"us-east-1\",\n  logger,\n};\n\nconst getConfig = () => {\n  return process.env.S3_LOCAL_ENDPOINT ? localConfig : awsConfig;\n};\n\nexport const createClient = () => new S3Client(getConfig());\n\nexport const parseS3Response = async (response: GetObjectCommandOutput) => {\n  const stringBody = await response.Body?.transformToString();\n  if (!stringBody) {\n    logger.warn(`Empty response from S3`);\n    return undefined;\n  }\n  return JSON.parse(stringBody);\n};\n"],
./.cdk/cdk.out/asset.5d289a595ce478db9df012f373875df4ba074358b98a94efabf988a6fe8201d6/index.js.map:4: TODO: remove\n\n\n  get _type() {\n    return this.type;\n  }\n\n  _typeCheck(_value) {\n    return true;\n  }\n\n  clone(spec) {\n    if (this._mutate) {\n      if (spec) Object.assign(this.spec, spec);\n      return this;\n    } // if the nested value is a schema we can skip cloning, since\n    // they are already immutable\n\n\n    const next = Object.create(Object.getPrototypeOf(this)); // @ts-expect-error this is readonly\n\n    next.type = this.type;\n    next._typeError = this._typeError;\n    next._whitelistError = this._whitelistError;\n    next._blacklistError = this._blacklistError;\n    next._whitelist = this._whitelist.clone();\n    next._blacklist = this._blacklist.clone();\n    next.exclusiveTests = _extends({}, this.exclusiveTests); // @ts-expect-error this is readonly\n\n    next.deps = [...this.deps];\n    next.conditions = [...this.conditions];\n    next.tests = [...this.tests];\n    next.transforms = [...this.transforms];\n    next.spec = (0, _nanoclone.default)(_extends({}, this.spec, spec));\n    return next;\n  }\n\n  label(label) {\n    let next = this.clone();\n    next.spec.label = label;\n    return next;\n  }\n\n  meta(...args) {\n    if (args.length === 0) return this.spec.meta;\n    let next = this.clone();\n    next.spec.meta = Object.assign(next.spec.meta || {}, args[0]);\n    return next;\n  } // withContext<TContext extends AnyObject>(): BaseSchema<\n  //   TCast,\n  //   TContext,\n  //   TOutput\n  // > {\n  //   return this as any;\n  // }\n\n\n  withMutation(fn) {\n    let before = this._mutate;\n    this._mutate = true;\n    let result = fn(this);\n    this._mutate = before;\n    return result;\n  }\n\n  concat(schema) {\n    if (!schema || schema === this) return this;\n    if (schema.type !== this.type && this.type !== 'mixed') throw new TypeError(`You cannot \\`concat()\\` schema's of different types: ${this.type} and ${schema.type}`);\n    let base = this;\n    let combined = schema.clone();\n\n    const mergedSpec = _extends({}, base.spec, combined.spec); // if (combined.spec.nullable === UNSET)\n    //   mergedSpec.nullable = base.spec.nullable;\n    // if (combined.spec.presence === UNSET)\n    //   mergedSpec.presence = base.spec.presence;\n\n\n    combined.spec = mergedSpec;\n    combined._typeError || (combined._typeError = base._typeError);\n    combined._whitelistError || (combined._whitelistError = base._whitelistError);\n    combined._blacklistError || (combined._blacklistError = base._blacklistError); // manually merge the blacklist/whitelist (the other `schema` takes\n    // precedence in case of conflicts)\n\n    combined._whitelist = base._whitelist.merge(schema._whitelist, schema._blacklist);\n    combined._blacklist = base._blacklist.merge(schema._blacklist, schema._whitelist); // start with the current tests\n\n    combined.tests = base.tests;\n    combined.exclusiveTests = base.exclusiveTests; // manually add the new tests to ensure\n    // the deduping logic is consistent\n\n    combined.withMutation(next => {\n      schema.tests.forEach(fn => {\n        next.test(fn.OPTIONS);\n      });\n    });\n    combined.transforms = [...base.transforms, ...combined.transforms];\n    return combined;\n  }\n\n  isType(v) {\n    if (this.spec.nullable && v === null) return true;\n    return this._typeCheck(v);\n  }\n\n  resolve(options) {\n    let schema = this;\n\n    if (schema.conditions.length) {\n      let conditions = schema.conditions;\n      schema = schema.clone();\n      schema.conditions = [];\n      schema = conditions.reduce((schema, condition) => condition.resolve(schema, options), schema);\n      schema = schema.resolve(options);\n    }\n\n    return schema;\n  }\n  /**\n   *\n   * @param {*} value\n   * @param {Object} options\n   * @param {*=} options.parent\n   * @param {*=} options.context\n   */\n\n\n  cast(value, options = {}) {\n    let resolvedSchema = this.resolve(_extends({\n      value\n    }, options));\n\n    let result = resolvedSchema._cast(value, options);\n\n    if (value !== undefined && options.assert !== false && resolvedSchema.isType(result) !== true) {\n      let formattedValue = (0, _printValue.default)(value);\n      let formattedResult = (0, _printValue.default)(result);\n      throw new TypeError(`The value of ${options.path || 'field'} could not be cast to a value ` + `that satisfies the schema type: \"${resolvedSchema._type}\". \\n\\n` + `attempted value: ${formattedValue} \\n` + (formattedResult !== formattedValue ? `result of cast: ${formattedResult}` : ''));\n    }\n\n    return result;\n  }\n\n  _cast(rawValue, _options) {\n    let value = rawValue === undefined ? rawValue : this.transforms.reduce((value, fn) => fn.call(this, value, rawValue, this), rawValue);\n\n    if (value === undefined) {\n      value = this.getDefault();\n    }\n\n    return value;\n  }\n\n  _validate(_value, options = {}, cb) {\n    let {\n      sync,\n      path,\n      from = [],\n      originalValue = _value,\n      strict = this.spec.strict,\n      abortEarly = this.spec.abortEarly\n    } = options;\n    let value = _value;\n\n    if (!strict) {\n      // this._validating = true;\n      value = this._cast(value, _extends({\n        assert: false\n      }, options)); // this._validating = false;\n    } // value is cast, we can check if it meets type requirements\n\n\n    let args = {\n      value,\n      path,\n      options,\n      originalValue,\n      schema: this,\n      label: this.spec.label,\n      sync,\n      from\n    };\n    let initialTests = [];\n    if (this._typeError) initialTests.push(this._typeError);\n    let finalTests = [];\n    if (this._whitelistError) finalTests.push(this._whitelistError);\n    if (this._blacklistError) finalTests.push(this._blacklistError);\n    (0, _runTests.default)({\n      args,\n      value,\n      path,\n      sync,\n      tests: initialTests,\n      endEarly: abortEarly\n    }, err => {\n      if (err) return void cb(err, value);\n      (0, _runTests.default)({\n        tests: this.tests.concat(finalTests),\n        args,\n        path,\n        sync,\n        value,\n        endEarly: abortEarly\n      }, cb);\n    });\n  }\n\n  validate(value, options, maybeCb) {\n    let schema = this.resolve(_extends({}, options, {\n      value\n    })); // callback case is for nested validations\n\n    return typeof maybeCb === 'function' ? schema._validate(value, options, maybeCb) : new Promise((resolve, reject) => schema._validate(value, options, (err, value) => {\n      if (err) reject(err);else resolve(value);\n    }));\n  }\n\n  validateSync(value, options) {\n    let schema = this.resolve(_extends({}, options, {\n      value\n    }));\n    let result;\n\n    schema._validate(value, _extends({}, options, {\n      sync: true\n    }), (err, value) => {\n      if (err) throw err;\n      result = value;\n    });\n\n    return result;\n  }\n\n  isValid(value, options) {\n    return this.validate(value, options).then(() => true, err => {\n      if (_ValidationError.default.isError(err)) return false;\n      throw err;\n    });\n  }\n\n  isValidSync(value, options) {\n    try {\n      this.validateSync(value, options);\n      return true;\n    } catch (err) {\n      if (_ValidationError.default.isError(err)) return false;\n      throw err;\n    }\n  }\n\n  _getDefault() {\n    let defaultValue = this.spec.default;\n\n    if (defaultValue == null) {\n      return defaultValue;\n    }\n\n    return typeof defaultValue === 'function' ? defaultValue.call(this) : (0, _nanoclone.default)(defaultValue);\n  }\n\n  getDefault(options) {\n    let schema = this.resolve(options || {});\n    return schema._getDefault();\n  }\n\n  default(def) {\n    if (arguments.length === 0) {\n      return this._getDefault();\n    }\n\n    let next = this.clone({\n      default: def\n    });\n    return next;\n  }\n\n  strict(isStrict = true) {\n    let next = this.clone();\n    next.spec.strict = isStrict;\n    return next;\n  }\n\n  _isPresent(value) {\n    return value != null;\n  }\n\n  defined(message = _locale.mixed.defined) {\n    return this.test({\n      message,\n      name: 'defined',\n      exclusive: true,\n\n      test(value) {\n        return value !== undefined;\n      }\n\n    });\n  }\n\n  required(message = _locale.mixed.required) {\n    return this.clone({\n      presence: 'required'\n    }).withMutation(s => s.test({\n      message,\n      name: 'required',\n      exclusive: true,\n\n      test(value) {\n        return this.schema._isPresent(value);\n      }\n\n    }));\n  }\n\n  notRequired() {\n    let next = this.clone({\n      presence: 'optional'\n    });\n    next.tests = next.tests.filter(test => test.OPTIONS.name !== 'required');\n    return next;\n  }\n\n  nullable(isNullable = true) {\n    let next = this.clone({\n      nullable: isNullable !== false\n    });\n    return next;\n  }\n\n  transform(fn) {\n    let next = this.clone();\n    next.transforms.push(fn);\n    return next;\n  }\n  /**\n   * Adds a test function to the schema's queue of tests.\n   * tests can be exclusive or non-exclusive.\n   *\n   * - exclusive tests, will replace any existing tests of the same name.\n   * - non-exclusive: can be stacked\n   *\n   * If a non-exclusive test is added to a schema with an exclusive test of the same name\n   * the exclusive test is removed and further tests of the same name will be stacked.\n   *\n   * If an exclusive test is added to a schema with non-exclusive tests of the same name\n   * the previous tests are removed and further tests of the same name will replace each other.\n   */\n\n\n  test(...args) {\n    let opts;\n\n    if (args.length === 1) {\n      if (typeof args[0] === 'function') {\n        opts = {\n          test: args[0]\n        };\n      } else {\n        opts = args[0];\n      }\n    } else if (args.length === 2) {\n      opts = {\n        name: args[0],\n        test: args[1]\n      };\n    } else {\n      opts = {\n        name: args[0],\n        message: args[1],\n        test: args[2]\n      };\n    }\n\n    if (opts.message === undefined) opts.message = _locale.mixed.default;\n    if (typeof opts.test !== 'function') throw new TypeError('`test` is a required parameters');\n    let next = this.clone();\n    let validate = (0, _createValidation.default)(opts);\n    let isExclusive = opts.exclusive || opts.name && next.exclusiveTests[opts.name] === true;\n\n    if (opts.exclusive) {\n      if (!opts.name) throw new TypeError('Exclusive tests must provide a unique `name` identifying the test');\n    }\n\n    if (opts.name) next.exclusiveTests[opts.name] = !!opts.exclusive;\n    next.tests = next.tests.filter(fn => {\n      if (fn.OPTIONS.name === opts.name) {\n        if (isExclusive) return false;\n        if (fn.OPTIONS.test === validate.OPTIONS.test) return false;\n      }\n\n      return true;\n    });\n    next.tests.push(validate);\n    return next;\n  }\n\n  when(keys, options) {\n    if (!Array.isArray(keys) && typeof keys !== 'string') {\n      options = keys;\n      keys = '.';\n    }\n\n    let next = this.clone();\n    let deps = (0, _toArray.default)(keys).map(key => new _Reference.default(key));\n    deps.forEach(dep => {\n      // @ts-ignore\n      if (dep.isSibling) next.deps.push(dep.key);\n    });\n    next.conditions.push(new _Condition.default(deps, options));\n    return next;\n  }\n\n  typeError(message) {\n    let next = this.clone();\n    next._typeError = (0, _createValidation.default)({\n      message,\n      name: 'typeError',\n\n      test(value) {\n        if (value !== undefined && !this.schema.isType(value)) return this.createError({\n          params: {\n            type: this.schema._type\n          }\n        });\n        return true;\n      }\n\n    });\n    return next;\n  }\n\n  oneOf(enums, message = _locale.mixed.oneOf) {\n    let next = this.clone();\n    enums.forEach(val => {\n      next._whitelist.add(val);\n\n      next._blacklist.delete(val);\n    });\n    next._whitelistError = (0, _createValidation.default)({\n      message,\n      name: 'oneOf',\n\n      test(value) {\n        if (value === undefined) return true;\n        let valids = this.schema._whitelist;\n        let resolved = valids.resolveAll(this.resolve);\n        return resolved.includes(value) ? true : this.createError({\n          params: {\n            values: valids.toArray().join(', '),\n            resolved\n          }\n        });\n      }\n\n    });\n    return next;\n  }\n\n  notOneOf(enums, message = _locale.mixed.notOneOf) {\n    let next = this.clone();\n    enums.forEach(val => {\n      next._blacklist.add(val);\n\n      next._whitelist.delete(val);\n    });\n    next._blacklistError = (0, _createValidation.default)({\n      message,\n      name: 'notOneOf',\n\n      test(value) {\n        let invalids = this.schema._blacklist;\n        let resolved = invalids.resolveAll(this.resolve);\n        if (resolved.includes(value)) return this.createError({\n          params: {\n            values: invalids.toArray().join(', '),\n            resolved\n          }\n        });\n        return true;\n      }\n\n    });\n    return next;\n  }\n\n  strip(strip = true) {\n    let next = this.clone();\n    next.spec.strip = strip;\n    return next;\n  }\n\n  describe() {\n    const next = this.clone();\n    const {\n      label,\n      meta\n    } = next.spec;\n    const description = {\n      meta,\n      label,\n      type: next.type,\n      oneOf: next._whitelist.describe(),\n      notOneOf: next._blacklist.describe(),\n      tests: next.tests.map(fn => ({\n        name: fn.OPTIONS.name,\n        params: fn.OPTIONS.params\n      })).filter((n, idx, list) => list.findIndex(c => c.name === n.name) === idx)\n    };\n    return description;\n  }\n\n} // eslint-disable-next-line @typescript-eslint/no-unused-vars\n\n\nexports.default = BaseSchema;\n// @ts-expect-error\nBaseSchema.prototype.__isYupSchema__ = true;\n\nfor (const method of ['validate', 'validateSync']) BaseSchema.prototype[`${method}At`] = function (path, value, options = {}) {\n  const {\n    parent,\n    parentPath,\n    schema\n  } = (0, _reach.getIn)(this, path, value, options.context);\n  return schema[method](parent && parent[parentPath], _extends({}, options, {\n    parent,\n    path\n  }));\n};\n\nfor (const alias of ['equals', 'is']) BaseSchema.prototype[alias] = BaseSchema.prototype.oneOf;\n\nfor (const alias of ['not', 'nope']) BaseSchema.prototype[alias] = BaseSchema.prototype.notOneOf;\n\nBaseSchema.prototype.optional = BaseSchema.prototype.notRequired;", "\"use strict\";\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\nexports.create = create;\nexports.default = void 0;\n\nvar _schema = _interopRequireDefault(require(\"./schema\"));\n\nfunction _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { default: obj }; }\n\nconst Mixed = _schema.default;\nvar _default = Mixed;\nexports.default = _default;\n\nfunction create() {\n  return new Mixed();\n} // XXX: this is using the Base schema so that `addMethod(mixed)` works as a base class\n\n\ncreate.prototype = Mixed.prototype;", "\"use strict\";\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\nexports.default = void 0;\n\nconst isAbsent = value => value == null;\n\nvar _default = isAbsent;\nexports.default = _default;", "\"use strict\";\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\nexports.create = create;\nexports.default = void 0;\n\nvar _schema = _interopRequireDefault(require(\"./schema\"));\n\nvar _locale = require(\"./locale\");\n\nvar _isAbsent = _interopRequireDefault(require(\"./util/isAbsent\"));\n\nfunction _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { default: obj }; }\n\nfunction create() {\n  return new BooleanSchema();\n}\n\nclass BooleanSchema extends _schema.default {\n  constructor() {\n    super({\n      type: 'boolean'\n    });\n    this.withMutation(() => {\n      this.transform(function (value) {\n        if (!this.isType(value)) {\n          if (/^(true|1)$/i.test(String(value))) return true;\n          if (/^(false|0)$/i.test(String(value))) return false;\n        }\n\n        return value;\n      });\n    });\n  }\n\n  _typeCheck(v) {\n    if (v instanceof Boolean) v = v.valueOf();\n    return typeof v === 'boolean';\n  }\n\n  isTrue(message = _locale.boolean.isValue) {\n    return this.test({\n      message,\n      name: 'is-value',\n      exclusive: true,\n      params: {\n        value: 'true'\n      },\n\n      test(value) {\n        return (0, _isAbsent.default)(value) || value === true;\n      }\n\n    });\n  }\n\n  isFalse(message = _locale.boolean.isValue) {\n    return this.test({\n      message,\n      name: 'is-value',\n      exclusive: true,\n      params: {\n        value: 'false'\n      },\n\n      test(value) {\n        return (0, _isAbsent.default)(value) || value === false;\n      }\n\n    });\n  }\n\n}\n\nexports.default = BooleanSchema;\ncreate.prototype = BooleanSchema.prototype;", "\"use strict\";\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\nexports.create = create;\nexports.default = void 0;\n\nvar _locale = require(\"./locale\");\n\nvar _isAbsent = _interopRequireDefault(require(\"./util/isAbsent\"));\n\nvar _schema = _interopRequireDefault(require(\"./schema\"));\n\nfunction _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { default: obj }; }\n\n// eslint-disable-next-line\nlet rEmail = /^((([a-z]|\\d|[!#\\$%&'\\*\\+\\-\\/=\\?\\^_`{\\|}~]|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])+(\\.([a-z]|\\d|[!#\\$%&'\\*\\+\\-\\/=\\?\\^_`{\\|}~]|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])+)*)|((\\x22)((((\\x20|\\x09)*(\\x0d\\x0a))?(\\x20|\\x09)+)?(([\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x7f]|\\x21|[\\x23-\\x5b]|[\\x5d-\\x7e]|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])|(\\\\([\\x01-\\x09\\x0b\\x0c\\x0d-\\x7f]|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF]))))*(((\\x20|\\x09)*(\\x0d\\x0a))?(\\x20|\\x09)+)?(\\x22)))@((([a-z]|\\d|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])|(([a-z]|\\d|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])([a-z]|\\d|-|\\.|_|~|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])*([a-z]|\\d|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])))\\.)+(([a-z]|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])|(([a-z]|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])([a-z]|\\d|-|\\.|_|~|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])*([a-z]|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])))$/i; // eslint-disable-next-line\n\nlet rUrl = /^((https?|ftp):)?\\/\\/(((([a-z]|\\d|-|\\.|_|~|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])|(%[\\da-f]{2})|[!\\$&'\\(\\)\\*\\+,;=]|:)*@)?(((\\d|[1-9]\\d|1\\d\\d|2[0-4]\\d|25[0-5])\\.(\\d|[1-9]\\d|1\\d\\d|2[0-4]\\d|25[0-5])\\.(\\d|[1-9]\\d|1\\d\\d|2[0-4]\\d|25[0-5])\\.(\\d|[1-9]\\d|1\\d\\d|2[0-4]\\d|25[0-5]))|((([a-z]|\\d|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])|(([a-z]|\\d|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])([a-z]|\\d|-|\\.|_|~|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])*([a-z]|\\d|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])))\\.)+(([a-z]|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])|(([a-z]|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])([a-z]|\\d|-|\\.|_|~|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])*([a-z]|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])))\\.?)(:\\d*)?)(\\/((([a-z]|\\d|-|\\.|_|~|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])|(%[\\da-f]{2})|[!\\$&'\\(\\)\\*\\+,;=]|:|@)+(\\/(([a-z]|\\d|-|\\.|_|~|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])|(%[\\da-f]{2})|[!\\$&'\\(\\)\\*\\+,;=]|:|@)*)*)?)?(\\?((([a-z]|\\d|-|\\.|_|~|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])|(%[\\da-f]{2})|[!\\$&'\\(\\)\\*\\+,;=]|:|@)|[\\uE000-\\uF8FF]|\\/|\\?)*)?(\\#((([a-z]|\\d|-|\\.|_|~|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])|(%[\\da-f]{2})|[!\\$&'\\(\\)\\*\\+,;=]|:|@)|\\/|\\?)*)?$/i; // eslint-disable-next-line\n\nlet rUUID = /^(?:[0-9a-f]{8}-[0-9a-f]{4}-[1-5][0-9a-f]{3}-[89ab][0-9a-f]{3}-[0-9a-f]{12}|00000000-0000-0000-0000-000000000000)$/i;\n\nlet isTrimmed = value => (0, _isAbsent.default)(value) || value === value.trim();\n\nlet objStringTag = {}.toString();\n\nfunction create() {\n  return new StringSchema();\n}\n\nclass StringSchema extends _schema.default {\n  constructor() {\n    super({\n      type: 'string'\n    });\n    this.withMutation(() => {\n      this.transform(function (value) {\n        if (this.isType(value)) return value;\n        if (Array.isArray(value)) return value;\n        const strValue = value != null && value.toString ? value.toString() : value;\n        if (strValue === objStringTag) return value;\n        return strValue;\n      });\n    });\n  }\n\n  _typeCheck(value) {\n    if (value instanceof String) value = value.valueOf();\n    return typeof value === 'string';\n  }\n\n  _isPresent(value) {\n    return super._isPresent(value) && !!value.length;\n  }\n\n  length(length, message = _locale.string.length) {\n    return this.test({\n      message,\n      name: 'length',\n      exclusive: true,\n      params: {\n        length\n      },\n\n      test(value) {\n        return (0, _isAbsent.default)(value) || value.length === this.resolve(length);\n      }\n\n    });\n  }\n\n  min(min, message = _locale.string.min) {\n    return this.test({\n      message,\n      name: 'min',\n      exclusive: true,\n      params: {\n        min\n      },\n\n      test(value) {\n        return (0, _isAbsent.default)(value) || value.length >= this.resolve(min);\n      }\n\n    });\n  }\n\n  max(max, message = _locale.string.max) {\n    return this.test({\n      name: 'max',\n      exclusive: true,\n      message,\n      params: {\n        max\n      },\n\n      test(value) {\n        return (0, _isAbsent.default)(value) || value.length <= this.resolve(max);\n      }\n\n    });\n  }\n\n  matches(regex, options) {\n    let excludeEmptyString = false;\n    let message;\n    let name;\n\n    if (options) {\n      if (typeof options === 'object') {\n        ({\n          excludeEmptyString = false,\n          message,\n          name\n        } = options);\n      } else {\n        message = options;\n      }\n    }\n\n    return this.test({\n      name: name || 'matches',\n      message: message || _locale.string.matches,\n      params: {\n        regex\n      },\n      test: value => (0, _isAbsent.default)(value) || value === '' && excludeEmptyString || value.search(regex) !== -1\n    });\n  }\n\n  email(message = _locale.string.email) {\n    return this.matches(rEmail, {\n      name: 'email',\n      message,\n      excludeEmptyString: true\n    });\n  }\n\n  url(message = _locale.string.url) {\n    return this.matches(rUrl, {\n      name: 'url',\n      message,\n      excludeEmptyString: true\n    });\n  }\n\n  uuid(message = _locale.string.uuid) {\n    return this.matches(rUUID, {\n      name: 'uuid',\n      message,\n      excludeEmptyString: false\n    });\n  } //-- transforms --\n\n\n  ensure() {\n    return this.default('').transform(val => val === null ? '' : val);\n  }\n\n  trim(message = _locale.string.trim) {\n    return this.transform(val => val != null ? val.trim() : val).test({\n      message,\n      name: 'trim',\n      test: isTrimmed\n    });\n  }\n\n  lowercase(message = _locale.string.lowercase) {\n    return this.transform(value => !(0, _isAbsent.default)(value) ? value.toLowerCase() : value).test({\n      message,\n      name: 'string_case',\n      exclusive: true,\n      test: value => (0, _isAbsent.default)(value) || value === value.toLowerCase()\n    });\n  }\n\n  uppercase(message = _locale.string.uppercase) {\n    return this.transform(value => !(0, _isAbsent.default)(value) ? value.toUpperCase() : value).test({\n      message,\n      name: 'string_case',\n      exclusive: true,\n      test: value => (0, _isAbsent.default)(value) || value === value.toUpperCase()\n    });\n  }\n\n}\n\nexports.default = StringSchema;\ncreate.prototype = StringSchema.prototype; //\n// String Interfaces\n//", "\"use strict\";\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\nexports.create = create;\nexports.default = void 0;\n\nvar _locale = require(\"./locale\");\n\nvar _isAbsent = _interopRequireDefault(require(\"./util/isAbsent\"));\n\nvar _schema = _interopRequireDefault(require(\"./schema\"));\n\nfunction _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { default: obj }; }\n\nlet isNaN = value => value != +value;\n\nfunction create() {\n  return new NumberSchema();\n}\n\nclass NumberSchema extends _schema.default {\n  constructor() {\n    super({\n      type: 'number'\n    });\n    this.withMutation(() => {\n      this.transform(function (value) {\n        let parsed = value;\n\n        if (typeof parsed === 'string') {\n          parsed = parsed.replace(/\\s/g, '');\n          if (parsed === '') return NaN; // don't use parseFloat to avoid positives on alpha-numeric strings\n\n          parsed = +parsed;\n        }\n\n        if (this.isType(parsed)) return parsed;\n        return parseFloat(parsed);\n      });\n    });\n  }\n\n  _typeCheck(value) {\n    if (value instanceof Number) value = value.valueOf();\n    return typeof value === 'number' && !isNaN(value);\n  }\n\n  min(min, message = _locale.number.min) {\n    return this.test({\n      message,\n      name: 'min',\n      exclusive: true,\n      params: {\n        min\n      },\n\n      test(value) {\n        return (0, _isAbsent.default)(value) || value >= this.resolve(min);\n      }\n\n    });\n  }\n\n  max(max, message = _locale.number.max) {\n    return this.test({\n      message,\n      name: 'max',\n      exclusive: true,\n      params: {\n        max\n      },\n\n      test(value) {\n        return (0, _isAbsent.default)(value) || value <= this.resolve(max);\n      }\n\n    });\n  }\n\n  lessThan(less, message = _locale.number.lessThan) {\n    return this.test({\n      message,\n      name: 'max',\n      exclusive: true,\n      params: {\n        less\n      },\n\n      test(value) {\n        return (0, _isAbsent.default)(value) || value < this.resolve(less);\n      }\n\n    });\n  }\n\n  moreThan(more, message = _locale.number.moreThan) {\n    return this.test({\n      message,\n      name: 'min',\n      exclusive: true,\n      params: {\n        more\n      },\n\n      test(value) {\n        return (0, _isAbsent.default)(value) || value > this.resolve(more);\n      }\n\n    });\n  }\n\n  positive(msg = _locale.number.positive) {\n    return this.moreThan(0, msg);\n  }\n\n  negative(msg = _locale.number.negative) {\n    return this.lessThan(0, msg);\n  }\n\n  integer(message = _locale.number.integer) {\n    return this.test({\n      name: 'integer',\n      message,\n      test: val => (0, _isAbsent.default)(val) || Number.isInteger(val)\n    });\n  }\n\n  truncate() {\n    return this.transform(value => !(0, _isAbsent.default)(value) ? value | 0 : value);\n  }\n\n  round(method) {\n    var _method;\n\n    let avail = ['ceil', 'floor', 'round', 'trunc'];\n    method = ((_method = method) == null ? void 0 : _method.toLowerCase()) || 'round'; // this exists for symemtry with the new Math.trunc\n\n    if (method === 'trunc') return this.truncate();\n    if (avail.indexOf(method.toLowerCase()) === -1) throw new TypeError('Only valid options for round() are: ' + avail.join(', '));\n    return this.transform(value => !(0, _isAbsent.default)(value) ? Math[method](value) : value);\n  }\n\n}\n\nexports.default = NumberSchema;\ncreate.prototype = NumberSchema.prototype; //\n// Number Interfaces\n//", "\"use strict\";\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\nexports.default = parseIsoDate;\n\n/* eslint-disable */\n\n/**\n *\n * Date.parse with progressive enhancement for ISO 8601 <https://github.com/csnover/js-iso8601>\n * NON-CONFORMANT EDITION.\n * \u00A9 2011 Colin Snover <http://zetafleet.com>\n * Released under MIT license.\n */\n//              1 YYYY                 2 MM        3 DD              4 HH     5 mm        6 ss            7 msec         8 Z 9 \u00B1    10 tzHH    11 tzmm\nvar isoReg = /^(\\d{4}|[+\\-]\\d{6})(?:-?(\\d{2})(?:-?(\\d{2}))?)?(?:[ T]?(\\d{2}):?(\\d{2})(?::?(\\d{2})(?:[,\\.](\\d{1,}))?)?(?:(Z)|([+\\-])(\\d{2})(?::?(\\d{2}))?)?)?$/;\n\nfunction parseIsoDate(date) {\n  var numericKeys = [1, 4, 5, 6, 7, 10, 11],\n      minutesOffset = 0,\n      timestamp,\n      struct;\n\n  if (struct = isoReg.exec(date)) {\n    // avoid NaN timestamps caused by \u201Cundefined\u201D values being passed to Date.UTC\n    for (var i = 0, k; k = numericKeys[i]; ++i) struct[k] = +struct[k] || 0; // allow undefined days and months\n\n\n    struct[2] = (+struct[2] || 1) - 1;\n    struct[3] = +struct[3] || 1; // allow arbitrary sub-second precision beyond milliseconds\n\n    struct[7] = struct[7] ? String(struct[7]).substr(0, 3) : 0; // timestamps without timezone identifiers should be considered local time\n\n    if ((struct[8] === undefined || struct[8] === '') && (struct[9] === undefined || struct[9] === '')) timestamp = +new Date(struct[1], struct[2], struct[3], struct[4], struct[5], struct[6], struct[7]);else {\n      if (struct[8] !== 'Z' && struct[9] !== undefined) {\n        minutesOffset = struct[10] * 60 + struct[11];\n        if (struct[9] === '+') minutesOffset = 0 - minutesOffset;\n      }\n\n      timestamp = Date.UTC(struct[1], struct[2], struct[3], struct[4], struct[5] + minutesOffset, struct[6], struct[7]);\n    }\n  } else timestamp = Date.parse ? Date.parse(date) : NaN;\n\n  return timestamp;\n}", "\"use strict\";\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\nexports.create = create;\nexports.default = void 0;\n\nvar _isodate = _interopRequireDefault(require(\"./util/isodate\"));\n\nvar _locale = require(\"./locale\");\n\nvar _isAbsent = _interopRequireDefault(require(\"./util/isAbsent\"));\n\nvar _Reference = _interopRequireDefault(require(\"./Reference\"));\n\nvar _schema = _interopRequireDefault(require(\"./schema\"));\n\nfunction _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { default: obj }; }\n\n// @ts-ignore\nlet invalidDate = new Date('');\n\nlet isDate = obj => Object.prototype.toString.call(obj) === '[object Date]';\n\nfunction create() {\n  return new DateSchema();\n}\n\nclass DateSchema extends _schema.default {\n  constructor() {\n    super({\n      type: 'date'\n    });\n    this.withMutation(() => {\n      this.transform(function (value) {\n        if (this.isType(value)) return value;\n        value = (0, _isodate.default)(value); // 0 is a valid timestamp equivalent to 1970-01-01T00:00:00Z(unix epoch) or before.\n\n        return !isNaN(value) ? new Date(value) : invalidDate;\n      });\n    });\n  }\n\n  _typeCheck(v) {\n    return isDate(v) && !isNaN(v.getTime());\n  }\n\n  prepareParam(ref, name) {\n    let param;\n\n    if (!_Reference.default.isRef(ref)) {\n      let cast = this.cast(ref);\n      if (!this._typeCheck(cast)) throw new TypeError(`\\`${name}\\` must be a Date or a value that can be \\`cast()\\` to a Date`);\n      param = cast;\n    } else {\n      param = ref;\n    }\n\n    return param;\n  }\n\n  min(min, message = _locale.date.min) {\n    let limit = this.prepareParam(min, 'min');\n    return this.test({\n      message,\n      name: 'min',\n      exclusive: true,\n      params: {\n        min\n      },\n\n      test(value) {\n        return (0, _isAbsent.default)(value) || value >= this.resolve(limit);\n      }\n\n    });\n  }\n\n  max(max, message = _locale.date.max) {\n    let limit = this.prepareParam(max, 'max');\n    return this.test({\n      message,\n      name: 'max',\n      exclusive: true,\n      params: {\n        max\n      },\n\n      test(value) {\n        return (0, _isAbsent.default)(value) || value <= this.resolve(limit);\n      }\n\n    });\n  }\n\n}\n\nexports.default = DateSchema;\nDateSchema.INVALID_DATE = invalidDate;\ncreate.prototype = DateSchema.prototype;\ncreate.INVALID_DATE = invalidDate;", "/**\n * A specialized version of `_.reduce` for arrays without support for\n * iteratee shorthands.\n *\n * @private\n * @param {Array} [array] The array to iterate over.\n * @param {Function} iteratee The function invoked per iteration.\n * @param {*} [accumulator] The initial value.\n * @param {boolean} [initAccum] Specify using the first element of `array` as\n *  the initial value.\n * @returns {*} Returns the accumulated value.\n */\nfunction arrayReduce(array, iteratee, accumulator, initAccum) {\n  var index = -1,\n      length = array == null ? 0 : array.length;\n\n  if (initAccum && length) {\n    accumulator = array[++index];\n  }\n  while (++index < length) {\n    accumulator = iteratee(accumulator, array[index], index, array);\n  }\n  return accumulator;\n}\n\nmodule.exports = arrayReduce;\n", "/**\n * The base implementation of `_.propertyOf` without support for deep paths.\n *\n * @private\n * @param {Object} object The object to query.\n * @returns {Function} Returns the new accessor function.\n */\nfunction basePropertyOf(object) {\n  return function(key) {\n    return object == null ? undefined : object[key];\n  };\n}\n\nmodule.exports = basePropertyOf;\n", "var basePropertyOf = require('./_basePropertyOf');\n\n/** Used to map Latin Unicode letters to basic Latin letters. */\nvar deburredLetters = {\n  // Latin-1 Supplement block.\n  '\\xc0': 'A',  '\\xc1': 'A', '\\xc2': 'A', '\\xc3': 'A', '\\xc4': 'A', '\\xc5': 'A',\n  '\\xe0': 'a',  '\\xe1': 'a', '\\xe2': 'a', '\\xe3': 'a', '\\xe4': 'a', '\\xe5': 'a',\n  '\\xc7': 'C',  '\\xe7': 'c',\n  '\\xd0': 'D',  '\\xf0': 'd',\n  '\\xc8': 'E',  '\\xc9': 'E', '\\xca': 'E', '\\xcb': 'E',\n  '\\xe8': 'e',  '\\xe9': 'e', '\\xea': 'e', '\\xeb': 'e',\n  '\\xcc': 'I',  '\\xcd': 'I', '\\xce': 'I', '\\xcf': 'I',\n  '\\xec': 'i',  '\\xed': 'i', '\\xee': 'i', '\\xef': 'i',\n  '\\xd1': 'N',  '\\xf1': 'n',\n  '\\xd2': 'O',  '\\xd3': 'O', '\\xd4': 'O', '\\xd5': 'O', '\\xd6': 'O', '\\xd8': 'O',\n  '\\xf2': 'o',  '\\xf3': 'o', '\\xf4': 'o', '\\xf5': 'o', '\\xf6': 'o', '\\xf8': 'o',\n  '\\xd9': 'U',  '\\xda': 'U', '\\xdb': 'U', '\\xdc': 'U',\n  '\\xf9': 'u',  '\\xfa': 'u', '\\xfb': 'u', '\\xfc': 'u',\n  '\\xdd': 'Y',  '\\xfd': 'y', '\\xff': 'y',\n  '\\xc6': 'Ae', '\\xe6': 'ae',\n  '\\xde': 'Th', '\\xfe': 'th',\n  '\\xdf': 'ss',\n  // Latin Extended-A block.\n  '\\u0100': 'A',  '\\u0102': 'A', '\\u0104': 'A',\n  '\\u0101': 'a',  '\\u0103': 'a', '\\u0105': 'a',\n  '\\u0106': 'C',  '\\u0108': 'C', '\\u010a': 'C', '\\u010c': 'C',\n  '\\u0107': 'c',  '\\u0109': 'c', '\\u010b': 'c', '\\u010d': 'c',\n  '\\u010e': 'D',  '\\u0110': 'D', '\\u010f': 'd', '\\u0111': 'd',\n  '\\u0112': 'E',  '\\u0114': 'E', '\\u0116': 'E', '\\u0118': 'E', '\\u011a': 'E',\n  '\\u0113': 'e',  '\\u0115': 'e', '\\u0117': 'e', '\\u0119': 'e', '\\u011b': 'e',\n  '\\u011c': 'G',  '\\u011e': 'G', '\\u0120': 'G', '\\u0122': 'G',\n  '\\u011d': 'g',  '\\u011f': 'g', '\\u0121': 'g', '\\u0123': 'g',\n  '\\u0124': 'H',  '\\u0126': 'H', '\\u0125': 'h', '\\u0127': 'h',\n  '\\u0128': 'I',  '\\u012a': 'I', '\\u012c': 'I', '\\u012e': 'I', '\\u0130': 'I',\n  '\\u0129': 'i',  '\\u012b': 'i', '\\u012d': 'i', '\\u012f': 'i', '\\u0131': 'i',\n  '\\u0134': 'J',  '\\u0135': 'j',\n  '\\u0136': 'K',  '\\u0137': 'k', '\\u0138': 'k',\n  '\\u0139': 'L',  '\\u013b': 'L', '\\u013d': 'L', '\\u013f': 'L', '\\u0141': 'L',\n  '\\u013a': 'l',  '\\u013c': 'l', '\\u013e': 'l', '\\u0140': 'l', '\\u0142': 'l',\n  '\\u0143': 'N',  '\\u0145': 'N', '\\u0147': 'N', '\\u014a': 'N',\n  '\\u0144': 'n',  '\\u0146': 'n', '\\u0148': 'n', '\\u014b': 'n',\n  '\\u014c': 'O',  '\\u014e': 'O', '\\u0150': 'O',\n  '\\u014d': 'o',  '\\u014f': 'o', '\\u0151': 'o',\n  '\\u0154': 'R',  '\\u0156': 'R', '\\u0158': 'R',\n  '\\u0155': 'r',  '\\u0157': 'r', '\\u0159': 'r',\n  '\\u015a': 'S',  '\\u015c': 'S', '\\u015e': 'S', '\\u0160': 'S',\n  '\\u015b': 's',  '\\u015d': 's', '\\u015f': 's', '\\u0161': 's',\n  '\\u0162': 'T',  '\\u0164': 'T', '\\u0166': 'T',\n  '\\u0163': 't',  '\\u0165': 't', '\\u0167': 't',\n  '\\u0168': 'U',  '\\u016a': 'U', '\\u016c': 'U', '\\u016e': 'U', '\\u0170': 'U', '\\u0172': 'U',\n  '\\u0169': 'u',  '\\u016b': 'u', '\\u016d': 'u', '\\u016f': 'u', '\\u0171': 'u', '\\u0173': 'u',\n  '\\u0174': 'W',  '\\u0175': 'w',\n  '\\u0176': 'Y',  '\\u0177': 'y', '\\u0178': 'Y',\n  '\\u0179': 'Z',  '\\u017b': 'Z', '\\u017d': 'Z',\n  '\\u017a': 'z',  '\\u017c': 'z', '\\u017e': 'z',\n  '\\u0132': 'IJ', '\\u0133': 'ij',\n  '\\u0152': 'Oe', '\\u0153': 'oe',\n  '\\u0149': \"'n\", '\\u017f': 's'\n};\n\n/**\n * Used by `_.deburr` to convert Latin-1 Supplement and Latin Extended-A\n * letters to basic Latin letters.\n *\n * @private\n * @param {string} letter The matched letter to deburr.\n * @returns {string} Returns the deburred letter.\n */\nvar deburrLetter = basePropertyOf(deburredLetters);\n\nmodule.exports = deburrLetter;\n", "var deburrLetter = require('./_deburrLetter'),\n    toString = require('./toString');\n\n/** Used to match Latin Unicode letters (excluding mathematical operators). */\nvar reLatin = /[\\xc0-\\xd6\\xd8-\\xf6\\xf8-\\xff\\u0100-\\u017f]/g;\n\n/** Used to compose unicode character classes. */\nvar rsComboMarksRange = '\\\\u0300-\\\\u036f',\n    reComboHalfMarksRange = '\\\\ufe20-\\\\ufe2f',\n    rsComboSymbolsRange = '\\\\u20d0-\\\\u20ff',\n    rsComboRange = rsComboMarksRange + reComboHalfMarksRange + rsComboSymbolsRange;\n\n/** Used to compose unicode capture groups. */\nvar rsCombo = '[' + rsComboRange + ']';\n\n/**\n * Used to match [combining diacritical marks](https://en.wikipedia.org/wiki/Combining_Diacritical_Marks) and\n * [combining diacritical marks for symbols](https://en.wikipedia.org/wiki/Combining_Diacritical_Marks_for_Symbols).\n */\nvar reComboMark = RegExp(rsCombo, 'g');\n\n/**\n * Deburrs `string` by converting\n * [Latin-1 Supplement](https://en.wikipedia.org/wiki/Latin-1_Supplement_(Unicode_block)#Character_table)\n * and [Latin Extended-A](https://en.wikipedia.org/wiki/Latin_Extended-A)\n * letters to basic Latin letters and removing\n * [combining diacritical marks](https://en.wikipedia.org/wiki/Combining_Diacritical_Marks).\n *\n * @static\n * @memberOf _\n * @since 3.0.0\n * @category String\n * @param {string} [string=''] The string to deburr.\n * @returns {string} Returns the deburred string.\n * @example\n *\n * _.deburr('d\u00E9j\u00E0 vu');\n * // => 'deja vu'\n */\nfunction deburr(string) {\n  string = toString(string);\n  return string && string.replace(reLatin, deburrLetter).replace(reComboMark, '');\n}\n\nmodule.exports = deburr;\n", "/** Used to match words composed of alphanumeric characters. */\nvar reAsciiWord = /[^\\x00-\\x2f\\x3a-\\x40\\x5b-\\x60\\x7b-\\x7f]+/g;\n\n/**\n * Splits an ASCII `string` into an array of its words.\n *\n * @private\n * @param {string} The string to inspect.\n * @returns {Array} Returns the words of `string`.\n */\nfunction asciiWords(string) {\n  return string.match(reAsciiWord) || [];\n}\n\nmodule.exports = asciiWords;\n", "/** Used to detect strings that need a more robust regexp to match words. */\nvar reHasUnicodeWord = /[a-z][A-Z]|[A-Z]{2}[a-z]|[0-9][a-zA-Z]|[a-zA-Z][0-9]|[^a-zA-Z0-9 ]/;\n\n/**\n * Checks if `string` contains a word composed of Unicode symbols.\n *\n * @private\n * @param {string} string The string to inspect.\n * @returns {boolean} Returns `true` if a word is found, else `false`.\n */\nfunction hasUnicodeWord(string) {\n  return reHasUnicodeWord.test(string);\n}\n\nmodule.exports = hasUnicodeWord;\n", "/** Used to compose unicode character classes. */\nvar rsAstralRange = '\\\\ud800-\\\\udfff',\n    rsComboMarksRange = '\\\\u0300-\\\\u036f',\n    reComboHalfMarksRange = '\\\\ufe20-\\\\ufe2f',\n    rsComboSymbolsRange = '\\\\u20d0-\\\\u20ff',\n    rsComboRange = rsComboMarksRange + reComboHalfMarksRange + rsComboSymbolsRange,\n    rsDingbatRange = '\\\\u2700-\\\\u27bf',\n    rsLowerRange = 'a-z\\\\xdf-\\\\xf6\\\\xf8-\\\\xff',\n    rsMathOpRange = '\\\\xac\\\\xb1\\\\xd7\\\\xf7',\n    rsNonCharRange = '\\\\x00-\\\\x2f\\\\x3a-\\\\x40\\\\x5b-\\\\x60\\\\x7b-\\\\xbf',\n    rsPunctuationRange = '\\\\u2000-\\\\u206f',\n    rsSpaceRange = ' \\\\t\\\\x0b\\\\f\\\\xa0\\\\ufeff\\\\n\\\\r\\\\u2028\\\\u2029\\\\u1680\\\\u180e\\\\u2000\\\\u2001\\\\u2002\\\\u2003\\\\u2004\\\\u2005\\\\u2006\\\\u2007\\\\u2008\\\\u2009\\\\u200a\\\\u202f\\\\u205f\\\\u3000',\n    rsUpperRange = 'A-Z\\\\xc0-\\\\xd6\\\\xd8-\\\\xde',\n    rsVarRange = '\\\\ufe0e\\\\ufe0f',\n    rsBreakRange = rsMathOpRange + rsNonCharRange + rsPunctuationRange + rsSpaceRange;\n\n/** Used to compose unicode capture groups. */\nvar rsApos = \"['\\u2019]\",\n    rsBreak = '[' + rsBreakRange + ']',\n    rsCombo = '[' + rsComboRange + ']',\n    rsDigits = '\\\\d+',\n    rsDingbat = '[' + rsDingbatRange + ']',\n    rsLower = '[' + rsLowerRange + ']',\n    rsMisc = '[^' + rsAstralRange + rsBreakRange + rsDigits + rsDingbatRange + rsLowerRange + rsUpperRange + ']',\n    rsFitz = '\\\\ud83c[\\\\udffb-\\\\udfff]',\n    rsModifier = '(?:' + rsCombo + '|' + rsFitz + ')',\n    rsNonAstral = '[^' + rsAstralRange + ']',\n    rsRegional = '(?:\\\\ud83c[\\\\udde6-\\\\uddff]){2}',\n    rsSurrPair = '[\\\\ud800-\\\\udbff][\\\\udc00-\\\\udfff]',\n    rsUpper = '[' + rsUpperRange + ']',\n    rsZWJ = '\\\\u200d';\n\n/** Used to compose unicode regexes. */\nvar rsMiscLower = '(?:' + rsLower + '|' + rsMisc + ')',\n    rsMiscUpper = '(?:' + rsUpper + '|' + rsMisc + ')',\n    rsOptContrLower = '(?:' + rsApos + '(?:d|ll|m|re|s|t|ve))?',\n    rsOptContrUpper = '(?:' + rsApos + '(?:D|LL|M|RE|S|T|VE))?',\n    reOptMod = rsModifier + '?',\n    rsOptVar = '[' + rsVarRange + ']?',\n    rsOptJoin = '(?:' + rsZWJ + '(?:' + [rsNonAstral, rsRegional, rsSurrPair].join('|') + ')' + rsOptVar + reOptMod + ')*',\n    rsOrdLower = '\\\\d*(?:1st|2nd|3rd|(?![123])\\\\dth)(?=\\\\b|[A-Z_])',\n    rsOrdUpper = '\\\\d*(?:1ST|2ND|3RD|(?![123])\\\\dTH)(?=\\\\b|[a-z_])',\n    rsSeq = rsOptVar + reOptMod + rsOptJoin,\n    rsEmoji = '(?:' + [rsDingbat, rsRegional, rsSurrPair].join('|') + ')' + rsSeq;\n\n/** Used to match complex or compound words. */\nvar reUnicodeWord = RegExp([\n  rsUpper + '?' + rsLower + '+' + rsOptContrLower + '(?=' + [rsBreak, rsUpper, '$'].join('|') + ')',\n  rsMiscUpper + '+' + rsOptContrUpper + '(?=' + [rsBreak, rsUpper + rsMiscLower, '$'].join('|') + ')',\n  rsUpper + '?' + rsMiscLower + '+' + rsOptContrLower,\n  rsUpper + '+' + rsOptContrUpper,\n  rsOrdUpper,\n  rsOrdLower,\n  rsDigits,\n  rsEmoji\n].join('|'), 'g');\n\n/**\n * Splits a Unicode `string` into an array of its words.\n *\n * @private\n * @param {string} The string to inspect.\n * @returns {Array} Returns the words of `string`.\n */\nfunction unicodeWords(string) {\n  return string.match(reUnicodeWord) || [];\n}\n\nmodule.exports = unicodeWords;\n", "var asciiWords = require('./_asciiWords'),\n    hasUnicodeWord = require('./_hasUnicodeWord'),\n    toString = require('./toString'),\n    unicodeWords = require('./_unicodeWords');\n\n/**\n * Splits `string` into an array of its words.\n *\n * @static\n * @memberOf _\n * @since 3.0.0\n * @category String\n * @param {string} [string=''] The string to inspect.\n * @param {RegExp|string} [pattern] The pattern to match words.\n * @param- {Object} [guard] Enables use as an iteratee for methods like `_.map`.\n * @returns {Array} Returns the words of `string`.\n * @example\n *\n * _.words('fred, barney, & pebbles');\n * // => ['fred', 'barney', 'pebbles']\n *\n * _.words('fred, barney, & pebbles', /[^, ]+/g);\n * // => ['fred', 'barney', '&', 'pebbles']\n */\nfunction words(string, pattern, guard) {\n  string = toString(string);\n  pattern = guard ? undefined : pattern;\n\n  if (pattern === undefined) {\n    return hasUnicodeWord(string) ? unicodeWords(string) : asciiWords(string);\n  }\n  return string.match(pattern) || [];\n}\n\nmodule.exports = words;\n", "var arrayReduce = require('./_arrayReduce'),\n    deburr = require('./deburr'),\n    words = require('./words');\n\n/** Used to compose unicode capture groups. */\nvar rsApos = \"['\\u2019]\";\n\n/** Used to match apostrophes. */\nvar reApos = RegExp(rsApos, 'g');\n\n/**\n * Creates a function like `_.camelCase`.\n *\n * @private\n * @param {Function} callback The function to combine each word.\n * @returns {Function} Returns the new compounder function.\n */\nfunction createCompounder(callback) {\n  return function(string) {\n    return arrayReduce(words(deburr(string).replace(reApos, '')), callback, '');\n  };\n}\n\nmodule.exports = createCompounder;\n", "var createCompounder = require('./_createCompounder');\n\n/**\n * Converts `string` to\n * [snake case](https://en.wikipedia.org/wiki/Snake_case).\n *\n * @static\n * @memberOf _\n * @since 3.0.0\n * @category String\n * @param {string} [string=''] The string to convert.\n * @returns {string} Returns the snake cased string.\n * @example\n *\n * _.snakeCase('Foo Bar');\n * // => 'foo_bar'\n *\n * _.snakeCase('fooBar');\n * // => 'foo_bar'\n *\n * _.snakeCase('--FOO-BAR--');\n * // => 'foo_bar'\n */\nvar snakeCase = createCompounder(function(result, word, index) {\n  return result + (index ? '_' : '') + word.toLowerCase();\n});\n\nmodule.exports = snakeCase;\n", "/**\n * The base implementation of `_.slice` without an iteratee call guard.\n *\n * @private\n * @param {Array} array The array to slice.\n * @param {number} [start=0] The start position.\n * @param {number} [end=array.length] The end position.\n * @returns {Array} Returns the slice of `array`.\n */\nfunction baseSlice(array, start, end) {\n  var index = -1,\n      length = array.length;\n\n  if (start < 0) {\n    start = -start > length ? 0 : (length + start);\n  }\n  end = end > length ? length : end;\n  if (end < 0) {\n    end += length;\n  }\n  length = start > end ? 0 : ((end - start) >>> 0);\n  start >>>= 0;\n\n  var result = Array(length);\n  while (++index < length) {\n    result[index] = array[index + start];\n  }\n  return result;\n}\n\nmodule.exports = baseSlice;\n", "var baseSlice = require('./_baseSlice');\n\n/**\n * Casts `array` to a slice if it's needed.\n *\n * @private\n * @param {Array} array The array to inspect.\n * @param {number} start The start position.\n * @param {number} [end=array.length] The end position.\n * @returns {Array} Returns the cast slice.\n */\nfunction castSlice(array, start, end) {\n  var length = array.length;\n  end = end === undefined ? length : end;\n  return (!start && end >= length) ? array : baseSlice(array, start, end);\n}\n\nmodule.exports = castSlice;\n", "/** Used to compose unicode character classes. */\nvar rsAstralRange = '\\\\ud800-\\\\udfff',\n    rsComboMarksRange = '\\\\u0300-\\\\u036f',\n    reComboHalfMarksRange = '\\\\ufe20-\\\\ufe2f',\n    rsComboSymbolsRange = '\\\\u20d0-\\\\u20ff',\n    rsComboRange = rsComboMarksRange + reComboHalfMarksRange + rsComboSymbolsRange,\n    rsVarRange = '\\\\ufe0e\\\\ufe0f';\n\n/** Used to compose unicode capture groups. */\nvar rsZWJ = '\\\\u200d';\n\n/** Used to detect strings with [zero-width joiners or code points from the astral planes](http://eev.ee/blog/2015/09/12/dark-corners-of-unicode/). */\nvar reHasUnicode = RegExp('[' + rsZWJ + rsAstralRange  + rsComboRange + rsVarRange + ']');\n\n/**\n * Checks if `string` contains Unicode symbols.\n *\n * @private\n * @param {string} string The string to inspect.\n * @returns {boolean} Returns `true` if a symbol is found, else `false`.\n */\nfunction hasUnicode(string) {\n  return reHasUnicode.test(string);\n}\n\nmodule.exports = hasUnicode;\n", "/**\n * Converts an ASCII `string` to an array.\n *\n * @private\n * @param {string} string The string to convert.\n * @returns {Array} Returns the converted array.\n */\nfunction asciiToArray(string) {\n  return string.split('');\n}\n\nmodule.exports = asciiToArray;\n", "/** Used to compose unicode character classes. */\nvar rsAstralRange = '\\\\ud800-\\\\udfff',\n    rsComboMarksRange = '\\\\u0300-\\\\u036f',\n    reComboHalfMarksRange = '\\\\ufe20-\\\\ufe2f',\n    rsComboSymbolsRange = '\\\\u20d0-\\\\u20ff',\n    rsComboRange = rsComboMarksRange + reComboHalfMarksRange + rsComboSymbolsRange,\n    rsVarRange = '\\\\ufe0e\\\\ufe0f';\n\n/** Used to compose unicode capture groups. */\nvar rsAstral = '[' + rsAstralRange + ']',\n    rsCombo = '[' + rsComboRange + ']',\n    rsFitz = '\\\\ud83c[\\\\udffb-\\\\udfff]',\n    rsModifier = '(?:' + rsCombo + '|' + rsFitz + ')',\n    rsNonAstral = '[^' + rsAstralRange + ']',\n    rsRegional = '(?:\\\\ud83c[\\\\udde6-\\\\uddff]){2}',\n    rsSurrPair = '[\\\\ud800-\\\\udbff][\\\\udc00-\\\\udfff]',\n    rsZWJ = '\\\\u200d';\n\n/** Used to compose unicode regexes. */\nvar reOptMod = rsModifier + '?',\n    rsOptVar = '[' + rsVarRange + ']?',\n    rsOptJoin = '(?:' + rsZWJ + '(?:' + [rsNonAstral, rsRegional, rsSurrPair].join('|') + ')' + rsOptVar + reOptMod + ')*',\n    rsSeq = rsOptVar + reOptMod + rsOptJoin,\n    rsSymbol = '(?:' + [rsNonAstral + rsCombo + '?', rsCombo, rsRegional, rsSurrPair, rsAstral].join('|') + ')';\n\n/** Used to match [string symbols](https://mathiasbynens.be/notes/javascript-unicode). */\nvar reUnicode = RegExp(rsFitz + '(?=' + rsFitz + ')|' + rsSymbol + rsSeq, 'g');\n\n/**\n * Converts a Unicode `string` to an array.\n *\n * @private\n * @param {string} string The string to convert.\n * @returns {Array} Returns the converted array.\n */\nfunction unicodeToArray(string) {\n  return string.match(reUnicode) || [];\n}\n\nmodule.exports = unicodeToArray;\n", "var asciiToArray = require('./_asciiToArray'),\n    hasUnicode = require('./_hasUnicode'),\n    unicodeToArray = require('./_unicodeToArray');\n\n/**\n * Converts `string` to an array.\n *\n * @private\n * @param {string} string The string to convert.\n * @returns {Array} Returns the converted array.\n */\nfunction stringToArray(string) {\n  return hasUnicode(string)\n    ? unicodeToArray(string)\n    : asciiToArray(string);\n}\n\nmodule.exports = stringToArray;\n", "var castSlice = require('./_castSlice'),\n    hasUnicode = require('./_hasUnicode'),\n    stringToArray = require('./_stringToArray'),\n    toString = require('./toString');\n\n/**\n * Creates a function like `_.lowerFirst`.\n *\n * @private\n * @param {string} methodName The name of the `String` case method to use.\n * @returns {Function} Returns the new case function.\n */\nfunction createCaseFirst(methodName) {\n  return function(string) {\n    string = toString(string);\n\n    var strSymbols = hasUnicode(string)\n      ? stringToArray(string)\n      : undefined;\n\n    var chr = strSymbols\n      ? strSymbols[0]\n      : string.charAt(0);\n\n    var trailing = strSymbols\n      ? castSlice(strSymbols, 1).join('')\n      : string.slice(1);\n\n    return chr[methodName]() + trailing;\n  };\n}\n\nmodule.exports = createCaseFirst;\n", "var createCaseFirst = require('./_createCaseFirst');\n\n/**\n * Converts the first character of `string` to upper case.\n *\n * @static\n * @memberOf _\n * @since 4.0.0\n * @category String\n * @param {string} [string=''] The string to convert.\n * @returns {string} Returns the converted string.\n * @example\n *\n * _.upperFirst('fred');\n * // => 'Fred'\n *\n * _.upperFirst('FRED');\n * // => 'FRED'\n */\nvar upperFirst = createCaseFirst('toUpperCase');\n\nmodule.exports = upperFirst;\n", "var toString = require('./toString'),\n    upperFirst = require('./upperFirst');\n\n/**\n * Converts the first character of `string` to upper case and the remaining\n * to lower case.\n *\n * @static\n * @memberOf _\n * @since 3.0.0\n * @category String\n * @param {string} [string=''] The string to capitalize.\n * @returns {string} Returns the capitalized string.\n * @example\n *\n * _.capitalize('FRED');\n * // => 'Fred'\n */\nfunction capitalize(string) {\n  return upperFirst(toString(string).toLowerCase());\n}\n\nmodule.exports = capitalize;\n", "var capitalize = require('./capitalize'),\n    createCompounder = require('./_createCompounder');\n\n/**\n * Converts `string` to [camel case](https://en.wikipedia.org/wiki/CamelCase).\n *\n * @static\n * @memberOf _\n * @since 3.0.0\n * @category String\n * @param {string} [string=''] The string to convert.\n * @returns {string} Returns the camel cased string.\n * @example\n *\n * _.camelCase('Foo Bar');\n * // => 'fooBar'\n *\n * _.camelCase('--foo-bar--');\n * // => 'fooBar'\n *\n * _.camelCase('__FOO_BAR__');\n * // => 'fooBar'\n */\nvar camelCase = createCompounder(function(result, word, index) {\n  word = word.toLowerCase();\n  return result + (index ? capitalize(word) : word);\n});\n\nmodule.exports = camelCase;\n", "var baseAssignValue = require('./_baseAssignValue'),\n    baseForOwn = require('./_baseForOwn'),\n    baseIteratee = require('./_baseIteratee');\n\n/**\n * The opposite of `_.mapValues`; this method creates an object with the\n * same values as `object` and keys generated by running each own enumerable\n * string keyed property of `object` thru `iteratee`. The iteratee is invoked\n * with three arguments: (value, key, object).\n *\n * @static\n * @memberOf _\n * @since 3.8.0\n * @category Object\n * @param {Object} object The object to iterate over.\n * @param {Function} [iteratee=_.identity] The function invoked per iteration.\n * @returns {Object} Returns the new mapped object.\n * @see _.mapValues\n * @example\n *\n * _.mapKeys({ 'a': 1, 'b': 2 }, function(value, key) {\n *   return key + value;\n * });\n * // => { 'a1': 1, 'b2': 2 }\n */\nfunction mapKeys(object, iteratee) {\n  var result = {};\n  iteratee = baseIteratee(iteratee, 3);\n\n  baseForOwn(object, function(value, key, object) {\n    baseAssignValue(result, iteratee(value, key, object), value);\n  });\n  return result;\n}\n\nmodule.exports = mapKeys;\n", "\n/**\n * Topological sorting function\n *\n * @param {Array} edges\n * @returns {Array}\n */\n\nmodule.exports = function(edges) {\n  return toposort(uniqueNodes(edges), edges)\n}\n\nmodule.exports.array = toposort\n\nfunction toposort(nodes, edges) {\n  var cursor = nodes.length\n    , sorted = new Array(cursor)\n    , visited = {}\n    , i = cursor\n    // Better data structures make algorithm much faster.\n    , outgoingEdges = makeOutgoingEdges(edges)\n    , nodesHash = makeNodesHash(nodes)\n\n  // check for unknown nodes\n  edges.forEach(function(edge) {\n    if (!nodesHash.has(edge[0]) || !nodesHash.has(edge[1])) {\n      throw new Error('Unknown node. There is an unknown node in the supplied edges.')\n    }\n  })\n\n  while (i--) {\n    if (!visited[i]) visit(nodes[i], i, new Set())\n  }\n\n  return sorted\n\n  function visit(node, i, predecessors) {\n    if(predecessors.has(node)) {\n      var nodeRep\n      try {\n        nodeRep = \", node was:\" + JSON.stringify(node)\n      } catch(e) {\n        nodeRep = \"\"\n      }\n      throw new Error('Cyclic dependency' + nodeRep)\n    }\n\n    if (!nodesHash.has(node)) {\n      throw new Error('Found unknown node. Make sure to provided all involved nodes. Unknown node: '+JSON.stringify(node))\n    }\n\n    if (visited[i]) return;\n    visited[i] = true\n\n    var outgoing = outgoingEdges.get(node) || new Set()\n    outgoing = Array.from(outgoing)\n\n    if (i = outgoing.length) {\n      predecessors.add(node)\n      do {\n        var child = outgoing[--i]\n        visit(child, nodesHash.get(child), predecessors)\n      } while (i)\n      predecessors.delete(node)\n    }\n\n    sorted[--cursor] = node\n  }\n}\n\nfunction uniqueNodes(arr){\n  var res = new Set()\n  for (var i = 0, len = arr.length; i < len; i++) {\n    var edge = arr[i]\n    res.add(edge[0])\n    res.add(edge[1])\n  }\n  return Array.from(res)\n}\n\nfunction makeOutgoingEdges(arr){\n  var edges = new Map()\n  for (var i = 0, len = arr.length; i < len; i++) {\n    var edge = arr[i]\n    if (!edges.has(edge[0])) edges.set(edge[0], new Set())\n    if (!edges.has(edge[1])) edges.set(edge[1], new Set())\n    edges.get(edge[0]).add(edge[1])\n  }\n  return edges\n}\n\nfunction makeNodesHash(arr){\n  var res = new Map()\n  for (var i = 0, len = arr.length; i < len; i++) {\n    res.set(arr[i], i)\n  }\n  return res\n}\n", "\"use strict\";\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\nexports.default = sortFields;\n\nvar _has = _interopRequireDefault(require(\"lodash/has\"));\n\nvar _toposort = _interopRequireDefault(require(\"toposort\"));\n\nvar _propertyExpr = require(\"property-expr\");\n\nvar _Reference = _interopRequireDefault(require(\"../Reference\"));\n\nvar _isSchema = _interopRequireDefault(require(\"./isSchema\"));\n\nfunction _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { default: obj }; }\n\n// @ts-expect-error\nfunction sortFields(fields, excludedEdges = []) {\n  let edges = [];\n  let nodes = new Set();\n  let excludes = new Set(excludedEdges.map(([a, b]) => `${a}-${b}`));\n\n  function addNode(depPath, key) {\n    let node = (0, _propertyExpr.split)(depPath)[0];\n    nodes.add(node);\n    if (!excludes.has(`${key}-${node}`)) edges.push([key, node]);\n  }\n\n  for (const key in fields) if ((0, _has.default)(fields, key)) {\n    let value = fields[key];\n    nodes.add(key);\n    if (_Reference.default.isRef(value) && value.isSibling) addNode(value.path, key);else if ((0, _isSchema.default)(value) && 'deps' in value) value.deps.forEach(path => addNode(path, key));\n  }\n\n  return _toposort.default.array(Array.from(nodes), edges).reverse();\n}", "\"use strict\";\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\nexports.default = sortByKeyOrder;\n\nfunction findIndex(arr, err) {\n  let idx = Infinity;\n  arr.some((key, ii) => {\n    var _err$path;\n\n    if (((_err$path = err.path) == null ? void 0 : _err$path.indexOf(key)) !== -1) {\n      idx = ii;\n      return true;\n    }\n  });\n  return idx;\n}\n\nfunction sortByKeyOrder(keys) {\n  return (a, b) => {\n    return findIndex(keys, a) - findIndex(keys, b);\n  };\n}", "\"use strict\";\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\nexports.create = create;\nexports.default = void 0;\n\nvar _has = _interopRequireDefault(require(\"lodash/has\"));\n\nvar _snakeCase = _interopRequireDefault(require(\"lodash/snakeCase\"));\n\nvar _camelCase = _interopRequireDefault(require(\"lodash/camelCase\"));\n\nvar _mapKeys = _interopRequireDefault(require(\"lodash/mapKeys\"));\n\nvar _mapValues = _interopRequireDefault(require(\"lodash/mapValues\"));\n\nvar _propertyExpr = require(\"property-expr\");\n\nvar _locale = require(\"./locale\");\n\nvar _sortFields = _interopRequireDefault(require(\"./util/sortFields\"));\n\nvar _sortByKeyOrder = _interopRequireDefault(require(\"./util/sortByKeyOrder\"));\n\nvar _runTests = _interopRequireDefault(require(\"./util/runTests\"));\n\nvar _ValidationError = _interopRequireDefault(require(\"./ValidationError\"));\n\nvar _schema = _interopRequireDefault(require(\"./schema\"));\n\nfunction _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { default: obj }; }\n\nfunction _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nlet isObject = obj => Object.prototype.toString.call(obj) === '[object Object]';\n\nfunction unknown(ctx, value) {\n  let known = Object.keys(ctx.fields);\n  return Object.keys(value).filter(key => known.indexOf(key) === -1);\n}\n\nconst defaultSort = (0, _sortByKeyOrder.default)([]);\n\nclass ObjectSchema extends _schema.default {\n  constructor(spec) {\n    super({\n      type: 'object'\n    });\n    this.fields = Object.create(null);\n    this._sortErrors = defaultSort;\n    this._nodes = [];\n    this._excludedEdges = [];\n    this.withMutation(() => {\n      this.transform(function coerce(value) {\n        if (typeof value === 'string') {\n          try {\n            value = JSON.parse(value);\n          } catch (err) {\n            value = null;\n          }\n        }\n\n        if (this.isType(value)) return value;\n        return null;\n      });\n\n      if (spec) {\n        this.shape(spec);\n      }\n    });\n  }\n\n  _typeCheck(value) {\n    return isObject(value) || typeof value === 'function';\n  }\n\n  _cast(_value, options = {}) {\n    var _options$stripUnknown;\n\n    let value = super._cast(_value, options); //should ignore nulls here\n\n\n    if (value === undefined) return this.getDefault();\n    if (!this._typeCheck(value)) return value;\n    let fields = this.fields;\n    let strip = (_options$stripUnknown = options.stripUnknown) != null ? _options$stripUnknown : this.spec.noUnknown;\n\n    let props = this._nodes.concat(Object.keys(value).filter(v => this._nodes.indexOf(v) === -1));\n\n    let intermediateValue = {}; // is filled during the transform below\n\n    let innerOptions = _extends({}, options, {\n      parent: intermediateValue,\n      __validating: options.__validating || false\n    });\n\n    let isChanged = false;\n\n    for (const prop of props) {\n      let field = fields[prop];\n      let exists = (0, _has.default)(value, prop);\n\n      if (field) {\n        let fieldValue;\n        let inputValue = value[prop]; // safe to mutate since this is fired in sequence\n\n        innerOptions.path = (options.path ? `${options.path}.` : '') + prop; // innerOptions.value = value[prop];\n\n        field = field.resolve({\n          value: inputValue,\n          context: options.context,\n          parent: intermediateValue\n        });\n        let fieldSpec = 'spec' in field ? field.spec : undefined;\n        let strict = fieldSpec == null ? void 0 : fieldSpec.strict;\n\n        if (fieldSpec == null ? void 0 : fieldSpec.strip) {\n          isChanged = isChanged || prop in value;\n          continue;\n        }\n\n        fieldValue = !options.__validating || !strict ? // TODO: use _cast, this is double resolving\n        field.cast(value[prop], innerOptions) : value[prop];\n\n        if (fieldValue !== undefined) {\n          intermediateValue[prop] = fieldValue;\n        }\n      } else if (exists && !strip) {\n        intermediateValue[prop] = value[prop];\n      }\n\n      if (intermediateValue[prop] !== value[prop]) {\n        isChanged = true;\n      }\n    }\n\n    return isChanged ? intermediateValue : value;\n  }\n\n  _validate(_value, opts = {}, callback) {\n    let errors = [];\n    let {\n      sync,\n      from = [],\n      originalValue = _value,\n      abortEarly = this.spec.abortEarly,\n      recursive = this.spec.recursive\n    } = opts;\n    from = [{\n      schema: this,\n      value: originalValue\n    }, ...from]; // this flag is needed for handling `strict` correctly in the context of\n    // validation vs just casting. e.g strict() on a field is only used when validating\n\n    opts.__validating = true;\n    opts.originalValue = originalValue;\n    opts.from = from;\n\n    super._validate(_value, opts, (err, value) => {\n      if (err) {\n        if (!_ValidationError.default.isError(err) || abortEarly) {\n          return void callback(err, value);\n        }\n\n        errors.push(err);\n      }\n\n      if (!recursive || !isObject(value)) {\n        callback(errors[0] || null, value);\n        return;\n      }\n\n      originalValue = originalValue || value;\n\n      let tests = this._nodes.map(key => (_, cb) => {\n        let path = key.indexOf('.') === -1 ? (opts.path ? `${opts.path}.` : '') + key : `${opts.path || ''}[\"${key}\"]`;\n        let field = this.fields[key];\n\n        if (field && 'validate' in field) {\n          field.validate(value[key], _extends({}, opts, {\n            // @ts-ignore\n            path,\n            from,\n            // inner fields are always strict:\n            // 1. this isn't strict so the casting will also have cast inner values\n            // 2. this is strict in which case the nested values weren't cast either\n            strict: true,\n            parent: value,\n            originalValue: originalValue[key]\n          }), cb);\n          return;\n        }\n\n        cb(null);\n      });\n\n      (0, _runTests.default)({\n        sync,\n        tests,\n        value,\n        errors,\n        endEarly: abortEarly,\n        sort: this._sortErrors,\n        path: opts.path\n      }, callback);\n    });\n  }\n\n  clone(spec) {\n    const next = super.clone(spec);\n    next.fields = _extends({}, this.fields);\n    next._nodes = this._nodes;\n    next._excludedEdges = this._excludedEdges;\n    next._sortErrors = this._sortErrors;\n    return next;\n  }\n\n  concat(schema) {\n    let next = super.concat(schema);\n    let nextFields = next.fields;\n\n    for (let [field, schemaOrRef] of Object.entries(this.fields)) {\n      const target = nextFields[field];\n\n      if (target === undefined) {\n        nextFields[field] = schemaOrRef;\n      } else if (target instanceof _schema.default && schemaOrRef instanceof _schema.default) {\n        nextFields[field] = schemaOrRef.concat(target);\n      }\n    }\n\n    return next.withMutation(() => next.shape(nextFields, this._excludedEdges));\n  }\n\n  getDefaultFromShape() {\n    let dft = {};\n\n    this._nodes.forEach(key => {\n      const field = this.fields[key];\n      dft[key] = 'default' in field ? field.getDefault() : undefined;\n    });\n\n    return dft;\n  }\n\n  _getDefault() {\n    if ('default' in this.spec) {\n      return super._getDefault();\n    } // if there is no default set invent one\n\n\n    if (!this._nodes.length) {\n      return undefined;\n    }\n\n    return this.getDefaultFromShape();\n  }\n\n  shape(additions, excludes = []) {\n    let next = this.clone();\n    let fields = Object.assign(next.fields, additions);\n    next.fields = fields;\n    next._sortErrors = (0, _sortByKeyOrder.default)(Object.keys(fields));\n\n    if (excludes.length) {\n      // this is a convenience for when users only supply a single pair\n      if (!Array.isArray(excludes[0])) excludes = [excludes];\n      next._excludedEdges = [...next._excludedEdges, ...excludes];\n    }\n\n    next._nodes = (0, _sortFields.default)(fields, next._excludedEdges);\n    return next;\n  }\n\n  pick(keys) {\n    const picked = {};\n\n    for (const key of keys) {\n      if (this.fields[key]) picked[key] = this.fields[key];\n    }\n\n    return this.clone().withMutation(next => {\n      next.fields = {};\n      return next.shape(picked);\n    });\n  }\n\n  omit(keys) {\n    const next = this.clone();\n    const fields = next.fields;\n    next.fields = {};\n\n    for (const key of keys) {\n      delete fields[key];\n    }\n\n    return next.withMutation(() => next.shape(fields));\n  }\n\n  from(from, to, alias) {\n    let fromGetter = (0, _propertyExpr.getter)(from, true);\n    return this.transform(obj => {\n      if (obj == null) return obj;\n      let newObj = obj;\n\n      if ((0, _has.default)(obj, from)) {\n        newObj = _extends({}, obj);\n        if (!alias) delete newObj[from];\n        newObj[to] = fromGetter(obj);\n      }\n\n      return newObj;\n    });\n  }\n\n  noUnknown(noAllow = true, message = _locale.object.noUnknown) {\n    if (typeof noAllow === 'string') {\n      message = noAllow;\n      noAllow = true;\n    }\n\n    let next = this.test({\n      name: 'noUnknown',\n      exclusive: true,\n      message: message,\n\n      test(value) {\n        if (value == null) return true;\n        const unknownKeys = unknown(this.schema, value);\n        return !noAllow || unknownKeys.length === 0 || this.createError({\n          params: {\n            unknown: unknownKeys.join(', ')\n          }\n        });\n      }\n\n    });\n    next.spec.noUnknown = noAllow;\n    return next;\n  }\n\n  unknown(allow = true, message = _locale.object.noUnknown) {\n    return this.noUnknown(!allow, message);\n  }\n\n  transformKeys(fn) {\n    return this.transform(obj => obj && (0, _mapKeys.default)(obj, (_, key) => fn(key)));\n  }\n\n  camelCase() {\n    return this.transformKeys(_camelCase.default);\n  }\n\n  snakeCase() {\n    return this.transformKeys(_snakeCase.default);\n  }\n\n  constantCase() {\n    return this.transformKeys(key => (0, _snakeCase.default)(key).toUpperCase());\n  }\n\n  describe() {\n    let base = super.describe();\n    base.fields = (0, _mapValues.default)(this.fields, value => value.describe());\n    return base;\n  }\n\n}\n\nexports.default = ObjectSchema;\n\nfunction create(spec) {\n  return new ObjectSchema(spec);\n}\n\ncreate.prototype = ObjectSchema.prototype;", "\"use strict\";\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\nexports.create = create;\nexports.default = void 0;\n\nvar _isAbsent = _interopRequireDefault(require(\"./util/isAbsent\"));\n\nvar _isSchema = _interopRequireDefault(require(\"./util/isSchema\"));\n\nvar _printValue = _interopRequireDefault(require(\"./util/printValue\"));\n\nvar _locale = require(\"./locale\");\n\nvar _runTests = _interopRequireDefault(require(\"./util/runTests\"));\n\nvar _ValidationError = _interopRequireDefault(require(\"./ValidationError\"));\n\nvar _schema = _interopRequireDefault(require(\"./schema\"));\n\nfunction _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { default: obj }; }\n\nfunction _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction create(type) {\n  return new ArraySchema(type);\n}\n\nclass ArraySchema extends _schema.default {\n  constructor(type) {\n    super({\n      type: 'array'\n    }); // `undefined` specifically means uninitialized, as opposed to\n    // \"no subtype\"\n\n    this.innerType = void 0;\n    this.innerType = type;\n    this.withMutation(() => {\n      this.transform(function (values) {\n        if (typeof values === 'string') try {\n          values = JSON.parse(values);\n        } catch (err) {\n          values = null;\n        }\n        return this.isType(values) ? values : null;\n      });\n    });\n  }\n\n  _typeCheck(v) {\n    return Array.isArray(v);\n  }\n\n  get _subType() {\n    return this.innerType;\n  }\n\n  _cast(_value, _opts) {\n    const value = super._cast(_value, _opts); //should ignore nulls here\n\n\n    if (!this._typeCheck(value) || !this.innerType) return value;\n    let isChanged = false;\n    const castArray = value.map((v, idx) => {\n      const castElement = this.innerType.cast(v, _extends({}, _opts, {\n        path: `${_opts.path || ''}[${idx}]`\n      }));\n\n      if (castElement !== v) {\n        isChanged = true;\n      }\n\n      return castElement;\n    });\n    return isChanged ? castArray : value;\n  }\n\n  _validate(_value, options = {}, callback) {\n    var _options$abortEarly, _options$recursive;\n\n    let errors = [];\n    let sync = options.sync;\n    let path = options.path;\n    let innerType = this.innerType;\n    let endEarly = (_options$abortEarly = options.abortEarly) != null ? _options$abortEarly : this.spec.abortEarly;\n    let recursive = (_options$recursive = options.recursive) != null ? _options$recursive : this.spec.recursive;\n    let originalValue = options.originalValue != null ? options.originalValue : _value;\n\n    super._validate(_value, options, (err, value) => {\n      if (err) {\n        if (!_ValidationError.default.isError(err) || endEarly) {\n          return void callback(err, value);\n        }\n\n        errors.push(err);\n      }\n\n      if (!recursive || !innerType || !this._typeCheck(value)) {\n        callback(errors[0] || null, value);\n        return;\n      }\n\n      originalValue = originalValue || value; // #950 Ensure that sparse array empty slots are validated\n\n      let tests = new Array(value.length);\n\n      for (let idx = 0; idx < value.length; idx++) {\n        let item = value[idx];\n        let path = `${options.path || ''}[${idx}]`; // object._validate note for isStrict explanation\n\n        let innerOptions = _extends({}, options, {\n          path,\n          strict: true,\n          parent: value,\n          index: idx,\n          originalValue: originalValue[idx]\n        });\n\n        tests[idx] = (_, cb) => innerType.validate(item, innerOptions, cb);\n      }\n\n      (0, _runTests.default)({\n        sync,\n        path,\n        value,\n        errors,\n        endEarly,\n        tests\n      }, callback);\n    });\n  }\n\n  clone(spec) {\n    const next = super.clone(spec);\n    next.innerType = this.innerType;\n    return next;\n  }\n\n  concat(schema) {\n    let next = super.concat(schema);\n    next.innerType = this.innerType;\n    if (schema.innerType) next.innerType = next.innerType ? // @ts-expect-error Lazy doesn't have concat()\n    next.innerType.concat(schema.innerType) : schema.innerType;\n    return next;\n  }\n\n  of(schema) {\n    // FIXME: this should return a new instance of array without the default to be\n    let next = this.clone();\n    if (!(0, _isSchema.default)(schema)) throw new TypeError('`array.of()` sub-schema must be a valid yup schema not: ' + (0, _printValue.default)(schema)); // FIXME(ts):\n\n    next.innerType = schema;\n    return next;\n  }\n\n  length(length, message = _locale.array.length) {\n    return this.test({\n      message,\n      name: 'length',\n      exclusive: true,\n      params: {\n        length\n      },\n\n      test(value) {\n        return (0, _isAbsent.default)(value) || value.length === this.resolve(length);\n      }\n\n    });\n  }\n\n  min(min, message) {\n    message = message || _locale.array.min;\n    return this.test({\n      message,\n      name: 'min',\n      exclusive: true,\n      params: {\n        min\n      },\n\n      // FIXME(ts): Array<typeof T>\n      test(value) {\n        return (0, _isAbsent.default)(value) || value.length >= this.resolve(min);\n      }\n\n    });\n  }\n\n  max(max, message) {\n    message = message || _locale.array.max;\n    return this.test({\n      message,\n      name: 'max',\n      exclusive: true,\n      params: {\n        max\n      },\n\n      test(value) {\n        return (0, _isAbsent.default)(value) || value.length <= this.resolve(max);\n      }\n\n    });\n  }\n\n  ensure() {\n    return this.default(() => []).transform((val, original) => {\n      // We don't want to return `null` for nullable schema\n      if (this._typeCheck(val)) return val;\n      return original == null ? [] : [].concat(original);\n    });\n  }\n\n  compact(rejector) {\n    let reject = !rejector ? v => !!v : (v, i, a) => !rejector(v, i, a);\n    return this.transform(values => values != null ? values.filter(reject) : values);\n  }\n\n  describe() {\n    let base = super.describe();\n    if (this.innerType) base.innerType = this.innerType.describe();\n    return base;\n  }\n\n  nullable(isNullable = true) {\n    return super.nullable(isNullable);\n  }\n\n  defined() {\n    return super.defined();\n  }\n\n  required(msg) {\n    return super.required(msg);\n  }\n\n}\n\nexports.default = ArraySchema;\ncreate.prototype = ArraySchema.prototype; //\n// Interfaces\n//", "\"use strict\";\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\nexports.create = create;\nexports.default = void 0;\n\nvar _isSchema = _interopRequireDefault(require(\"./util/isSchema\"));\n\nfunction _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { default: obj }; }\n\nfunction create(builder) {\n  return new Lazy(builder);\n}\n\nclass Lazy {\n  constructor(builder) {\n    this.type = 'lazy';\n    this.__isYupSchema__ = true;\n    this.__inputType = void 0;\n    this.__outputType = void 0;\n\n    this._resolve = (value, options = {}) => {\n      let schema = this.builder(value, options);\n      if (!(0, _isSchema.default)(schema)) throw new TypeError('lazy() functions must return a valid schema');\n      return schema.resolve(options);\n    };\n\n    this.builder = builder;\n  }\n\n  resolve(options) {\n    return this._resolve(options.value, options);\n  }\n\n  cast(value, options) {\n    return this._resolve(value, options).cast(value, options);\n  }\n\n  validate(value, options, maybeCb) {\n    // @ts-expect-error missing public callback on type\n    return this._resolve(value, options).validate(value, options, maybeCb);\n  }\n\n  validateSync(value, options) {\n    return this._resolve(value, options).validateSync(value, options);\n  }\n\n  validateAt(path, value, options) {\n    return this._resolve(value, options).validateAt(path, value, options);\n  }\n\n  validateSyncAt(path, value, options) {\n    return this._resolve(value, options).validateSyncAt(path, value, options);\n  }\n\n  describe() {\n    return null;\n  }\n\n  isValid(value, options) {\n    return this._resolve(value, options).isValid(value, options);\n  }\n\n  isValidSync(value, options) {\n    return this._resolve(value, options).isValidSync(value, options);\n  }\n\n}\n\nvar _default = Lazy;\nexports.default = _default;", "\"use strict\";\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\nexports.default = setLocale;\n\nvar _locale = _interopRequireDefault(require(\"./locale\"));\n\nfunction _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { default: obj }; }\n\nfunction setLocale(custom) {\n  Object.keys(custom).forEach(type => {\n    // @ts-ignore\n    Object.keys(custom[type]).forEach(method => {\n      // @ts-ignore\n      _locale.default[type][method] = custom[type][method];\n    });\n  });\n}", "\"use strict\";\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\nObject.defineProperty(exports, \"ArraySchema\", {\n  enumerable: true,\n  get: function () {\n    return _array.default;\n  }\n});\nObject.defineProperty(exports, \"BaseSchema\", {\n  enumerable: true,\n  get: function () {\n    return _schema.default;\n  }\n});\nObject.defineProperty(exports, \"BooleanSchema\", {\n  enumerable: true,\n  get: function () {\n    return _boolean.default;\n  }\n});\nObject.defineProperty(exports, \"DateSchema\", {\n  enumerable: true,\n  get: function () {\n    return _date.default;\n  }\n});\nObject.defineProperty(exports, \"MixedSchema\", {\n  enumerable: true,\n  get: function () {\n    return _mixed.default;\n  }\n});\nObject.defineProperty(exports, \"NumberSchema\", {\n  enumerable: true,\n  get: function () {\n    return _number.default;\n  }\n});\nObject.defineProperty(exports, \"ObjectSchema\", {\n  enumerable: true,\n  get: function () {\n    return _object.default;\n  }\n});\nObject.defineProperty(exports, \"StringSchema\", {\n  enumerable: true,\n  get: function () {\n    return _string.default;\n  }\n});\nObject.defineProperty(exports, \"ValidationError\", {\n  enumerable: true,\n  get: function () {\n    return _ValidationError.default;\n  }\n});\nexports.addMethod = addMethod;\nObject.defineProperty(exports, \"array\", {\n  enumerable: true,\n  get: function () {\n    return _array.create;\n  }\n});\nObject.defineProperty(exports, \"bool\", {\n  enumerable: true,\n  get: function () {\n    return _boolean.create;\n  }\n});\nObject.defineProperty(exports, \"boolean\", {\n  enumerable: true,\n  get: function () {\n    return _boolean.create;\n  }\n});\nObject.defineProperty(exports, \"date\", {\n  enumerable: true,\n  get: function () {\n    return _date.create;\n  }\n});\nObject.defineProperty(exports, \"isSchema\", {\n  enumerable: true,\n  get: function () {\n    return _isSchema.default;\n  }\n});\nObject.defineProperty(exports, \"lazy\", {\n  enumerable: true,\n  get: function () {\n    return _Lazy.create;\n  }\n});\nObject.defineProperty(exports, \"mixed\", {\n  enumerable: true,\n  get: function () {\n    return _mixed.create;\n  }\n});\nObject.defineProperty(exports, \"number\", {\n  enumerable: true,\n  get: function () {\n    return _number.create;\n  }\n});\nObject.defineProperty(exports, \"object\", {\n  enumerable: true,\n  get: function () {\n    return _object.create;\n  }\n});\nObject.defineProperty(exports, \"reach\", {\n  enumerable: true,\n  get: function () {\n    return _reach.default;\n  }\n});\nObject.defineProperty(exports, \"ref\", {\n  enumerable: true,\n  get: function () {\n    return _Reference.create;\n  }\n});\nObject.defineProperty(exports, \"setLocale\", {\n  enumerable: true,\n  get: function () {\n    return _setLocale.default;\n  }\n});\nObject.defineProperty(exports, \"string\", {\n  enumerable: true,\n  get: function () {\n    return _string.create;\n  }\n});\n\nvar _mixed = _interopRequireWildcard(require(\"./mixed\"));\n\nvar _boolean = _interopRequireWildcard(require(\"./boolean\"));\n\nvar _string = _interopRequireWildcard(require(\"./string\"));\n\nvar _number = _interopRequireWildcard(require(\"./number\"));\n\nvar _date = _interopRequireWildcard(require(\"./date\"));\n\nvar _object = _interopRequireWildcard(require(\"./object\"));\n\nvar _array = _interopRequireWildcard(require(\"./array\"));\n\nvar _Reference = require(\"./Reference\");\n\nvar _Lazy = require(\"./Lazy\");\n\nvar _ValidationError = _interopRequireDefault(require(\"./ValidationError\"));\n\nvar _reach = _interopRequireDefault(require(\"./util/reach\"));\n\nvar _isSchema = _interopRequireDefault(require(\"./util/isSchema\"));\n\nvar _setLocale = _interopRequireDefault(require(\"./setLocale\"));\n\nvar _schema = _interopRequireDefault(require(\"./schema\"));\n\nfunction _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { default: obj }; }\n\nfunction _getRequireWildcardCache(nodeInterop) { if (typeof WeakMap !== \"function\") return null; var cacheBabelInterop = new WeakMap(); var cacheNodeInterop = new WeakMap(); return (_getRequireWildcardCache = function (nodeInterop) { return nodeInterop ? cacheNodeInterop : cacheBabelInterop; })(nodeInterop); }\n\nfunction _interopRequireWildcard(obj, nodeInterop) { if (!nodeInterop && obj && obj.__esModule) { return obj; } if (obj === null || typeof obj !== \"object\" && typeof obj !== \"function\") { return { default: obj }; } var cache = _getRequireWildcardCache(nodeInterop); if (cache && cache.has(obj)) { return cache.get(obj); } var newObj = {}; var hasPropertyDescriptor = Object.defineProperty && Object.getOwnPropertyDescriptor; for (var key in obj) { if (key !== \"default\" && Object.prototype.hasOwnProperty.call(obj, key)) { var desc = hasPropertyDescriptor ? Object.getOwnPropertyDescriptor(obj, key) : null; if (desc && (desc.get || desc.set)) { Object.defineProperty(newObj, key, desc); } else { newObj[key] = obj[key]; } } } newObj.default = obj; if (cache) { cache.set(obj, newObj); } return newObj; }\n\nfunction addMethod(schemaType, name, fn) {\n  if (!schemaType || !(0, _isSchema.default)(schemaType.prototype)) throw new TypeError('You must provide a yup schema constructor function');\n  if (typeof name !== 'string') throw new TypeError('A Method name must be provided');\n  if (typeof fn !== 'function') throw new TypeError('Method function must be provided');\n  schemaType.prototype[name] = fn;\n}", "import KSUID from \"ksuid\";\nimport handler from \"../handler-lib\";\n// utils\nimport { hasPermissions } from \"../../utils/auth/authorization\";\nimport { parseStateReportParameters } from \"../../utils/auth/parameters\";\nimport {\n  validateData,\n  validateFieldData,\n} from \"../../utils/validation/validation\";\nimport { metadataValidationSchema } from \"../../utils/validation/schemas\";\nimport { error, reportNames } from \"../../utils/constants/constants\";\nimport {\n  calculateDueDate,\n  calculatePeriod,\n  calculateCurrentYear,\n} from \"../../utils/time/time\";\nimport {\n  createReportName,\n  getEligibleWorkPlan,\n  getReportPeriod,\n  getReportYear,\n} from \"../../utils/other/other\";\nimport {\n  putReportFieldData,\n  putReportMetadata,\n  queryReportMetadatasForState,\n} from \"../../storage/reports\";\nimport { getOrCreateFormTemplate } from \"../../utils/formTemplates/formTemplates\";\nimport { logger } from \"../../utils/debugging/debug-lib\";\nimport { copyFieldDataFromSource } from \"../../utils/other/copy\";\nimport { extractWorkPlanData } from \"../../utils/transformations/transformations\";\nimport assert from \"node:assert\";\n// types\nimport {\n  AnyObject,\n  APIGatewayProxyEvent,\n  ReportMetadataShape,\n  ReportType,\n  UserRoles,\n} from \"../../utils/types\";\nimport {\n  badRequest,\n  created,\n  forbidden,\n  internalServerError,\n  notFound,\n} from \"../../utils/responses/response-lib\";\n\nexport const createReport = handler(\n  async (event: APIGatewayProxyEvent, _context) => {\n    const { allParamsValid, reportType, state } =\n      parseStateReportParameters(event);\n    if (!allParamsValid) {\n      return badRequest(error.NO_KEY);\n    }\n\n    if (!hasPermissions(event, [UserRoles.STATE_USER], state)) {\n      return forbidden(error.UNAUTHORIZED);\n    }\n\n    const reportTypeExpanded = reportNames[reportType];\n\n    /*\n     * Begin Section - If creating a SAR Submission, find the oldest Work Plan created that hasn't been used\n     * to create a different SAR and attach all of its fieldData to the SAR Submissions FieldData\n     */\n    const {\n      workPlanMetadata,\n      workPlanFieldData,\n    }: {\n      workPlanMetadata?: ReportMetadataShape;\n      workPlanFieldData?: AnyObject;\n    } =\n      reportType === ReportType.SAR\n        ? await getEligibleWorkPlan(state)\n        : { workPlanMetadata: undefined, workPlanFieldData: undefined };\n\n    // If we received no work plan information and we're trying to create a SAR, return NO_WORKPLANS_FOUND\n    if (\n      reportType === ReportType.SAR &&\n      (!workPlanMetadata || !workPlanFieldData)\n    ) {\n      return notFound(error.NO_WORKPLANS_FOUND);\n    }\n\n    // Check the payload that was sent with the request and setup validation\n    const unvalidatedPayload = JSON.parse(event.body!);\n    const { metadata: unvalidatedMetadata, fieldData: unvalidatedFieldData } =\n      unvalidatedPayload;\n\n    const creationFieldDataValidationJson = {\n      stateName: \"text\",\n      stateOrTerritory: \"text\",\n      submissionCount: \"number\",\n      submissionName: \"text\",\n      targetPopulations: \"objectArray\",\n    };\n\n    const currentDate = Date.now();\n\n    const overrideCopyOver =\n      unvalidatedMetadata?.copyReport &&\n      unvalidatedMetadata?.copyReport?.isCopyOverTest;\n\n    /**\n     * If the report is a WP, determine reportYear from the unvalidated metadata. Otherwise, a SAR will use the workplan metadata.\n     */\n    let reportData =\n      reportType === ReportType.WP ? unvalidatedMetadata : workPlanMetadata;\n\n    const reportYear = getReportYear(reportData, overrideCopyOver);\n    const reportPeriod = getReportPeriod(reportData, overrideCopyOver);\n\n    // Begin Section - Getting/Creating newest Form Template based on reportType\n    let formTemplate, formTemplateVersion;\n    try {\n      ({ formTemplate, formTemplateVersion } = await getOrCreateFormTemplate(\n        reportType,\n        reportPeriod,\n        reportYear,\n        workPlanFieldData\n      ));\n    } catch (err) {\n      logger.error(err, \"Error getting or creating template\");\n      throw err;\n    }\n    // End Section - Getting/Creating newest Form Template based on reportType\n\n    // Return MISSING_DATA error if missing unvalidated data or validators.\n    if (!unvalidatedFieldData || !formTemplate.validationJson) {\n      return badRequest(error.MISSING_DATA);\n    }\n\n    // Setup validation for what we expect to see in the payload\n    let validatedFieldData;\n    try {\n      validatedFieldData = await validateFieldData(\n        creationFieldDataValidationJson,\n        unvalidatedFieldData\n      );\n    } catch {\n      return badRequest(error.INVALID_DATA);\n    }\n\n    // If we are creating a SAR and found a Work Plan to copy from, grab its field data and add it to our SAR\n    if (workPlanFieldData) {\n      validatedFieldData = {\n        ...validatedFieldData,\n        ...workPlanFieldData,\n      };\n\n      extractWorkPlanData(validatedFieldData!, reportYear, reportPeriod);\n    }\n\n    // Return INVALID_DATA error field data has no valid entries\n    if (validatedFieldData && Object.keys(validatedFieldData).length === 0) {\n      return badRequest(error.INVALID_DATA);\n    }\n    // End Section - Check the payload that was sent with the request and validate it\n\n    // Being Section - Check if metadata has filled parameter for copyReport\n    let newFieldData;\n\n    const updatedValidatedFieldData = {\n      ...validatedFieldData,\n      generalInformation_resubmissionInformation: \"N/A\",\n    };\n\n    if (unvalidatedMetadata.copyReport) {\n      const reportPeriod = calculatePeriod(Date.now(), workPlanMetadata);\n      const isCurrentPeriod =\n        calculateCurrentYear() === unvalidatedMetadata.copyReport.reportYear &&\n        reportPeriod === unvalidatedMetadata.copyReport.reportPeriod;\n\n      //do not allow user to create a copy if it's the same period\n      if (isCurrentPeriod && !overrideCopyOver) {\n        return badRequest(error.UNABLE_TO_COPY);\n      }\n\n      newFieldData = await copyFieldDataFromSource(\n        state,\n        unvalidatedMetadata.copyReport?.fieldDataId,\n        formTemplate,\n        updatedValidatedFieldData\n      );\n    } else {\n      newFieldData = updatedValidatedFieldData;\n    }\n\n    // End Section - Check if metadata has filled parameter for copyReport\n    /*\n     * End Section - If creating a SAR Submission, find the last Work Plan created that hasn't been used\n     * to create a different SAR and attach all of its fieldData to the SAR Submissions FieldData\n     */\n\n    // Validate the metadata for the submission\n    let validatedMetadata;\n    try {\n      validatedMetadata = await validateData(metadataValidationSchema, {\n        ...unvalidatedMetadata,\n      });\n    } catch {\n      // Return INVALID_DATA error if metadata is not valid.\n      return badRequest(error.INVALID_DATA);\n    }\n\n    const existingReports: ReportMetadataShape[] =\n      await queryReportMetadatasForState(reportType, state);\n\n    for (const report of existingReports) {\n      const {\n        reportYear: existingReportYear,\n        reportPeriod: existingReportPeriod,\n        archived,\n      } = report;\n      if (\n        !archived &&\n        existingReportYear === reportYear &&\n        existingReportPeriod === reportPeriod\n      ) {\n        return badRequest(error.INVALID_DATA);\n      }\n    }\n    const reportId: string = KSUID.randomSync().string;\n    const fieldDataId: string = KSUID.randomSync().string;\n    const formTemplateId: string = formTemplateVersion?.id;\n\n    try {\n      await putReportFieldData(\n        { reportType, state, fieldDataId },\n        newFieldData\n      );\n    } catch {\n      return internalServerError(error.S3_OBJECT_CREATION_ERROR);\n    }\n\n    // Begin Section - Create DyanmoDB record\n    const createdReportMetadata: ReportMetadataShape = {\n      ...validatedMetadata,\n      state,\n      id: reportId,\n      fieldDataId,\n      status: \"Not started\",\n      formTemplateId,\n      createdAt: currentDate,\n      lastAltered: currentDate,\n      versionNumber: formTemplateVersion?.versionNumber,\n      submissionName: createReportName(\n        reportTypeExpanded,\n        reportPeriod,\n        state,\n        reportYear,\n        workPlanMetadata\n      ),\n      reportYear,\n      reportPeriod,\n      isCopied: overrideCopyOver ? true : false,\n      dueDate: calculateDueDate(reportYear, reportPeriod, reportType),\n      associatedWorkPlan: workPlanMetadata?.id,\n    };\n\n    try {\n      await putReportMetadata(createdReportMetadata);\n    } catch {\n      return internalServerError(error.DYNAMO_CREATION_ERROR);\n    }\n    // End Section - Create DynamoDB record.\n\n    // Begin Section - Let the Workplan know that its been tied to a SAR that was just created\n    if (reportType === ReportType.SAR) {\n      assert(workPlanMetadata !== undefined);\n      const workPlanWithSarConnection = {\n        ...workPlanMetadata,\n        associatedSar: reportId,\n      };\n      try {\n        await putReportMetadata(workPlanWithSarConnection);\n      } catch {\n        return internalServerError(error.DYNAMO_CREATION_ERROR);\n      }\n    }\n    // End Section - Let the Workplan know that its been tied to a SAR that was just created\n\n    // Return successful creation response!\n    return created({\n      ...createdReportMetadata,\n      fieldData: validatedFieldData,\n      formTemplate,\n      formTemplateVersion: formTemplateVersion?.versionNumber,\n    });\n  }\n);\n", "import util from \"util\";\nimport { Logger } from \"@smithy/types\";\n\ntype LogLevel = \"trace\" | \"debug\" | \"info\" | \"warn\" | \"error\";\ntype LogEntry = {\n  date: Date;\n  level: LogLevel;\n  string: string;\n};\n\nconst logs: LogEntry[] = [];\n\nconst buildLoggerForLevel = (level: LogLevel) => {\n  return function (...content: any[]) {\n    logs.push({\n      date: new Date(),\n      level: level,\n      string: util.format.apply(null, content),\n    });\n\n    /*\n     * If we have a function logging thousands of messages,\n     * better to take the console performance hit mid-operation\n     * than to let memory usage run away as well.\n     */\n    if (logs.length > 99) {\n      flush();\n    }\n  };\n};\n\n/*\n * Individual functions are exported to support handler-lib;\n * This integrates SDK client logging with lambda logging.\n */\nexport const trace = buildLoggerForLevel(\"trace\");\nexport const debug = buildLoggerForLevel(\"debug\");\nexport const info = buildLoggerForLevel(\"info\");\nexport const warn = buildLoggerForLevel(\"warn\");\nexport const error = buildLoggerForLevel(\"error\");\n\nexport function flush() {\n  while (logs.length > 0) {\n    const { date, level, string } = logs.shift()!;\n    // eslint-disable-next-line no-console\n    console[level](date, string);\n  }\n}\n\n/*\n * This is only called at the beginning of a lambda handler,\n * so the log buffer should be empty anyway. But it doesn't\n * hurt to make sure!\n */\nexport const init = flush;\n\n/**\n * A logger suitable for passing to any AWS client constructor.\n * Note that the `trace` log level is excluded.\n *\n * This logger accumulates log messages in an internal buffer,\n * eventually flushing them to the console.\n */\nexport const logger: Logger = { debug, info, warn, error };\n", "import { SSMClient, GetParameterCommand } from \"@aws-sdk/client-ssm\";\nimport jwt_decode from \"jwt-decode\";\nimport { CognitoJwtVerifier } from \"aws-jwt-verify\";\n// types\nimport { APIGatewayProxyEvent, UserRoles } from \"../types\";\nimport { logger } from \"../debugging/debug-lib\";\n\ninterface DecodedToken {\n  \"custom:cms_roles\": UserRoles;\n  \"custom:cms_state\": string | undefined;\n}\n\nconst loadCognitoValues = async () => {\n  if (\n    process.env.COGNITO_USER_POOL_ID &&\n    process.env.COGNITO_USER_POOL_CLIENT_ID\n  ) {\n    return {\n      userPoolId: process.env.COGNITO_USER_POOL_ID,\n      userPoolClientId: process.env.COGNITO_USER_POOL_CLIENT_ID,\n    };\n  } else {\n    const ssmClient = new SSMClient({ logger });\n    const stage = process.env.stage!;\n    const getParam = async (identifier: string) => {\n      const command = new GetParameterCommand({\n        Name: `/${stage}/ui-auth/${identifier}`,\n      });\n      const result = await ssmClient.send(command);\n      return result.Parameter?.Value;\n    };\n    const userPoolId = await getParam(\"cognito_user_pool_id\");\n    const userPoolClientId = await getParam(\"cognito_user_pool_client_id\");\n    if (userPoolId && userPoolClientId) {\n      process.env[\"COGNITO_USER_POOL_ID\"] = userPoolId;\n      process.env[\"COGNITO_USER_POOL_CLIENT_ID\"] = userPoolClientId;\n      return { userPoolId, userPoolClientId };\n    } else {\n      throw new Error(\"cannot load cognito values\");\n    }\n  }\n};\n\nexport const isAuthenticated = async (event: APIGatewayProxyEvent) => {\n  const isLocalStack = event.requestContext.accountId === \"000000000000\";\n  if (isLocalStack) {\n    return true;\n  }\n\n  const cognitoValues = await loadCognitoValues();\n\n  // Verifier that expects valid access tokens:\n  const verifier = CognitoJwtVerifier.create({\n    userPoolId: cognitoValues.userPoolId,\n    tokenUse: \"id\",\n    clientId: cognitoValues.userPoolClientId,\n  });\n\n  try {\n    await verifier.verify(event?.headers?.[\"x-api-key\"]!);\n    return true;\n  } catch {\n    return false;\n  }\n};\n\nexport const hasPermissions = (\n  event: APIGatewayProxyEvent,\n  allowedRoles: UserRoles[],\n  state?: string\n) => {\n  let isAllowed = false;\n  // decode the idToken\n  if (event?.headers?.[\"x-api-key\"]) {\n    const decoded = jwt_decode(event.headers[\"x-api-key\"]) as DecodedToken;\n    const idmUserRoles = decoded[\"custom:cms_roles\"];\n    const idmUserState = decoded[\"custom:cms_state\"];\n    const mfpUserRole = idmUserRoles\n      ?.split(\",\")\n      .find((role) => role.includes(\"mdctmfp\")) as UserRoles;\n\n    isAllowed =\n      allowedRoles.includes(mfpUserRole) &&\n      (!state || idmUserState?.includes(state))!;\n  }\n\n  return isAllowed;\n};\n\nexport const isAuthorizedToFetchState = (\n  event: APIGatewayProxyEvent,\n  state: string\n) => {\n  // If this is a state user for the matching state, authorize them.\n  if (hasPermissions(event, [UserRoles.STATE_USER], state)) {\n    return true;\n  }\n\n  const nonStateUserRoles = Object.values(UserRoles).filter(\n    (role) => role !== UserRoles.STATE_USER\n  );\n\n  // If they are any other user type, they don't need to belong to this state.\n  return hasPermissions(event, nonStateUserRoles);\n};\n", "// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n// SPDX-License-Identifier: Apache-2.0\n/**\n * Base Error for all other errors in this file\n */\nexport class JwtBaseError extends Error {\n}\n/**\n * An error that is raised because an actual value does not match with the expected value\n */\nexport class FailedAssertionError extends JwtBaseError {\n    constructor(msg, actual, expected) {\n        super(msg);\n        this.failedAssertion = {\n            actual,\n            expected,\n        };\n    }\n}\n/**\n * JWT errors\n */\nexport class JwtParseError extends JwtBaseError {\n    constructor(msg, error) {\n        const message = error != null ? `${msg}: ${error}` : msg;\n        super(message);\n    }\n}\nexport class ParameterValidationError extends JwtBaseError {\n}\nexport class JwtInvalidSignatureError extends JwtBaseError {\n}\nexport class JwtInvalidSignatureAlgorithmError extends FailedAssertionError {\n}\nexport class JwtInvalidClaimError extends FailedAssertionError {\n    withRawJwt({ header, payload }) {\n        this.rawJwt = {\n            header,\n            payload,\n        };\n        return this;\n    }\n}\nexport class JwtInvalidIssuerError extends JwtInvalidClaimError {\n}\nexport class JwtInvalidAudienceError extends JwtInvalidClaimError {\n}\nexport class JwtInvalidScopeError extends JwtInvalidClaimError {\n}\nexport class JwtExpiredError extends JwtInvalidClaimError {\n}\nexport class JwtNotBeforeError extends JwtInvalidClaimError {\n}\n/**\n * Amazon Cognito specific erros\n */\nexport class CognitoJwtInvalidGroupError extends JwtInvalidClaimError {\n}\nexport class CognitoJwtInvalidTokenUseError extends JwtInvalidClaimError {\n}\nexport class CognitoJwtInvalidClientIdError extends JwtInvalidClaimError {\n}\n/**\n * ASN.1 errors\n */\nexport class Asn1DecodingError extends JwtBaseError {\n}\n/**\n * JWK errors\n */\nexport class JwksValidationError extends JwtBaseError {\n}\nexport class JwkValidationError extends JwtBaseError {\n}\nexport class JwtWithoutValidKidError extends JwtBaseError {\n}\nexport class KidNotFoundInJwksError extends JwtBaseError {\n}\nexport class WaitPeriodNotYetEndedJwkError extends JwtBaseError {\n}\nexport class JwksNotAvailableInCacheError extends JwtBaseError {\n}\nexport class JwkInvalidUseError extends FailedAssertionError {\n}\nexport class JwkInvalidKtyError extends FailedAssertionError {\n}\n/**\n * HTTPS fetch errors\n */\nexport class FetchError extends JwtBaseError {\n    constructor(uri, msg) {\n        super(`Failed to fetch ${uri}: ${msg}`);\n    }\n}\nexport class NonRetryableFetchError extends FetchError {\n}\n/**\n * Web compatibility errors\n */\nexport class NotSupportedError extends JwtBaseError {\n}\n", "// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n// SPDX-License-Identifier: Apache-2.0\n//\n// Node.js implementations for the node-web-compatibility layer\nimport { createPublicKey, createVerify } from \"crypto\";\nimport { constructPublicKeyInDerFormat } from \"./asn1.js\";\nimport { fetchJson } from \"./https-node.js\";\n/**\n * Enum to map supported JWT signature algorithms with OpenSSL message digest algorithm names\n */\nvar JwtSignatureAlgorithms;\n(function (JwtSignatureAlgorithms) {\n    JwtSignatureAlgorithms[\"RS256\"] = \"RSA-SHA256\";\n    JwtSignatureAlgorithms[\"RS384\"] = \"RSA-SHA384\";\n    JwtSignatureAlgorithms[\"RS512\"] = \"RSA-SHA512\";\n})(JwtSignatureAlgorithms || (JwtSignatureAlgorithms = {}));\nexport const nodeWebCompat = {\n    fetchJson,\n    transformJwkToKeyObjectSync: (jwk) => createPublicKey({\n        key: constructPublicKeyInDerFormat(Buffer.from(jwk.n, \"base64\"), Buffer.from(jwk.e, \"base64\")),\n        format: \"der\",\n        type: \"spki\",\n    }),\n    transformJwkToKeyObjectAsync: async (jwk) => createPublicKey({\n        key: constructPublicKeyInDerFormat(Buffer.from(jwk.n, \"base64\"), Buffer.from(jwk.e, \"base64\")),\n        format: \"der\",\n        type: \"spki\",\n    }),\n    parseB64UrlString: (b64) => Buffer.from(b64, \"base64\").toString(\"utf8\"),\n    verifySignatureSync: ({ alg, keyObject, jwsSigningInput, signature }) => \n    // eslint-disable-next-line security/detect-object-injection\n    createVerify(JwtSignatureAlgorithms[alg])\n        .update(jwsSigningInput)\n        .verify(keyObject, signature, \"base64\"),\n    verifySignatureAsync: async ({ alg, keyObject, jwsSigningInput, signature, }) => \n    // eslint-disable-next-line security/detect-object-injection\n    createVerify(JwtSignatureAlgorithms[alg])\n        .update(jwsSigningInput)\n        .verify(keyObject, signature, \"base64\"),\n    defaultFetchTimeouts: {\n        socketIdle: 500,\n        response: 1500,\n    },\n    setTimeoutUnref: (...args) => setTimeout(...args).unref(),\n};\n", "// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n// SPDX-License-Identifier: Apache-2.0\n//\n// Utility to encode RSA public keys (a pair of modulus (n) and exponent (e)) into DER-encoding, per ASN.1 specification.\nimport { Asn1DecodingError } from \"./error.js\";\n/** Enum with possible values for supported ASN.1 classes */\nvar Asn1Class;\n(function (Asn1Class) {\n    Asn1Class[Asn1Class[\"Universal\"] = 0] = \"Universal\";\n})(Asn1Class || (Asn1Class = {}));\n/** Enum with possible values for supported ASN.1 encodings */\nvar Asn1Encoding;\n(function (Asn1Encoding) {\n    Asn1Encoding[Asn1Encoding[\"Primitive\"] = 0] = \"Primitive\";\n    Asn1Encoding[Asn1Encoding[\"Constructed\"] = 1] = \"Constructed\";\n})(Asn1Encoding || (Asn1Encoding = {}));\n/** Enum with possible values for supported ASN.1 tags */\nvar Asn1Tag;\n(function (Asn1Tag) {\n    Asn1Tag[Asn1Tag[\"BitString\"] = 3] = \"BitString\";\n    Asn1Tag[Asn1Tag[\"ObjectIdentifier\"] = 6] = \"ObjectIdentifier\";\n    Asn1Tag[Asn1Tag[\"Sequence\"] = 16] = \"Sequence\";\n    Asn1Tag[Asn1Tag[\"Null\"] = 5] = \"Null\";\n    Asn1Tag[Asn1Tag[\"Integer\"] = 2] = \"Integer\";\n})(Asn1Tag || (Asn1Tag = {}));\n/**\n * Encode an ASN.1 identifier per ASN.1 spec (DER-encoding)\n * See https://www.itu.int/ITU-T/studygroups/com17/languages/X.690-0207.pdf chapter 8.1.2\n *\n * @param identifier - The ASN.1 identifier\n * @returns The buffer\n */\nfunction encodeIdentifier(identifier) {\n    const identifierAsNumber = (identifier.class << 7) |\n        (identifier.primitiveOrConstructed << 5) |\n        identifier.tag;\n    return Buffer.from([identifierAsNumber]);\n}\n/**\n * Encode the length of an ASN.1 type per ASN.1 spec (DER-encoding)\n * See https://www.itu.int/ITU-T/studygroups/com17/languages/X.690-0207.pdf chapter 8.1.3\n *\n * @param length - The length of the ASN.1 type\n * @returns The buffer\n */\nfunction encodeLength(length) {\n    if (length < 128) {\n        return Buffer.from([length]);\n    }\n    const integers = [];\n    while (length > 0) {\n        integers.push(length % 256);\n        length = length >> 8;\n    }\n    integers.reverse();\n    return Buffer.from([128 | integers.length, ...integers]);\n}\n/**\n * Encode a buffer (that represent an integer) as integer per ASN.1 spec (DER-encoding)\n * See https://www.itu.int/ITU-T/studygroups/com17/languages/X.690-0207.pdf chapter 8.3\n *\n * @param buffer - The buffer that represent an integer to encode\n * @returns The buffer\n */\nfunction encodeBufferAsInteger(buffer) {\n    return Buffer.concat([\n        encodeIdentifier({\n            class: Asn1Class.Universal,\n            primitiveOrConstructed: Asn1Encoding.Primitive,\n            tag: Asn1Tag.Integer,\n        }),\n        encodeLength(buffer.length),\n        buffer,\n    ]);\n}\n/**\n * Encode an object identifier (a string such as \"1.2.840.113549.1.1.1\") per ASN.1 spec (DER-encoding)\n * See https://www.itu.int/ITU-T/studygroups/com17/languages/X.690-0207.pdf chapter 8.19\n *\n * @param oid - The object identifier to encode\n * @returns The buffer\n */\nfunction encodeObjectIdentifier(oid) {\n    const oidComponents = oid.split(\".\").map((i) => parseInt(i));\n    const firstSubidentifier = oidComponents[0] * 40 + oidComponents[1];\n    const subsequentSubidentifiers = oidComponents\n        .slice(2)\n        .reduce((expanded, component) => {\n        const bytes = [];\n        do {\n            bytes.push(component % 128);\n            component = component >> 7;\n        } while (component);\n        return expanded.concat(bytes.map((b, index) => (index ? b + 128 : b)).reverse());\n    }, []);\n    const oidBuffer = Buffer.from([\n        firstSubidentifier,\n        ...subsequentSubidentifiers,\n    ]);\n    return Buffer.concat([\n        encodeIdentifier({\n            class: Asn1Class.Universal,\n            primitiveOrConstructed: Asn1Encoding.Primitive,\n            tag: Asn1Tag.ObjectIdentifier,\n        }),\n        encodeLength(oidBuffer.length),\n        oidBuffer,\n    ]);\n}\n/**\n * Encode a buffer as bit string per ASN.1 spec (DER-encoding)\n * See https://www.itu.int/ITU-T/studygroups/com17/languages/X.690-0207.pdf chapter 8.6\n *\n * @param buffer - The buffer to encode\n * @returns The buffer\n */\nfunction encodeBufferAsBitString(buffer) {\n    const bitString = Buffer.concat([Buffer.from([0]), buffer]);\n    return Buffer.concat([\n        encodeIdentifier({\n            class: Asn1Class.Universal,\n            primitiveOrConstructed: Asn1Encoding.Primitive,\n            tag: Asn1Tag.BitString,\n        }),\n        encodeLength(bitString.length),\n        bitString,\n    ]);\n}\n/**\n * Encode a sequence of DER-encoded items per ASN.1 spec (DER-encoding)\n * See https://www.itu.int/ITU-T/studygroups/com17/languages/X.690-0207.pdf chapter 8.9\n *\n * @param sequenceItems - The sequence of DER-encoded items\n * @returns The buffer\n */\nfunction encodeSequence(sequenceItems) {\n    const concatenated = Buffer.concat(sequenceItems);\n    return Buffer.concat([\n        encodeIdentifier({\n            class: Asn1Class.Universal,\n            primitiveOrConstructed: Asn1Encoding.Constructed,\n            tag: Asn1Tag.Sequence,\n        }),\n        encodeLength(concatenated.length),\n        concatenated,\n    ]);\n}\n/**\n * Encode null per ASN.1 spec (DER-encoding)\n * See https://www.itu.int/ITU-T/studygroups/com17/languages/X.690-0207.pdf chapter 8.8\n *\n * @returns The buffer\n */\nfunction encodeNull() {\n    return Buffer.concat([\n        encodeIdentifier({\n            class: Asn1Class.Universal,\n            primitiveOrConstructed: Asn1Encoding.Primitive,\n            tag: Asn1Tag.Null,\n        }),\n        encodeLength(0),\n    ]);\n}\n/**\n * RSA encryption object identifier constant\n *\n * From: https://tools.ietf.org/html/rfc8017\n *\n * pkcs-1    OBJECT IDENTIFIER ::= {\n *     iso(1) member-body(2) us(840) rsadsi(113549) pkcs(1) 1\n * }\n *\n * -- When rsaEncryption is used in an AlgorithmIdentifier,\n * -- the parameters MUST be present and MUST be NULL.\n * --\n * rsaEncryption    OBJECT IDENTIFIER ::= { pkcs-1 1 }\n *\n * See also: http://www.oid-info.com/get/1.2.840.113549.1.1.1\n */\nconst ALGORITHM_RSA_ENCRYPTION = encodeSequence([\n    encodeObjectIdentifier(\"1.2.840.113549.1.1.1\"),\n    encodeNull(), // parameters\n]);\n/**\n * Transform an RSA public key, which is a pair of modulus (n) and exponent (e),\n *  into a buffer per ASN.1 spec (DER-encoding)\n *\n * @param n - The modulus of the public key as buffer\n * @param e - The exponent of the public key as buffer\n * @returns The buffer, which is the public key encoded per ASN.1 spec (DER-encoding)\n */\nexport function constructPublicKeyInDerFormat(n, e) {\n    return encodeSequence([\n        ALGORITHM_RSA_ENCRYPTION,\n        encodeBufferAsBitString(encodeSequence([encodeBufferAsInteger(n), encodeBufferAsInteger(e)])),\n    ]);\n}\n/**\n * Decode an ASN.1 identifier (a number) into its parts: class, primitiveOrConstructed, tag\n * See https://www.itu.int/ITU-T/studygroups/com17/languages/X.690-0207.pdf chapter 8.1.2\n *\n * @param identifier - The identifier\n * @returns An object with properties class, primitiveOrConstructed, tag\n */\nfunction decodeIdentifier(identifier) {\n    if (identifier >> 3 === 0b11111) {\n        throw new Asn1DecodingError(\"Decoding of identifier with tag > 30 not implemented\");\n    }\n    return {\n        class: identifier >> 6,\n        primitiveOrConstructed: (identifier >> 5) & 0b001,\n        tag: identifier & 0b11111, // bit 1-5\n    };\n}\n/**\n * Decode an ASN.1 block of length value combinations,\n * and return the length and byte range of the first length value combination.\n * See https://www.itu.int/ITU-T/studygroups/com17/languages/X.690-0207.pdf chapter 8.1.3 - 8.1.5\n *\n * @param blockOfLengthValues - The ASN.1 length value\n * @returns The length and byte range of the first included length value\n */\nfunction decodeLengthValue(blockOfLengthValues) {\n    if (!(blockOfLengthValues[0] & 0b10000000)) {\n        return {\n            length: blockOfLengthValues[0],\n            firstByteOffset: 1,\n            lastByteOffset: 1 + blockOfLengthValues[0],\n        };\n    }\n    const nrLengthOctets = blockOfLengthValues[0] & 0b01111111;\n    const length = Buffer.from(blockOfLengthValues.subarray(1, 1 + 1 + nrLengthOctets)).readUIntBE(0, nrLengthOctets);\n    return {\n        length,\n        firstByteOffset: 1 + nrLengthOctets,\n        lastByteOffset: 1 + nrLengthOctets + length,\n    };\n}\n/**\n * Decode an ASN.1 sequence into its constituent parts, each part being an identifier-length-value triplet\n * See https://www.itu.int/ITU-T/studygroups/com17/languages/X.690-0207.pdf chapter 8.9\n *\n * @param sequenceValue - The ASN.1 sequence value\n * @returns Array of identifier-length-value triplets\n */\nfunction decodeSequence(sequence) {\n    const { tag } = decodeIdentifier(sequence[0]);\n    if (tag !== Asn1Tag.Sequence) {\n        throw new Asn1DecodingError(`Expected a sequence to decode, but got tag ${tag}`);\n    }\n    const { firstByteOffset, lastByteOffset } = decodeLengthValue(sequence.subarray(1));\n    const sequenceValue = sequence.subarray(1 + firstByteOffset, 1 + 1 + lastByteOffset);\n    const parts = [];\n    let offset = 0;\n    while (offset < sequenceValue.length) {\n        // Silence false postive: accessing an octet in a Buffer at a particular index\n        // is to be done with index operator: [index]\n        // eslint-disable-next-line security/detect-object-injection\n        const identifier = decodeIdentifier(sequenceValue[offset]);\n        const next = decodeLengthValue(sequenceValue.subarray(offset + 1));\n        const value = sequenceValue.subarray(offset + 1 + next.firstByteOffset, offset + 1 + next.lastByteOffset);\n        parts.push({ identifier, length: next.length, value });\n        offset += 1 + next.lastByteOffset;\n    }\n    return parts;\n}\n/**\n * Decode an ASN.1 sequence that is wrapped in a bit string\n * (Which is the way RSA public keys are encoded in ASN.1 DER-encoding)\n * See https://www.itu.int/ITU-T/studygroups/com17/languages/X.690-0207.pdf chapter 8.6 and 8.9\n *\n * @param bitStringValue - The ASN.1 bit string value\n * @returns Array of identifier-length-value triplets\n */\nfunction decodeBitStringWrappedSequenceValue(bitStringValue) {\n    const wrappedSequence = bitStringValue.subarray(1);\n    return decodeSequence(wrappedSequence);\n}\n/**\n * Decode an ASN.1 DER-encoded public key, into its modulus (n) and exponent (e)\n *\n * @param publicKey - The ASN.1 DER-encoded public key\n * @returns Object with modulus (n) and exponent (e)\n */\nexport function deconstructPublicKeyInDerFormat(publicKey) {\n    const [, pubkeyinfo] = decodeSequence(publicKey);\n    const [n, e] = decodeBitStringWrappedSequenceValue(pubkeyinfo.value);\n    return { n: n.value, e: e.value };\n}\n", "// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n// SPDX-License-Identifier: Apache-2.0\n//\n// NodeJS implementation for fetching JSON documents over HTTPS\nimport { request } from \"https\";\nimport { validateHttpsJsonResponse } from \"./https-common.js\";\nimport { pipeline } from \"stream\";\nimport { TextDecoder } from \"util\";\nimport { safeJsonParse } from \"./safe-json-parse.js\";\nimport { FetchError, NonRetryableFetchError } from \"./error.js\";\n/**\n * Execute a HTTPS request\n * @param uri - The URI\n * @param requestOptions - The RequestOptions to use\n * @param data - Data to send to the URI (e.g. POST data)\n * @returns - The response as parsed JSON\n */\nexport async function fetchJson(uri, requestOptions, data) {\n    let responseTimeout;\n    return new Promise((resolve, reject) => {\n        const req = request(uri, {\n            method: \"GET\",\n            ...requestOptions,\n        }, (response) => {\n            // Capture response data\n            // @types/node is incomplete so cast to any\n            // eslint-disable-next-line @typescript-eslint/no-explicit-any\n            pipeline([\n                response,\n                getJsonDestination(uri, response.statusCode, response.headers),\n            ], done);\n        });\n        if (requestOptions?.responseTimeout) {\n            responseTimeout = setTimeout(() => done(new FetchError(uri, `Response time-out (after ${requestOptions.responseTimeout} ms.)`)), requestOptions.responseTimeout);\n            responseTimeout.unref(); // Don't block Node from exiting\n        }\n        function done(...args) {\n            if (responseTimeout)\n                clearTimeout(responseTimeout);\n            if (args[0] == null) {\n                resolve(args[1]);\n                return;\n            }\n            // In case of errors, let the Agent (if any) know to abandon the socket\n            // This is probably best, because the socket may have become stale\n            /* istanbul ignore next */\n            req.socket?.emit(\"agentRemove\");\n            // Turn error into FetchError so the URI is nicely captured in the message\n            let error = args[0];\n            if (!(error instanceof FetchError)) {\n                error = new FetchError(uri, error.message);\n            }\n            req.destroy();\n            reject(error);\n        }\n        // Handle errors while sending request\n        req.on(\"error\", done);\n        // Signal end of request (include optional data)\n        req.end(data);\n    });\n}\n/**\n * Ensures the HTTPS response contains valid JSON\n *\n * @param uri - The URI you were requesting\n * @param statusCode - The response status code to your HTTPS request\n * @param headers - The response headers to your HTTPS request\n *\n * @returns - Async function that can be used as destination in a stream.pipeline, it will return the JSON, if valid, or throw an error otherwise\n */\nfunction getJsonDestination(uri, statusCode, headers) {\n    return async (responseIterable) => {\n        validateHttpsJsonResponse(uri, statusCode, headers[\"content-type\"]);\n        const collected = [];\n        for await (const chunk of responseIterable) {\n            collected.push(chunk);\n        }\n        try {\n            return safeJsonParse(new TextDecoder(\"utf8\", { fatal: true, ignoreBOM: true }).decode(Buffer.concat(collected)));\n        }\n        catch (err) {\n            throw new NonRetryableFetchError(uri, err);\n        }\n    };\n}\n", "// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n// SPDX-License-Identifier: Apache-2.0\n//\n// Lower-level HTTPS functionalities, common for Node.js and Web\nimport { FetchError, NonRetryableFetchError } from \"./error.js\";\n/**\n * Sanity check a HTTPS response where we expect to get JSON data back\n *\n * @param uri the uri that was being requested\n * @param statusCode the HTTP status code, should be 200\n * @param contentType the value of the \"Content-Type\" header in the response, should start with \"application/json\"\n * @returns void - throws an error if the status code or content type aren't as expected\n */\nexport function validateHttpsJsonResponse(uri, statusCode, contentType) {\n    if (statusCode === 429) {\n        throw new FetchError(uri, \"Too many requests\");\n    }\n    else if (statusCode !== 200) {\n        throw new NonRetryableFetchError(uri, `Status code is ${statusCode}, expected 200`);\n    }\n    if (!contentType ||\n        !contentType.toLowerCase().startsWith(\"application/json\")) {\n        throw new NonRetryableFetchError(uri, `Content-type is \"${contentType}\", expected \"application/json\"`);\n    }\n}\n", "// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n// SPDX-License-Identifier: Apache-2.0\n//\n// Utility to parse JSON safely\n/**\n * Check if a piece of JSON is a JSON object, and not e.g. a mere string or null\n *\n * @param j - the JSON\n */\nexport function isJsonObject(j) {\n    // It is not enough to check that `typeof j === \"object\"`\n    // because in JS `typeof null` is also \"object\", and so is `typeof []`.\n    // So we need to check that j is an object, and not null, and not an array\n    return typeof j === \"object\" && !Array.isArray(j) && j !== null;\n}\n/**\n * Parse a string as JSON, while removing __proto__ and constructor, so JS prototype pollution is prevented\n *\n * @param s - the string to JSON parse\n */\nexport function safeJsonParse(s) {\n    return JSON.parse(s, (_, value) => {\n        if (typeof value === \"object\" && !Array.isArray(value) && value !== null) {\n            delete value.__proto__;\n            delete value.constructor;\n        }\n        return value;\n    });\n}\n", "// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n// SPDX-License-Identifier: Apache-2.0\n//\n// Utilities for fetching the JWKS URI, to get the public keys with which to verify JWTs\nimport { NonRetryableFetchError } from \"./error.js\";\nimport { nodeWebCompat } from \"#node-web-compat\";\n/**\n * Execute a HTTPS request\n * @param uri - The URI\n * @param requestOptions - The RequestOptions to use (depending on the runtime context, either Node.js RequestOptions or Web Fetch init)\n * @param data - Data to send to the URI (e.g. POST data)\n * @returns - The response as parsed JSON\n */\nexport const fetchJson = nodeWebCompat.fetchJson;\n/**\n * HTTPS Fetcher for URIs with JSON body\n *\n * @param defaultRequestOptions - The default RequestOptions to use on individual HTTPS requests\n */\nexport class SimpleJsonFetcher {\n    constructor(props) {\n        this.defaultRequestOptions = {\n            timeout: nodeWebCompat.defaultFetchTimeouts.socketIdle,\n            responseTimeout: nodeWebCompat.defaultFetchTimeouts.response,\n            ...props?.defaultRequestOptions,\n        };\n    }\n    /**\n     * Execute a HTTPS request (with 1 immediate retry in case of errors)\n     * @param uri - The URI\n     * @param requestOptions - The RequestOptions to use\n     * @param data - Data to send to the URI (e.g. POST data)\n     * @returns - The response as parsed JSON\n     */\n    async fetch(uri, requestOptions, data) {\n        requestOptions = { ...this.defaultRequestOptions, ...requestOptions };\n        try {\n            return await fetchJson(uri, requestOptions, data);\n        }\n        catch (err) {\n            if (err instanceof NonRetryableFetchError) {\n                throw err;\n            }\n            // Retry once, immediately\n            return fetchJson(uri, requestOptions, data);\n        }\n    }\n}\n", "// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n// SPDX-License-Identifier: Apache-2.0\n//\n// Utilities to assert that supplied values match with expected values\nimport { FailedAssertionError } from \"./error.js\";\n/**\n * Assert value is a non-empty string and equal to the expected value,\n * or throw an error otherwise\n *\n * @param name - Name for the value being checked\n * @param actual - The value to check\n * @param expected - The expected value\n * @param errorConstructor - Constructor for the concrete error to be thrown\n */\nexport function assertStringEquals(name, actual, expected, errorConstructor = FailedAssertionError) {\n    if (!actual) {\n        throw new errorConstructor(`Missing ${name}. Expected: ${expected}`, actual, expected);\n    }\n    if (typeof actual !== \"string\") {\n        throw new errorConstructor(`${name} is not of type string`, actual, expected);\n    }\n    if (expected !== actual) {\n        throw new errorConstructor(`${name} not allowed: ${actual}. Expected: ${expected}`, actual, expected);\n    }\n}\n/**\n * Assert value is a non-empty string and is indeed one of the expected values,\n * or throw an error otherwise\n *\n * @param name - Name for the value being checked\n * @param actual - The value to check\n * @param expected - The array of expected values. For your convenience you can provide\n * @param errorConstructor - Constructor for the concrete error to be thrown\n * a string here as well, which will mean an array with just that string\n */\nexport function assertStringArrayContainsString(name, actual, expected, errorConstructor = FailedAssertionError) {\n    if (!actual) {\n        throw new errorConstructor(`Missing ${name}. ${expectationMessage(expected)}`, actual, expected);\n    }\n    if (typeof actual !== \"string\") {\n        throw new errorConstructor(`${name} is not of type string`, actual, expected);\n    }\n    return assertStringArraysOverlap(name, actual, expected, errorConstructor);\n}\n/**\n * Assert value is an array of strings, where at least one of the strings is indeed one of the expected values,\n * or throw an error otherwise\n *\n * @param name - Name for the value being checked\n * @param actual - The value to check, must be an array of strings, or a single string (which will be treated\n * as an array with just that string)\n * @param expected - The array of expected values. For your convenience you can provide\n * a string here as well, which will mean an array with just that string\n * @param errorConstructor - Constructor for the concrete error to be thrown\n */\nexport function assertStringArraysOverlap(name, actual, expected, errorConstructor = FailedAssertionError) {\n    if (!actual) {\n        throw new errorConstructor(`Missing ${name}. ${expectationMessage(expected)}`, actual, expected);\n    }\n    const expectedAsSet = new Set(Array.isArray(expected) ? expected : [expected]);\n    if (typeof actual === \"string\") {\n        actual = [actual];\n    }\n    if (!Array.isArray(actual)) {\n        throw new errorConstructor(`${name} is not an array`, actual, expected);\n    }\n    const overlaps = actual.some((actualItem) => {\n        if (typeof actualItem !== \"string\") {\n            throw new errorConstructor(`${name} includes elements that are not of type string`, actual, expected);\n        }\n        return expectedAsSet.has(actualItem);\n    });\n    if (!overlaps) {\n        throw new errorConstructor(`${name} not allowed: ${actual.join(\", \")}. ${expectationMessage(expected)}`, actual, expected);\n    }\n}\n/**\n * Get a nicely readable message regarding an expectation\n *\n * @param expected - The expected value.\n */\nfunction expectationMessage(expected) {\n    if (Array.isArray(expected)) {\n        if (expected.length > 1) {\n            return `Expected one of: ${expected.join(\", \")}`;\n        }\n        return `Expected: ${expected[0]}`;\n    }\n    return `Expected: ${expected}`;\n}\n/**\n * Assert value is not a promise, or throw an error otherwise\n *\n * @param actual - The value to check\n * @param errorFactory - Function that returns the error to be thrown\n */\nexport function assertIsNotPromise(actual, errorFactory) {\n    if (actual && typeof actual.then === \"function\") {\n        throw errorFactory();\n    }\n}\n", "// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n// SPDX-License-Identifier: Apache-2.0\nimport { SimpleJsonFetcher, fetchJson } from \"./https.js\";\nimport { isJsonObject } from \"./safe-json-parse.js\";\nimport { JwkValidationError, JwksNotAvailableInCacheError, JwksValidationError, KidNotFoundInJwksError, WaitPeriodNotYetEndedJwkError, JwtWithoutValidKidError, JwkInvalidUseError, JwkInvalidKtyError, } from \"./error.js\";\nimport { nodeWebCompat } from \"#node-web-compat\";\nimport { assertStringEquals } from \"./assert.js\";\nconst optionalJwkFieldNames = [\n    \"use\",\n    \"alg\",\n    \"kid\",\n    \"n\",\n    \"e\", // https://datatracker.ietf.org/doc/html/rfc7518#section-6.3.1.2\n];\nconst mandatoryJwkFieldNames = [\n    \"kty\", // https://datatracker.ietf.org/doc/html/rfc7517#section-4.1\n];\nexport function findJwkInJwks(jwks, kid) {\n    return jwks.keys.find((jwk) => jwk.kid != null && jwk.kid === kid);\n}\nexport async function fetchJwks(jwksUri) {\n    const jwks = await fetchJson(jwksUri);\n    assertIsJwks(jwks);\n    return jwks;\n}\nexport async function fetchJwk(jwksUri, decomposedJwt) {\n    if (!decomposedJwt.header.kid) {\n        throw new JwtWithoutValidKidError(\"JWT header does not have valid kid claim\");\n    }\n    const jwks = await fetchJwks(jwksUri);\n    const jwk = findJwkInJwks(jwks, decomposedJwt.header.kid);\n    if (!jwk) {\n        throw new KidNotFoundInJwksError(`JWK for kid \"${decomposedJwt.header.kid}\" not found in the JWKS`);\n    }\n    return jwk;\n}\nexport function assertIsJwks(jwks) {\n    if (!jwks) {\n        throw new JwksValidationError(\"JWKS empty\");\n    }\n    if (!isJsonObject(jwks)) {\n        throw new JwksValidationError(\"JWKS should be an object\");\n    }\n    if (!Object.keys(jwks).includes(\"keys\")) {\n        throw new JwksValidationError(\"JWKS does not include keys\");\n    }\n    if (!Array.isArray(jwks.keys)) {\n        throw new JwksValidationError(\"JWKS keys should be an array\");\n    }\n    for (const jwk of jwks.keys) {\n        assertIsJwk(jwk);\n    }\n}\nexport function assertIsRsaSignatureJwk(jwk) {\n    // Check JWK use\n    assertStringEquals(\"JWK use\", jwk.use, \"sig\", JwkInvalidUseError);\n    // Check JWK kty\n    assertStringEquals(\"JWK kty\", jwk.kty, \"RSA\", JwkInvalidKtyError);\n    // Check modulus (n) has a value\n    if (!jwk.n)\n        throw new JwkValidationError(\"Missing modulus (n)\");\n    // Check exponent (e) has a value\n    if (!jwk.e)\n        throw new JwkValidationError(\"Missing exponent (e)\");\n}\nexport function assertIsJwk(jwk) {\n    if (!jwk) {\n        throw new JwkValidationError(\"JWK empty\");\n    }\n    if (!isJsonObject(jwk)) {\n        throw new JwkValidationError(\"JWK should be an object\");\n    }\n    for (const field of mandatoryJwkFieldNames) {\n        // disable eslint rule because `field` is trusted\n        // eslint-disable-next-line security/detect-object-injection\n        if (typeof jwk[field] !== \"string\") {\n            throw new JwkValidationError(`JWK ${field} should be a string`);\n        }\n    }\n    for (const field of optionalJwkFieldNames) {\n        // disable eslint rule because `field` is trusted\n        // eslint-disable-next-line security/detect-object-injection\n        if (field in jwk && typeof jwk[field] !== \"string\") {\n            throw new JwkValidationError(`JWK ${field} should be a string`);\n        }\n    }\n}\nexport function isJwks(jwks) {\n    try {\n        assertIsJwks(jwks);\n        return true;\n    }\n    catch {\n        return false;\n    }\n}\nexport function isJwk(jwk) {\n    try {\n        assertIsJwk(jwk);\n        return true;\n    }\n    catch {\n        return false;\n    }\n}\nexport class SimplePenaltyBox {\n    constructor(props) {\n        this.waitingUris = new Map();\n        this.waitSeconds = props?.waitSeconds ?? 10;\n    }\n    async wait(jwksUri) {\n        // SimplePenaltyBox does not actually wait but bluntly throws an error\n        // Any waiting and retries are expected to be done upstream (e.g. in the browser / app)\n        if (this.waitingUris.has(jwksUri)) {\n            throw new WaitPeriodNotYetEndedJwkError(\"Not allowed to fetch JWKS yet, still waiting for back off period to end\");\n        }\n    }\n    release(jwksUri) {\n        const i = this.waitingUris.get(jwksUri);\n        if (i) {\n            clearTimeout(i);\n            this.waitingUris.delete(jwksUri);\n        }\n    }\n    registerFailedAttempt(jwksUri) {\n        const i = nodeWebCompat.setTimeoutUnref(() => {\n            this.waitingUris.delete(jwksUri);\n        }, this.waitSeconds * 1000);\n        this.waitingUris.set(jwksUri, i);\n    }\n    registerSuccessfulAttempt(jwksUri) {\n        this.release(jwksUri);\n    }\n}\nexport class SimpleJwksCache {\n    constructor(props) {\n        this.jwksCache = new Map();\n        this.fetchingJwks = new Map();\n        this.penaltyBox = props?.penaltyBox ?? new SimplePenaltyBox();\n        this.fetcher = props?.fetcher ?? new SimpleJsonFetcher();\n    }\n    addJwks(jwksUri, jwks) {\n        this.jwksCache.set(jwksUri, jwks);\n    }\n    async getJwks(jwksUri) {\n        const existingFetch = this.fetchingJwks.get(jwksUri);\n        if (existingFetch) {\n            return existingFetch;\n        }\n        const jwksPromise = this.fetcher.fetch(jwksUri).then((res) => {\n            assertIsJwks(res);\n            return res;\n        });\n        this.fetchingJwks.set(jwksUri, jwksPromise);\n        let jwks;\n        try {\n            jwks = await jwksPromise;\n        }\n        finally {\n            this.fetchingJwks.delete(jwksUri);\n        }\n        this.jwksCache.set(jwksUri, jwks);\n        return jwks;\n    }\n    getCachedJwk(jwksUri, decomposedJwt) {\n        if (typeof decomposedJwt.header.kid !== \"string\") {\n            throw new JwtWithoutValidKidError(\"JWT header does not have valid kid claim\");\n        }\n        if (!this.jwksCache.has(jwksUri)) {\n            throw new JwksNotAvailableInCacheError(`JWKS for uri ${jwksUri} not yet available in cache`);\n        }\n        const jwk = findJwkInJwks(this.jwksCache.get(jwksUri), decomposedJwt.header.kid);\n        if (!jwk) {\n            throw new KidNotFoundInJwksError(`JWK for kid ${decomposedJwt.header.kid} not found in the JWKS`);\n        }\n        return jwk;\n    }\n    async getJwk(jwksUri, decomposedJwt) {\n        if (typeof decomposedJwt.header.kid !== \"string\") {\n            throw new JwtWithoutValidKidError(\"JWT header does not have valid kid claim\");\n        }\n        // Try to get JWK from cache:\n        const cachedJwks = this.jwksCache.get(jwksUri);\n        if (cachedJwks) {\n            const cachedJwk = findJwkInJwks(cachedJwks, decomposedJwt.header.kid);\n            if (cachedJwk) {\n                return cachedJwk;\n            }\n        }\n        // Await any wait period that is currently in effect\n        // This prevents us from flooding the JWKS URI with requests\n        await this.penaltyBox.wait(jwksUri, decomposedJwt.header.kid);\n        // Fetch the JWKS and (try to) locate the JWK\n        const jwks = await this.getJwks(jwksUri);\n        const jwk = findJwkInJwks(jwks, decomposedJwt.header.kid);\n        // If the JWK could not be located, someone might be messing around with us\n        // Register the failed attempt with the penaltyBox, so it can enforce a wait period\n        // before trying again next time (instead of flooding the JWKS URI with requests)\n        if (!jwk) {\n            this.penaltyBox.registerFailedAttempt(jwksUri, decomposedJwt.header.kid);\n            throw new KidNotFoundInJwksError(`JWK for kid \"${decomposedJwt.header.kid}\" not found in the JWKS`);\n        }\n        else {\n            this.penaltyBox.registerSuccessfulAttempt(jwksUri, decomposedJwt.header.kid);\n        }\n        return jwk;\n    }\n}\n", "// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n// SPDX-License-Identifier: Apache-2.0\nexport const supportedSignatureAlgorithms = [\n    \"RS256\",\n    \"RS384\",\n    \"RS512\",\n];\n", "// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n// SPDX-License-Identifier: Apache-2.0\nimport { assertStringArrayContainsString, assertStringArraysOverlap, } from \"./assert.js\";\nimport { safeJsonParse, isJsonObject } from \"./safe-json-parse.js\";\nimport { JwtExpiredError, JwtNotBeforeError, JwtInvalidIssuerError, JwtInvalidAudienceError, JwtInvalidScopeError, JwtParseError, ParameterValidationError, } from \"./error.js\";\nimport { nodeWebCompat } from \"#node-web-compat\";\n/**\n * Assert that the argument is a valid JWT header object.\n * Throws an error in case it is not.\n *\n * @param header\n * @returns void\n */\nfunction assertJwtHeader(header) {\n    if (!isJsonObject(header)) {\n        throw new JwtParseError(\"JWT header is not an object\");\n    }\n    if (header.alg !== undefined && typeof header.alg !== \"string\") {\n        throw new JwtParseError(\"JWT header alg claim is not a string\");\n    }\n    if (header.kid !== undefined && typeof header.kid !== \"string\") {\n        throw new JwtParseError(\"JWT header kid claim is not a string\");\n    }\n}\n/**\n * Assert that the argument is a valid JWT payload object.\n * Throws an error in case it is not.\n *\n * @param payload\n * @returns void\n */\nfunction assertJwtPayload(payload) {\n    if (!isJsonObject(payload)) {\n        throw new JwtParseError(\"JWT payload is not an object\");\n    }\n    if (payload.exp !== undefined && !Number.isFinite(payload.exp)) {\n        throw new JwtParseError(\"JWT payload exp claim is not a number\");\n    }\n    if (payload.iss !== undefined && typeof payload.iss !== \"string\") {\n        throw new JwtParseError(\"JWT payload iss claim is not a string\");\n    }\n    if (payload.aud !== undefined &&\n        typeof payload.aud !== \"string\" &&\n        (!Array.isArray(payload.aud) ||\n            payload.aud.some((aud) => typeof aud !== \"string\"))) {\n        throw new JwtParseError(\"JWT payload aud claim is not a string or array of strings\");\n    }\n    if (payload.nbf !== undefined && !Number.isFinite(payload.nbf)) {\n        throw new JwtParseError(\"JWT payload nbf claim is not a number\");\n    }\n    if (payload.iat !== undefined && !Number.isFinite(payload.iat)) {\n        throw new JwtParseError(\"JWT payload iat claim is not a number\");\n    }\n    if (payload.scope !== undefined && typeof payload.scope !== \"string\") {\n        throw new JwtParseError(\"JWT payload scope claim is not a string\");\n    }\n    if (payload.jti !== undefined && typeof payload.jti !== \"string\") {\n        throw new JwtParseError(\"JWT payload jti claim is not a string\");\n    }\n}\n/**\n * Sanity check, decompose and JSON parse a JWT string into its constituent parts:\n * - header object\n * - payload object\n * - signature string\n *\n * @param jwt The JWT (as string)\n * @returns the decomposed JWT\n */\nexport function decomposeJwt(jwt) {\n    // Sanity checks on JWT\n    if (!jwt) {\n        throw new JwtParseError(\"Empty JWT\");\n    }\n    if (typeof jwt !== \"string\") {\n        throw new JwtParseError(\"JWT is not a string\");\n    }\n    if (!jwt.match(/^[A-Za-z0-9_-]+\\.[A-Za-z0-9_-]+\\.[A-Za-z0-9_-]+$/)) {\n        throw new JwtParseError(\"JWT string does not consist of exactly 3 parts (header, payload, signature)\");\n    }\n    const [headerB64, payloadB64, signatureB64] = jwt.split(\".\");\n    // B64 decode header and payload\n    const [headerString, payloadString] = [headerB64, payloadB64].map(nodeWebCompat.parseB64UrlString);\n    // Parse header\n    let header;\n    try {\n        header = safeJsonParse(headerString);\n    }\n    catch (err) {\n        throw new JwtParseError(\"Invalid JWT. Header is not a valid JSON object\", err);\n    }\n    assertJwtHeader(header);\n    // parse payload\n    let payload;\n    try {\n        payload = safeJsonParse(payloadString);\n    }\n    catch (err) {\n        throw new JwtParseError(\"Invalid JWT. Payload is not a valid JSON object\", err);\n    }\n    assertJwtPayload(payload);\n    return {\n        header,\n        headerB64,\n        payload,\n        payloadB64,\n        signatureB64,\n    };\n}\n/**\n * Validate JWT payload fields. Throws an error in case there's any validation issue.\n *\n * @param payload The (JSON parsed) JWT payload\n * @param options The options to use during validation\n * @returns void\n */\nexport function validateJwtFields(payload, options) {\n    // Check expiry\n    if (payload.exp !== undefined) {\n        if (payload.exp + (options.graceSeconds ?? 0) < Date.now() / 1000) {\n            throw new JwtExpiredError(`Token expired at ${new Date(payload.exp * 1000).toISOString()}`, payload.exp);\n        }\n    }\n    // Check not before\n    if (payload.nbf !== undefined) {\n        if (payload.nbf - (options.graceSeconds ?? 0) > Date.now() / 1000) {\n            throw new JwtNotBeforeError(`Token can't be used before ${new Date(payload.nbf * 1000).toISOString()}`, payload.nbf);\n        }\n    }\n    // Check JWT issuer\n    if (options.issuer !== null) {\n        if (options.issuer === undefined) {\n            throw new ParameterValidationError(\"issuer must be provided or set to null explicitly\");\n        }\n        assertStringArrayContainsString(\"Issuer\", payload.iss, options.issuer, JwtInvalidIssuerError);\n    }\n    // Check audience\n    if (options.audience !== null) {\n        if (options.audience === undefined) {\n            throw new ParameterValidationError(\"audience must be provided or set to null explicitly\");\n        }\n        assertStringArraysOverlap(\"Audience\", payload.aud, options.audience, JwtInvalidAudienceError);\n    }\n    // Check scope\n    if (options.scope != null) {\n        assertStringArraysOverlap(\"Scope\", payload.scope?.split(\" \"), options.scope, JwtInvalidScopeError);\n    }\n}\n", "// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n// SPDX-License-Identifier: Apache-2.0\nimport { SimpleJwksCache, isJwk, isJwks, fetchJwk, assertIsRsaSignatureJwk, findJwkInJwks, } from \"./jwk.js\";\nimport { assertIsNotPromise, assertStringArrayContainsString, assertStringEquals, } from \"./assert.js\";\nimport { supportedSignatureAlgorithms, } from \"./jwt-model.js\";\nimport { decomposeJwt, validateJwtFields } from \"./jwt.js\";\nimport { JwtInvalidClaimError, JwtInvalidIssuerError, JwtInvalidSignatureAlgorithmError, JwtInvalidSignatureError, KidNotFoundInJwksError, ParameterValidationError, } from \"./error.js\";\nimport { nodeWebCompat } from \"#node-web-compat\";\n/**\n * Sanity check the JWT header and the selected JWK\n *\n * @param header: the JWT header (decoded and JSON parsed)\n * @param jwk: the JWK\n */\nfunction validateJwtHeaderAndJwk(header, jwk) {\n    // Check that the JWK is in fact a JWK for RSA signatures\n    assertIsRsaSignatureJwk(jwk);\n    // Check that JWT signature algorithm matches JWK\n    if (jwk.alg) {\n        assertStringEquals(\"JWT signature algorithm\", header.alg, jwk.alg, JwtInvalidSignatureAlgorithmError);\n    }\n    // Check JWT signature algorithm is one of the supported signature algorithms\n    assertStringArrayContainsString(\"JWT signature algorithm\", header.alg, supportedSignatureAlgorithms, JwtInvalidSignatureAlgorithmError);\n}\n/**\n * Verify a JWT asynchronously (thus allowing for the JWKS to be fetched from the JWKS URI)\n *\n * @param jwt The JWT\n * @param jwksUri The JWKS URI, where the JWKS can be fetched from\n * @param options Verification options\n * @returns Promise that resolves to the payload of the JWT\u2013\u2013if the JWT is valid, otherwise the promise rejects\n */\nexport async function verifyJwt(jwt, jwksUri, options) {\n    return verifyDecomposedJwt(decomposeJwt(jwt), jwksUri, options);\n}\n/**\n * Verify (asynchronously) a JWT that is already decomposed (by function `decomposeJwt`)\n *\n * @param decomposedJwt The decomposed JWT\n * @param jwksUri The JWKS URI, where the JWKS can be fetched from\n * @param options Verification options\n * @param jwkFetcher A function that can execute the fetch of the JWKS from the JWKS URI\n * @param transformJwkToKeyObjectFn A function that can transform a JWK into a crypto native key object\n * @returns Promise that resolves to the payload of the JWT\u2013\u2013if the JWT is valid, otherwise the promise rejects\n */\nasync function verifyDecomposedJwt(decomposedJwt, jwksUri, options, jwkFetcher = fetchJwk, transformJwkToKeyObjectFn = nodeWebCompat.transformJwkToKeyObjectAsync) {\n    const { header, headerB64, payload, payloadB64, signatureB64 } = decomposedJwt;\n    const jwk = await jwkFetcher(jwksUri, decomposedJwt);\n    validateJwtHeaderAndJwk(decomposedJwt.header, jwk);\n    // Transform the JWK to native key format, that can be used with verifySignature\n    const keyObject = await transformJwkToKeyObjectFn(jwk, header.alg, payload.iss);\n    // Verify the JWT signature\n    const valid = await nodeWebCompat.verifySignatureAsync({\n        jwsSigningInput: `${headerB64}.${payloadB64}`,\n        signature: signatureB64,\n        alg: header.alg,\n        keyObject,\n    });\n    if (!valid) {\n        throw new JwtInvalidSignatureError(\"Invalid signature\");\n    }\n    try {\n        validateJwtFields(payload, options);\n        if (options.customJwtCheck) {\n            await options.customJwtCheck({ header, payload, jwk });\n        }\n    }\n    catch (err) {\n        if (options.includeRawJwtInErrors && err instanceof JwtInvalidClaimError) {\n            throw err.withRawJwt(decomposedJwt);\n        }\n        throw err;\n    }\n    return payload;\n}\n/**\n * Verify a JWT synchronously, using a JWKS or JWK that has already been fetched\n *\n * @param jwt The JWT\n * @param jwkOrJwks The JWKS that includes the right JWK (indexed by kid). Alternatively, provide the right JWK directly\n * @param options Verification options\n * @param transformJwkToKeyObjectFn A function that can transform a JWK into a crypto native key object\n * @returns The (JSON parsed) payload of the JWT\u2013\u2013if the JWT is valid, otherwise an error is thrown\n */\nexport function verifyJwtSync(jwt, jwkOrJwks, options, transformJwkToKeyObjectFn = nodeWebCompat.transformJwkToKeyObjectSync) {\n    return verifyDecomposedJwtSync(decomposeJwt(jwt), jwkOrJwks, options, transformJwkToKeyObjectFn);\n}\n/**\n * Verify (synchronously) a JWT that is already decomposed (by function `decomposeJwt`)\n *\n * @param decomposedJwt The decomposed JWT\n * @param jwkOrJwks The JWKS that includes the right JWK (indexed by kid). Alternatively, provide the right JWK directly\n * @param options Verification options\n * @param transformJwkToKeyObjectFn A function that can transform a JWK into a crypto native key object\n * @returns The (JSON parsed) payload of the JWT\u2013\u2013if the JWT is valid, otherwise an error is thrown\n */\nfunction verifyDecomposedJwtSync(decomposedJwt, jwkOrJwks, options, transformJwkToKeyObjectFn) {\n    const { header, headerB64, payload, payloadB64, signatureB64 } = decomposedJwt;\n    let jwk;\n    if (isJwk(jwkOrJwks)) {\n        jwk = jwkOrJwks;\n    }\n    else if (isJwks(jwkOrJwks)) {\n        const locatedJwk = header.kid\n            ? findJwkInJwks(jwkOrJwks, header.kid)\n            : undefined;\n        if (!locatedJwk) {\n            throw new KidNotFoundInJwksError(`JWK for kid ${header.kid} not found in the JWKS`);\n        }\n        jwk = locatedJwk;\n    }\n    else {\n        throw new ParameterValidationError([\n            `Expected a valid JWK or JWKS (parsed as JavaScript object), but received: ${jwkOrJwks}.`,\n            \"If you're passing a JWKS URI, use the async verify() method instead, it will download and parse the JWKS for you\",\n        ].join());\n    }\n    validateJwtHeaderAndJwk(decomposedJwt.header, jwk);\n    // Transform the JWK to native key format, that can be used with verifySignature\n    const keyObject = transformJwkToKeyObjectFn(jwk, header.alg, payload.iss);\n    // Verify the JWT signature (JWS)\n    const valid = nodeWebCompat.verifySignatureSync({\n        jwsSigningInput: `${headerB64}.${payloadB64}`,\n        signature: signatureB64,\n        alg: header.alg,\n        keyObject,\n    });\n    if (!valid) {\n        throw new JwtInvalidSignatureError(\"Invalid signature\");\n    }\n    try {\n        validateJwtFields(payload, options);\n        if (options.customJwtCheck) {\n            const res = options.customJwtCheck({ header, payload, jwk });\n            assertIsNotPromise(res, () => new ParameterValidationError(\"Custom JWT checks must be synchronous but a promise was returned\"));\n        }\n    }\n    catch (err) {\n        if (options.includeRawJwtInErrors && err instanceof JwtInvalidClaimError) {\n            throw err.withRawJwt(decomposedJwt);\n        }\n        throw err;\n    }\n    return payload;\n}\n/**\n * Abstract class representing a verifier for JWTs signed with RSA (e.g. RS256, RS384, RS512)\n *\n * A class is used, because there is state:\n * - The JWKS is fetched (downloaded) from the JWKS URI and cached in memory\n * - Verification properties at verifier level, are used as default options for individual verify calls\n *\n * When instantiating this class, relevant type parameters should be provided, for your concrete case:\n * @param StillToProvide The verification options that you want callers of verify to provide on individual verify calls\n * @param SpecificVerifyProperties The verification options that you'll use\n * @param IssuerConfig The issuer config that you'll use (config options are used as default verification options)\n * @param MultiIssuer Verify multiple issuers (true) or just a single one (false)\n */\nexport class JwtRsaVerifierBase {\n    constructor(verifyProperties, jwksCache = new SimpleJwksCache()) {\n        this.jwksCache = jwksCache;\n        this.issuersConfig = new Map();\n        this.publicKeyCache = new KeyObjectCache();\n        if (Array.isArray(verifyProperties)) {\n            if (!verifyProperties.length) {\n                throw new ParameterValidationError(\"Provide at least one issuer configuration\");\n            }\n            for (const prop of verifyProperties) {\n                if (this.issuersConfig.has(prop.issuer)) {\n                    throw new ParameterValidationError(`issuer ${prop.issuer} supplied multiple times`);\n                }\n                this.issuersConfig.set(prop.issuer, this.withJwksUri(prop));\n            }\n        }\n        else {\n            this.issuersConfig.set(verifyProperties.issuer, this.withJwksUri(verifyProperties));\n        }\n    }\n    get expectedIssuers() {\n        return Array.from(this.issuersConfig.keys());\n    }\n    getIssuerConfig(issuer) {\n        if (!issuer) {\n            if (this.issuersConfig.size !== 1) {\n                throw new ParameterValidationError(\"issuer must be provided\");\n            }\n            issuer = this.issuersConfig.keys().next().value;\n        }\n        const config = this.issuersConfig.get(issuer);\n        if (!config) {\n            throw new ParameterValidationError(`issuer not configured: ${issuer}`);\n        }\n        return config;\n    }\n    /**\n     * This method loads a JWKS that you provide, into the JWKS cache, so that it is\n     * available for JWT verification. Use this method to speed up the first JWT verification\n     * (when the JWKS would otherwise have to be downloaded from the JWKS uri), or to provide the JWKS\n     * in case the JwtVerifier does not have internet access to download the JWKS\n     *\n     * @param jwksThe JWKS\n     * @param issuer The issuer for which you want to cache the JWKS\n     *  Supply this field, if you instantiated the JwtVerifier with multiple issuers\n     * @returns void\n     */\n    cacheJwks(...[jwks, issuer]) {\n        const issuerConfig = this.getIssuerConfig(issuer);\n        this.jwksCache.addJwks(issuerConfig.jwksUri, jwks);\n        this.publicKeyCache.clearCache(issuerConfig.issuer);\n    }\n    /**\n     * Hydrate the JWKS cache for (all of) the configured issuer(s).\n     * This will fetch and cache the latest and greatest JWKS for concerned issuer(s).\n     *\n     * @param issuer The issuer to fetch the JWKS for\n     * @returns void\n     */\n    async hydrate() {\n        const jwksFetches = this.expectedIssuers\n            .map((issuer) => this.getIssuerConfig(issuer).jwksUri)\n            .map((jwksUri) => this.jwksCache.getJwks(jwksUri));\n        await Promise.all(jwksFetches);\n    }\n    /**\n     * Verify (synchronously) a JWT that is signed using RS256 / RS384 / RS512.\n     *\n     * @param jwt The JWT, as string\n     * @param props Verification properties\n     * @returns The payload of the JWT\u2013\u2013if the JWT is valid, otherwise an error is thrown\n     */\n    verifySync(...[jwt, properties]) {\n        const { decomposedJwt, jwksUri, verifyProperties } = this.getVerifyParameters(jwt, properties);\n        return this.verifyDecomposedJwtSync(decomposedJwt, jwksUri, verifyProperties);\n    }\n    /**\n     * Verify (synchronously) an already decomposed JWT, that is signed using RS256 / RS384 / RS512.\n     *\n     * @param decomposedJwt The decomposed Jwt\n     * @param jwk The JWK to verify the JWTs signature with\n     * @param verifyProperties The properties to use for verification\n     * @returns The payload of the JWT\u2013\u2013if the JWT is valid, otherwise an error is thrown\n     */\n    verifyDecomposedJwtSync(decomposedJwt, jwksUri, verifyProperties) {\n        const jwk = this.jwksCache.getCachedJwk(jwksUri, decomposedJwt);\n        return verifyDecomposedJwtSync(decomposedJwt, jwk, verifyProperties, this.publicKeyCache.transformJwkToKeyObjectSync.bind(this.publicKeyCache));\n    }\n    /**\n     * Verify (asynchronously) a JWT that is signed using RS256 / RS384 / RS512.\n     * This call is asynchronous, and the JWKS will be fetched from the JWKS uri,\n     * in case it is not yet available in the cache.\n     *\n     * @param jwt The JWT, as string\n     * @param props Verification properties\n     * @returns Promise that resolves to the payload of the JWT\u2013\u2013if the JWT is valid, otherwise the promise rejects\n     */\n    async verify(...[jwt, properties]) {\n        const { decomposedJwt, jwksUri, verifyProperties } = this.getVerifyParameters(jwt, properties);\n        return this.verifyDecomposedJwt(decomposedJwt, jwksUri, verifyProperties);\n    }\n    /**\n     * Verify (asynchronously) an already decomposed JWT, that is signed using RS256 / RS384 / RS512.\n     *\n     * @param decomposedJwt The decomposed Jwt\n     * @param jwk The JWK to verify the JWTs signature with\n     * @param verifyProperties The properties to use for verification\n     * @returns The payload of the JWT\u2013\u2013if the JWT is valid, otherwise an error is thrown\n     */\n    verifyDecomposedJwt(decomposedJwt, jwksUri, verifyProperties) {\n        return verifyDecomposedJwt(decomposedJwt, jwksUri, verifyProperties, this.jwksCache.getJwk.bind(this.jwksCache), this.publicKeyCache.transformJwkToKeyObjectAsync.bind(this.publicKeyCache));\n    }\n    /**\n     * Get the verification parameters to use, by merging the issuer configuration,\n     * with the overriding properties that are now provided\n     *\n     * @param jwt: the JWT that is going to be verified\n     * @param verifyProperties: the overriding properties, that override the issuer configuration\n     * @returns The merged verification parameters\n     */\n    getVerifyParameters(jwt, verifyProperties) {\n        const decomposedJwt = decomposeJwt(jwt);\n        assertStringArrayContainsString(\"Issuer\", decomposedJwt.payload.iss, this.expectedIssuers, JwtInvalidIssuerError);\n        const issuerConfig = this.getIssuerConfig(decomposedJwt.payload.iss);\n        return {\n            decomposedJwt,\n            jwksUri: issuerConfig.jwksUri,\n            verifyProperties: {\n                ...issuerConfig,\n                ...verifyProperties,\n            },\n        };\n    }\n    /**\n     * Get issuer config with JWKS URI, by adding a default JWKS URI if needed\n     *\n     * @param config: the issuer config.\n     * @returns The config with JWKS URI\n     */\n    withJwksUri(config) {\n        if (config.jwksUri) {\n            return config;\n        }\n        const issuerUri = new URL(config.issuer).pathname.replace(/\\/$/, \"\");\n        return {\n            jwksUri: new URL(`${issuerUri}/.well-known/jwks.json`, config.issuer)\n                .href,\n            ...config,\n        };\n    }\n}\n/**\n * Class representing a verifier for JWTs signed with RSA (e.g. RS256 / RS384 / RS512)\n */\nexport class JwtRsaVerifier extends JwtRsaVerifierBase {\n    // eslint-disable-next-line @typescript-eslint/explicit-module-boundary-types\n    static create(verifyProperties, additionalProperties) {\n        return new this(verifyProperties, additionalProperties?.jwksCache);\n    }\n}\n/**\n * Class representing a cache of RSA public keys in native key object format\n *\n * Because it takes a bit of compute time to turn a JWK into native key object format,\n * we want to cache this computation.\n */\nexport class KeyObjectCache {\n    constructor(transformJwkToKeyObjectSyncFn = nodeWebCompat.transformJwkToKeyObjectSync, transformJwkToKeyObjectAsyncFn = nodeWebCompat.transformJwkToKeyObjectAsync) {\n        this.transformJwkToKeyObjectSyncFn = transformJwkToKeyObjectSyncFn;\n        this.transformJwkToKeyObjectAsyncFn = transformJwkToKeyObjectAsyncFn;\n        this.publicKeys = new Map();\n    }\n    /**\n     * Transform the JWK into an RSA public key in native key object format.\n     * If the transformed JWK is already in the cache, it is returned from the cache instead.\n     *\n     * @param jwk: the JWK\n     * @param jwtHeaderAlg: the alg from the JWT header (used if absent on JWK)\n     * @param issuer: the issuer that uses the JWK for signing JWTs (used for caching the transformation)\n     * @returns the RSA public key in native key object format\n     */\n    transformJwkToKeyObjectSync(jwk, jwtHeaderAlg, issuer) {\n        const alg = jwk.alg ?? jwtHeaderAlg;\n        if (!issuer || !jwk.kid || !alg) {\n            return this.transformJwkToKeyObjectSyncFn(jwk, alg, issuer);\n        }\n        const fromCache = this.publicKeys.get(issuer)?.get(jwk.kid)?.get(alg);\n        if (fromCache)\n            return fromCache;\n        const publicKey = this.transformJwkToKeyObjectSyncFn(jwk, alg, issuer);\n        this.putKeyObjectInCache(issuer, jwk.kid, alg, publicKey);\n        return publicKey;\n    }\n    /**\n     * Transform the JWK into an RSA public key in native key object format (async).\n     * If the transformed JWK is already in the cache, it is returned from the cache instead.\n     *\n     * @param jwk: the JWK\n     * @param jwtHeaderAlg: the alg from the JWT header (used if absent on JWK)\n     * @param issuer: the issuer that uses the JWK for signing JWTs (used for caching the transformation)\n     * @returns the RSA public key in native key object format\n     */\n    async transformJwkToKeyObjectAsync(jwk, jwtHeaderAlg, issuer) {\n        const alg = jwk.alg ?? jwtHeaderAlg;\n        if (!issuer || !jwk.kid || !alg) {\n            return this.transformJwkToKeyObjectAsyncFn(jwk, alg, issuer);\n        }\n        const fromCache = this.publicKeys.get(issuer)?.get(jwk.kid)?.get(alg);\n        if (fromCache)\n            return fromCache;\n        const publicKey = await this.transformJwkToKeyObjectAsyncFn(jwk, alg, issuer);\n        this.putKeyObjectInCache(issuer, jwk.kid, alg, publicKey);\n        return publicKey;\n    }\n    putKeyObjectInCache(issuer, kid, alg, publicKey) {\n        const cachedIssuer = this.publicKeys.get(issuer);\n        const cachedIssuerKid = cachedIssuer?.get(kid);\n        if (cachedIssuerKid) {\n            cachedIssuerKid.set(alg, publicKey);\n        }\n        else if (cachedIssuer) {\n            cachedIssuer.set(kid, new Map([[alg, publicKey]]));\n        }\n        else {\n            this.publicKeys.set(issuer, new Map([[kid, new Map([[alg, publicKey]])]]));\n        }\n    }\n    clearCache(issuer) {\n        this.publicKeys.delete(issuer);\n    }\n}\n", "// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n// SPDX-License-Identifier: Apache-2.0\nimport { CognitoJwtInvalidClientIdError, CognitoJwtInvalidGroupError, CognitoJwtInvalidTokenUseError, JwtInvalidClaimError, ParameterValidationError, } from \"./error.js\";\nimport { JwtRsaVerifierBase } from \"./jwt-rsa.js\";\nimport { assertStringArrayContainsString, assertStringEquals, assertStringArraysOverlap, } from \"./assert.js\";\n/**\n * Validate claims of a decoded Cognito JWT.\n * This function throws an error in case there's any validation issue.\n *\n * @param payload - The JSON parsed payload of the Cognito JWT\n * @param options - Validation options\n * @param options.groups - The cognito groups, of which at least one must be present in the JWT's cognito:groups claim\n * @param options.tokenUse - The required token use of the JWT: \"id\" or \"access\"\n * @param options.clientId - The required clientId of the JWT. May be an array of string, of which at least one must match\n * @returns void\n */\nexport function validateCognitoJwtFields(payload, options) {\n    // Check groups\n    if (options.groups != null) {\n        assertStringArraysOverlap(\"Cognito group\", payload[\"cognito:groups\"], options.groups, CognitoJwtInvalidGroupError);\n    }\n    // Check token use\n    assertStringArrayContainsString(\"Token use\", payload.token_use, [\"id\", \"access\"], CognitoJwtInvalidTokenUseError);\n    if (options.tokenUse !== null) {\n        if (options.tokenUse === undefined) {\n            throw new ParameterValidationError(\"tokenUse must be provided or set to null explicitly\");\n        }\n        assertStringEquals(\"Token use\", payload.token_use, options.tokenUse, CognitoJwtInvalidTokenUseError);\n    }\n    // Check clientId aka audience\n    if (options.clientId !== null) {\n        if (options.clientId === undefined) {\n            throw new ParameterValidationError(\"clientId must be provided or set to null explicitly\");\n        }\n        if (payload.token_use === \"id\") {\n            assertStringArrayContainsString('Client ID (\"audience\")', payload.aud, options.clientId, CognitoJwtInvalidClientIdError);\n        }\n        else {\n            assertStringArrayContainsString(\"Client ID\", payload.client_id, options.clientId, CognitoJwtInvalidClientIdError);\n        }\n    }\n}\n/**\n * Class representing a verifier for JWTs signed by Amazon Cognito\n */\nexport class CognitoJwtVerifier extends JwtRsaVerifierBase {\n    constructor(props, jwksCache) {\n        const issuerConfig = Array.isArray(props)\n            ? props.map((p) => ({\n                ...p,\n                ...CognitoJwtVerifier.parseUserPoolId(p.userPoolId),\n                audience: null, // checked instead by validateCognitoJwtFields\n            }))\n            : {\n                ...props,\n                ...CognitoJwtVerifier.parseUserPoolId(props.userPoolId),\n                audience: null, // checked instead by validateCognitoJwtFields\n            };\n        super(issuerConfig, jwksCache);\n    }\n    /**\n     * Parse a User Pool ID, to extract the issuer and JWKS URI\n     *\n     * @param userPoolId The User Pool ID\n     * @returns The issuer and JWKS URI for the User Pool\n     */\n    static parseUserPoolId(userPoolId) {\n        // Disable safe regexp check as userPoolId is provided by developer, i.e. is not user input\n        // eslint-disable-next-line security/detect-unsafe-regex\n        const match = userPoolId.match(/^(?<region>(\\w+-)?\\w+-\\w+-\\d)+_\\w+$/);\n        if (!match) {\n            throw new ParameterValidationError(`Invalid Cognito User Pool ID: ${userPoolId}`);\n        }\n        const region = match.groups.region;\n        const issuer = `https://cognito-idp.${region}.amazonaws.com/${userPoolId}`;\n        return {\n            issuer,\n            jwksUri: `${issuer}/.well-known/jwks.json`,\n        };\n    }\n    // eslint-disable-next-line @typescript-eslint/explicit-module-boundary-types\n    static create(verifyProperties, additionalProperties) {\n        return new this(verifyProperties, additionalProperties?.jwksCache);\n    }\n    /**\n     * Verify (synchronously) a JWT that is signed by Amazon Cognito.\n     *\n     * @param jwt The JWT, as string\n     * @param props Verification properties\n     * @returns The payload of the JWT\u2013\u2013if the JWT is valid, otherwise an error is thrown\n     */\n    verifySync(...[jwt, properties]) {\n        const { decomposedJwt, jwksUri, verifyProperties } = this.getVerifyParameters(jwt, properties);\n        this.verifyDecomposedJwtSync(decomposedJwt, jwksUri, verifyProperties);\n        try {\n            validateCognitoJwtFields(decomposedJwt.payload, verifyProperties);\n        }\n        catch (err) {\n            if (verifyProperties.includeRawJwtInErrors &&\n                err instanceof JwtInvalidClaimError) {\n                throw err.withRawJwt(decomposedJwt);\n            }\n            throw err;\n        }\n        return decomposedJwt.payload;\n    }\n    /**\n     * Verify (asynchronously) a JWT that is signed by Amazon Cognito.\n     * This call is asynchronous, and the JWKS will be fetched from the JWKS uri,\n     * in case it is not yet available in the cache.\n     *\n     * @param jwt The JWT, as string\n     * @param props Verification properties\n     * @returns Promise that resolves to the payload of the JWT\u2013\u2013if the JWT is valid, otherwise the promise rejects\n     */\n    async verify(...[jwt, properties]) {\n        const { decomposedJwt, jwksUri, verifyProperties } = this.getVerifyParameters(jwt, properties);\n        await this.verifyDecomposedJwt(decomposedJwt, jwksUri, verifyProperties);\n        try {\n            validateCognitoJwtFields(decomposedJwt.payload, verifyProperties);\n        }\n        catch (err) {\n            if (verifyProperties.includeRawJwtInErrors &&\n                err instanceof JwtInvalidClaimError) {\n                throw err.withRawJwt(decomposedJwt);\n            }\n            throw err;\n        }\n        return decomposedJwt.payload;\n    }\n    /**\n     * This method loads a JWKS that you provide, into the JWKS cache, so that it is\n     * available for JWT verification. Use this method to speed up the first JWT verification\n     * (when the JWKS would otherwise have to be downloaded from the JWKS uri), or to provide the JWKS\n     * in case the JwtVerifier does not have internet access to download the JWKS\n     *\n     * @param jwks The JWKS\n     * @param userPoolId The userPoolId for which you want to cache the JWKS\n     *  Supply this field, if you instantiated the CognitoJwtVerifier with multiple userPoolIds\n     * @returns void\n     */\n    cacheJwks(...[jwks, userPoolId]) {\n        let issuer;\n        if (userPoolId !== undefined) {\n            issuer = CognitoJwtVerifier.parseUserPoolId(userPoolId).issuer;\n        }\n        else if (this.expectedIssuers.length > 1) {\n            throw new ParameterValidationError(\"userPoolId must be provided\");\n        }\n        const issuerConfig = this.getIssuerConfig(issuer);\n        super.cacheJwks(jwks, issuerConfig.issuer);\n    }\n}\n", "// GLOBAL\n\nexport interface AnyObject {\n  [key: string]: any;\n}\n\nexport interface CompletionData {\n  [key: string]: boolean | CompletionData;\n}\n\n/**\n * Abridged copy of the type used by `aws-lambda@1.0.7` (from `@types/aws-lambda@8.10.88`)\n * We only this package for these types, and we use only a subset of the\n * properties. Since `aws-lambda` depends on `aws-sdk` (that is, SDK v2),\n * we can save ourselves a big dependency with this small redundancy.\n */\n\nexport interface APIGatewayProxyEventPathParameters {\n  [name: string]: string | undefined;\n}\n\nexport interface APIGatewayProxyEvent {\n  body: string | null;\n  headers: Record<string, string | undefined>;\n  multiValueHeaders: Record<string, string | undefined>;\n  httpMethod: string;\n  isBase64Encoded: boolean;\n  path: string;\n  pathParameters: Record<string, string | undefined> | null;\n  queryStringParameters: Record<string, string | undefined> | null;\n  multiValueQueryStringParameters: Record<string, string | undefined> | null;\n  stageVariables: Record<string, string | undefined> | null;\n  /** The context is complicated, and we don't (as of 2023) use it at all. */\n  requestContext: any;\n  resource: string;\n}\n\n// ALERTS\n\nexport enum AlertTypes {\n  ERROR = \"error\",\n  INFO = \"info\",\n  SUCCESS = \"success\",\n  WARNING = \"warning\",\n}\n\n// TIME\n\nexport interface DateShape {\n  year: number;\n  month: number;\n  day: number;\n}\n\nexport interface TimeShape {\n  hour: number;\n  minute: number;\n  second: number;\n}\n\n// OTHER\n\nexport interface CustomHtmlElement {\n  type: string;\n  content: string | any;\n  as?: string;\n  props?: AnyObject;\n}\n\nexport interface ErrorVerbiage {\n  title: string;\n  description: string | CustomHtmlElement[];\n}\n\nconst states = [\n  \"AL\",\n  \"AK\",\n  \"AS\", // American Samoa\n  \"AZ\",\n  \"AR\",\n  \"CA\",\n  \"CO\",\n  \"CT\",\n  \"DE\",\n  \"DC\",\n  \"FL\",\n  \"FM\", // Federated States of Micronesia\n  \"GA\",\n  \"GU\", // Guam\n  \"HI\",\n  \"ID\",\n  \"IL\",\n  \"IN\",\n  \"IA\",\n  \"KS\",\n  \"KY\",\n  \"LA\",\n  \"ME\",\n  \"MH\", // Marshall Islands\n  \"MD\",\n  \"MA\",\n  \"MI\",\n  \"MN\",\n  \"MS\",\n  \"MO\",\n  \"MP\", // Northern Mariana Islands\n  \"MT\",\n  \"NE\",\n  \"NV\",\n  \"NH\",\n  \"NJ\",\n  \"NM\",\n  \"NY\",\n  \"NC\",\n  \"ND\",\n  \"OH\",\n  \"OK\",\n  \"OR\",\n  \"PA\",\n  \"PR\",\n  \"PW\", // Palau\n  \"RI\",\n  \"SC\",\n  \"SD\",\n  \"TN\",\n  \"TX\",\n  \"UT\",\n  \"VT\",\n  \"VA\",\n  \"VI\", // Virgin Islands\n  \"WA\",\n  \"WV\",\n  \"WI\",\n  \"WY\",\n] as const;\nexport type State = typeof states[number];\n\nexport const isState = (state: unknown): state is State => {\n  return states.includes(state as State);\n};\n\nexport interface FormTemplateVersion {\n  md5Hash: string;\n  versionNumber: number;\n  id: string;\n  lastAltered: string;\n  reportType: string;\n}\n\nexport const enum TemplateKeys {\n  WP = \"templates/MFP-Work-Plan-Help-File.pdf\",\n  SAR = \"templates/MFP-Semi-Annual-Rprt-Help-File.pdf\",\n}\n\n/**\n * S3Create event\n * https://docs.aws.amazon.com/AmazonS3/latest/dev/notification-content-structure.html\n */\n\nexport interface S3EventRecordGlacierRestoreEventData {\n  lifecycleRestorationExpiryTime: string;\n  lifecycleRestoreStorageClass: string;\n}\nexport interface S3EventRecordGlacierEventData {\n  restoreEventData: S3EventRecordGlacierRestoreEventData;\n}\n\nexport interface S3EventRecord {\n  eventVersion: string;\n  eventSource: string;\n  awsRegion: string;\n  eventTime: string;\n  eventName: string;\n  userIdentity: {\n    principalId: string;\n  };\n  requestParameters: {\n    sourceIPAddress: string;\n  };\n  responseElements: {\n    \"x-amz-request-id\": string;\n    \"x-amz-id-2\": string;\n  };\n  s3: {\n    s3SchemaVersion: string;\n    configurationId: string;\n    bucket: {\n      name: string;\n      ownerIdentity: {\n        principalId: string;\n      };\n      arn: string;\n    };\n    object: {\n      key: string;\n      size: number;\n      eTag: string;\n      versionId?: string | undefined;\n      sequencer: string;\n    };\n  };\n  glacierEventData?: S3EventRecordGlacierEventData | undefined;\n}\n\n/**\n * Use this type to create a type guard for filtering arrays of objects\n * by the presence of certain attributes.\n *\n * @example\n * interface Foo {\n *    bar: string;\n *    baz?: string;\n *    buzz?: string;\n *    bizz?: string;\n * }\n * type RequireBaz = SomeRequired<Foo, 'baz'>\n * const array: Foo[] = [\n *  { bar: 'always here' },\n *  { bar: 'always here', baz: 'sometimes here' }\n * ]\n * array.filter((f): f is RequireBaz => typeof f.baz !== 'undefined' )\n * // `array`'s type now shows bar and baz as required.\n * array.map((f) => return f.baz)\n */\nexport type SomeRequired<T, K extends keyof T> = Required<Pick<T, K>> &\n  Omit<T, K>;\n\n/**\n * Instructs Typescript to complain if it detects that this function may be reachable.\n * Useful for the default branch of a switch statement that verifiably covers every case.\n */\nexport const assertExhaustive = (_: never): void => {};\n", "import { FormJson } from \"./formFields\";\nimport { AnyObject, CustomHtmlElement } from \"./other\";\n\n// REPORT STRUCTURE\n\nexport interface ReportJson {\n  id?: string;\n  type: ReportType;\n  name: string;\n  basePath: string;\n  routes: ReportRoute[];\n  validationSchema?: AnyObject;\n  /**\n   * The validationJson property is populated at the moment any form template\n   * is stored in S3 for the first time. It will be populated from that moment on.\n   */\n  validationJson?: AnyObject;\n}\n\nexport type ReportRoute = ReportRouteWithForm | ReportRouteWithoutForm;\n\nexport interface ReportRouteBase {\n  name: string;\n  path: string;\n  pageType?: string;\n  conditionallyRender?: string;\n}\n\nexport type ReportRouteWithForm =\n  | StandardReportPageShape\n  | DrawerReportPageShape\n  | ModalDrawerReportPageShape\n  | ModalOverlayReportPageShape\n  | OverlayModalPageShape\n  | EntityDetailsOverlayShape\n  | DynamicModalOverlayReportPageShape;\n\nexport interface ReportPageShapeBase extends ReportRouteBase {\n  children?: never;\n  verbiage: ReportPageVerbiage;\n}\n\nexport interface StandardReportPageShape extends ReportPageShapeBase {\n  form: FormJson;\n  dashboard?: never;\n  modalForm?: never;\n  drawerForm?: never;\n  entityType?: never;\n  entitySteps?: never;\n  template?: never;\n  initiatives?: never;\n  objectiveCards?: never;\n}\n\nexport interface DrawerReportPageShape extends ReportPageShapeBase {\n  entityType: string;\n  verbiage: DrawerReportPageVerbiage;\n  drawerForm: FormJson;\n  modalForm?: never;\n  form?: never;\n  entitySteps?: never;\n  dashboard?: never;\n  template?: never;\n  initiatives?: never;\n  objectiveCards?: never;\n}\n\nexport interface ModalDrawerReportPageShape extends ReportPageShapeBase {\n  entityType: string;\n  verbiage: ModalDrawerReportPageVerbiage;\n  modalForm: FormJson;\n  drawerForm: FormJson;\n  form?: never;\n  entitySteps?: never;\n  dashboard?: never;\n  template?: never;\n  initiatives?: never;\n  objectiveCards?: never;\n}\n\nexport interface ModalOverlayReportPageShape extends ReportPageShapeBase {\n  initiativeId: string | undefined;\n  entityType: string;\n  entityInfo?: string[];\n  verbiage: ModalOverlayReportPageVerbiage;\n  modalForm: FormJson;\n  drawerForm?: never;\n  form?: never;\n  dashboard: EntityDetailsDashboardOverlayShape;\n  entitySteps?: (EntityDetailsOverlayShape | OverlayModalPageShape)[];\n  template?: never;\n  initiatives?: never;\n  objectiveCards?: never;\n}\n\nexport interface DynamicModalOverlayReportPageShape\n  extends ReportPageShapeBase {\n  entityType: string;\n  entityInfo: string[];\n  verbiage: ModalOverlayReportPageVerbiage;\n  drawerForm?: never;\n  modalForm?: never;\n  form?: never;\n  entitySteps?: never;\n  dashboard?: never;\n  initiatives: {\n    initiativeId: string;\n    name: string;\n    topic: string;\n    dashboard: FormJson;\n    entitySteps: (EntityDetailsOverlayShape | OverlayModalPageShape)[];\n  }[];\n  objectiveCards?: never;\n  /** Only used during form template transformation; will be absent after transformation */\n  template?: AnyObject;\n}\n\nexport interface OverlayModalPageShape extends ReportPageShapeBase {\n  entityType: string;\n  stepName: string;\n  hint: string;\n  verbiage: ModalOverlayReportPageVerbiage;\n  modalForm: FormJson;\n  drawerForm?: FormJson;\n  form?: never;\n  entitySteps?: never;\n  dashboard?: never;\n  template?: never;\n  initiatives?: never;\n  objectiveCards?: {\n    modalForm?: FormJson;\n  }[];\n}\n\nexport interface EntityDetailsOverlayShape extends ReportPageShapeBase {\n  stepName: string;\n  hint: string;\n  form: FormJson;\n  verbiage: EntityOverlayPageVerbiage;\n  entityType?: never;\n  dashboard?: never;\n  modalForm?: never;\n  drawerForm?: never;\n  entitySteps?: never;\n  template?: never;\n  initiatives?: never;\n  objectiveCards?: {\n    modalForm?: FormJson;\n  }[];\n}\n\nexport interface EntityDetailsDashboardOverlayShape\n  extends ReportPageShapeBase {\n  dashboard?: never;\n  template?: never;\n  initiatives?: never;\n  objectiveCards?: never;\n}\n\nexport interface ReportRouteWithoutForm extends ReportRouteBase {\n  children?: ReportRoute[];\n  pageType?: string;\n  entityType?: never;\n  verbiage?: never;\n  modalForm?: never;\n  drawerForm?: never;\n  form?: never;\n  entitySteps?: never;\n  dashboard?: never;\n  template?: never;\n  initiatives?: never;\n  objectiveCards?: never;\n}\n\nexport interface ReportPageVerbiage {\n  intro: {\n    section: string;\n    subsection?: string;\n    hint?: string;\n    info?: string | CustomHtmlElement[];\n  };\n  closeOutWarning?: AnyObject;\n  closeOutModal?: AnyObject;\n}\n\nexport interface DrawerReportPageVerbiage extends ReportPageVerbiage {\n  dashboardTitle: string;\n  countEntitiesInTitle?: boolean;\n  drawerTitle: string;\n  drawerInfo?: CustomHtmlElement[];\n  missingEntityMessage?: CustomHtmlElement[];\n}\n\nexport interface ModalDrawerReportPageVerbiage\n  extends DrawerReportPageVerbiage {\n  addEntityButtonText: string;\n  editEntityButtonText: string;\n  readOnlyEntityButtonText: string;\n  addEditModalAddTitle: string;\n  addEditModalEditTitle: string;\n  addEditModalMessage: string;\n  deleteEntityButtonAltText: string;\n  deleteModalTitle: string;\n  deleteModalConfirmButtonText: string;\n  deleteModalWarning: string;\n  entityUnfinishedMessage: string;\n  enterEntityDetailsButtonText: string;\n  readOnlyEntityDetailsButtonText: string;\n  editEntityDetailsButtonText: string;\n}\n\nexport interface ModalOverlayReportPageVerbiage\n  extends EntityOverlayPageVerbiage {\n  addEntityButtonText: string;\n  dashboardTitle: string;\n  countEntitiesInTitle: boolean;\n  tableHeader: string;\n  addEditModalHint: string;\n  emptyDashboardText: string;\n}\n\nexport interface EntityOverlayPageVerbiage extends ReportPageVerbiage {\n  closeOutWarning?: {\n    title?: string;\n    description?: string;\n  };\n  closeOutModal?: {\n    closeOutModalButtonText?: string;\n    closeOutModalTitle?: string;\n    closeOutModalBodyText?: string;\n    closeOutModalConfirmButtonText?: string;\n  };\n}\n\nexport enum ReportType {\n  WP = \"WP\",\n  SAR = \"SAR\",\n}\n/**\n * Check if unknown value is a report type\n *\n * @param reportType possible report type value\n * @returns type assertion for value\n */\nexport function isReportType(reportType: unknown): reportType is ReportType {\n  return Object.values(ReportType).includes(reportType as ReportType);\n}\n", "import { Choice } from \"./formFields\";\n\n/** This type is intended to be used exclusively in the form template transforms */\nexport type WorkPlanFieldDataForTransforms = {\n  targetPopulations?: TargetPopulation[];\n  initiative?: {\n    id: string;\n    initiative_name: string;\n    initiative_wpTopic: Choice[];\n    fundingSources: {\n      id: string;\n      fundingSources_wpTopic: Choice[];\n    }[];\n    evaluationPlan: any;\n  }[];\n};\n\n/** This type is intended to be used exclusively in the form template transforms */\nexport type TargetPopulation = {\n  transitionBenchmarks_applicableToMfpDemonstration: Choice[];\n  transitionBenchmarks_targetPopulationName: string;\n  isRequired?: boolean;\n};\n\n/** This type is intended to be used exclusively in the form template transforms */\nexport type Initiative = {\n  id: string;\n  initiative_name: string;\n  initiative_wpTopic: Choice[];\n  fundingSources: FundingSource[];\n};\n\nexport type FundingSource = {\n  id: string;\n  fundingSources_wpTopic: Choice[];\n  initiative_wp_otherTopic?: string;\n};\n\nexport const isUsableForTransforms = (\n  workPlanFieldData?: any\n): workPlanFieldData is WorkPlanFieldDataForTransforms | undefined => {\n  /*\n   * If we are running a WP transformation,\n   * we don't expect to have any WP field data at hand.\n   * None of the WP transforms expect field data,\n   * so leaving this undefined is perfectly fine.\n   */\n  if (workPlanFieldData === undefined) {\n    return true;\n  }\n\n  /*\n   * In real life a work plan will always have targetPopulations.\n   * We allow it to be optional here for ease of testing.\n   * It is checked for nullishness before running the related transformation.\n   */\n  if (workPlanFieldData.targetPopulations !== undefined) {\n    const populations = workPlanFieldData.targetPopulations;\n    if (!Array.isArray(populations)) return false;\n    if (!populations.every(isTargetPopulation)) return false;\n  }\n\n  // As with populations, this is optional only for east of testing.\n  if (workPlanFieldData.initiative !== undefined) {\n    const initiatives = workPlanFieldData.initiative;\n    if (!Array.isArray(initiatives)) return false;\n    if (!initiatives.every(isInitiative)) return false;\n  }\n\n  return true;\n};\n\nconst isTargetPopulation = (pop: any): pop is TargetPopulation => {\n  if (typeof pop !== \"object\") return false;\n  if (pop.isRequired !== undefined && typeof pop.isRequired !== \"boolean\")\n    return false;\n  if (typeof pop.transitionBenchmarks_targetPopulationName !== \"string\")\n    return false;\n  if (!isChoiceArray(pop.transitionBenchmarks_applicableToMfpDemonstration))\n    return false;\n  if (pop.transitionBenchmarks_applicableToMfpDemonstration.length !== 1)\n    return false;\n  return true;\n};\n\nconst isInitiative = (initiative: any): initiative is Initiative => {\n  if (typeof initiative !== \"object\") return false;\n  if (typeof initiative.id !== \"string\") return false;\n  if (typeof initiative.initiative_name !== \"string\") return false;\n  if (!isChoiceArray(initiative.initiative_wpTopic)) return false;\n  if (!Array.isArray(initiative.fundingSources)) return false;\n  if (!initiative.fundingSources.every(isFundingSource)) return false;\n  for (let fundingSource of initiative.fundingSources) {\n    if (!isChoiceArray(fundingSource.fundingSources_wpTopic)) return false;\n  }\n  return true;\n};\n\nconst isFundingSource = (\n  fundingSource: any\n): fundingSource is FundingSource => {\n  if (typeof fundingSource !== \"object\") return false;\n  if (typeof fundingSource.id !== \"string\") return false;\n  if (!isChoiceArray(fundingSource.fundingSources_wpTopic)) return false;\n  if (\n    fundingSource.initiative_wp_otherTopic !== undefined &&\n    typeof fundingSource.initiative_wp_otherTopic !== \"string\"\n  )\n    return false;\n  return true;\n};\n\nconst isChoiceArray = (choices: any): choices is Choice[] => {\n  if (!Array.isArray(choices)) return false;\n  for (let choice of choices) {\n    if (typeof choice.key !== \"string\") return false;\n    if (typeof choice.value !== \"string\") return false;\n  }\n  return true;\n};\n", "/**\n * The response for a successful request.\n * Should include a body for GET, PUT, or POST.\n * Need not include a body for DELETE\n */\nexport const ok = (body?: Object) => new HttpResponse(StatusCodes.Ok, body);\n\n/**\n * The response for a successful POST or PUT request,\n * which resulted in the creation of a new resource.\n */\nexport const created = (body: Object) =>\n  new HttpResponse(StatusCodes.Created, body);\n\n/**\n * The response for a successful request which returns no content.\n */\nexport const noContent = (body?: Object) =>\n  new HttpResponse(StatusCodes.NoContent, body);\n\n/**\n * The response for a failed request, due to client-side issues.\n * Typically indicates a missing parameter or malformed body.\n */\nexport const badRequest = (body?: Object) =>\n  new HttpResponse(StatusCodes.BadRequest, body);\n\n/**\n * The response for a client without any authorization.\n * Typically indicates an issue with the request's headers or token.\n *\n * Note: The usual name for HTTP 401 is \"Unauthorized\", but that's misleading.\n * Authentication is for identity; authorization is for permissions.\n */\nexport const unauthenticated = (body?: Object) =>\n  new HttpResponse(StatusCodes.Unauthenticated, body);\n\n/**\n * The response for a client without sufficient permissions.\n * This is specific to the requested operation.\n * For example, a regular user requesting an admin-only endpoint.\n */\nexport const forbidden = (body?: Object) =>\n  new HttpResponse(StatusCodes.Forbidden, body);\n\n/**\n * The response for a request that assumes the existence of a missing resource.\n * For example, attempting to submit a report that isn't in the database.\n */\nexport const notFound = (body?: Object) =>\n  new HttpResponse(StatusCodes.NotFound, body);\n\n/**\n * The response for a request that assumes the server is in a different state.\n * For example, attempting to submit a report that's already submitted.\n */\nexport const conflict = (body?: Object) =>\n  new HttpResponse(StatusCodes.Conflict, body);\n\n/**\n * The response for a request that errored out on the server side.\n * Typically indicates there is nothing the client can do to resolve the issue.\n */\nexport const internalServerError = (body?: Object) =>\n  new HttpResponse(StatusCodes.InternalServerError, body);\n\n/**\n * Note: Production code shouldn't need to reference this directly.\n * Use a helper method instead.\n *\n * This enum is listed mainly for the purpose of unit testing.\n */\nexport enum StatusCodes {\n  Ok = 200,\n  Created = 201,\n  NoContent = 204,\n  BadRequest = 400,\n  Unauthenticated = 401,\n  Forbidden = 403,\n  NotFound = 404,\n  Conflict = 409,\n  InternalServerError = 500,\n}\n\n/**\n * Note: Production code shouldn't need to reference this directly.\n * Use a helper method instead.\n */\nexport class HttpResponse {\n  readonly statusCode: number;\n  readonly body: string | undefined;\n  readonly headers = {\n    \"Access-Control-Allow-Origin\": \"*\",\n    \"Access-Control-Allow-Credentials\": true,\n  };\n  constructor(statusCode: number, body?: Object | undefined) {\n    this.statusCode = statusCode;\n    if (body !== undefined) {\n      this.body = JSON.stringify(body);\n    }\n  }\n}\n", "import { ReportType } from \"../types/reports\";\n\nexport const error = {\n  // generic errors\n  UNAUTHORIZED: \"User is not authorized to access this resource.\",\n  NO_KEY: \"Must provide key for table.\",\n  MISSING_DATA: \"Missing required data.\",\n  INVALID_DATA: \"Provided data is not valid.\",\n  NO_MATCHING_RECORD: \"No matching record found.\",\n  SERVER_ERROR: \"An unspecified server error occured.\",\n  // bucket errors\n  S3_OBJECT_CREATION_ERROR: \"Report could not be created due to an S3 error.\",\n  S3_OBJECT_UPDATE_ERROR: \"Report could not be updated due to an S3 error.\",\n  S3_OBJECT_GET_ERROR: \"Error while fetching report.\",\n  // dynamo errors\n  DYNAMO_CREATION_ERROR: \"Report could not be created due to a database error.\",\n  DYNAMO_UPDATE_ERROR: \"Report could not be updated due to a database error.\",\n  // template errors\n  NO_TEMPLATE_NAME: \"Must request template for download.\",\n  INVALID_TEMPLATE_NAME: \"Requested template does not exist or does not match.\",\n  NOT_IN_DATABASE: \"Record not found in database.\",\n  UNABLE_TO_COPY:\n    \"Unable to copy over report if todays date is in the same period and year as a previous report.\",\n  MISSING_FORM_TEMPLATE: \"Form Template not found in S3.\",\n  MISSING_FIELD_DATA: \"Field Data not found in S3.\",\n  NO_WORKPLANS_FOUND: \"No record of Work Plans found in database\",\n  // admin action errors\n  ALREADY_ARCHIVED: \"Cannot update archived report.\",\n  ALREADY_LOCKED: \"Cannot update locked report.\",\n  REPORT_INCOMPLETE: \"Cannot submit incomplete form.\",\n} as const;\n\nexport const buckets = {\n  FORM_TEMPLATE: \"formTemplates\",\n  FIELD_DATA: \"fieldData\",\n};\n\n// STATES\nexport enum States {\n  AL = \"Alabama\",\n  AK = \"Alaska\",\n  AS = \"American Samoa\",\n  AZ = \"Arizona\",\n  AR = \"Arkansas\",\n  CA = \"California\",\n  CO = \"Colorado\",\n  CT = \"Connecticut\",\n  DE = \"Delaware\",\n  DC = \"District of Columbia\",\n  FM = \"Federated States of Micronesia\",\n  FL = \"Florida\",\n  GA = \"Georgia\",\n  GU = \"Guam\",\n  HI = \"Hawaii\",\n  ID = \"Idaho\",\n  IL = \"Illinois\",\n  IN = \"Indiana\",\n  IA = \"Iowa\",\n  KS = \"Kansas\",\n  KY = \"Kentucky\",\n  LA = \"Louisiana\",\n  ME = \"Maine\",\n  MH = \"Marshall Islands\",\n  MD = \"Maryland\",\n  MA = \"Massachusetts\",\n  MI = \"Michigan\",\n  MN = \"Minnesota\",\n  MS = \"Mississippi\",\n  MO = \"Missouri\",\n  MT = \"Montana\",\n  NE = \"Nebraska\",\n  NV = \"Nevada\",\n  NH = \"New Hampshire\",\n  NJ = \"New Jersey\",\n  NM = \"New Mexico\",\n  NY = \"New York\",\n  NC = \"North Carolina\",\n  ND = \"North Dakota\",\n  MP = \"Northern Mariana Islands\",\n  OH = \"Ohio\",\n  OK = \"Oklahoma\",\n  OR = \"Oregon\",\n  PW = \"Palau\",\n  PA = \"Pennsylvania\",\n  PR = \"Puerto Rico\",\n  RI = \"Rhode Island\",\n  SC = \"South Carolina\",\n  SD = \"South Dakota\",\n  TN = \"Tennessee\",\n  TX = \"Texas\",\n  UT = \"Utah\",\n  VT = \"Vermont\",\n  VI = \"Virgin Islands\",\n  VA = \"Virginia\",\n  WA = \"Washington\",\n  WV = \"West Virginia\",\n  WI = \"Wisconsin\",\n  WY = \"Wyoming\",\n}\n\n// REPORTS\n\nexport const reportTables: { [key in ReportType]: string } = {\n  SAR: process.env.SarReportsTable!,\n  WP: process.env.WpReportsTable!,\n};\n\nexport const reportBuckets: { [key in ReportType]: string } = {\n  SAR: process.env.SAR_FORM_BUCKET!,\n  WP: process.env.WP_FORM_BUCKET!,\n};\n\nexport const reportNames: { [key in ReportType]: string } = {\n  SAR: \"SAR\",\n  WP: \"Work Plan\",\n};\n\nexport const tableTopics: { [key in ReportType]: string } = {\n  SAR: \"sar-reports\",\n  WP: \"wp-reports\",\n};\n\nexport const bucketTopics: { [key in ReportType]: string } = {\n  SAR: \"sar-form\",\n  WP: \"wp-form\",\n};\n\nexport const DEFAULT_TARGET_POPULATION_NAMES = [\n  \"Older adults\",\n  \"Individuals with physical disabilities (PD)\",\n  \"Individuals with intellectual and developmental disabilities (I/DD)\",\n  \"Individuals with mental health and substance use disorders (MH/SUD)\",\n  \"HCBS infrastructure/system-level development\",\n];\n", "/*! @license DOMPurify 3.2.4 | (c) Cure53 and other contributors | Released under the Apache license 2.0 and Mozilla Public License 2.0 | github.com/cure53/DOMPurify/blob/3.2.4/LICENSE */\n\nconst {\n  entries,\n  setPrototypeOf,\n  isFrozen,\n  getPrototypeOf,\n  getOwnPropertyDescriptor\n} = Object;\nlet {\n  freeze,\n  seal,\n  create\n} = Object; // eslint-disable-line import/no-mutable-exports\nlet {\n  apply,\n  construct\n} = typeof Reflect !== 'undefined' && Reflect;\nif (!freeze) {\n  freeze = function freeze(x) {\n    return x;\n  };\n}\nif (!seal) {\n  seal = function seal(x) {\n    return x;\n  };\n}\nif (!apply) {\n  apply = function apply(fun, thisValue, args) {\n    return fun.apply(thisValue, args);\n  };\n}\nif (!construct) {\n  construct = function construct(Func, args) {\n    return new Func(...args);\n  };\n}\nconst arrayForEach = unapply(Array.prototype.forEach);\nconst arrayLastIndexOf = unapply(Array.prototype.lastIndexOf);\nconst arrayPop = unapply(Array.prototype.pop);\nconst arrayPush = unapply(Array.prototype.push);\nconst arraySplice = unapply(Array.prototype.splice);\nconst stringToLowerCase = unapply(String.prototype.toLowerCase);\nconst stringToString = unapply(String.prototype.toString);\nconst stringMatch = unapply(String.prototype.match);\nconst stringReplace = unapply(String.prototype.replace);\nconst stringIndexOf = unapply(String.prototype.indexOf);\nconst stringTrim = unapply(String.prototype.trim);\nconst objectHasOwnProperty = unapply(Object.prototype.hasOwnProperty);\nconst regExpTest = unapply(RegExp.prototype.test);\nconst typeErrorCreate = unconstruct(TypeError);\n/**\n * Creates a new function that calls the given function with a specified thisArg and arguments.\n *\n * @param func - The function to be wrapped and called.\n * @returns A new function that calls the given function with a specified thisArg and arguments.\n */\nfunction unapply(func) {\n  return function (thisArg) {\n    for (var _len = arguments.length, args = new Array(_len > 1 ? _len - 1 : 0), _key = 1; _key < _len; _key++) {\n      args[_key - 1] = arguments[_key];\n    }\n    return apply(func, thisArg, args);\n  };\n}\n/**\n * Creates a new function that constructs an instance of the given constructor function with the provided arguments.\n *\n * @param func - The constructor function to be wrapped and called.\n * @returns A new function that constructs an instance of the given constructor function with the provided arguments.\n */\nfunction unconstruct(func) {\n  return function () {\n    for (var _len2 = arguments.length, args = new Array(_len2), _key2 = 0; _key2 < _len2; _key2++) {\n      args[_key2] = arguments[_key2];\n    }\n    return construct(func, args);\n  };\n}\n/**\n * Add properties to a lookup table\n *\n * @param set - The set to which elements will be added.\n * @param array - The array containing elements to be added to the set.\n * @param transformCaseFunc - An optional function to transform the case of each element before adding to the set.\n * @returns The modified set with added elements.\n */\nfunction addToSet(set, array) {\n  let transformCaseFunc = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : stringToLowerCase;\n  if (setPrototypeOf) {\n    // Make 'in' and truthy checks like Boolean(set.constructor)\n    // independent of any properties defined on Object.prototype.\n    // Prevent prototype setters from intercepting set as a this value.\n    setPrototypeOf(set, null);\n  }\n  let l = array.length;\n  while (l--) {\n    let element = array[l];\n    if (typeof element === 'string') {\n      const lcElement = transformCaseFunc(element);\n      if (lcElement !== element) {\n        // Config presets (e.g. tags.js, attrs.js) are immutable.\n        if (!isFrozen(array)) {\n          array[l] = lcElement;\n        }\n        element = lcElement;\n      }\n    }\n    set[element] = true;\n  }\n  return set;\n}\n/**\n * Clean up an array to harden against CSPP\n *\n * @param array - The array to be cleaned.\n * @returns The cleaned version of the array\n */\nfunction cleanArray(array) {\n  for (let index = 0; index < array.length; index++) {\n    const isPropertyExist = objectHasOwnProperty(array, index);\n    if (!isPropertyExist) {\n      array[index] = null;\n    }\n  }\n  return array;\n}\n/**\n * Shallow clone an object\n *\n * @param object - The object to be cloned.\n * @returns A new object that copies the original.\n */\nfunction clone(object) {\n  const newObject = create(null);\n  for (const [property, value] of entries(object)) {\n    const isPropertyExist = objectHasOwnProperty(object, property);\n    if (isPropertyExist) {\n      if (Array.isArray(value)) {\n        newObject[property] = cleanArray(value);\n      } else if (value && typeof value === 'object' && value.constructor === Object) {\n        newObject[property] = clone(value);\n      } else {\n        newObject[property] = value;\n      }\n    }\n  }\n  return newObject;\n}\n/**\n * This method automatically checks if the prop is function or getter and behaves accordingly.\n *\n * @param object - The object to look up the getter function in its prototype chain.\n * @param prop - The property name for which to find the getter function.\n * @returns The getter function found in the prototype chain or a fallback function.\n */\nfunction lookupGetter(object, prop) {\n  while (object !== null) {\n    const desc = getOwnPropertyDescriptor(object, prop);\n    if (desc) {\n      if (desc.get) {\n        return unapply(desc.get);\n      }\n      if (typeof desc.value === 'function') {\n        return unapply(desc.value);\n      }\n    }\n    object = getPrototypeOf(object);\n  }\n  function fallbackValue() {\n    return null;\n  }\n  return fallbackValue;\n}\n\nconst html$1 = freeze(['a', 'abbr', 'acronym', 'address', 'area', 'article', 'aside', 'audio', 'b', 'bdi', 'bdo', 'big', 'blink', 'blockquote', 'body', 'br', 'button', 'canvas', 'caption', 'center', 'cite', 'code', 'col', 'colgroup', 'content', 'data', 'datalist', 'dd', 'decorator', 'del', 'details', 'dfn', 'dialog', 'dir', 'div', 'dl', 'dt', 'element', 'em', 'fieldset', 'figcaption', 'figure', 'font', 'footer', 'form', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'head', 'header', 'hgroup', 'hr', 'html', 'i', 'img', 'input', 'ins', 'kbd', 'label', 'legend', 'li', 'main', 'map', 'mark', 'marquee', 'menu', 'menuitem', 'meter', 'nav', 'nobr', 'ol', 'optgroup', 'option', 'output', 'p', 'picture', 'pre', 'progress', 'q', 'rp', 'rt', 'ruby', 's', 'samp', 'section', 'select', 'shadow', 'small', 'source', 'spacer', 'span', 'strike', 'strong', 'style', 'sub', 'summary', 'sup', 'table', 'tbody', 'td', 'template', 'textarea', 'tfoot', 'th', 'thead', 'time', 'tr', 'track', 'tt', 'u', 'ul', 'var', 'video', 'wbr']);\nconst svg$1 = freeze(['svg', 'a', 'altglyph', 'altglyphdef', 'altglyphitem', 'animatecolor', 'animatemotion', 'animatetransform', 'circle', 'clippath', 'defs', 'desc', 'ellipse', 'filter', 'font', 'g', 'glyph', 'glyphref', 'hkern', 'image', 'line', 'lineargradient', 'marker', 'mask', 'metadata', 'mpath', 'path', 'pattern', 'polygon', 'polyline', 'radialgradient', 'rect', 'stop', 'style', 'switch', 'symbol', 'text', 'textpath', 'title', 'tref', 'tspan', 'view', 'vkern']);\nconst svgFilters = freeze(['feBlend', 'feColorMatrix', 'feComponentTransfer', 'feComposite', 'feConvolveMatrix', 'feDiffuseLighting', 'feDisplacementMap', 'feDistantLight', 'feDropShadow', 'feFlood', 'feFuncA', 'feFuncB', 'feFuncG', 'feFuncR', 'feGaussianBlur', 'feImage', 'feMerge', 'feMergeNode', 'feMorphology', 'feOffset', 'fePointLight', 'feSpecularLighting', 'feSpotLight', 'feTile', 'feTurbulence']);\n// List of SVG elements that are disallowed by default.\n// We still need to know them so that we can do namespace\n// checks properly in case one wants to add them to\n// allow-list.\nconst svgDisallowed = freeze(['animate', 'color-profile', 'cursor', 'discard', 'font-face', 'font-face-format', 'font-face-name', 'font-face-src', 'font-face-uri', 'foreignobject', 'hatch', 'hatchpath', 'mesh', 'meshgradient', 'meshpatch', 'meshrow', 'missing-glyph', 'script', 'set', 'solidcolor', 'unknown', 'use']);\nconst mathMl$1 = freeze(['math', 'menclose', 'merror', 'mfenced', 'mfrac', 'mglyph', 'mi', 'mlabeledtr', 'mmultiscripts', 'mn', 'mo', 'mover', 'mpadded', 'mphantom', 'mroot', 'mrow', 'ms', 'mspace', 'msqrt', 'mstyle', 'msub', 'msup', 'msubsup', 'mtable', 'mtd', 'mtext', 'mtr', 'munder', 'munderover', 'mprescripts']);\n// Similarly to SVG, we want to know all MathML elements,\n// even those that we disallow by default.\nconst mathMlDisallowed = freeze(['maction', 'maligngroup', 'malignmark', 'mlongdiv', 'mscarries', 'mscarry', 'msgroup', 'mstack', 'msline', 'msrow', 'semantics', 'annotation', 'annotation-xml', 'mprescripts', 'none']);\nconst text = freeze(['#text']);\n\nconst html = freeze(['accept', 'action', 'align', 'alt', 'autocapitalize', 'autocomplete', 'autopictureinpicture', 'autoplay', 'background', 'bgcolor', 'border', 'capture', 'cellpadding', 'cellspacing', 'checked', 'cite', 'class', 'clear', 'color', 'cols', 'colspan', 'controls', 'controlslist', 'coords', 'crossorigin', 'datetime', 'decoding', 'default', 'dir', 'disabled', 'disablepictureinpicture', 'disableremoteplayback', 'download', 'draggable', 'enctype', 'enterkeyhint', 'face', 'for', 'headers', 'height', 'hidden', 'high', 'href', 'hreflang', 'id', 'inputmode', 'integrity', 'ismap', 'kind', 'label', 'lang', 'list', 'loading', 'loop', 'low', 'max', 'maxlength', 'media', 'method', 'min', 'minlength', 'multiple', 'muted', 'name', 'nonce', 'noshade', 'novalidate', 'nowrap', 'open', 'optimum', 'pattern', 'placeholder', 'playsinline', 'popover', 'popovertarget', 'popovertargetaction', 'poster', 'preload', 'pubdate', 'radiogroup', 'readonly', 'rel', 'required', 'rev', 'reversed', 'role', 'rows', 'rowspan', 'spellcheck', 'scope', 'selected', 'shape', 'size', 'sizes', 'span', 'srclang', 'start', 'src', 'srcset', 'step', 'style', 'summary', 'tabindex', 'title', 'translate', 'type', 'usemap', 'valign', 'value', 'width', 'wrap', 'xmlns', 'slot']);\nconst svg = freeze(['accent-height', 'accumulate', 'additive', 'alignment-baseline', 'amplitude', 'ascent', 'attributename', 'attributetype', 'azimuth', 'basefrequency', 'baseline-shift', 'begin', 'bias', 'by', 'class', 'clip', 'clippathunits', 'clip-path', 'clip-rule', 'color', 'color-interpolation', 'color-interpolation-filters', 'color-profile', 'color-rendering', 'cx', 'cy', 'd', 'dx', 'dy', 'diffuseconstant', 'direction', 'display', 'divisor', 'dur', 'edgemode', 'elevation', 'end', 'exponent', 'fill', 'fill-opacity', 'fill-rule', 'filter', 'filterunits', 'flood-color', 'flood-opacity', 'font-family', 'font-size', 'font-size-adjust', 'font-stretch', 'font-style', 'font-variant', 'font-weight', 'fx', 'fy', 'g1', 'g2', 'glyph-name', 'glyphref', 'gradientunits', 'gradienttransform', 'height', 'href', 'id', 'image-rendering', 'in', 'in2', 'intercept', 'k', 'k1', 'k2', 'k3', 'k4', 'kerning', 'keypoints', 'keysplines', 'keytimes', 'lang', 'lengthadjust', 'letter-spacing', 'kernelmatrix', 'kernelunitlength', 'lighting-color', 'local', 'marker-end', 'marker-mid', 'marker-start', 'markerheight', 'markerunits', 'markerwidth', 'maskcontentunits', 'maskunits', 'max', 'mask', 'media', 'method', 'mode', 'min', 'name', 'numoctaves', 'offset', 'operator', 'opacity', 'order', 'orient', 'orientation', 'origin', 'overflow', 'paint-order', 'path', 'pathlength', 'patterncontentunits', 'patterntransform', 'patternunits', 'points', 'preservealpha', 'preserveaspectratio', 'primitiveunits', 'r', 'rx', 'ry', 'radius', 'refx', 'refy', 'repeatcount', 'repeatdur', 'restart', 'result', 'rotate', 'scale', 'seed', 'shape-rendering', 'slope', 'specularconstant', 'specularexponent', 'spreadmethod', 'startoffset', 'stddeviation', 'stitchtiles', 'stop-color', 'stop-opacity', 'stroke-dasharray', 'stroke-dashoffset', 'stroke-linecap', 'stroke-linejoin', 'stroke-miterlimit', 'stroke-opacity', 'stroke', 'stroke-width', 'style', 'surfacescale', 'systemlanguage', 'tabindex', 'tablevalues', 'targetx', 'targety', 'transform', 'transform-origin', 'text-anchor', 'text-decoration', 'text-rendering', 'textlength', 'type', 'u1', 'u2', 'unicode', 'values', 'viewbox', 'visibility', 'version', 'vert-adv-y', 'vert-origin-x', 'vert-origin-y', 'width', 'word-spacing', 'wrap', 'writing-mode', 'xchannelselector', 'ychannelselector', 'x', 'x1', 'x2', 'xmlns', 'y', 'y1', 'y2', 'z', 'zoomandpan']);\nconst mathMl = freeze(['accent', 'accentunder', 'align', 'bevelled', 'close', 'columnsalign', 'columnlines', 'columnspan', 'denomalign', 'depth', 'dir', 'display', 'displaystyle', 'encoding', 'fence', 'frame', 'height', 'href', 'id', 'largeop', 'length', 'linethickness', 'lspace', 'lquote', 'mathbackground', 'mathcolor', 'mathsize', 'mathvariant', 'maxsize', 'minsize', 'movablelimits', 'notation', 'numalign', 'open', 'rowalign', 'rowlines', 'rowspacing', 'rowspan', 'rspace', 'rquote', 'scriptlevel', 'scriptminsize', 'scriptsizemultiplier', 'selection', 'separator', 'separators', 'stretchy', 'subscriptshift', 'supscriptshift', 'symmetric', 'voffset', 'width', 'xmlns']);\nconst xml = freeze(['xlink:href', 'xml:id', 'xlink:title', 'xml:space', 'xmlns:xlink']);\n\n// eslint-disable-next-line unicorn/better-regex\nconst MUSTACHE_EXPR = seal(/\\{\\{[\\w\\W]*|[\\w\\W]*\\}\\}/gm); // Specify template detection regex for SAFE_FOR_TEMPLATES mode\nconst ERB_EXPR = seal(/<%[\\w\\W]*|[\\w\\W]*%>/gm);\nconst TMPLIT_EXPR = seal(/\\$\\{[\\w\\W]*/gm); // eslint-disable-line unicorn/better-regex\nconst DATA_ATTR = seal(/^data-[\\-\\w.\\u00B7-\\uFFFF]+$/); // eslint-disable-line no-useless-escape\nconst ARIA_ATTR = seal(/^aria-[\\-\\w]+$/); // eslint-disable-line no-useless-escape\nconst IS_ALLOWED_URI = seal(/^(?:(?:(?:f|ht)tps?|mailto|tel|callto|sms|cid|xmpp):|[^a-z]|[a-z+.\\-]+(?:[^a-z+.\\-:]|$))/i // eslint-disable-line no-useless-escape\n);\nconst IS_SCRIPT_OR_DATA = seal(/^(?:\\w+script|data):/i);\nconst ATTR_WHITESPACE = seal(/[\\u0000-\\u0020\\u00A0\\u1680\\u180E\\u2000-\\u2029\\u205F\\u3000]/g // eslint-disable-line no-control-regex\n);\nconst DOCTYPE_NAME = seal(/^html$/i);\nconst CUSTOM_ELEMENT = seal(/^[a-z][.\\w]*(-[.\\w]+)+$/i);\n\nvar EXPRESSIONS = /*#__PURE__*/Object.freeze({\n  __proto__: null,\n  ARIA_ATTR: ARIA_ATTR,\n  ATTR_WHITESPACE: ATTR_WHITESPACE,\n  CUSTOM_ELEMENT: CUSTOM_ELEMENT,\n  DATA_ATTR: DATA_ATTR,\n  DOCTYPE_NAME: DOCTYPE_NAME,\n  ERB_EXPR: ERB_EXPR,\n  IS_ALLOWED_URI: IS_ALLOWED_URI,\n  IS_SCRIPT_OR_DATA: IS_SCRIPT_OR_DATA,\n  MUSTACHE_EXPR: MUSTACHE_EXPR,\n  TMPLIT_EXPR: TMPLIT_EXPR\n});\n\n/* eslint-disable @typescript-eslint/indent */\n// https://developer.mozilla.org/en-US/docs/Web/API/Node/nodeType\nconst NODE_TYPE = {\n  element: 1,\n  attribute: 2,\n  text: 3,\n  cdataSection: 4,\n  entityReference: 5,\n  // Deprecated\n  entityNode: 6,\n  // Deprecated\n  progressingInstruction: 7,\n  comment: 8,\n  document: 9,\n  documentType: 10,\n  documentFragment: 11,\n  notation: 12 // Deprecated\n};\nconst getGlobal = function getGlobal() {\n  return typeof window === 'undefined' ? null : window;\n};\n/**\n * Creates a no-op policy for internal use only.\n * Don't export this function outside this module!\n * @param trustedTypes The policy factory.\n * @param purifyHostElement The Script element used to load DOMPurify (to determine policy name suffix).\n * @return The policy created (or null, if Trusted Types\n * are not supported or creating the policy failed).\n */\nconst _createTrustedTypesPolicy = function _createTrustedTypesPolicy(trustedTypes, purifyHostElement) {\n  if (typeof trustedTypes !== 'object' || typeof trustedTypes.createPolicy !== 'function') {\n    return null;\n  }\n  // Allow the callers to control the unique policy name\n  // by adding a data-tt-policy-suffix to the script element with the DOMPurify.\n  // Policy creation with duplicate names throws in Trusted Types.\n  let suffix = null;\n  const ATTR_NAME = 'data-tt-policy-suffix';\n  if (purifyHostElement && purifyHostElement.hasAttribute(ATTR_NAME)) {\n    suffix = purifyHostElement.getAttribute(ATTR_NAME);\n  }\n  const policyName = 'dompurify' + (suffix ? '#' + suffix : '');\n  try {\n    return trustedTypes.createPolicy(policyName, {\n      createHTML(html) {\n        return html;\n      },\n      createScriptURL(scriptUrl) {\n        return scriptUrl;\n      }\n    });\n  } catch (_) {\n    // Policy creation failed (most likely another DOMPurify script has\n    // already run). Skip creating the policy, as this will only cause errors\n    // if TT are enforced.\n    console.warn('TrustedTypes policy ' + policyName + ' could not be created.');\n    return null;\n  }\n};\nconst _createHooksMap = function _createHooksMap() {\n  return {\n    afterSanitizeAttributes: [],\n    afterSanitizeElements: [],\n    afterSanitizeShadowDOM: [],\n    beforeSanitizeAttributes: [],\n    beforeSanitizeElements: [],\n    beforeSanitizeShadowDOM: [],\n    uponSanitizeAttribute: [],\n    uponSanitizeElement: [],\n    uponSanitizeShadowNode: []\n  };\n};\nfunction createDOMPurify() {\n  let window = arguments.length > 0 && arguments[0] !== undefined ? arguments[0] : getGlobal();\n  const DOMPurify = root => createDOMPurify(root);\n  DOMPurify.version = '3.2.4';\n  DOMPurify.removed = [];\n  if (!window || !window.document || window.document.nodeType !== NODE_TYPE.document || !window.Element) {\n    // Not running in a browser, provide a factory function\n    // so that you can pass your own Window\n    DOMPurify.isSupported = false;\n    return DOMPurify;\n  }\n  let {\n    document\n  } = window;\n  const originalDocument = document;\n  const currentScript = originalDocument.currentScript;\n  const {\n    DocumentFragment,\n    HTMLTemplateElement,\n    Node,\n    Element,\n    NodeFilter,\n    NamedNodeMap = window.NamedNodeMap || window.MozNamedAttrMap,\n    HTMLFormElement,\n    DOMParser,\n    trustedTypes\n  } = window;\n  const ElementPrototype = Element.prototype;\n  const cloneNode = lookupGetter(ElementPrototype, 'cloneNode');\n  const remove = lookupGetter(ElementPrototype, 'remove');\n  const getNextSibling = lookupGetter(ElementPrototype, 'nextSibling');\n  const getChildNodes = lookupGetter(ElementPrototype, 'childNodes');\n  const getParentNode = lookupGetter(ElementPrototype, 'parentNode');\n  // As per issue #47, the web-components registry is inherited by a\n  // new document created via createHTMLDocument. As per the spec\n  // (http://w3c.github.io/webcomponents/spec/custom/#creating-and-passing-registries)\n  // a new empty registry is used when creating a template contents owner\n  // document, so we use that as our parent document to ensure nothing\n  // is inherited.\n  if (typeof HTMLTemplateElement === 'function') {\n    const template = document.createElement('template');\n    if (template.content && template.content.ownerDocument) {\n      document = template.content.ownerDocument;\n    }\n  }\n  let trustedTypesPolicy;\n  let emptyHTML = '';\n  const {\n    implementation,\n    createNodeIterator,\n    createDocumentFragment,\n    getElementsByTagName\n  } = document;\n  const {\n    importNode\n  } = originalDocument;\n  let hooks = _createHooksMap();\n  /**\n   * Expose whether this browser supports running the full DOMPurify.\n   */\n  DOMPurify.isSupported = typeof entries === 'function' && typeof getParentNode === 'function' && implementation && implementation.createHTMLDocument !== undefined;\n  const {\n    MUSTACHE_EXPR,\n    ERB_EXPR,\n    TMPLIT_EXPR,\n    DATA_ATTR,\n    ARIA_ATTR,\n    IS_SCRIPT_OR_DATA,\n    ATTR_WHITESPACE,\n    CUSTOM_ELEMENT\n  } = EXPRESSIONS;\n  let {\n    IS_ALLOWED_URI: IS_ALLOWED_URI$1\n  } = EXPRESSIONS;\n  /**\n   * We consider the elements and attributes below to be safe. Ideally\n   * don't add any new ones but feel free to remove unwanted ones.\n   */\n  /* allowed element names */\n  let ALLOWED_TAGS = null;\n  const DEFAULT_ALLOWED_TAGS = addToSet({}, [...html$1, ...svg$1, ...svgFilters, ...mathMl$1, ...text]);\n  /* Allowed attribute names */\n  let ALLOWED_ATTR = null;\n  const DEFAULT_ALLOWED_ATTR = addToSet({}, [...html, ...svg, ...mathMl, ...xml]);\n  /*\n   * Configure how DOMPurify should handle custom elements and their attributes as well as customized built-in elements.\n   * @property {RegExp|Function|null} tagNameCheck one of [null, regexPattern, predicate]. Default: `null` (disallow any custom elements)\n   * @property {RegExp|Function|null} attributeNameCheck one of [null, regexPattern, predicate]. Default: `null` (disallow any attributes not on the allow list)\n   * @property {boolean} allowCustomizedBuiltInElements allow custom elements derived from built-ins if they pass CUSTOM_ELEMENT_HANDLING.tagNameCheck. Default: `false`.\n   */\n  let CUSTOM_ELEMENT_HANDLING = Object.seal(create(null, {\n    tagNameCheck: {\n      writable: true,\n      configurable: false,\n      enumerable: true,\n      value: null\n    },\n    attributeNameCheck: {\n      writable: true,\n      configurable: false,\n      enumerable: true,\n      value: null\n    },\n    allowCustomizedBuiltInElements: {\n      writable: true,\n      configurable: false,\n      enumerable: true,\n      value: false\n    }\n  }));\n  /* Explicitly forbidden tags (overrides ALLOWED_TAGS/ADD_TAGS) */\n  let FORBID_TAGS = null;\n  /* Explicitly forbidden attributes (overrides ALLOWED_ATTR/ADD_ATTR) */\n  let FORBID_ATTR = null;\n  /* Decide if ARIA attributes are okay */\n  let ALLOW_ARIA_ATTR = true;\n  /* Decide if custom data attributes are okay */\n  let ALLOW_DATA_ATTR = true;\n  /* Decide if unknown protocols are okay */\n  let ALLOW_UNKNOWN_PROTOCOLS = false;\n  /* Decide if self-closing tags in attributes are allowed.\n   * Usually removed due to a mXSS issue in jQuery 3.0 */\n  let ALLOW_SELF_CLOSE_IN_ATTR = true;\n  /* Output should be safe for common template engines.\n   * This means, DOMPurify removes data attributes, mustaches and ERB\n   */\n  let SAFE_FOR_TEMPLATES = false;\n  /* Output should be safe even for XML used within HTML and alike.\n   * This means, DOMPurify removes comments when containing risky content.\n   */\n  let SAFE_FOR_XML = true;\n  /* Decide if document with <html>... should be returned */\n  let WHOLE_DOCUMENT = false;\n  /* Track whether config is already set on this instance of DOMPurify. */\n  let SET_CONFIG = false;\n  /* Decide if all elements (e.g. style, script) must be children of\n   * document.body. By default, browsers might move them to document.head */\n  let FORCE_BODY = false;\n  /* Decide if a DOM `HTMLBodyElement` should be returned, instead of a html\n   * string (or a TrustedHTML object if Trusted Types are supported).\n   * If `WHOLE_DOCUMENT` is enabled a `HTMLHtmlElement` will be returned instead\n   */\n  let RETURN_DOM = false;\n  /* Decide if a DOM `DocumentFragment` should be returned, instead of a html\n   * string  (or a TrustedHTML object if Trusted Types are supported) */\n  let RETURN_DOM_FRAGMENT = false;\n  /* Try to return a Trusted Type object instead of a string, return a string in\n   * case Trusted Types are not supported  */\n  let RETURN_TRUSTED_TYPE = false;\n  /* Output should be free from DOM clobbering attacks?\n   * This sanitizes markups named with colliding, clobberable built-in DOM APIs.\n   */\n  let SANITIZE_DOM = true;\n  /* Achieve full DOM Clobbering protection by isolating the namespace of named\n   * properties and JS variables, mitigating attacks that abuse the HTML/DOM spec rules.\n   *\n   * HTML/DOM spec rules that enable DOM Clobbering:\n   *   - Named Access on Window (\u00A77.3.3)\n   *   - DOM Tree Accessors (\u00A73.1.5)\n   *   - Form Element Parent-Child Relations (\u00A74.10.3)\n   *   - Iframe srcdoc / Nested WindowProxies (\u00A74.8.5)\n   *   - HTMLCollection (\u00A74.2.10.2)\n   *\n   * Namespace isolation is implemented by prefixing `id` and `name` attributes\n   * with a constant string, i.e., `user-content-`\n   */\n  let SANITIZE_NAMED_PROPS = false;\n  const SANITIZE_NAMED_PROPS_PREFIX = 'user-content-';\n  /* Keep element content when removing element? */\n  let KEEP_CONTENT = true;\n  /* If a `Node` is passed to sanitize(), then performs sanitization in-place instead\n   * of importing it into a new Document and returning a sanitized copy */\n  let IN_PLACE = false;\n  /* Allow usage of profiles like html, svg and mathMl */\n  let USE_PROFILES = {};\n  /* Tags to ignore content of when KEEP_CONTENT is true */\n  let FORBID_CONTENTS = null;\n  const DEFAULT_FORBID_CONTENTS = addToSet({}, ['annotation-xml', 'audio', 'colgroup', 'desc', 'foreignobject', 'head', 'iframe', 'math', 'mi', 'mn', 'mo', 'ms', 'mtext', 'noembed', 'noframes', 'noscript', 'plaintext', 'script', 'style', 'svg', 'template', 'thead', 'title', 'video', 'xmp']);\n  /* Tags that are safe for data: URIs */\n  let DATA_URI_TAGS = null;\n  const DEFAULT_DATA_URI_TAGS = addToSet({}, ['audio', 'video', 'img', 'source', 'image', 'track']);\n  /* Attributes safe for values like \"javascript:\" */\n  let URI_SAFE_ATTRIBUTES = null;\n  const DEFAULT_URI_SAFE_ATTRIBUTES = addToSet({}, ['alt', 'class', 'for', 'id', 'label', 'name', 'pattern', 'placeholder', 'role', 'summary', 'title', 'value', 'style', 'xmlns']);\n  const MATHML_NAMESPACE = 'http://www.w3.org/1998/Math/MathML';\n  const SVG_NAMESPACE = 'http://www.w3.org/2000/svg';\n  const HTML_NAMESPACE = 'http://www.w3.org/1999/xhtml';\n  /* Document namespace */\n  let NAMESPACE = HTML_NAMESPACE;\n  let IS_EMPTY_INPUT = false;\n  /* Allowed XHTML+XML namespaces */\n  let ALLOWED_NAMESPACES = null;\n  const DEFAULT_ALLOWED_NAMESPACES = addToSet({}, [MATHML_NAMESPACE, SVG_NAMESPACE, HTML_NAMESPACE], stringToString);\n  let MATHML_TEXT_INTEGRATION_POINTS = addToSet({}, ['mi', 'mo', 'mn', 'ms', 'mtext']);\n  let HTML_INTEGRATION_POINTS = addToSet({}, ['annotation-xml']);\n  // Certain elements are allowed in both SVG and HTML\n  // namespace. We need to specify them explicitly\n  // so that they don't get erroneously deleted from\n  // HTML namespace.\n  const COMMON_SVG_AND_HTML_ELEMENTS = addToSet({}, ['title', 'style', 'font', 'a', 'script']);\n  /* Parsing of strict XHTML documents */\n  let PARSER_MEDIA_TYPE = null;\n  const SUPPORTED_PARSER_MEDIA_TYPES = ['application/xhtml+xml', 'text/html'];\n  const DEFAULT_PARSER_MEDIA_TYPE = 'text/html';\n  let transformCaseFunc = null;\n  /* Keep a reference to config to pass to hooks */\n  let CONFIG = null;\n  /* Ideally, do not touch anything below this line */\n  /* ______________________________________________ */\n  const formElement = document.createElement('form');\n  const isRegexOrFunction = function isRegexOrFunction(testValue) {\n    return testValue instanceof RegExp || testValue instanceof Function;\n  };\n  /**\n   * _parseConfig\n   *\n   * @param cfg optional config literal\n   */\n  // eslint-disable-next-line complexity\n  const _parseConfig = function _parseConfig() {\n    let cfg = arguments.length > 0 && arguments[0] !== undefined ? arguments[0] : {};\n    if (CONFIG && CONFIG === cfg) {\n      return;\n    }\n    /* Shield configuration object from tampering */\n    if (!cfg || typeof cfg !== 'object') {\n      cfg = {};\n    }\n    /* Shield configuration object from prototype pollution */\n    cfg = clone(cfg);\n    PARSER_MEDIA_TYPE =\n    // eslint-disable-next-line unicorn/prefer-includes\n    SUPPORTED_PARSER_MEDIA_TYPES.indexOf(cfg.PARSER_MEDIA_TYPE) === -1 ? DEFAULT_PARSER_MEDIA_TYPE : cfg.PARSER_MEDIA_TYPE;\n    // HTML tags and attributes are not case-sensitive, converting to lowercase. Keeping XHTML as is.\n    transformCaseFunc = PARSER_MEDIA_TYPE === 'application/xhtml+xml' ? stringToString : stringToLowerCase;\n    /* Set configuration parameters */\n    ALLOWED_TAGS = objectHasOwnProperty(cfg, 'ALLOWED_TAGS') ? addToSet({}, cfg.ALLOWED_TAGS, transformCaseFunc) : DEFAULT_ALLOWED_TAGS;\n    ALLOWED_ATTR = objectHasOwnProperty(cfg, 'ALLOWED_ATTR') ? addToSet({}, cfg.ALLOWED_ATTR, transformCaseFunc) : DEFAULT_ALLOWED_ATTR;\n    ALLOWED_NAMESPACES = objectHasOwnProperty(cfg, 'ALLOWED_NAMESPACES') ? addToSet({}, cfg.ALLOWED_NAMESPACES, stringToString) : DEFAULT_ALLOWED_NAMESPACES;\n    URI_SAFE_ATTRIBUTES = objectHasOwnProperty(cfg, 'ADD_URI_SAFE_ATTR') ? addToSet(clone(DEFAULT_URI_SAFE_ATTRIBUTES), cfg.ADD_URI_SAFE_ATTR, transformCaseFunc) : DEFAULT_URI_SAFE_ATTRIBUTES;\n    DATA_URI_TAGS = objectHasOwnProperty(cfg, 'ADD_DATA_URI_TAGS') ? addToSet(clone(DEFAULT_DATA_URI_TAGS), cfg.ADD_DATA_URI_TAGS, transformCaseFunc) : DEFAULT_DATA_URI_TAGS;\n    FORBID_CONTENTS = objectHasOwnProperty(cfg, 'FORBID_CONTENTS') ? addToSet({}, cfg.FORBID_CONTENTS, transformCaseFunc) : DEFAULT_FORBID_CONTENTS;\n    FORBID_TAGS = objectHasOwnProperty(cfg, 'FORBID_TAGS') ? addToSet({}, cfg.FORBID_TAGS, transformCaseFunc) : {};\n    FORBID_ATTR = objectHasOwnProperty(cfg, 'FORBID_ATTR') ? addToSet({}, cfg.FORBID_ATTR, transformCaseFunc) : {};\n    USE_PROFILES = objectHasOwnProperty(cfg, 'USE_PROFILES') ? cfg.USE_PROFILES : false;\n    ALLOW_ARIA_ATTR = cfg.ALLOW_ARIA_ATTR !== false; // Default true\n    ALLOW_DATA_ATTR = cfg.ALLOW_DATA_ATTR !== false; // Default true\n    ALLOW_UNKNOWN_PROTOCOLS = cfg.ALLOW_UNKNOWN_PROTOCOLS || false; // Default false\n    ALLOW_SELF_CLOSE_IN_ATTR = cfg.ALLOW_SELF_CLOSE_IN_ATTR !== false; // Default true\n    SAFE_FOR_TEMPLATES = cfg.SAFE_FOR_TEMPLATES || false; // Default false\n    SAFE_FOR_XML = cfg.SAFE_FOR_XML !== false; // Default true\n    WHOLE_DOCUMENT = cfg.WHOLE_DOCUMENT || false; // Default false\n    RETURN_DOM = cfg.RETURN_DOM || false; // Default false\n    RETURN_DOM_FRAGMENT = cfg.RETURN_DOM_FRAGMENT || false; // Default false\n    RETURN_TRUSTED_TYPE = cfg.RETURN_TRUSTED_TYPE || false; // Default false\n    FORCE_BODY = cfg.FORCE_BODY || false; // Default false\n    SANITIZE_DOM = cfg.SANITIZE_DOM !== false; // Default true\n    SANITIZE_NAMED_PROPS = cfg.SANITIZE_NAMED_PROPS || false; // Default false\n    KEEP_CONTENT = cfg.KEEP_CONTENT !== false; // Default true\n    IN_PLACE = cfg.IN_PLACE || false; // Default false\n    IS_ALLOWED_URI$1 = cfg.ALLOWED_URI_REGEXP || IS_ALLOWED_URI;\n    NAMESPACE = cfg.NAMESPACE || HTML_NAMESPACE;\n    MATHML_TEXT_INTEGRATION_POINTS = cfg.MATHML_TEXT_INTEGRATION_POINTS || MATHML_TEXT_INTEGRATION_POINTS;\n    HTML_INTEGRATION_POINTS = cfg.HTML_INTEGRATION_POINTS || HTML_INTEGRATION_POINTS;\n    CUSTOM_ELEMENT_HANDLING = cfg.CUSTOM_ELEMENT_HANDLING || {};\n    if (cfg.CUSTOM_ELEMENT_HANDLING && isRegexOrFunction(cfg.CUSTOM_ELEMENT_HANDLING.tagNameCheck)) {\n      CUSTOM_ELEMENT_HANDLING.tagNameCheck = cfg.CUSTOM_ELEMENT_HANDLING.tagNameCheck;\n    }\n    if (cfg.CUSTOM_ELEMENT_HANDLING && isRegexOrFunction(cfg.CUSTOM_ELEMENT_HANDLING.attributeNameCheck)) {\n      CUSTOM_ELEMENT_HANDLING.attributeNameCheck = cfg.CUSTOM_ELEMENT_HANDLING.attributeNameCheck;\n    }\n    if (cfg.CUSTOM_ELEMENT_HANDLING && typeof cfg.CUSTOM_ELEMENT_HANDLING.allowCustomizedBuiltInElements === 'boolean') {\n      CUSTOM_ELEMENT_HANDLING.allowCustomizedBuiltInElements = cfg.CUSTOM_ELEMENT_HANDLING.allowCustomizedBuiltInElements;\n    }\n    if (SAFE_FOR_TEMPLATES) {\n      ALLOW_DATA_ATTR = false;\n    }\n    if (RETURN_DOM_FRAGMENT) {\n      RETURN_DOM = true;\n    }\n    /* Parse profile info */\n    if (USE_PROFILES) {\n      ALLOWED_TAGS = addToSet({}, text);\n      ALLOWED_ATTR = [];\n      if (USE_PROFILES.html === true) {\n        addToSet(ALLOWED_TAGS, html$1);\n        addToSet(ALLOWED_ATTR, html);\n      }\n      if (USE_PROFILES.svg === true) {\n        addToSet(ALLOWED_TAGS, svg$1);\n        addToSet(ALLOWED_ATTR, svg);\n        addToSet(ALLOWED_ATTR, xml);\n      }\n      if (USE_PROFILES.svgFilters === true) {\n        addToSet(ALLOWED_TAGS, svgFilters);\n        addToSet(ALLOWED_ATTR, svg);\n        addToSet(ALLOWED_ATTR, xml);\n      }\n      if (USE_PROFILES.mathMl === true) {\n        addToSet(ALLOWED_TAGS, mathMl$1);\n        addToSet(ALLOWED_ATTR, mathMl);\n        addToSet(ALLOWED_ATTR, xml);\n      }\n    }\n    /* Merge configuration parameters */\n    if (cfg.ADD_TAGS) {\n      if (ALLOWED_TAGS === DEFAULT_ALLOWED_TAGS) {\n        ALLOWED_TAGS = clone(ALLOWED_TAGS);\n      }\n      addToSet(ALLOWED_TAGS, cfg.ADD_TAGS, transformCaseFunc);\n    }\n    if (cfg.ADD_ATTR) {\n      if (ALLOWED_ATTR === DEFAULT_ALLOWED_ATTR) {\n        ALLOWED_ATTR = clone(ALLOWED_ATTR);\n      }\n      addToSet(ALLOWED_ATTR, cfg.ADD_ATTR, transformCaseFunc);\n    }\n    if (cfg.ADD_URI_SAFE_ATTR) {\n      addToSet(URI_SAFE_ATTRIBUTES, cfg.ADD_URI_SAFE_ATTR, transformCaseFunc);\n    }\n    if (cfg.FORBID_CONTENTS) {\n      if (FORBID_CONTENTS === DEFAULT_FORBID_CONTENTS) {\n        FORBID_CONTENTS = clone(FORBID_CONTENTS);\n      }\n      addToSet(FORBID_CONTENTS, cfg.FORBID_CONTENTS, transformCaseFunc);\n    }\n    /* Add #text in case KEEP_CONTENT is set to true */\n    if (KEEP_CONTENT) {\n      ALLOWED_TAGS['#text'] = true;\n    }\n    /* Add html, head and body to ALLOWED_TAGS in case WHOLE_DOCUMENT is true */\n    if (WHOLE_DOCUMENT) {\n      addToSet(ALLOWED_TAGS, ['html', 'head', 'body']);\n    }\n    /* Add tbody to ALLOWED_TAGS in case tables are permitted, see #286, #365 */\n    if (ALLOWED_TAGS.table) {\n      addToSet(ALLOWED_TAGS, ['tbody']);\n      delete FORBID_TAGS.tbody;\n    }\n    if (cfg.TRUSTED_TYPES_POLICY) {\n      if (typeof cfg.TRUSTED_TYPES_POLICY.createHTML !== 'function') {\n        throw typeErrorCreate('TRUSTED_TYPES_POLICY configuration option must provide a \"createHTML\" hook.');\n      }\n      if (typeof cfg.TRUSTED_TYPES_POLICY.createScriptURL !== 'function') {\n        throw typeErrorCreate('TRUSTED_TYPES_POLICY configuration option must provide a \"createScriptURL\" hook.');\n      }\n      // Overwrite existing TrustedTypes policy.\n      trustedTypesPolicy = cfg.TRUSTED_TYPES_POLICY;\n      // Sign local variables required by `sanitize`.\n      emptyHTML = trustedTypesPolicy.createHTML('');\n    } else {\n      // Uninitialized policy, attempt to initialize the internal dompurify policy.\n      if (trustedTypesPolicy === undefined) {\n        trustedTypesPolicy = _createTrustedTypesPolicy(trustedTypes, currentScript);\n      }\n      // If creating the internal policy succeeded sign internal variables.\n      if (trustedTypesPolicy !== null && typeof emptyHTML === 'string') {\n        emptyHTML = trustedTypesPolicy.createHTML('');\n      }\n    }\n    // Prevent further manipulation of configuration.\n    // Not available in IE8, Safari 5, etc.\n    if (freeze) {\n      freeze(cfg);\n    }\n    CONFIG = cfg;\n  };\n  /* Keep track of all possible SVG and MathML tags\n   * so that we can perform the namespace checks\n   * correctly. */\n  const ALL_SVG_TAGS = addToSet({}, [...svg$1, ...svgFilters, ...svgDisallowed]);\n  const ALL_MATHML_TAGS = addToSet({}, [...mathMl$1, ...mathMlDisallowed]);\n  /**\n   * @param element a DOM element whose namespace is being checked\n   * @returns Return false if the element has a\n   *  namespace that a spec-compliant parser would never\n   *  return. Return true otherwise.\n   */\n  const _checkValidNamespace = function _checkValidNamespace(element) {\n    let parent = getParentNode(element);\n    // In JSDOM, if we're inside shadow DOM, then parentNode\n    // can be null. We just simulate parent in this case.\n    if (!parent || !parent.tagName) {\n      parent = {\n        namespaceURI: NAMESPACE,\n        tagName: 'template'\n      };\n    }\n    const tagName = stringToLowerCase(element.tagName);\n    const parentTagName = stringToLowerCase(parent.tagName);\n    if (!ALLOWED_NAMESPACES[element.namespaceURI]) {\n      return false;\n    }\n    if (element.namespaceURI === SVG_NAMESPACE) {\n      // The only way to switch from HTML namespace to SVG\n      // is via <svg>. If it happens via any other tag, then\n      // it should be killed.\n      if (parent.namespaceURI === HTML_NAMESPACE) {\n        return tagName === 'svg';\n      }\n      // The only way to switch from MathML to SVG is via`\n      // svg if parent is either <annotation-xml> or MathML\n      // text integration points.\n      if (parent.namespaceURI === MATHML_NAMESPACE) {\n        return tagName === 'svg' && (parentTagName === 'annotation-xml' || MATHML_TEXT_INTEGRATION_POINTS[parentTagName]);\n      }\n      // We only allow elements that are defined in SVG\n      // spec. All others are disallowed in SVG namespace.\n      return Boolean(ALL_SVG_TAGS[tagName]);\n    }\n    if (element.namespaceURI === MATHML_NAMESPACE) {\n      // The only way to switch from HTML namespace to MathML\n      // is via <math>. If it happens via any other tag, then\n      // it should be killed.\n      if (parent.namespaceURI === HTML_NAMESPACE) {\n        return tagName === 'math';\n      }\n      // The only way to switch from SVG to MathML is via\n      // <math> and HTML integration points\n      if (parent.namespaceURI === SVG_NAMESPACE) {\n        return tagName === 'math' && HTML_INTEGRATION_POINTS[parentTagName];\n      }\n      // We only allow elements that are defined in MathML\n      // spec. All others are disallowed in MathML namespace.\n      return Boolean(ALL_MATHML_TAGS[tagName]);\n    }\n    if (element.namespaceURI === HTML_NAMESPACE) {\n      // The only way to switch from SVG to HTML is via\n      // HTML integration points, and from MathML to HTML\n      // is via MathML text integration points\n      if (parent.namespaceURI === SVG_NAMESPACE && !HTML_INTEGRATION_POINTS[parentTagName]) {\n        return false;\n      }\n      if (parent.namespaceURI === MATHML_NAMESPACE && !MATHML_TEXT_INTEGRATION_POINTS[parentTagName]) {\n        return false;\n      }\n      // We disallow tags that are specific for MathML\n      // or SVG and should never appear in HTML namespace\n      return !ALL_MATHML_TAGS[tagName] && (COMMON_SVG_AND_HTML_ELEMENTS[tagName] || !ALL_SVG_TAGS[tagName]);\n    }\n    // For XHTML and XML documents that support custom namespaces\n    if (PARSER_MEDIA_TYPE === 'application/xhtml+xml' && ALLOWED_NAMESPACES[element.namespaceURI]) {\n      return true;\n    }\n    // The code should never reach this place (this means\n    // that the element somehow got namespace that is not\n    // HTML, SVG, MathML or allowed via ALLOWED_NAMESPACES).\n    // Return false just in case.\n    return false;\n  };\n  /**\n   * _forceRemove\n   *\n   * @param node a DOM node\n   */\n  const _forceRemove = function _forceRemove(node) {\n    arrayPush(DOMPurify.removed, {\n      element: node\n    });\n    try {\n      // eslint-disable-next-line unicorn/prefer-dom-node-remove\n      getParentNode(node).removeChild(node);\n    } catch (_) {\n      remove(node);\n    }\n  };\n  /**\n   * _removeAttribute\n   *\n   * @param name an Attribute name\n   * @param element a DOM node\n   */\n  const _removeAttribute = function _removeAttribute(name, element) {\n    try {\n      arrayPush(DOMPurify.removed, {\n        attribute: element.getAttributeNode(name),\n        from: element\n      });\n    } catch (_) {\n      arrayPush(DOMPurify.removed, {\n        attribute: null,\n        from: element\n      });\n    }\n    element.removeAttribute(name);\n    // We void attribute values for unremovable \"is\" attributes\n    if (name === 'is') {\n      if (RETURN_DOM || RETURN_DOM_FRAGMENT) {\n        try {\n          _forceRemove(element);\n        } catch (_) {}\n      } else {\n        try {\n          element.setAttribute(name, '');\n        } catch (_) {}\n      }\n    }\n  };\n  /**\n   * _initDocument\n   *\n   * @param dirty - a string of dirty markup\n   * @return a DOM, filled with the dirty markup\n   */\n  const _initDocument = function _initDocument(dirty) {\n    /* Create a HTML document */\n    let doc = null;\n    let leadingWhitespace = null;\n    if (FORCE_BODY) {\n      dirty = '<remove></remove>' + dirty;\n    } else {\n      /* If FORCE_BODY isn't used, leading whitespace needs to be preserved manually */\n      const matches = stringMatch(dirty, /^[\\r\\n\\t ]+/);\n      leadingWhitespace = matches && matches[0];\n    }\n    if (PARSER_MEDIA_TYPE === 'application/xhtml+xml' && NAMESPACE === HTML_NAMESPACE) {\n      // Root of XHTML doc must contain xmlns declaration (see https://www.w3.org/TR/xhtml1/normative.html#strict)\n      dirty = '<html xmlns=\"http://www.w3.org/1999/xhtml\"><head></head><body>' + dirty + '</body></html>';\n    }\n    const dirtyPayload = trustedTypesPolicy ? trustedTypesPolicy.createHTML(dirty) : dirty;\n    /*\n     * Use the DOMParser API by default, fallback later if needs be\n     * DOMParser not work for svg when has multiple root element.\n     */\n    if (NAMESPACE === HTML_NAMESPACE) {\n      try {\n        doc = new DOMParser().parseFromString(dirtyPayload, PARSER_MEDIA_TYPE);\n      } catch (_) {}\n    }\n    /* Use createHTMLDocument in case DOMParser is not available */\n    if (!doc || !doc.documentElement) {\n      doc = implementation.createDocument(NAMESPACE, 'template', null);\n      try {\n        doc.documentElement.innerHTML = IS_EMPTY_INPUT ? emptyHTML : dirtyPayload;\n      } catch (_) {\n        // Syntax error if dirtyPayload is invalid xml\n      }\n    }\n    const body = doc.body || doc.documentElement;\n    if (dirty && leadingWhitespace) {\n      body.insertBefore(document.createTextNode(leadingWhitespace), body.childNodes[0] || null);\n    }\n    /* Work on whole document or just its body */\n    if (NAMESPACE === HTML_NAMESPACE) {\n      return getElementsByTagName.call(doc, WHOLE_DOCUMENT ? 'html' : 'body')[0];\n    }\n    return WHOLE_DOCUMENT ? doc.documentElement : body;\n  };\n  /**\n   * Creates a NodeIterator object that you can use to traverse filtered lists of nodes or elements in a document.\n   *\n   * @param root The root element or node to start traversing on.\n   * @return The created NodeIterator\n   */\n  const _createNodeIterator = function _createNodeIterator(root) {\n    return createNodeIterator.call(root.ownerDocument || root, root,\n    // eslint-disable-next-line no-bitwise\n    NodeFilter.SHOW_ELEMENT | NodeFilter.SHOW_COMMENT | NodeFilter.SHOW_TEXT | NodeFilter.SHOW_PROCESSING_INSTRUCTION | NodeFilter.SHOW_CDATA_SECTION, null);\n  };\n  /**\n   * _isClobbered\n   *\n   * @param element element to check for clobbering attacks\n   * @return true if clobbered, false if safe\n   */\n  const _isClobbered = function _isClobbered(element) {\n    return element instanceof HTMLFormElement && (typeof element.nodeName !== 'string' || typeof element.textContent !== 'string' || typeof element.removeChild !== 'function' || !(element.attributes instanceof NamedNodeMap) || typeof element.removeAttribute !== 'function' || typeof element.setAttribute !== 'function' || typeof element.namespaceURI !== 'string' || typeof element.insertBefore !== 'function' || typeof element.hasChildNodes !== 'function');\n  };\n  /**\n   * Checks whether the given object is a DOM node.\n   *\n   * @param value object to check whether it's a DOM node\n   * @return true is object is a DOM node\n   */\n  const _isNode = function _isNode(value) {\n    return typeof Node === 'function' && value instanceof Node;\n  };\n  function _executeHooks(hooks, currentNode, data) {\n    arrayForEach(hooks, hook => {\n      hook.call(DOMPurify, currentNode, data, CONFIG);\n    });\n  }\n  /**\n   * _sanitizeElements\n   *\n   * @protect nodeName\n   * @protect textContent\n   * @protect removeChild\n   * @param currentNode to check for permission to exist\n   * @return true if node was killed, false if left alive\n   */\n  const _sanitizeElements = function _sanitizeElements(currentNode) {\n    let content = null;\n    /* Execute a hook if present */\n    _executeHooks(hooks.beforeSanitizeElements, currentNode, null);\n    /* Check if element is clobbered or can clobber */\n    if (_isClobbered(currentNode)) {\n      _forceRemove(currentNode);\n      return true;\n    }\n    /* Now let's check the element's type and name */\n    const tagName = transformCaseFunc(currentNode.nodeName);\n    /* Execute a hook if present */\n    _executeHooks(hooks.uponSanitizeElement, currentNode, {\n      tagName,\n      allowedTags: ALLOWED_TAGS\n    });\n    /* Detect mXSS attempts abusing namespace confusion */\n    if (currentNode.hasChildNodes() && !_isNode(currentNode.firstElementChild) && regExpTest(/<[/\\w]/g, currentNode.innerHTML) && regExpTest(/<[/\\w]/g, currentNode.textContent)) {\n      _forceRemove(currentNode);\n      return true;\n    }\n    /* Remove any occurrence of processing instructions */\n    if (currentNode.nodeType === NODE_TYPE.progressingInstruction) {\n      _forceRemove(currentNode);\n      return true;\n    }\n    /* Remove any kind of possibly harmful comments */\n    if (SAFE_FOR_XML && currentNode.nodeType === NODE_TYPE.comment && regExpTest(/<[/\\w]/g, currentNode.data)) {\n      _forceRemove(currentNode);\n      return true;\n    }\n    /* Remove element if anything forbids its presence */\n    if (!ALLOWED_TAGS[tagName] || FORBID_TAGS[tagName]) {\n      /* Check if we have a custom element to handle */\n      if (!FORBID_TAGS[tagName] && _isBasicCustomElement(tagName)) {\n        if (CUSTOM_ELEMENT_HANDLING.tagNameCheck instanceof RegExp && regExpTest(CUSTOM_ELEMENT_HANDLING.tagNameCheck, tagName)) {\n          return false;\n        }\n        if (CUSTOM_ELEMENT_HANDLING.tagNameCheck instanceof Function && CUSTOM_ELEMENT_HANDLING.tagNameCheck(tagName)) {\n          return false;\n        }\n      }\n      /* Keep content except for bad-listed elements */\n      if (KEEP_CONTENT && !FORBID_CONTENTS[tagName]) {\n        const parentNode = getParentNode(currentNode) || currentNode.parentNode;\n        const childNodes = getChildNodes(currentNode) || currentNode.childNodes;\n        if (childNodes && parentNode) {\n          const childCount = childNodes.length;\n          for (let i = childCount - 1; i >= 0; --i) {\n            const childClone = cloneNode(childNodes[i], true);\n            childClone.__removalCount = (currentNode.__removalCount || 0) + 1;\n            parentNode.insertBefore(childClone, getNextSibling(currentNode));\n          }\n        }\n      }\n      _forceRemove(currentNode);\n      return true;\n    }\n    /* Check whether element has a valid namespace */\n    if (currentNode instanceof Element && !_checkValidNamespace(currentNode)) {\n      _forceRemove(currentNode);\n      return true;\n    }\n    /* Make sure that older browsers don't get fallback-tag mXSS */\n    if ((tagName === 'noscript' || tagName === 'noembed' || tagName === 'noframes') && regExpTest(/<\\/no(script|embed|frames)/i, currentNode.innerHTML)) {\n      _forceRemove(currentNode);\n      return true;\n    }\n    /* Sanitize element content to be template-safe */\n    if (SAFE_FOR_TEMPLATES && currentNode.nodeType === NODE_TYPE.text) {\n      /* Get the element's text content */\n      content = currentNode.textContent;\n      arrayForEach([MUSTACHE_EXPR, ERB_EXPR, TMPLIT_EXPR], expr => {\n        content = stringReplace(content, expr, ' ');\n      });\n      if (currentNode.textContent !== content) {\n        arrayPush(DOMPurify.removed, {\n          element: currentNode.cloneNode()\n        });\n        currentNode.textContent = content;\n      }\n    }\n    /* Execute a hook if present */\n    _executeHooks(hooks.afterSanitizeElements, currentNode, null);\n    return false;\n  };\n  /**\n   * _isValidAttribute\n   *\n   * @param lcTag Lowercase tag name of containing element.\n   * @param lcName Lowercase attribute name.\n   * @param value Attribute value.\n   * @return Returns true if `value` is valid, otherwise false.\n   */\n  // eslint-disable-next-line complexity\n  const _isValidAttribute = function _isValidAttribute(lcTag, lcName, value) {\n    /* Make sure attribute cannot clobber */\n    if (SANITIZE_DOM && (lcName === 'id' || lcName === 'name') && (value in document || value in formElement)) {\n      return false;\n    }\n    /* Allow valid data-* attributes: At least one character after \"-\"\n        (https://html.spec.whatwg.org/multipage/dom.html#embedding-custom-non-visible-data-with-the-data-*-attributes)\n        XML-compatible (https://html.spec.whatwg.org/multipage/infrastructure.html#xml-compatible and http://www.w3.org/TR/xml/#d0e804)\n        We don't need to check the value; it's always URI safe. */\n    if (ALLOW_DATA_ATTR && !FORBID_ATTR[lcName] && regExpTest(DATA_ATTR, lcName)) ; else if (ALLOW_ARIA_ATTR && regExpTest(ARIA_ATTR, lcName)) ; else if (!ALLOWED_ATTR[lcName] || FORBID_ATTR[lcName]) {\n      if (\n      // First condition does a very basic check if a) it's basically a valid custom element tagname AND\n      // b) if the tagName passes whatever the user has configured for CUSTOM_ELEMENT_HANDLING.tagNameCheck\n      // and c) if the attribute name passes whatever the user has configured for CUSTOM_ELEMENT_HANDLING.attributeNameCheck\n      _isBasicCustomElement(lcTag) && (CUSTOM_ELEMENT_HANDLING.tagNameCheck instanceof RegExp && regExpTest(CUSTOM_ELEMENT_HANDLING.tagNameCheck, lcTag) || CUSTOM_ELEMENT_HANDLING.tagNameCheck instanceof Function && CUSTOM_ELEMENT_HANDLING.tagNameCheck(lcTag)) && (CUSTOM_ELEMENT_HANDLING.attributeNameCheck instanceof RegExp && regExpTest(CUSTOM_ELEMENT_HANDLING.attributeNameCheck, lcName) || CUSTOM_ELEMENT_HANDLING.attributeNameCheck instanceof Function && CUSTOM_ELEMENT_HANDLING.attributeNameCheck(lcName)) ||\n      // Alternative, second condition checks if it's an `is`-attribute, AND\n      // the value passes whatever the user has configured for CUSTOM_ELEMENT_HANDLING.tagNameCheck\n      lcName === 'is' && CUSTOM_ELEMENT_HANDLING.allowCustomizedBuiltInElements && (CUSTOM_ELEMENT_HANDLING.tagNameCheck instanceof RegExp && regExpTest(CUSTOM_ELEMENT_HANDLING.tagNameCheck, value) || CUSTOM_ELEMENT_HANDLING.tagNameCheck instanceof Function && CUSTOM_ELEMENT_HANDLING.tagNameCheck(value))) ; else {\n        return false;\n      }\n      /* Check value is safe. First, is attr inert? If so, is safe */\n    } else if (URI_SAFE_ATTRIBUTES[lcName]) ; else if (regExpTest(IS_ALLOWED_URI$1, stringReplace(value, ATTR_WHITESPACE, ''))) ; else if ((lcName === 'src' || lcName === 'xlink:href' || lcName === 'href') && lcTag !== 'script' && stringIndexOf(value, 'data:') === 0 && DATA_URI_TAGS[lcTag]) ; else if (ALLOW_UNKNOWN_PROTOCOLS && !regExpTest(IS_SCRIPT_OR_DATA, stringReplace(value, ATTR_WHITESPACE, ''))) ; else if (value) {\n      return false;\n    } else ;\n    return true;\n  };\n  /**\n   * _isBasicCustomElement\n   * checks if at least one dash is included in tagName, and it's not the first char\n   * for more sophisticated checking see https://github.com/sindresorhus/validate-element-name\n   *\n   * @param tagName name of the tag of the node to sanitize\n   * @returns Returns true if the tag name meets the basic criteria for a custom element, otherwise false.\n   */\n  const _isBasicCustomElement = function _isBasicCustomElement(tagName) {\n    return tagName !== 'annotation-xml' && stringMatch(tagName, CUSTOM_ELEMENT);\n  };\n  /**\n   * _sanitizeAttributes\n   *\n   * @protect attributes\n   * @protect nodeName\n   * @protect removeAttribute\n   * @protect setAttribute\n   *\n   * @param currentNode to sanitize\n   */\n  const _sanitizeAttributes = function _sanitizeAttributes(currentNode) {\n    /* Execute a hook if present */\n    _executeHooks(hooks.beforeSanitizeAttributes, currentNode, null);\n    const {\n      attributes\n    } = currentNode;\n    /* Check if we have attributes; if not we might have a text node */\n    if (!attributes || _isClobbered(currentNode)) {\n      return;\n    }\n    const hookEvent = {\n      attrName: '',\n      attrValue: '',\n      keepAttr: true,\n      allowedAttributes: ALLOWED_ATTR,\n      forceKeepAttr: undefined\n    };\n    let l = attributes.length;\n    /* Go backwards over all attributes; safely remove bad ones */\n    while (l--) {\n      const attr = attributes[l];\n      const {\n        name,\n        namespaceURI,\n        value: attrValue\n      } = attr;\n      const lcName = transformCaseFunc(name);\n      let value = name === 'value' ? attrValue : stringTrim(attrValue);\n      /* Execute a hook if present */\n      hookEvent.attrName = lcName;\n      hookEvent.attrValue = value;\n      hookEvent.keepAttr = true;\n      hookEvent.forceKeepAttr = undefined; // Allows developers to see this is a property they can set\n      _executeHooks(hooks.uponSanitizeAttribute, currentNode, hookEvent);\n      value = hookEvent.attrValue;\n      /* Full DOM Clobbering protection via namespace isolation,\n       * Prefix id and name attributes with `user-content-`\n       */\n      if (SANITIZE_NAMED_PROPS && (lcName === 'id' || lcName === 'name')) {\n        // Remove the attribute with this value\n        _removeAttribute(name, currentNode);\n        // Prefix the value and later re-create the attribute with the sanitized value\n        value = SANITIZE_NAMED_PROPS_PREFIX + value;\n      }\n      /* Work around a security issue with comments inside attributes */\n      if (SAFE_FOR_XML && regExpTest(/((--!?|])>)|<\\/(style|title)/i, value)) {\n        _removeAttribute(name, currentNode);\n        continue;\n      }\n      /* Did the hooks approve of the attribute? */\n      if (hookEvent.forceKeepAttr) {\n        continue;\n      }\n      /* Remove attribute */\n      _removeAttribute(name, currentNode);\n      /* Did the hooks approve of the attribute? */\n      if (!hookEvent.keepAttr) {\n        continue;\n      }\n      /* Work around a security issue in jQuery 3.0 */\n      if (!ALLOW_SELF_CLOSE_IN_ATTR && regExpTest(/\\/>/i, value)) {\n        _removeAttribute(name, currentNode);\n        continue;\n      }\n      /* Sanitize attribute content to be template-safe */\n      if (SAFE_FOR_TEMPLATES) {\n        arrayForEach([MUSTACHE_EXPR, ERB_EXPR, TMPLIT_EXPR], expr => {\n          value = stringReplace(value, expr, ' ');\n        });\n      }\n      /* Is `value` valid for this attribute? */\n      const lcTag = transformCaseFunc(currentNode.nodeName);\n      if (!_isValidAttribute(lcTag, lcName, value)) {\n        continue;\n      }\n      /* Handle attributes that require Trusted Types */\n      if (trustedTypesPolicy && typeof trustedTypes === 'object' && typeof trustedTypes.getAttributeType === 'function') {\n        if (namespaceURI) ; else {\n          switch (trustedTypes.getAttributeType(lcTag, lcName)) {\n            case 'TrustedHTML':\n              {\n                value = trustedTypesPolicy.createHTML(value);\n                break;\n              }\n            case 'TrustedScriptURL':\n              {\n                value = trustedTypesPolicy.createScriptURL(value);\n                break;\n              }\n          }\n        }\n      }\n      /* Handle invalid data-* attribute set by try-catching it */\n      try {\n        if (namespaceURI) {\n          currentNode.setAttributeNS(namespaceURI, name, value);\n        } else {\n          /* Fallback to setAttribute() for browser-unrecognized namespaces e.g. \"x-schema\". */\n          currentNode.setAttribute(name, value);\n        }\n        if (_isClobbered(currentNode)) {\n          _forceRemove(currentNode);\n        } else {\n          arrayPop(DOMPurify.removed);\n        }\n      } catch (_) {}\n    }\n    /* Execute a hook if present */\n    _executeHooks(hooks.afterSanitizeAttributes, currentNode, null);\n  };\n  /**\n   * _sanitizeShadowDOM\n   *\n   * @param fragment to iterate over recursively\n   */\n  const _sanitizeShadowDOM = function _sanitizeShadowDOM(fragment) {\n    let shadowNode = null;\n    const shadowIterator = _createNodeIterator(fragment);\n    /* Execute a hook if present */\n    _executeHooks(hooks.beforeSanitizeShadowDOM, fragment, null);\n    while (shadowNode = shadowIterator.nextNode()) {\n      /* Execute a hook if present */\n      _executeHooks(hooks.uponSanitizeShadowNode, shadowNode, null);\n      /* Sanitize tags and elements */\n      _sanitizeElements(shadowNode);\n      /* Check attributes next */\n      _sanitizeAttributes(shadowNode);\n      /* Deep shadow DOM detected */\n      if (shadowNode.content instanceof DocumentFragment) {\n        _sanitizeShadowDOM(shadowNode.content);\n      }\n    }\n    /* Execute a hook if present */\n    _executeHooks(hooks.afterSanitizeShadowDOM, fragment, null);\n  };\n  // eslint-disable-next-line complexity\n  DOMPurify.sanitize = function (dirty) {\n    let cfg = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : {};\n    let body = null;\n    let importedNode = null;\n    let currentNode = null;\n    let returnNode = null;\n    /* Make sure we have a string to sanitize.\n      DO NOT return early, as this will return the wrong type if\n      the user has requested a DOM object rather than a string */\n    IS_EMPTY_INPUT = !dirty;\n    if (IS_EMPTY_INPUT) {\n      dirty = '<!-->';\n    }\n    /* Stringify, in case dirty is an object */\n    if (typeof dirty !== 'string' && !_isNode(dirty)) {\n      if (typeof dirty.toString === 'function') {\n        dirty = dirty.toString();\n        if (typeof dirty !== 'string') {\n          throw typeErrorCreate('dirty is not a string, aborting');\n        }\n      } else {\n        throw typeErrorCreate('toString is not a function');\n      }\n    }\n    /* Return dirty HTML if DOMPurify cannot run */\n    if (!DOMPurify.isSupported) {\n      return dirty;\n    }\n    /* Assign config vars */\n    if (!SET_CONFIG) {\n      _parseConfig(cfg);\n    }\n    /* Clean up removed elements */\n    DOMPurify.removed = [];\n    /* Check if dirty is correctly typed for IN_PLACE */\n    if (typeof dirty === 'string') {\n      IN_PLACE = false;\n    }\n    if (IN_PLACE) {\n      /* Do some early pre-sanitization to avoid unsafe root nodes */\n      if (dirty.nodeName) {\n        const tagName = transformCaseFunc(dirty.nodeName);\n        if (!ALLOWED_TAGS[tagName] || FORBID_TAGS[tagName]) {\n          throw typeErrorCreate('root node is forbidden and cannot be sanitized in-place');\n        }\n      }\n    } else if (dirty instanceof Node) {\n      /* If dirty is a DOM element, append to an empty document to avoid\n         elements being stripped by the parser */\n      body = _initDocument('<!---->');\n      importedNode = body.ownerDocument.importNode(dirty, true);\n      if (importedNode.nodeType === NODE_TYPE.element && importedNode.nodeName === 'BODY') {\n        /* Node is already a body, use as is */\n        body = importedNode;\n      } else if (importedNode.nodeName === 'HTML') {\n        body = importedNode;\n      } else {\n        // eslint-disable-next-line unicorn/prefer-dom-node-append\n        body.appendChild(importedNode);\n      }\n    } else {\n      /* Exit directly if we have nothing to do */\n      if (!RETURN_DOM && !SAFE_FOR_TEMPLATES && !WHOLE_DOCUMENT &&\n      // eslint-disable-next-line unicorn/prefer-includes\n      dirty.indexOf('<') === -1) {\n        return trustedTypesPolicy && RETURN_TRUSTED_TYPE ? trustedTypesPolicy.createHTML(dirty) : dirty;\n      }\n      /* Initialize the document to work on */\n      body = _initDocument(dirty);\n      /* Check we have a DOM node from the data */\n      if (!body) {\n        return RETURN_DOM ? null : RETURN_TRUSTED_TYPE ? emptyHTML : '';\n      }\n    }\n    /* Remove first element node (ours) if FORCE_BODY is set */\n    if (body && FORCE_BODY) {\n      _forceRemove(body.firstChild);\n    }\n    /* Get node iterator */\n    const nodeIterator = _createNodeIterator(IN_PLACE ? dirty : body);\n    /* Now start iterating over the created document */\n    while (currentNode = nodeIterator.nextNode()) {\n      /* Sanitize tags and elements */\n      _sanitizeElements(currentNode);\n      /* Check attributes next */\n      _sanitizeAttributes(currentNode);\n      /* Shadow DOM detected, sanitize it */\n      if (currentNode.content instanceof DocumentFragment) {\n        _sanitizeShadowDOM(currentNode.content);\n      }\n    }\n    /* If we sanitized `dirty` in-place, return it. */\n    if (IN_PLACE) {\n      return dirty;\n    }\n    /* Return sanitized string or DOM */\n    if (RETURN_DOM) {\n      if (RETURN_DOM_FRAGMENT) {\n        returnNode = createDocumentFragment.call(body.ownerDocument);\n        while (body.firstChild) {\n          // eslint-disable-next-line unicorn/prefer-dom-node-append\n          returnNode.appendChild(body.firstChild);\n        }\n      } else {\n        returnNode = body;\n      }\n      if (ALLOWED_ATTR.shadowroot || ALLOWED_ATTR.shadowrootmode) {\n        /*\n          AdoptNode() is not used because internal state is not reset\n          (e.g. the past names map of a HTMLFormElement), this is safe\n          in theory but we would rather not risk another attack vector.\n          The state that is cloned by importNode() is explicitly defined\n          by the specs.\n        */\n        returnNode = importNode.call(originalDocument, returnNode, true);\n      }\n      return returnNode;\n    }\n    let serializedHTML = WHOLE_DOCUMENT ? body.outerHTML : body.innerHTML;\n    /* Serialize doctype if allowed */\n    if (WHOLE_DOCUMENT && ALLOWED_TAGS['!doctype'] && body.ownerDocument && body.ownerDocument.doctype && body.ownerDocument.doctype.name && regExpTest(DOCTYPE_NAME, body.ownerDocument.doctype.name)) {\n      serializedHTML = '<!DOCTYPE ' + body.ownerDocument.doctype.name + '>\\n' + serializedHTML;\n    }\n    /* Sanitize final string template-safe */\n    if (SAFE_FOR_TEMPLATES) {\n      arrayForEach([MUSTACHE_EXPR, ERB_EXPR, TMPLIT_EXPR], expr => {\n        serializedHTML = stringReplace(serializedHTML, expr, ' ');\n      });\n    }\n    return trustedTypesPolicy && RETURN_TRUSTED_TYPE ? trustedTypesPolicy.createHTML(serializedHTML) : serializedHTML;\n  };\n  DOMPurify.setConfig = function () {\n    let cfg = arguments.length > 0 && arguments[0] !== undefined ? arguments[0] : {};\n    _parseConfig(cfg);\n    SET_CONFIG = true;\n  };\n  DOMPurify.clearConfig = function () {\n    CONFIG = null;\n    SET_CONFIG = false;\n  };\n  DOMPurify.isValidAttribute = function (tag, attr, value) {\n    /* Initialize shared config vars if necessary. */\n    if (!CONFIG) {\n      _parseConfig({});\n    }\n    const lcTag = transformCaseFunc(tag);\n    const lcName = transformCaseFunc(attr);\n    return _isValidAttribute(lcTag, lcName, value);\n  };\n  DOMPurify.addHook = function (entryPoint, hookFunction) {\n    if (typeof hookFunction !== 'function') {\n      return;\n    }\n    arrayPush(hooks[entryPoint], hookFunction);\n  };\n  DOMPurify.removeHook = function (entryPoint, hookFunction) {\n    if (hookFunction !== undefined) {\n      const index = arrayLastIndexOf(hooks[entryPoint], hookFunction);\n      return index === -1 ? undefined : arraySplice(hooks[entryPoint], index, 1)[0];\n    }\n    return arrayPop(hooks[entryPoint]);\n  };\n  DOMPurify.removeHooks = function (entryPoint) {\n    hooks[entryPoint] = [];\n  };\n  DOMPurify.removeAllHooks = function () {\n    hooks = _createHooksMap();\n  };\n  return DOMPurify;\n}\nvar purify = createDOMPurify();\n\nexport { purify as default };\n//# sourceMappingURL=purify.es.mjs.map\n", "import createDOMPurify from \"dompurify\";\nimport { JSDOM } from \"jsdom\";\n\nconst windowEmulator: any = new JSDOM(\"\").window;\nconst DOMPurify = createDOMPurify(windowEmulator);\n\n/*\n * DOMPurify prevents all XSS attacks by default. With these settings, it also\n * prevents \"deception\" attacks. If an attacker could put <div style=\"...\">\n * into the site's admin banner, they could make give the banner any appearance,\n * overlaid anywhere on the page. For example, a fake \"session expired\" modal\n * with a malicious link. Thus, this very strict DOMPurify config.\n */\nDOMPurify.setConfig({\n  // Only these tags will be allowed through\n  ALLOWED_TAGS: [\"ul\", \"ol\", \"li\", \"a\", \"#text\"],\n  // On those tags, only these attributes are allowed\n  ALLOWED_ATTR: [\"href\", \"alt\"],\n  // If a tag is removed, so will all its child elements & text\n  KEEP_CONTENT: false,\n});\n\n// sanitize string\nexport const sanitizeString = (string: string) => {\n  if (DOMPurify.isSupported) {\n    return DOMPurify.sanitize(string);\n  }\n};\n\n// iterates over array items, sanitizing items recursively\nexport const sanitizeArray = (array: unknown[]): unknown[] =>\n  array.map((entry: unknown) => sanitizeEntry(entry));\n\n// iterates over object key-value pairs, sanitizing values recursively\nexport const sanitizeObject = (object: { [key: string]: unknown }) => {\n  if (object) {\n    const entries = Object.entries(object);\n    const sanitizedEntries = entries.map((entry: [string, unknown]) => {\n      const [key, value] = entry;\n      return [key, sanitizeEntry(value)];\n    });\n    return Object.fromEntries(sanitizedEntries);\n  }\n};\n\nconst sanitizerMap: any = {\n  string: sanitizeString,\n  array: sanitizeArray,\n  object: sanitizeObject,\n};\n\n// return sanitized entry, or if safe type, return entry\nconst sanitizeEntry = (entry: unknown) => {\n  const entryType = Array.isArray(entry) ? \"array\" : typeof entry;\n  const sanitizer = sanitizerMap[entryType];\n  return sanitizer?.(entry) || entry;\n};\n", "// utils\nimport * as logger from \"../utils/debugging/debug-lib\";\nimport { isAuthenticated } from \"../utils/auth/authorization\";\nimport {\n  HttpResponse,\n  internalServerError,\n  unauthenticated,\n} from \"../utils/responses/response-lib\";\nimport { error } from \"../utils/constants/constants\";\nimport { sanitizeObject } from \"../utils/sanitize/sanitize\";\n// types\nimport { APIGatewayProxyEvent } from \"../utils/types\";\n\ntype LambdaFunction = (\n  event: APIGatewayProxyEvent, // eslint-disable-line no-unused-vars\n  context: any // eslint-disable-line no-unused-vars\n) => Promise<HttpResponse>;\n\nexport default function handler(lambda: LambdaFunction) {\n  return async function (event: APIGatewayProxyEvent, context: any) {\n    // Start debugger\n    logger.init();\n    logger.debug(\"API event: %O\", {\n      body: event.body,\n      pathParameters: event.pathParameters,\n      queryStringParameters: event.queryStringParameters,\n    });\n    if (await isAuthenticated(event)) {\n      try {\n        if (event.body) {\n          const newEventBody = sanitizeObject(JSON.parse(event.body));\n          event.body = JSON.stringify(newEventBody);\n        }\n        return await lambda(event, context);\n      } catch (error: any) {\n        logger.error(\"Error: %O\", error);\n\n        const body = { error: error.message };\n        return internalServerError(body);\n      } finally {\n        logger.flush();\n      }\n    } else {\n      return unauthenticated(error.UNAUTHORIZED);\n    }\n  };\n}\n", "import { APIGatewayProxyEvent, isReportType, isState } from \"../types\";\nimport { logger } from \"../debugging/debug-lib\";\n\nexport const parseSpecificReportParameters = (event: APIGatewayProxyEvent) => {\n  const { reportType, state, id } = event.pathParameters ?? {};\n  if (!isReportType(reportType)) {\n    logger.warn(\"Invalid report type in path\");\n    return { allParamsValid: false as const };\n  }\n  if (!isState(state)) {\n    logger.warn(\"Invalid state in path\");\n    return { allParamsValid: false as const };\n  }\n  if (!id) {\n    logger.warn(\"Invalid report ID in path\");\n    return { allParamsValid: false as const };\n  }\n  return { allParamsValid: true as const, reportType, state, id };\n};\n\nexport const parseStateReportParameters = (event: APIGatewayProxyEvent) => {\n  const { reportType, state } = event.pathParameters ?? {};\n  if (!isReportType(reportType)) {\n    logger.warn(\"Invalid report type in path\");\n    return { allParamsValid: false as const };\n  }\n  if (!isState(state)) {\n    logger.warn(\"Invalid state in path\");\n    return { allParamsValid: false as const };\n  }\n  return { allParamsValid: true as const, reportType, state };\n};\n", "import { object } from \"yup\";\nimport { error } from \"../constants/constants\";\n// types\nimport { AnyObject } from \"../types\";\n// utils\nimport { nested, endDate, schemaMap } from \"./schemaMap\";\n\n// compare payload data against validation schema\nexport const validateData = async (\n  validationSchema: AnyObject,\n  data: AnyObject,\n  options?: AnyObject\n) => {\n  try {\n    // returns valid data to be passed through API\n    return await validationSchema.validate(data, {\n      stripUnknown: true,\n      ...options,\n    });\n  } catch {\n    throw new Error(error.INVALID_DATA);\n  }\n};\n\n// filter field validation to just what's needed for the passed fields\nexport const filterValidationSchema = (\n  validationObject: AnyObject,\n  data: AnyObject\n): AnyObject => {\n  const validationEntries = Object.entries(validationObject);\n  const dataKeys = Object.keys(data);\n  const filteredEntries = validationEntries.filter(\n    (entry: [string, string | AnyObject]) => {\n      const [entryKey] = entry;\n      return dataKeys.includes(entryKey);\n    }\n  );\n  return Object.fromEntries(filteredEntries);\n};\n\n// map field validation types to validation schema\nexport const mapValidationTypesToSchema = (fieldValidationTypes: AnyObject) => {\n  let validationSchema: AnyObject = {};\n  // for each field to be validated,\n  Object.entries(fieldValidationTypes).forEach(\n    (fieldValidationType: [string, string | AnyObject]) => {\n      const [key, fieldValidation] = fieldValidationType;\n      // if standard validation type, set corresponding schema from map\n      if (typeof fieldValidation === \"string\") {\n        const correspondingSchema = schemaMap[fieldValidation];\n        if (correspondingSchema) {\n          validationSchema[key] = correspondingSchema;\n        }\n      }\n      // else if nested validation type, make and set nested schema\n      else if (fieldValidation.nested) {\n        validationSchema[key] = makeNestedFieldSchema(fieldValidation);\n        // else if not nested, make and set other dependent field types\n      } else if (fieldValidation.type === \"endDate\") {\n        validationSchema[key] = makeEndDateFieldSchema(fieldValidation);\n      }\n    }\n  );\n  return validationSchema;\n};\n\n// return created endDate schema\nexport const makeEndDateFieldSchema = (fieldValidationObject: AnyObject) => {\n  const { dependentFieldName } = fieldValidationObject;\n  return endDate(dependentFieldName);\n};\n\n// return created nested field schema\nexport const makeNestedFieldSchema = (fieldValidationObject: AnyObject) => {\n  const { type, parentFieldName, parentOptionId } = fieldValidationObject;\n  if (fieldValidationObject.type === \"endDate\") {\n    return nested(\n      () => makeEndDateFieldSchema(fieldValidationObject),\n      parentFieldName,\n      parentOptionId\n    );\n  } else {\n    const fieldValidationSchema = schemaMap[type];\n    return nested(() => fieldValidationSchema, parentFieldName, parentOptionId);\n  }\n};\n\nexport const validateFieldData = async (\n  validationJson: AnyObject,\n  unvalidatedFieldData: AnyObject\n) => {\n  let validatedFieldData: AnyObject | undefined = undefined;\n  // filter field validation to just what's needed for the passed fields\n  const filteredFieldDataValidationJson = filterValidationSchema(\n    validationJson,\n    unvalidatedFieldData\n  );\n  // transform field validation instructions to yup validation schema\n  const fieldDataValidationSchema = object().shape(\n    mapValidationTypesToSchema(filteredFieldDataValidationJson)\n  );\n  if (fieldDataValidationSchema) {\n    validatedFieldData = await validateData(\n      fieldDataValidationSchema,\n      unvalidatedFieldData\n    );\n  }\n  return validatedFieldData;\n};\n", "import { array, boolean, mixed, object, string } from \"yup\";\nimport { Choice } from \"../types/index\";\nimport {\n  checkRatioInputAgainstRegexes,\n  checkStandardIntegerInputAgainstRegexes,\n  checkStandardNumberInputAgainstRegexes,\n} from \"./checkInputValidity\";\n\nconst error = {\n  REQUIRED_GENERIC: \"A response is required\",\n  REQUIRED_CHECKBOX: \"Select at least one response\",\n  INVALID_GENERIC: \"Response must be valid\",\n  INVALID_EMAIL: \"Response must be a valid email address\",\n  INVALID_URL: \"Response must be a valid hyperlink/URL\",\n  INVALID_DATE: \"Response must be a valid date\",\n  INVALID_END_DATE: \"End date can't be before start date\",\n  NUMBER_LESS_THAN_ZERO: \"Response must be greater than or equal to zero\",\n  INVALID_NUMBER: \"Response must be a valid number\",\n  INVALID_NUMBER_OR_NA: 'Response must be a valid number or \"N/A\"',\n  INVALID_RATIO: \"Response must be a valid ratio\",\n};\n\n// TEXT - Helpers\nconst isWhitespaceString = (value?: string) => value?.trim().length === 0;\n\n// TEXT\nexport const text = () =>\n  string()\n    .typeError(error.INVALID_GENERIC)\n    .required(error.REQUIRED_GENERIC)\n    .test({\n      test: (value) => !isWhitespaceString(value),\n      message: error.REQUIRED_GENERIC,\n    });\nexport const textOptional = () => string().typeError(error.INVALID_GENERIC);\n\n// NUMBER - Helpers\nconst validNAValues = [\"N/A\", \"Data not available\"];\n// const validNumberRegex = /^\\.$|[0-9]/;\nconst validIntegerRegex = /^[0-9\\s,$%]+$/;\n\n// NUMBER - Number or Valid Strings\nexport const numberSchema = () =>\n  string()\n    .test({\n      message: error.INVALID_NUMBER_OR_NA,\n      test: (value) => {\n        if (value) {\n          const isValidStringValue = validNAValues.includes(value);\n          const isValidNumberValue =\n            checkStandardNumberInputAgainstRegexes(value);\n          return isValidStringValue || isValidNumberValue;\n        } else return true;\n      },\n    })\n    .test({\n      test: (value) => {\n        if (checkStandardNumberInputAgainstRegexes(value!)) {\n          return parseFloat(value!) >= 0;\n        } else return true;\n      },\n      message: error.NUMBER_LESS_THAN_ZERO,\n    });\n\nexport const number = () =>\n  numberSchema()\n    .required(error.REQUIRED_GENERIC)\n    .test({\n      test: (value) => !isWhitespaceString(value),\n      message: error.REQUIRED_GENERIC,\n    });\n\nexport const numberOptional = () => numberSchema().notRequired().nullable();\n\n// Integer or Valid Strings\nexport const validIntegerSchema = () =>\n  string()\n    .test({\n      message: error.INVALID_NUMBER_OR_NA,\n      test: (value) => {\n        if (value) {\n          const isValidStringValue = validNAValues.includes(value);\n          const isValidIntegerValue = validIntegerRegex.test(value);\n          return isValidStringValue || isValidIntegerValue;\n        } else return true;\n      },\n    })\n    .test({\n      test: (value) => {\n        if (checkStandardIntegerInputAgainstRegexes(value!)) {\n          return parseFloat(value!) >= 0;\n        } else return true;\n      },\n      message: error.NUMBER_LESS_THAN_ZERO,\n    });\n\nexport const validInteger = () =>\n  validIntegerSchema().required(error.REQUIRED_GENERIC);\n\nexport const validIntegerOptional = () =>\n  validIntegerSchema().notRequired().nullable();\n\n// Number - Ratio\nexport const ratio = () =>\n  mixed()\n    .test({\n      message: error.REQUIRED_GENERIC,\n      test: (val) => val != \"\",\n    })\n    .required(error.REQUIRED_GENERIC)\n    .test({\n      message: error.INVALID_RATIO,\n      test: (val) => {\n        return checkRatioInputAgainstRegexes(val).isValid;\n      },\n    });\n\n// EMAIL\nexport const email = () => text().email(error.INVALID_EMAIL);\nexport const emailOptional = () => email().notRequired();\n\n// URL\nexport const url = () => text().url(error.INVALID_URL);\nexport const urlOptional = () => url().notRequired();\n\n// DATE\nexport const date = () =>\n  string()\n    .required(error.REQUIRED_GENERIC)\n    .matches(dateFormatRegex, error.INVALID_DATE)\n    .test({\n      message: error.REQUIRED_GENERIC,\n      test: (value) => !isWhitespaceString(value),\n    });\n\nexport const dateOptional = () =>\n  string()\n    .typeError(error.INVALID_GENERIC)\n    .test({\n      message: error.INVALID_DATE,\n      test: (value) => dateFormatRegex.test(value!),\n    });\n\nexport const endDate = (startDateField: string) =>\n  date()\n    .typeError(error.INVALID_DATE)\n    .test({\n      message: error.INVALID_END_DATE,\n      test: (endDateString, context) => {\n        return isEndDateAfterStartDate(\n          context.parent[startDateField],\n          endDateString as string\n        );\n      },\n    });\n\nexport const isEndDateAfterStartDate = (\n  startDateString: string,\n  endDateString: string\n) => {\n  const startDate = new Date(startDateString);\n  const endDate = new Date(endDateString!);\n  return endDate >= startDate;\n};\n\n// DROPDOWN\nexport const dropdown = () =>\n  object({ label: text(), value: text() }).required(error.REQUIRED_GENERIC);\n\n// CHECKBOX\nexport const checkbox = () =>\n  array()\n    .min(1, error.REQUIRED_CHECKBOX)\n    .of(object({ key: text(), value: text() }))\n    .required(error.REQUIRED_CHECKBOX);\nexport const checkboxOptional = () =>\n  array().notRequired().typeError(error.INVALID_GENERIC);\nexport const checkboxSingle = () => boolean();\n\n// RADIO\nexport const radio = () =>\n  array()\n    .min(1, error.REQUIRED_GENERIC)\n    .of(object({ key: text(), value: text() }))\n    .required(error.REQUIRED_GENERIC);\nexport const radioOptional = () => radio().notRequired();\n\n// DYNAMIC\nexport const dynamic = () =>\n  array()\n    .min(1)\n    .of(\n      object().shape({\n        id: text(),\n        name: text(),\n      })\n    )\n    .required(error.REQUIRED_GENERIC);\nexport const dynamicOptional = () => dynamic().notRequired();\n\n// NESTED\nexport const nested = (\n  fieldSchema: Function,\n  parentFieldName: string,\n  parentOptionId: string\n) => {\n  const fieldTypeMap = {\n    array: array(),\n    string: string(),\n    date: date(),\n    object: object(),\n  };\n  const fieldType: keyof typeof fieldTypeMap = fieldSchema().type;\n  const baseSchema: any = fieldTypeMap[fieldType];\n  return baseSchema.when(parentFieldName, {\n    is: (value: Choice[]) =>\n      // look for parentOptionId in checked choices\n      value?.find((option: Choice) => option.key.endsWith(parentOptionId)),\n    then: () => fieldSchema(), // returns standard field schema (required)\n    otherwise: () => baseSchema, // returns not-required Yup base schema\n  });\n};\n\n// OBJECT ARRAY\nexport const objectArray = () => array().of(mixed());\n\n// REGEX\nexport const dateFormatRegex =\n  /^((0[1-9]|1[0-2])\\/(0[1-9]|1\\d|2\\d|3[01])\\/(19|20)\\d{2})|((0[1-9]|1[0-2])(0[1-9]|1\\d|2\\d|3[01])(19|20)\\d{2})$/;\n\n// SCHEMA MAP\nexport const schemaMap: any = {\n  checkbox: checkbox(),\n  checkboxOptional: checkboxOptional(),\n  checkboxSingle: checkboxSingle(),\n  date: date(),\n  dateOptional: dateOptional(),\n  dropdown: dropdown(),\n  dynamic: dynamic(),\n  dynamicOptional: dynamicOptional(),\n  email: email(),\n  emailOptional: emailOptional(),\n  number: number(),\n  numberOptional: numberOptional(),\n  objectArray: objectArray(),\n  radio: radio(),\n  radioOptional: radioOptional(),\n  ratio: ratio(),\n  text: text(),\n  textOptional: textOptional(),\n  url: url(),\n  urlOptional: urlOptional(),\n  validInteger: validInteger(),\n  validIntegerOptional: validIntegerOptional(),\n};\n", "// REGEX\n\n// basic check for all possible characters -- standard number\nconst validCharactersStandardNumberRegex = /^[0-9\\s.,$%-]+$/;\n// basic check for all possible characters -- standard number\nconst validCharactersStandardIntegerRegex = /^[0-9\\s,$%]+$/;\n// basic check for all possible characters -- ratio\nconst validCharactersRatioNumberRegex = /^[0-9.,-]+$/;\n// at most 1 decimal point\nconst atMost1DecimalPointRegex = /^[^.]*\\.?[^.]*$/;\n// commas only exist before decimal point\nconst validCommaLocationRegex = /^[0-9,$-]*\\.?[0-9%]*$/;\n// at most 1 $%\nconst atMost1SpecialCharacterRegex = /^([^$%]*\\$[^$%]*|[^$%]*%[^$%]*|[^$%]*)$/;\n// at most 1 $ at the beginning of the input\nconst validDollarSignPlacementRegex = /^[$]?[^$%]+$/;\n// at most 1 % at the end of the input\nconst validPercentSignPlacementRegex = /^[^%$]+[%]?$/;\n// at most 1 - at the beginning of the input (but after any potential $s)\nconst validNegativeSignPlacementRegex = /^[$]?[-]?[^$-]+[%]?$/;\n// exactly one ratio character in between other characters\nconst exactlyOneRatioCharacterRegex = /^[^:]+:[^:]+$/;\n\nexport const checkStandardNumberInputAgainstRegexes = (\n  value: string\n): boolean => {\n  if (\n    !validCharactersStandardNumberRegex.test(value) ||\n    !atMost1DecimalPointRegex.test(value) ||\n    !validCommaLocationRegex.test(value) ||\n    !atMost1SpecialCharacterRegex.test(value) ||\n    !(\n      validDollarSignPlacementRegex.test(value) ||\n      validPercentSignPlacementRegex.test(value)\n    ) ||\n    !validNegativeSignPlacementRegex.test(value)\n  )\n    return false;\n  return true;\n};\n\nexport const checkStandardIntegerInputAgainstRegexes = (\n  value: string\n): boolean => {\n  if (\n    !validCharactersStandardIntegerRegex.test(value) ||\n    !validCommaLocationRegex.test(value) ||\n    !atMost1SpecialCharacterRegex.test(value) ||\n    !(\n      validDollarSignPlacementRegex.test(value) ||\n      validPercentSignPlacementRegex.test(value)\n    )\n  )\n    return false;\n  return true;\n};\n\nexport const checkRatioInputAgainstRegexes = (\n  value: string\n): { isValid: boolean; leftSide: string; rightSide: string } => {\n  if (!exactlyOneRatioCharacterRegex.test(value))\n    return { isValid: false, leftSide: \"\", rightSide: \"\" };\n\n  // Grab the left and right side of the ratio sign\n  let values = value.split(\":\");\n\n  // Check left and right side for valid inputs\n  if (\n    !checkASideOfRatioAgainstRegexes(values[0]) ||\n    !checkASideOfRatioAgainstRegexes(values[1])\n  )\n    return { isValid: false, leftSide: values[0], rightSide: values[1] };\n\n  return { isValid: true, leftSide: values[0], rightSide: values[1] };\n};\n\nexport const checkASideOfRatioAgainstRegexes = (value: string): boolean => {\n  if (\n    !validCharactersRatioNumberRegex.test(value) ||\n    !atMost1DecimalPointRegex.test(value) ||\n    !validCommaLocationRegex.test(value) ||\n    !validNegativeSignPlacementRegex.test(value)\n  )\n    return false;\n  return true;\n};\n", "import * as yup from \"yup\";\n\nexport const metadataValidationSchema = yup.object().shape({\n  submissionName: yup.string(),\n  reportPeriod: yup.number(),\n  reportType: yup.string(),\n  locked: yup.bool(),\n  status: yup.string(),\n  lastAlteredBy: yup.string(),\n  submittedBy: yup.string(),\n  submittedOnDate: yup.string(),\n  previousRevisions: yup.array(),\n  submissionCount: yup.number(),\n  completionStatus: yup.mixed(),\n  finalSar: yup.mixed(),\n  populations: yup.mixed(),\n});\n", "import { ReportMetadataShape, ReportType } from \"../types\";\n\n/**\n * @param date A UTC timestamp, as from `Date.now()`\n * @returns A MM/dd/yyyy string, for that date on the US East Coast.\n */\nexport const convertDateUtcToEt = (date: number) => {\n  return Intl.DateTimeFormat(\"en-US\", {\n    timeZone: \"America/New_York\",\n    year: \"numeric\",\n    month: \"2-digit\",\n    day: \"2-digit\",\n  }).format(new Date(date));\n};\n\n/*\n * Calculates the period given the current date.\n * The periods are defined as follows:\n *     Period 1 is from 01/01 to 06/30.\n *     Period 2 is from 07/01 to 12/31.\n */\nexport const calculatePeriod = (\n  currentDate: number,\n  workPlan?: ReportMetadataShape\n) => {\n  if (workPlan) return workPlan.reportPeriod;\n  const date = new Date(currentDate);\n  const period = Math.ceil((date.getMonth() + 1) / 6);\n  return period;\n};\n\n/**\n * Calculates if the given year is a leap year\n * @param year The year.\n * @returns if the given year is a leap year.\n */\nexport const isLeapYear = (year: number) => {\n  return (year % 4 === 0 && year % 100 !== 0) || year % 400 === 0;\n};\n\n/**\n * This method returns a date in ISO format to a date in mm/dd/yyyy format.\n * @param date The given date in ISO format\n * @returns a date in MM/dd/yyyy format\n */\nexport const convertToFormattedDate = (date: Date) => {\n  const year = date.getFullYear();\n  const month = (1 + date.getMonth()).toString().padStart(2, \"0\");\n  const day = date.getDate().toString().padStart(2, \"0\");\n\n  return `${month}/${day}/${year}`;\n};\n\n/**\n * Calculates the due date given the period and the report type.\n * WP due date is May 1 for Period 1, and Nov 1 for Period 2\n * SAR due date for Period 1{*}: 60 days from June 30, due date is Aug 29.\n * SAR due date for Period 2{*}: 60 days from December 31(for a non leap year Mar 1, for leap years it\u2019s Feb 29).\n *\n * @param currentYear The current year of the report\n * @param reportPeriod The period (1 or 2) for the given report\n * @param reportType The report type (WP or SAR)\n */\n\nexport const calculateDueDate = (\n  currentYear: number,\n  reportPeriod: number,\n  reportType: ReportType\n) => {\n  let dueDate = \"5/1\";\n  let year = currentYear;\n\n  if (reportPeriod === 2) {\n    dueDate = \"11/1\";\n  }\n\n  if (reportType === ReportType.SAR) {\n    dueDate = \"8/29\";\n\n    if (reportPeriod === 2) {\n      year++;\n      dueDate = isLeapYear(year) ? \"2/29\" : \"3/1\";\n    }\n  }\n\n  const [month, day] = dueDate.split(\"/\").map((i) => parseInt(i, 10));\n  const date = new Date(year, month - 1, day);\n\n  return convertToFormattedDate(date);\n};\n\nexport const calculateCurrentQuarter = () => {\n  const quarter = Math.ceil((new Date().getMonth() + 1) / 3);\n  return quarter;\n};\n\nexport const calculateCurrentYear = () => {\n  const year = new Date().getFullYear();\n  return year;\n};\n\nexport const incrementQuarterAndYear = (quarter: number, year: number) => {\n  if (quarter >= 4) {\n    quarter = 1;\n    year++;\n  } else {\n    quarter++;\n  }\n  return [quarter, year];\n};\n", "import { GetObjectCommand, PutObjectCommand } from \"@aws-sdk/client-s3\";\nimport {\n  GetCommand,\n  paginateQuery,\n  PutCommand,\n  QueryCommand,\n} from \"@aws-sdk/lib-dynamodb\";\nimport {\n  FormTemplateVersion,\n  ReportFieldData,\n  ReportJson,\n  ReportMetadataShape,\n  ReportType,\n  State,\n} from \"../utils/types\";\nimport {\n  createClient as createDynamoClient,\n  collectPageItems,\n} from \"./dynamodb-lib\";\nimport { createClient as createS3Client, parseS3Response } from \"./s3-lib\";\nimport { reportBuckets, reportTables } from \"../utils/constants/constants\";\n\nconst dynamoClient = createDynamoClient();\nconst s3Client = createS3Client();\n\nconst formTemplateVersionTable = process.env.FormTemplateVersionsTable!;\n\n/* METADATA (dynamo) */\n\nexport const putReportMetadata = async (metadata: ReportMetadataShape) => {\n  await dynamoClient.send(\n    new PutCommand({\n      TableName: reportTables[metadata.reportType],\n      Item: metadata,\n    })\n  );\n};\n\nexport const queryReportMetadatasForState = async (\n  reportType: ReportType,\n  state: State\n) => {\n  const table = reportTables[reportType];\n  const responsePages = paginateQuery(\n    { client: dynamoClient },\n    {\n      TableName: table,\n      KeyConditionExpression: \"#state = :state\",\n      ExpressionAttributeNames: { \"#state\": \"state\" },\n      ExpressionAttributeValues: { \":state\": state },\n    }\n  );\n  const metadatas = await collectPageItems(responsePages);\n  return metadatas as ReportMetadataShape[];\n};\n\nexport const getReportMetadata = async (\n  reportType: ReportType,\n  state: State,\n  id: string\n) => {\n  const table = reportTables[reportType];\n  const response = await dynamoClient.send(\n    new GetCommand({\n      TableName: table,\n      Key: { state, id },\n    })\n  );\n  return response.Item as ReportMetadataShape | undefined;\n};\n\n/* FIELD DATA (s3) */\n\nexport const putReportFieldData = async (\n  {\n    reportType,\n    state,\n    fieldDataId,\n  }: Pick<ReportMetadataShape, \"reportType\" | \"state\" | \"fieldDataId\">,\n  fieldData: ReportFieldData\n) => {\n  const bucket = reportBuckets[reportType];\n  await s3Client.send(\n    new PutObjectCommand({\n      Bucket: bucket,\n      ContentType: \"application/json\",\n      Key: `fieldData/${state}/${fieldDataId}.json`,\n      Body: JSON.stringify(fieldData),\n    })\n  );\n};\n\nexport const getReportFieldData = async ({\n  reportType,\n  state,\n  fieldDataId,\n}: Pick<ReportMetadataShape, \"reportType\" | \"state\" | \"fieldDataId\">) => {\n  const bucket = reportBuckets[reportType];\n  const response = await s3Client.send(\n    new GetObjectCommand({\n      Bucket: bucket,\n      Key: `fieldData/${state}/${fieldDataId}.json`,\n    })\n  );\n  const fieldData = await parseS3Response(response);\n  return fieldData as ReportFieldData | undefined;\n};\n\n/* FORM TEMPLATES (s3) */\n\nexport const putReportFormTemplate = async (\n  {\n    reportType,\n    formTemplateId,\n  }: Pick<ReportMetadataShape, \"reportType\" | \"formTemplateId\">,\n  formTemplate: ReportJson\n) => {\n  const bucket = reportBuckets[reportType];\n  await s3Client.send(\n    new PutObjectCommand({\n      Bucket: bucket,\n      ContentType: \"application/json\",\n      Key: `formTemplates/${formTemplateId}.json`,\n      Body: JSON.stringify(formTemplate),\n    })\n  );\n};\n\nexport const getReportFormTemplate = async ({\n  reportType,\n  formTemplateId,\n}: Pick<ReportMetadataShape, \"reportType\" | \"formTemplateId\">) => {\n  const bucket = reportBuckets[reportType];\n  const response = await s3Client.send(\n    new GetObjectCommand({\n      Bucket: bucket,\n      Key: `formTemplates/${formTemplateId}.json`,\n    })\n  );\n  const fieldData = await parseS3Response(response);\n  return fieldData as ReportJson | undefined;\n};\n\n/* FORM TEMPLATE VERSIONS (dynamo) */\n\nexport const putFormTemplateVersion = async (\n  formTemplateVersion: FormTemplateVersion\n) => {\n  await dynamoClient.send(\n    new PutCommand({\n      TableName: formTemplateVersionTable,\n      Item: formTemplateVersion,\n    })\n  );\n};\n\nexport const queryFormTemplateVersionByHash = async (\n  reportType: ReportType,\n  md5Hash: string\n) => {\n  const response = await dynamoClient.send(\n    new QueryCommand({\n      TableName: formTemplateVersionTable,\n      IndexName: \"HashIndex\",\n      KeyConditionExpression: \"reportType = :reportType AND md5Hash = :md5Hash\",\n      ExpressionAttributeValues: {\n        \":reportType\": reportType,\n        \":md5Hash\": md5Hash,\n      },\n      Limit: 1,\n    })\n  );\n  return response.Items?.[0] as FormTemplateVersion | undefined;\n};\n\nexport const queryLatestFormTemplateVersionNumber = async (\n  reportType: ReportType\n) => {\n  const response = await dynamoClient.send(\n    new QueryCommand({\n      TableName: formTemplateVersionTable,\n      KeyConditionExpression: \"reportType = :reportType\",\n      ExpressionAttributeValues: { \":reportType\": reportType },\n      Limit: 1,\n      ScanIndexForward: false, // false -> backwards -> highest version first\n    })\n  );\n  const latestFormTemplate = response.Items?.[0] as\n    | FormTemplateVersion\n    | undefined;\n  return latestFormTemplate?.versionNumber ?? 0;\n};\n", "import {\n  DynamoDBClient,\n  QueryCommandOutput,\n  ScanCommandOutput,\n} from \"@aws-sdk/client-dynamodb\";\nimport { DynamoDBDocumentClient, Paginator } from \"@aws-sdk/lib-dynamodb\";\n// utils\nimport { logger } from \"../utils/debugging/debug-lib\";\n\nconst localConfig = {\n  endpoint: process.env.DYNAMODB_URL,\n  region: \"localhost\",\n  credentials: {\n    accessKeyId: \"LOCALFAKEKEY\", // pragma: allowlist secret\n    secretAccessKey: \"LOCALFAKESECRET\", // pragma: allowlist secret\n  },\n  logger,\n};\n\nconst awsConfig = {\n  region: \"us-east-1\",\n  logger,\n};\n\nconst getConfig = () => {\n  return process.env.DYNAMODB_URL ? localConfig : awsConfig;\n};\n\nexport const createClient = () => {\n  return DynamoDBDocumentClient.from(new DynamoDBClient(getConfig()));\n};\n\nexport const collectPageItems = async <\n  T extends QueryCommandOutput | ScanCommandOutput\n>(\n  paginator: Paginator<T>\n) => {\n  let items: Record<string, any>[] = [];\n  for await (let page of paginator) {\n    items = items.concat(page.Items ?? []);\n  }\n  return items;\n};\n", "import { S3Client, GetObjectCommandOutput } from \"@aws-sdk/client-s3\";\nimport { logger } from \"../utils/debugging/debug-lib\";\n\nconst localConfig = {\n  endpoint: process.env.S3_LOCAL_ENDPOINT,\n  region: \"localhost\",\n  forcePathStyle: true,\n  credentials: {\n    accessKeyId: \"S3RVER\", // pragma: allowlist secret\n    secretAccessKey: \"S3RVER\", // pragma: allowlist secret\n  },\n  logger,\n};\n\nconst awsConfig = {\n  region: \"us-east-1\",\n  logger,\n};\n\nconst getConfig = () => {\n  return process.env.S3_LOCAL_ENDPOINT ? localConfig : awsConfig;\n};\n\nexport const createClient = () => new S3Client(getConfig());\n\nexport const parseS3Response = async (response: GetObjectCommandOutput) => {\n  const stringBody = await response.Body?.transformToString();\n  if (!stringBody) {\n    logger.warn(`Empty response from S3`);\n    return undefined;\n  }\n  return JSON.parse(stringBody);\n};\n", "import {\n  getReportFieldData,\n  queryReportMetadatasForState,\n} from \"../../storage/reports\";\nimport { States } from \"../constants/constants\";\nimport {\n  AnyObject,\n  ReportFieldData,\n  ReportMetadataShape,\n  ReportStatus,\n  ReportType,\n  State,\n} from \"../types\";\n\nexport const createReportName = (\n  reportType: string,\n  reportPeriod: number,\n  state: State,\n  reportYear?: number,\n  workPlan?: ReportMetadataShape\n) => {\n  const reportName = reportType;\n  const period =\n    reportType === ReportType.SAR ? workPlan?.reportPeriod : reportPeriod;\n\n  const fullStateName = States[state];\n  return `${fullStateName} MFP ${reportName} ${reportYear} - Period ${period}`;\n};\n\nexport const getEligibleWorkPlan = async (\n  state: State\n): Promise<{\n  workPlanMetadata?: ReportMetadataShape;\n  workPlanFieldData?: ReportFieldData;\n}> => {\n  const allWorkPlans = await queryReportMetadatasForState(ReportType.WP, state);\n  const eligibleWorkPlans = allWorkPlans.filter(\n    (wp) =>\n      wp.status === ReportStatus.APPROVED && !wp.associatedSar && !wp?.archived\n  );\n  if (eligibleWorkPlans.length === 0) {\n    // There were no eligible work plans to treat as a base for this SAR\n    return { workPlanMetadata: undefined, workPlanFieldData: undefined };\n  }\n\n  const workPlanMetadata = eligibleWorkPlans.reduce((mostRecent, wp) =>\n    mostRecent.createdAt < wp.createdAt ? mostRecent : wp\n  );\n  const workPlanFieldData = await getReportFieldData(workPlanMetadata);\n  return { workPlanMetadata, workPlanFieldData };\n};\n\nexport const getReportYear = (\n  reportData: AnyObject,\n  isCopyOver: boolean = false\n): number => {\n  if (isCopyOver) {\n    if (typeof reportData?.copyReport?.reportYear !== \"number\") {\n      throw new Error(\"Invalid value for reportYear\");\n    }\n    const prevReportYear = reportData?.copyReport?.reportYear;\n    const prevReportPeriod = reportData?.copyReport?.reportPeriod;\n\n    return prevReportPeriod === 2 ? prevReportYear + 1 : prevReportYear;\n  }\n\n  if (typeof reportData.reportYear !== \"number\") {\n    throw new Error(\"Invalid value for reportYear\");\n  }\n\n  return reportData?.reportYear;\n};\n\nexport const getReportPeriod = (\n  reportData: AnyObject,\n  isCopyOver: boolean = false\n): number => {\n  if (isCopyOver) {\n    if (typeof reportData?.copyReport?.reportPeriod !== \"number\") {\n      throw new Error(\"Invalid value for reportPeriod\");\n    }\n\n    let prevReportPeriod = reportData?.copyReport?.reportPeriod;\n\n    return (prevReportPeriod % 2) + 1;\n  }\n\n  if (typeof reportData.reportPeriod !== \"number\") {\n    throw new Error(\"Invalid value for reportPeriod\");\n  }\n\n  return reportData?.reportPeriod;\n};\n", "{\n  \"type\": \"WP\",\n  \"name\": \"MFP Work Plan\",\n  \"basePath\": \"/wp\",\n  \"version\": \"WP_2023-08-21\",\n  \"entities\": {\n    \"targetPopulation\": { \"required\": true },\n    \"initiative\": { \"required\": true }\n  },\n  \"routes\": [\n    {\n      \"name\": \"General Information\",\n      \"path\": \"/wp/general-information\",\n      \"pageType\": \"standard\",\n      \"verbiage\": {\n        \"intro\": {\n          \"section\": \"\",\n          \"subsection\": \"General Information\",\n          \"info\": [\n            {\n              \"type\": \"html\",\n              \"content\": \"The Money Follows the Person (MFP) Demonstration Work Plan is the state or territory\u2019s road map for accomplishing the rebalancing objective described in section \"\n            },\n            {\n              \"type\": \"externalLink\",\n              \"content\": \"6071(a)(1) of the Deficit Reduction Act (DRA)\",\n              \"props\": {\n                \"href\": \"https://www.govinfo.gov/content/pkg/PLAW-109publ171/pdf/PLAW-109publ171.pdf\",\n                \"target\": \"_blank\",\n                \"aria-label\": \"6071(a)(1) of the Deficit Reduction Act (DRA) (Link opens in new tab)\"\n              }\n            },\n            {\n              \"type\": \"html\",\n              \"content\": \" as \u201Cincreasing the use of home and community-based, rather than institutional, long-term care services.\u201D The MFP Work Plan presents MFP Demonstration initiatives that support the state or territory\u2019s unique rebalancing goals and objectives. The MFP Work Plan enables states or territories and Centers for Medicare & Medicaid Services (CMS) to monitor state or territory-specific initiatives throughout the grant and make course corrections where needed. While the MFP Work Plan describes state or territory initiatives and sets performance measures, the MFP Semi-Annual Progress Report will capture progress on these initiatives and performance measures, alongside other information. CMS may amend or add new MFP Work Plan fields during the Demonstration period.\"\n            }\n          ]\n        },\n        \"praDisclosure\": [\n          {\n            \"type\": \"p\",\n            \"content\": \"<b>PRA Disclosure Statement</b>\"\n          },\n          {\n            \"type\": \"p\",\n            \"content\": \"Under the Privacy Act of 1974 any personally identifying information obtained will be kept private to the extent of the law. According to the Paperwork Reduction Act of 1995, no persons are required to respond to a collection of information unless it displays a valid OMB control number. The valid OMB control number for this information collection is 0938-1053. The time required to complete this information collection is estimated to average 2.5 hours per response, including the time to review instructions, search existing data resources, gather the data needed, and complete and review the information collection. If you have comments concerning the accuracy of the time estimate(s) or suggestions for improving this form, please write to: CMS, 7500 Security Boulevard, Attn: PRA Reports Clearance Officer, Mail Stop C4-26-05, Baltimore, Maryland 21244-1850\"\n          }\n        ]\n      },\n      \"form\": { \"id\": \"wp-gi\", \"fields\": [] }\n    },\n    {\n      \"name\": \"Transition Benchmarks\",\n      \"path\": \"/wp/transition-benchmarks\",\n      \"pageType\": \"modalDrawer\",\n      \"entityType\": \"targetPopulations\",\n      \"entityInfo\": [\"transitionBenchmarks_targetPopulationName\"],\n      \"verbiage\": {\n        \"accordion\": {\n          \"buttonLabel\": \"Considerations around target populations\",\n          \"intro\": [\n            {\n              \"type\": \"html\",\n              \"content\": \"An approach for defining target populations should be outlined in your Operational Protocol. Those definitions should be used to report the transition benchmarks. You may need to track some populations outside of these categories to see utilization to address questions from stakeholders and those circumstances should be discussed with your CMS MFP Project Officer. <br><br> In the next section, you will be asked to describe your state or territory-specific initiatives, which may inform which target populations you include here when you think about who is being served by your different planned initiatives.\u00A0<br><br>Additional information on strategies to achieve transition targets will be included in the state or territory-specific initiative on transitions and transition coordination services in the next section.\"\n            }\n          ]\n        },\n        \"intro\": {\n          \"section\": \"\",\n          \"subsection\": \"Transition Benchmark Projections\",\n          \"info\": [\n            {\n              \"type\": \"html\",\n              \"content\": \"Provide the projected number of transitions for each target population during each quarter. This number includes qualified institutional residents who enroll in MFP, and are anticipated to be discharged from an institution to a qualified residence during the reporting period in the quarter.\"\n            }\n          ]\n        },\n        \"dashboardTitle\": \"Report projected number of transitions for each target population\",\n        \"pdfDashboardTitle\": \"Transition Benchmark Totals\",\n        \"addEntityButtonText\": \"Add other target population\",\n        \"editEntityButtonText\": \"Edit name\",\n        \"readOnlyEntityButtonText\": \"View name\",\n        \"addEditModalAddTitle\": \"Add other target population\",\n        \"addEditModalEditTitle\": \"Edit other target population\",\n        \"deleteEntityButtonAltText\": \"Delete other target population\",\n        \"deleteModalTitle\": \"Are you sure you want to delete this target population?\",\n        \"deleteModalConfirmButtonText\": \"Yes, delete population\",\n        \"deleteModalWarning\": \"Are you sure you want to proceed? You will lose all information entered for this population in the MFP Work Plan. The population will remain in previously submitted MFP Semi-Annual Progress Reports if applicable.\",\n        \"entityUnfinishedMessage\": \"Complete the remaining indicators for this access measure by entering details.\",\n        \"enterEntityDetailsButtonText\": \"Edit\",\n        \"readOnlyEntityDetailsButtonText\": \"View\",\n        \"reviewPdfHint\": \"To view Transition Benchmark Totals by target population and by quarter, click \\\"Review PDF\\\" and it will open a summary in a new tab.\",\n        \"drawerTitle\": \"Report transition benchmarks for \",\n        \"drawerInfo\": [\n          {\n            \"type\": \"span\",\n            \"content\": \"Provide the projected number of transitions for the target population during each quarter. This number includes institutional residents who are discharged from an institution to a qualified residence during the reporting period, enroll in MFP, and begin using Medicaid home and community-based services (HCBS).\"\n          },\n          {\n            \"type\": \"p\",\n            \"content\": \"Complete all fields and select the \\\"Save & close\\\" button to save this section.\"\n          }\n        ]\n      },\n      \"modalForm\": {\n        \"id\": \"tb-modal\",\n        \"fields\": [\n          {\n            \"id\": \"transitionBenchmarks_targetPopulationName\",\n            \"type\": \"text\",\n            \"validation\": \"text\",\n            \"props\": {\n              \"label\": \"Target population name\",\n              \"hint\": \"Specify an \\\"other\\\" target population applicable to your MFP Demonstration project. (e.g., HIV/AIDS, brain injury).\"\n            }\n          }\n        ]\n      },\n      \"drawerForm\": {\n        \"id\": \"tb-drawer\",\n        \"fields\": [\n          {\n            \"id\": \"transitionBenchmarks_applicableToMfpDemonstration\",\n            \"type\": \"radio\",\n            \"validation\": \"radio\",\n            \"props\": {\n              \"label\": \"Is this target population applicable to your MFP Demonstration?\",\n              \"hint\": \"Enter 0 for quarters with no projected transitions. Enter N/A for quarters you do not expect to report.\",\n              \"choices\": [\n                {\n                  \"id\": \"2UObIwERkSKEGVUU1g8E1v\",\n                  \"label\": \"No\"\n                },\n                {\n                  \"id\": \"2UObIuHjl15upf6tLcgcWd\",\n                  \"label\": \"Yes\",\n                  \"children\": [\n                    {\n                      \"id\": \"quarterlyProjections\",\n                      \"type\": \"number\",\n                      \"validation\": {\n                        \"type\": \"validInteger\",\n                        \"parentFieldName\": \"transitionBenchmarks_applicableToMfpDemonstration\",\n                        \"parentOptionId\": \"transitionBenchmarks_applicableToMfpDemonstration-2UObIuHjl15upf6tLcgcWd\",\n                        \"nested\": true\n                      },\n                      \"transformation\": {\n                        \"rule\": \"nextTwelveQuarters\"\n                      }\n                    }\n                  ]\n                }\n              ]\n            }\n          }\n        ]\n      }\n    },\n    {\n      \"name\": \"Transition Benchmark Strategy\",\n      \"path\": \"/wp/transition-benchmark-strategy\",\n      \"pageType\": \"standard\",\n      \"verbiage\": {\n        \"intro\": {\n          \"section\": \"\",\n          \"subsection\": \"Transition Benchmark Strategy\"\n        }\n      },\n      \"form\": {\n        \"id\": \"tbs\",\n        \"fields\": [\n          {\n            \"id\": \"strategy_explaination\",\n            \"type\": \"textarea\",\n            \"validation\": \"text\",\n            \"props\": {\n              \"label\": \"Explain how you formulated your projected numbers, which should include descriptions of the data sources used, the time period for the analysis, and the methods used to project the number of transitions.\"\n            }\n          },\n          {\n            \"id\": \"strategy_additionalDetails\",\n            \"type\": \"textarea\",\n            \"validation\": \"text\",\n            \"props\": {\n              \"label\": \"Provide additional detail on strategies or approaches the state or territory will use to achieve transition targets here and through a required state or territory-specific initiative.\",\n              \"className\": \"tbs-bottom-margin\"\n            }\n          }\n        ]\n      }\n    },\n    {\n      \"name\": \"State or Territory-Specific Initiatives\",\n      \"path\": \"/wp/state-or-territory-specific-initiatives\",\n      \"children\": [\n        {\n          \"name\": \"State or Territory-Specific Initiatives Instructions\",\n          \"path\": \"/wp/state-or-territory-specific-initiatives/instructions\",\n          \"pageType\": \"standard\",\n          \"verbiage\": {\n            \"intro\": {\n              \"section\": \"\",\n              \"subsection\": \"State or Territory-Specific Initiatives Instructions\",\n              \"spreadsheet\": \"\",\n              \"info\": [\n                {\n                  \"type\": \"html\",\n                  \"content\": \"State or territory-specific initiatives are a distinct set of activities designed to increase the use of home and community-based services (HCBS) rather than institutional long-term services and supports. These initiatives can be funded using one or more of these funding sources:<br>\"\n                },\n                {\n                  \"type\": \"ul\",\n                  \"content\": \"\",\n                  \"props\": {\n                    \"style\": {\n                      \"marginLeft\": \"1.5rem\",\n                      \"padding\": \"1rem\"\n                    }\n                  },\n                  \"children\": [\n                    {\n                      \"type\": \"li\",\n                      \"content\": \"MFP cooperative agreement funds for:\"\n                    },\n                    {\n                      \"content\": \"\",\n                      \"type\": \"ul\",\n                      \"props\": {\n                        \"style\": {\n                          \"marginLeft\": \"1rem\",\n                          \"paddingTop\": \"1rem\"\n                        }\n                      },\n                      \"children\": [\n                        {\n                          \"type\": \"li\",\n                          \"props\": {\n                            \"style\": {\n                              \"paddingBottom\": \"1rem\"\n                            }\n                          },\n                          \"content\": \"Qualified HCBS and demonstration services\"\n                        },\n                        {\n                          \"type\": \"li\",\n                          \"props\": {\n                            \"style\": {\n                              \"paddingBottom\": \"1rem\"\n                            }\n                          },\n                          \"content\": \"Supplemental services\"\n                        },\n                        {\n                          \"type\": \"li\",\n                          \"props\": {\n                            \"style\": {\n                              \"paddingBottom\": \"1rem\"\n                            }\n                          },\n                          \"content\": \"Administrative activities\"\n                        },\n                        {\n                          \"type\": \"li\",\n                          \"props\": {\n                            \"style\": {\n                              \"paddingBottom\": \"1rem\"\n                            }\n                          },\n                          \"content\": \"Capacity building initiatives\"\n                        }\n                      ]\n                    },\n                    {\n                      \"type\": \"li\",\n                      \"content\": \"State or territory equivalent funds attributable to the MFP-enhanced match\"\n                    }\n                  ]\n                },\n                {\n                  \"type\": \"p\",\n                  \"content\": \"Recipients must identify and describe the required initiatives below and have the option to identify additional initiatives on other topics.\",\n                  \"paddingBottom\": \"1rem\"\n                },\n                {\n                  \"type\": \"table\",\n                  \"content\": \"\",\n                  \"props\": {\n                    \"className\": \"mdct-table\"\n                  },\n                  \"children\": [\n                    {\n                      \"type\": \"thead\",\n                      \"content\": \"\",\n                      \"children\": [\n                        {\n                          \"type\": \"tr\",\n                          \"content\": \"\",\n                          \"children\": [\n                            {\n                              \"type\": \"th\",\n                              \"content\": \"<span aria-label='Required by Program Terms and Conditions'>Required* Initiatives</span>\"\n                            },\n                            {\n                              \"type\": \"th\",\n                              \"content\": \"Optional Initiatives\"\n                            }\n                          ]\n                        }\n                      ]\n                    },\n                    {\n                      \"type\": \"tbody\",\n                      \"content\": \"\",\n                      \"children\": [\n                        {\n                          \"type\": \"tr\",\n                          \"content\": \"\",\n                          \"children\": [\n                            {\n                              \"type\": \"td\",\n                              \"content\": \"\",\n                              \"props\": {\n                                \"style\": {\n                                  \"verticalAlign\": \"top\",\n                                  \"borderBottom\": \"1px solid #D9D9D9\"\n                                }\n                              },\n                              \"children\": [\n                                {\n                                  \"type\": \"ul\",\n                                  \"content\": \"\",\n                                  \"children\": [\n                                    {\n                                      \"type\": \"li\",\n                                      \"content\": \"Transitions and transition coordination services\",\n                                      \"props\": {\n                                        \"style\": {\n                                          \"paddingBottom\": \".5rem\"\n                                        }\n                                      }\n                                    },\n                                    {\n                                      \"type\": \"li\",\n                                      \"content\": \"Housing-related supports\",\n                                      \"props\": {\n                                        \"style\": {\n                                          \"paddingBottom\": \".5rem\"\n                                        }\n                                      }\n                                    },\n                                    {\n                                      \"type\": \"li\",\n                                      \"content\": \"Quality measurement and improvement\",\n                                      \"props\": {\n                                        \"style\": {\n                                          \"paddingBottom\": \".5rem\"\n                                        }\n                                      }\n                                    },\n                                    {\n                                      \"type\": \"li\",\n                                      \"content\": \"Self-direction (if applicable)\",\n                                      \"props\": {\n                                        \"style\": {\n                                          \"paddingBottom\": \".5rem\"\n                                        }\n                                      }\n                                    },\n                                    {\n                                      \"type\": \"li\",\n                                      \"content\": \"Tribal initiative (if applicable)\",\n                                      \"props\": {\n                                        \"style\": {\n                                          \"paddingBottom\": \".5rem\"\n                                        }\n                                      }\n                                    }\n                                  ]\n                                }\n                              ]\n                            },\n                            {\n                              \"type\": \"td\",\n                              \"content\": \"\",\n                              \"props\": {\n                                \"style\": {\n                                  \"borderBottom\": \"1px solid #D9D9D9\"\n                                }\n                              },\n                              \"children\": [\n                                {\n                                  \"type\": \"ul\",\n                                  \"content\": \"\",\n                                  \"children\": [\n                                    {\n                                      \"type\": \"li\",\n                                      \"content\": \"Recruitment and enrollment\",\n                                      \"props\": {\n                                        \"style\": {\n                                          \"paddingBottom\": \".5rem\"\n                                        }\n                                      }\n                                    },\n                                    {\n                                      \"type\": \"li\",\n                                      \"content\": \"Person-centered planning and services\",\n                                      \"props\": {\n                                        \"style\": {\n                                          \"paddingBottom\": \".5rem\"\n                                        }\n                                      }\n                                    },\n                                    {\n                                      \"type\": \"li\",\n                                      \"content\": \"No Wrong Door systems\",\n                                      \"props\": {\n                                        \"style\": {\n                                          \"paddingBottom\": \".5rem\"\n                                        }\n                                      }\n                                    },\n                                    {\n                                      \"type\": \"li\",\n                                      \"content\": \"Community transition support\",\n                                      \"props\": {\n                                        \"style\": {\n                                          \"paddingBottom\": \".5rem\"\n                                        }\n                                      }\n                                    },\n                                    {\n                                      \"type\": \"li\",\n                                      \"content\": \"Direct service workforce and caregivers\",\n                                      \"props\": {\n                                        \"style\": {\n                                          \"paddingBottom\": \".5rem\"\n                                        }\n                                      }\n                                    },\n                                    {\n                                      \"type\": \"li\",\n                                      \"content\": \"Employment support\",\n                                      \"props\": {\n                                        \"style\": {\n                                          \"paddingBottom\": \".5rem\"\n                                        }\n                                      }\n                                    },\n                                    {\n                                      \"type\": \"li\",\n                                      \"content\": \"Convenient and accessible transportation options\",\n                                      \"props\": {\n                                        \"style\": {\n                                          \"paddingBottom\": \".5rem\"\n                                        }\n                                      }\n                                    },\n                                    {\n                                      \"type\": \"li\",\n                                      \"content\": \"Data-based decision-making\",\n                                      \"props\": {\n                                        \"style\": {\n                                          \"paddingBottom\": \".5rem\"\n                                        }\n                                      }\n                                    },\n                                    {\n                                      \"type\": \"li\",\n                                      \"content\": \"Financing approaches\",\n                                      \"props\": {\n                                        \"style\": {\n                                          \"paddingBottom\": \".5rem\"\n                                        }\n                                      }\n                                    },\n                                    {\n                                      \"type\": \"li\",\n                                      \"content\": \"Stakeholder engagement\",\n                                      \"props\": {\n                                        \"style\": {\n                                          \"paddingBottom\": \".5rem\"\n                                        }\n                                      }\n                                    },\n                                    {\n                                      \"type\": \"li\",\n                                      \"content\": \"Equity and social determinants of health (SDOH)\",\n                                      \"props\": {\n                                        \"style\": {\n                                          \"paddingBottom\": \".5rem\"\n                                        }\n                                      }\n                                    },\n                                    {\n                                      \"type\": \"li\",\n                                      \"content\": \"Other\"\n                                    }\n                                  ]\n                                }\n                              ]\n                            }\n                          ]\n                        }\n                      ]\n                    },\n                    {\n                      \"type\": \"tfoot\",\n                      \"content\": \"\"\n                    }\n                  ]\n                },\n                {\n                  \"type\": \"text\",\n                  \"content\": \"*Required by Program Terms and Conditions\",\n                  \"props\": {\n                    \"className\": \"mdct-smalltext\"\n                  }\n                },\n                {\n                  \"type\": \"html\",\n                  \"content\": \"<br>For each initiative, recipients will be asked to provide:<br>\"\n                },\n                {\n                  \"type\": \"ol\",\n                  \"content\": \"\",\n                  \"props\": {\n                    \"styleType\": \"upper-roman\",\n                    \"style\": {\n                      \"marginLeft\": \"1.5rem\",\n                      \"padding\": \"1rem\"\n                    }\n                  },\n                  \"children\": [\n                    {\n                      \"type\": \"li\",\n                      \"content\": \"Initiative description, including target populations and timeframe\",\n                      \"props\": {\n                        \"style\": {\n                          \"padding\": \".5rem\"\n                        }\n                      }\n                    },\n                    {\n                      \"type\": \"li\",\n                      \"content\": \"An evaluation plan, including measurable objectives\",\n                      \"props\": {\n                        \"style\": {\n                          \"padding\": \".5rem\"\n                        }\n                      }\n                    },\n                    {\n                      \"type\": \"li\",\n                      \"content\": \"Funding sources, with projected quarterly expenditures\",\n                      \"props\": {\n                        \"style\": {\n                          \"padding\": \".5rem\"\n                        }\n                      }\n                    },\n                    {\n                      \"type\": \"li\",\n                      \"content\": \"Initiative close-out information, to be completed as appropriate during MFP Work Plan revisions\",\n                      \"props\": {\n                        \"style\": {\n                          \"padding\": \".5rem\"\n                        }\n                      }\n                    }\n                  ]\n                },\n                {\n                  \"type\": \"html\",\n                  \"content\": \"<br>The MFP Work Plan should establish one or more demonstrable objectives for each initiative, set associated performance measures or indicators to monitor progress, and clearly articulate the actions necessary to achieve the objectives. Progress towards meeting these objectives indicates a state\u2019s or territory\u2019s increased capacity to provide HCBS, rather than institutional, long-term care services.<br>\"\n                },\n                {\n                  \"type\": \"html\",\n                  \"content\": \"<br>The recipient must identify the MFP funding source(s) for each initiative and provide quarterly projected spending by funding source. Funding sources for initiatives include state or territory funds equivalent to the MFP-enhanced Federal Medical Assistance Percentage (FMAP); MFP capacity building funding; MFP funding for qualified HCBS, demonstration services, and supplemental services; or MFP administrative cooperative agreement funding.<br>\"\n                },\n                {\n                  \"type\": \"html\",\n                  \"content\": \"<br>If a recipient updates the MFP Work Plan to indicate that an initiative will no longer be sustained with MFP funding or state or territory-equivalent funding, the recipient must explain whether the initiative will be terminated or sustained through another funding source.<br><br>\"\n                },\n                {\n                  \"type\": \"text\",\n                  \"content\": \"Answer the following questions regarding required initiative topics. This is necessary in order to track completion of required data.\",\n                  \"props\": {\n                    \"style\": {\n                      \"color\": \"#5B616B\",\n                      \"borderTop\": \"1px solid #D9D9D9\",\n                      \"paddingTop\": \"2rem\"\n                    }\n                  }\n                }\n              ]\n            }\n          },\n          \"form\": {\n            \"id\": \"sdii\",\n            \"fields\": [\n              {\n                \"id\": \"instructions_selfDirectedInitiatives\",\n                \"type\": \"radio\",\n                \"validation\": \"radio\",\n                \"props\": {\n                  \"label\": \"Are self-directed initiatives applicable to your state or territory?\",\n                  \"choices\": [\n                    {\n                      \"id\": \"UG7uunqq5UCtUq1is3iyiw\",\n                      \"label\": \"Yes\"\n                    },\n                    {\n                      \"id\": \"3DGAqqnOBE2kwKVFMxUt3A\",\n                      \"label\": \"No\"\n                    }\n                  ]\n                }\n              },\n              {\n                \"id\": \"instructions_tribalInitiatives\",\n                \"type\": \"radio\",\n                \"validation\": \"radio\",\n                \"props\": {\n                  \"label\": \"Are Tribal Initiatives applicable to your state or territory?\",\n                  \"choices\": [\n                    {\n                      \"id\": \"UG7uunqq5UCtUq1is3iyiw\",\n                      \"label\": \"Yes\"\n                    },\n                    {\n                      \"id\": \"3DGAqqnOBE2kwKVFMxUt3A\",\n                      \"label\": \"No\"\n                    }\n                  ]\n                }\n              }\n            ]\n          }\n        },\n        {\n          \"name\": \"State or Territory-Specific Initiatives\",\n          \"path\": \"/wp/state-or-territory-specific-initiatives/initiatives\",\n          \"pageType\": \"modalOverlay\",\n          \"entityType\": \"initiative\",\n          \"entityInfo\": [\"initiative_name\", \"initiative_wpTopic\"],\n          \"verbiage\": {\n            \"intro\": {\n              \"section\": \"\",\n              \"subsection\": \"State or Territory-Specific Initiatives\",\n              \"info\": [\n                {\n                  \"type\": \"html\",\n                  \"content\": \"See \"\n                },\n                {\n                  \"type\": \"internalLink\",\n                  \"content\": \"previous page\",\n                  \"props\": {\n                    \"to\": \"/wp/state-or-territory-specific-initiatives/instructions\",\n                    \"style\": {\n                      \"textDecoration\": \"underline\"\n                    }\n                  }\n                },\n                {\n                  \"type\": \"html\",\n                  \"content\": \" for detailed instructions.\"\n                }\n              ]\n            },\n            \"addEntityButtonText\": \"Add initiative\",\n            \"editEntityHint\": \"Select \\\"Edit\\\" to complete the details.\",\n\n            \"editEntityButtonText\": \"Edit name/topic\",\n            \"readOnlyEntityButtonText\": \"View name/topic\",\n            \"addEditModalAddTitle\": \"Add initiative\",\n            \"addEditModalEditTitle\": \"Edit initiative\",\n            \"deleteModalTitle\": \"Are you sure you want to delete this initiative?\",\n            \"deleteModalConfirmButtonText\": \"Yes, delete initiative\",\n            \"deleteModalWarning\": \"Are you sure you want to proceed? You will lose all information entered for this initiative in the MFP Work Plan.\",\n            \"enterEntityDetailsButtonText\": \"Edit\",\n            \"readOnlyEntityDetailsButtonText\": \"View\",\n            \"dashboardTitle\": \"Initiative total count:\",\n            \"countEntitiesInTitle\": true,\n            \"tableHeader\": \"Initiative name <br/> MFP Work Plan topic\",\n            \"addEditModalHint\": \"Provide the name of one initiative. You will be then be asked to complete details for this initiative including a description, evaluation plan and funding sources.\"\n          },\n          \"modalForm\": {\n            \"id\": \"add_initiative\",\n            \"fields\": [\n              {\n                \"id\": \"initiative_name\",\n                \"type\": \"textarea\",\n                \"validation\": \"text\",\n                \"props\": {\n                  \"label\": \"Initiative name\"\n                }\n              },\n              {\n                \"id\": \"initiative_wpTopic\",\n                \"type\": \"radio\",\n                \"validation\": \"radio\",\n                \"props\": {\n                  \"label\": \"MFP Work Plan topic:\",\n                  \"hint\": \"Note: Initiative topics with <span aria-label='asterisk'>*</span> are required and must be selected at least once across all initiatives.\",\n                  \"choices\": [\n                    {\n                      \"id\": \"VjQ0OFqior9Dxu5RRNiZ5u\",\n                      \"label\": \"Transitions and transition coordination services*\"\n                    },\n                    {\n                      \"id\": \"wbUsMMqVP7q1n10szK5h5S\",\n                      \"label\": \"Housing-related supports*\"\n                    },\n                    {\n                      \"id\": \"SdaFlF3DJyzKcHCCu3Zylm\",\n                      \"label\": \"Quality measurement and improvement*\"\n                    },\n                    {\n                      \"id\": \"8CpFrev6sMfRijIhafMj7V\",\n                      \"label\": \"Self-direction (*if applicable)\"\n                    },\n                    {\n                      \"id\": \"tVURShWTPfVKGU94QmIwDn\",\n                      \"label\": \"Tribal Initiative (*if applicable)\"\n                    },\n                    {\n                      \"id\": \"1k3EnM5WrizX3hsa6Zn85G\",\n                      \"label\": \"Recruitment and enrollment\"\n                    },\n                    {\n                      \"id\": \"dtybJ8ZucoIn7a4LnMpWg2\",\n                      \"label\": \"Person-centered planning and services\"\n                    },\n                    {\n                      \"id\": \"rSTGMVEOaJ4OZ6amTQetaa\",\n                      \"label\": \"No Wrong Door systems\"\n                    },\n                    {\n                      \"id\": \"8In9QpCC7O3XBkDOyB36vy\",\n                      \"label\": \"Community transition support\"\n                    },\n                    {\n                      \"id\": \"GCBzQ9GDWMwILW0sBQ2dhN\",\n                      \"label\": \"Direct service workforce and caregivers\"\n                    },\n                    {\n                      \"id\": \"jZCOy2pgOiAxgnHDfizer4\",\n                      \"label\": \"Employment support\"\n                    },\n                    {\n                      \"id\": \"xpdOyHiM2GesrYekAT2s1U\",\n                      \"label\": \"Convenient and accessible transportation options\"\n                    },\n                    {\n                      \"id\": \"K8WifjAU3SymG751jAvv6j\",\n                      \"label\": \"Data-based decision-making\"\n                    },\n                    {\n                      \"id\": \"39oSwSqVoDpLGbD9HnfUhg\",\n                      \"label\": \"Financing approaches\"\n                    },\n                    {\n                      \"id\": \"I9A6C2SY0Dk3ezfvywqqwB\",\n                      \"label\": \"Stakeholder engagement\"\n                    },\n                    {\n                      \"id\": \"2qjBuLtpA5pDvUM1HSHMVq\",\n                      \"label\": \"Equity and social determinants of health (SDOH)\"\n                    },\n                    {\n                      \"id\": \"18Wb9b2zMIF13pZwWstdJF\",\n                      \"label\": \"Other, specify\",\n                      \"children\": [\n                        {\n                          \"id\": \"initiative_wp_otherTopic\",\n                          \"type\": \"text\",\n                          \"validation\": {\n                            \"type\": \"text\",\n                            \"nested\": true,\n                            \"parentFieldName\": \"initiative_wpTopic\"\n                          }\n                        }\n                      ]\n                    }\n                  ]\n                }\n              }\n            ]\n          },\n          \"dashboard\": {\n            \"name\": \"Intitiative Details Dashboard\",\n            \"verbiage\": {\n              \"intro\": {\n                \"section\": \"State or Territory-Specific Initiatives\",\n                \"info\": [\n                  {\n                    \"type\": \"span\",\n                    \"content\": \"Complete each of the first 3 steps for this initiative. Step IV is to be completed when this initiative will no longer be sustained with MFP funding and/or state or territory equivalent funding, and will become available once your initiative information is complete and approved by your CMS MFP Project Officer.\"\n                  }\n                ]\n              },\n              \"closeDashboardButtonText\": \"Close Initiative\"\n            }\n          },\n          \"entitySteps\": [\n            {\n              \"name\": \"State or Territory-Specific Initiatives: I. Define initiative\",\n              \"path\": \"/wp/state-or-territory-specific-initiatives/define-initiative\",\n              \"pageType\": \"entityOverlay\",\n              \"entityType\": \"initiative\",\n              \"stepType\": \"defineInitiative\",\n              \"stepInfo\": [\"stepName\", \"hint\"],\n              \"stepName\": \"I. Define initiative\",\n              \"hint\": \"Provide initiative description, including target populations and timeframe\",\n              \"isRequired\": true,\n              \"verbiage\": {\n                \"intro\": {\n                  \"section\": \"State or Territory-Specific Initiatives: I. Define initiative\",\n                  \"info\": [\n                    {\n                      \"type\": \"html\",\n                      \"content\": \"Provide initiative description, including target populations and timeframe.\"\n                    }\n                  ],\n                  \"exportSectionHeader\": \"exportSectionHeader\"\n                },\n                \"enterEntityDetailsButtonText\": \"Edit\",\n                \"readOnlyEntityDetailsButtonText\": \"View\",\n                \"editEntityHint\": \"Select \\\"Edit\\\" to complete initiative definition.\"\n              },\n              \"form\": {\n                \"id\": \"stsidi\",\n                \"fields\": [\n                  {\n                    \"id\": \"defineInitiative_describeInitiative\",\n                    \"type\": \"textarea\",\n                    \"validation\": \"text\",\n                    \"props\": {\n                      \"label\": \"Describe the initiative, including key activities:\"\n                    }\n                  },\n                  {\n                    \"id\": \"defineInitiative_targetPopulations\",\n                    \"type\": \"checkbox\",\n                    \"validation\": \"checkbox\",\n                    \"props\": {\n                      \"label\": \"Target population(s):\",\n                      \"hint\": [\n                        {\n                          \"type\": \"html\",\n                          \"content\": \"Select all that apply. \u201COther\u201D population(s) selected and defined in the \"\n                        },\n                        {\n                          \"type\": \"internalLink\",\n                          \"content\": \"Transition Benchmarks\",\n                          \"props\": {\n                            \"to\": \"/wp/transition-benchmarks\"\n                          }\n                        },\n                        {\n                          \"type\": \"html\",\n                          \"content\": \" section automatically upload. Select \u201CHCBS infrastructure/system-level development\u201D for initiatives that strengthen or expand home and community-based services (HCBS).\"\n                        }\n                      ],\n                      \"choices\": []\n                    }\n                  },\n                  {\n                    \"id\": \"defineInitiative_projectedStartDate\",\n                    \"type\": \"date\",\n                    \"validation\": \"date\",\n                    \"props\": {\n                      \"label\": \"Start date\",\n                      \"hint\": \"Enter projected start month/year for future initiatives or enter past start month/year for initiatives in process.\"\n                    }\n                  },\n                  {\n                    \"id\": \"defineInitiative_projectedEndDate\",\n                    \"type\": \"radio\",\n                    \"validation\": \"radio\",\n                    \"props\": {\n                      \"label\": \"Does the initiative have a projected end date?\",\n                      \"hint\": \"Select 'No' if the initiative will be ongoing without a set end point.\",\n                      \"choices\": [\n                        {\n                          \"id\": \"WNsSaAHeDvRD2Pjkz6DcOE\",\n                          \"label\": \"Yes\",\n                          \"children\": [\n                            {\n                              \"id\": \"defineInitiative_projectedEndDate_value\",\n                              \"type\": \"date\",\n                              \"validation\": {\n                                \"type\": \"endDate\",\n                                \"parentOptionId\": \"defineInitiative_projectedEndDate-WNsSaAHeDvRD2Pjkz6DcOE\",\n                                \"parentFieldName\": \"defineInitiative_projectedEndDate\",\n                                \"dependentFieldName\": \"defineInitiative_projectedStartDate\",\n                                \"nested\": true\n                              },\n                              \"props\": {\n                                \"label\": \"Projected end date\",\n                                \"hint\": \"Enter projected end date.\"\n                              }\n                            }\n                          ]\n                        },\n                        {\n                          \"id\": \"TR6HoXF3Unf2QX0zzDg2Kp\",\n                          \"label\": \"No\"\n                        }\n                      ]\n                    }\n                  }\n                ]\n              }\n            },\n            {\n              \"name\": \"State or Territory-Specific Initiatives: II. Evaluation Plan\",\n              \"path\": \"/wp/state-or-territory-specific-initiatives/evaluation-plan\",\n              \"pageType\": \"overlayModal\",\n              \"entityType\": \"initiative\",\n              \"stepType\": \"evaluationPlan\",\n              \"stepInfo\": [\"stepName\", \"hint\"],\n              \"stepName\": \"II. Evaluation plan\",\n              \"hint\": \"Add evaluation plan, including measurable objectives\",\n              \"isRequired\": true,\n              \"verbiage\": {\n                \"intro\": {\n                  \"section\": \"State or Territory-Specific Initiatives: II. Evaluation Plan\",\n                  \"info\": [\n                    {\n                      \"type\": \"html\",\n                      \"content\": \"To complete your evaluation plan, create measurable objectives. Select \u201CAdd objective\u201D button for each objective you need to add to the system.\"\n                    }\n                  ],\n                  \"exportSectionHeader\": \"exportSectionHeader\"\n                },\n                \"accordion\": {\n                  \"buttonLabel\": \"Instructions\",\n                  \"intro\": [\n                    {\n                      \"type\": \"html\",\n                      \"content\": \"The evaluation plan captures expected results for each state or territory-specific initiative. Recipients should identify one or more objectives per initiative and set associated performance measures or indicators to monitor progress toward each objective and evaluate success. In addition, recipients must articulate how they will achieve targets and meet milestones. For more information on developing objectives and identifying appropriate performance measures, see \"\n                    },\n                    {\n                      \"type\": \"externalLink\",\n                      \"content\": \"\u201CUsing Data to Improve Money Follows the Person Program Performance.\u201C\",\n                      \"props\": {\n                        \"href\": \"https://www.medicaid.gov/sites/default/files/2023-01/MFP-Technical-Assistance-Brief.pdf\",\n                        \"target\": \"_blank\",\n                        \"aria-label\": \"\u201CUsing Data to Improve Money Follows the Person Program Performance.\u201C (Link opens in new tab)\"\n                      }\n                    },\n                    {\n                      \"type\": \"html\",\n                      \"content\": \"<br><br><strong>Identify one or more objectives.</strong> Objectives should be framed as SMART goals and have associated time-bound measures of success, including targets or milestones. As a reminder, SMART stands for:\"\n                    }\n                  ],\n                  \"list\": [\n                    \"<strong>Specific:</strong> Specifies the activities, actors, and beneficiaries\",\n                    \"<strong>Measurable:</strong> Defines how a change will be measured\",\n                    \"<strong>Achievable:</strong> Confirms the feasibility of implementing the intervention as planned\",\n                    \"<strong>Realistic/relevant:</strong> Ensures the intervention relates to the goal\",\n                    \"<strong>Time-bound:</strong> Specifies when the results are expected\"\n                  ],\n                  \"text\": \"\"\n                },\n                \"text\": \"To complete your evaluation plan, create measurable objectives. Select \u201CAdd objective\u201D button for each objective you need to add to the system.\",\n                \"addEntityButtonText\": \"Add objective\",\n                \"editEntityHint\": \"Select \\\"Edit\\\" to complete initiative evaluation plan.\",\n                \"editEntityButtonText\": \"Edit objective\",\n                \"readOnlyEntityButtonText\": \"View objective\",\n                \"addEditModalAddTitle\": \"Add objective for \",\n                \"addEditModalHint\": \"Objectives should be framed as SMART goals and have associated time-bound measures of success, including targets or milestones.\",\n                \"addEditModalEditTitle\": \"Edit objective for \",\n                \"deleteModalTitle\": \"Delete objective for this report?\",\n                \"deleteModalConfirmButtonText\": \"Yes, delete objective\",\n                \"deleteModalWarning\": \"You will lose all information entered for this objective. Objective will not be deleted from archived MFP Work Plans and Semi-Annual Progress Reports, but it will not be available in future MFP Work Plans and Semi-Annual Progress Reports. <br><br>Are you sure you want to proceed?\",\n                \"enterEntityDetailsButtonText\": \"Edit\",\n                \"readOnlyEntityDetailsButtonText\": \"View\",\n                \"dashboardTitle\": \"Objective total count\",\n                \"countEntitiesInTitle\": true,\n                \"deleteEntityButtonAltText\": \"\",\n                \"entityUnfinishedMessage\": \"Add the quantitative targets for the next 2 quarters by editing the objective.\",\n                \"drawerTitle\": \"\"\n              },\n              \"modalForm\": {\n                \"id\": \"tb-modal\",\n                \"fields\": [\n                  {\n                    \"id\": \"evaluationPlan_objectiveName\",\n                    \"type\": \"textarea\",\n                    \"validation\": \"text\",\n                    \"props\": {\n                      \"label\": \"Objective\"\n                    }\n                  },\n                  {\n                    \"id\": \"evaluationPlan_description\",\n                    \"type\": \"textarea\",\n                    \"validation\": \"text\",\n                    \"props\": {\n                      \"label\": \"Describe the performance measures or indicators your state or territory will use to monitor progress toward achieving this objective, including details on the calculation of measures if relevant. Describe any key deliverables.\",\n                      \"hint\": \"(e.g., data sources and limitations)\"\n                    }\n                  },\n                  {\n                    \"id\": \"evaluationPlan_targets\",\n                    \"type\": \"textarea\",\n                    \"validation\": \"text\",\n                    \"props\": {\n                      \"label\": \"Provide targets for the performance measures or indicators listed above. Include milestones and expected time frames for key deliverables.\",\n                      \"hint\": \"If a performance measure includes quantitative targets, complete the quarterly fields below.\"\n                    }\n                  },\n                  {\n                    \"id\": \"evaluationPlan_includesTargets\",\n                    \"type\": \"radio\",\n                    \"validation\": \"radio\",\n                    \"props\": {\n                      \"label\": \"Does the performance measure include quantitative targets?\",\n                      \"hint\": \"Fields allow percentages or numbers. If you wish to report percentages, enter the number in the fields and the percentage sign \u201C%\u201D. Enter N/A for quarters you do not expect to report.\",\n                      \"choices\": [\n                        {\n                          \"id\": \"UL4dAeyyvCFAXttxZioacR\",\n                          \"label\": \"No\"\n                        },\n                        {\n                          \"id\": \"7FP4jcg4jK7Ssqp3cCW5vQ\",\n                          \"label\": \"Yes\",\n                          \"children\": [\n                            {\n                              \"id\": \"quarterlyProjections\",\n                              \"type\": \"text\",\n                              \"validation\": {\n                                \"type\": \"text\",\n                                \"parentFieldName\": \"evaluationPlan_includesTargets\",\n                                \"parentOptionId\": \"evaluationPlan_includesTargets-7FP4jcg4jK7Ssqp3cCW5vQ\",\n                                \"nested\": true\n                              },\n                              \"props\": {\n                                \"className\": \"number-field\"\n                              },\n                              \"transformation\": {\n                                \"rule\": \"nextTwelveQuarters\"\n                              }\n                            }\n                          ]\n                        }\n                      ]\n                    }\n                  },\n                  {\n                    \"id\": \"evaluationPlan_additionalDetails\",\n                    \"type\": \"textarea\",\n                    \"validation\": \"text\",\n                    \"props\": {\n                      \"label\": \"Provide additional detail on strategies/approaches to the state or territory will use to achieve targets and/or milestones (building on the initiative description).\",\n                      \"hint\": \"List the responsible state or territory agency parties and any key external partners for achieving this objective.\"\n                    }\n                  }\n                ]\n              }\n            },\n            {\n              \"name\": \"State or Territory-Specific Initiatives: III. Funding sources\",\n              \"path\": \"/wp/state-or-territory-specific-initiatives/funding-sources\",\n              \"pageType\": \"overlayModal\",\n              \"entityType\": \"initiative\",\n              \"stepType\": \"fundingSources\",\n              \"stepInfo\": [\"stepName\", \"hint\"],\n              \"stepName\": \"III. Funding sources\",\n              \"hint\": \"Add funding sources with projected quarterly expenditures\",\n              \"isRequired\": true,\n              \"verbiage\": {\n                \"intro\": {\n                  \"section\": \"State or Territory-Specific Initiatives: III. Funding sources\",\n                  \"info\": [\n                    {\n                      \"type\": \"html\",\n                      \"content\": \"<br>Provide projected quarterly expenditures, by funding source, for this initiative. Actual quarterly expenditures will be reported in the recipient\u2019s MFP Semi-Annual Progress Report.</br>\"\n                    }\n                  ],\n                  \"exportSectionHeader\": \"exportSectionHeader\"\n                },\n                \"addEntityButtonText\": \"Add funding source\",\n                \"editEntityHint\": \"Select \\\"Edit\\\" to complete initiative funding information.\",\n                \"editEntityButtonText\": \"Edit funding source\",\n                \"readOnlyEntityButtonText\": \"View funding source\",\n                \"addEditModalAddTitle\": \"Add funding source and projected expenditures for \",\n                \"addEditModalEditTitle\": \"Edit funding source and projected expenditures for \",\n                \"deleteEntityButtonAltText\": \"Delete other target population\",\n                \"deleteModalTitle\": \"Delete funding source for this report? \",\n                \"deleteModalConfirmButtonText\": \"Yes, delete funding source\",\n                \"deleteModalWarning\": \"You will lose all information entered for this funding source. Funding source will not be deleted from archived MFP Work Plans and Semi-Annual Progress Reports, but it will not be available in future MFP Work Plans and Semi-Annual Progress Reports. <br><br>Are you sure you want to proceed?\",\n                \"tableHeader\": \"Initiative name <br/> MFP Work Plan topic\",\n                \"addEditModalHint\": \"Provide projected quarterly expenditures, by funding source, for this initiative. Actual quarterly expenditures will be reported in the recipient\u2019s MFP Semi-Annual Progress Report.\",\n                \"dashboardTitle\": \"Funding Sources\",\n                \"enterEntityDetailsButtonText\": \"Edit\",\n                \"readOnlyEntityDetailsButtonText\": \"View\",\n                \"entityUnfinishedMessage\": \"Add the projected quarterly expenditures for the next 2 quarters by editing funding source.\",\n                \"countEntitiesInTitle\": true\n              },\n              \"modalForm\": {\n                \"id\": \"tb-modal\",\n                \"fields\": [\n                  {\n                    \"id\": \"fundingSources_wpTopic\",\n                    \"type\": \"radio\",\n                    \"validation\": \"radio\",\n                    \"props\": {\n                      \"label\": \"Funding source:\",\n                      \"hint\": \"Enter a dollar amount. Enter 0 for quarters with no projected expenditures. Enter N/A for quarters you do not expect to report.\",\n                      \"choices\": [\n                        {\n                          \"id\": \"2VLpZ9A92OivbZhKvY8pE4hB65c\",\n                          \"label\": \"MFP cooperative agreement funds for qualified HCBS and demonstration services\"\n                        },\n                        {\n                          \"id\": \"2VLpZ9A92OivbZhKvY8pE4\",\n                          \"label\": \"MFP cooperative agreement funds for supplemental services\"\n                        },\n                        {\n                          \"id\": \"2VLpZCNtbcjRPq3evd1NI6\",\n                          \"label\": \"MFP cooperative agreement funds for administrative activities\"\n                        },\n                        {\n                          \"id\": \"2VLpZDJ9qaKKOk78ztBdiB\",\n                          \"label\": \"MFP cooperative agreement funds for capacity-building initiatives\"\n                        },\n                        {\n                          \"id\": \"2VLpZCRWieGr1Z49QX5Aqc\",\n                          \"label\": \"State or territory equivalent funds attributable to the MFP-enhanced match\"\n                        },\n                        {\n                          \"id\": \"2VLq8ASzZNfxbu520if529\",\n                          \"label\": \"Other, specify\",\n                          \"children\": [\n                            {\n                              \"id\": \"initiative_wp_otherTopic\",\n                              \"type\": \"textarea\",\n                              \"validation\": {\n                                \"type\": \"text\",\n                                \"nested\": true,\n                                \"parentFieldName\": \"fundingSources_wpTopic\"\n                              }\n                            }\n                          ]\n                        }\n                      ]\n                    }\n                  },\n                  {\n                    \"id\": \"fundingSources_quarters\",\n                    \"type\": \"number\",\n                    \"validation\": \"number\",\n                    \"transformation\": {\n                      \"rule\": \"nextTwelveQuarters\"\n                    },\n                    \"props\": {\n                      \"mask\": \"currency\"\n                    }\n                  }\n                ]\n              }\n            },\n            {\n              \"name\": \"State or Territory-Specific Initiatives: IV. Initiative close-out information\",\n              \"path\": \"/wp/state-or-territory-specific-initiatives/close-out-information\",\n              \"pageType\": \"entityOverlay\",\n              \"entityType\": \"initiative\",\n              \"stepType\": \"closeOutInformation\",\n              \"stepInfo\": [\"stepName\", \"hint\"],\n              \"stepName\": \"IV. Initiative close-out information (if applicable)\",\n              \"hint\": \"To be completed as appropriate during MFP Work Plan revisions\",\n              \"isRequired\": true,\n              \"verbiage\": {\n                \"intro\": {\n                  \"section\": \"State or Territory-Specific Initiatives: IV. Initiative close-out information\",\n                  \"info\": [\n                    {\n                      \"type\": \"html\",\n                      \"content\": \"Complete the section below for initiatives with an end date during the upcoming semi-annual reporting period.\"\n                    }\n                  ]\n                },\n                \"closeOutWarning\": {\n                  \"title\": \"Warning\",\n                  \"description\": \"Once you select \\\"Close out initiative\\\", this initiative will be closed out and will no longer be editable. You will be able to continue to view this response. If you are not ready to close out an initiative, select \\\"Save & return\\\" and you\u2019ll be able to save your draft data. You will not be able to close out an initiative until you complete the fields above. <br><br> This action cannot be undone.\"\n                },\n                \"closeOutModal\": {\n                  \"closeOutModalButtonText\": \"Close out initiative\",\n                  \"closeOutModalTitle\": \"Close out \",\n                  \"closeOutModalBodyText\": \"This initiative will be closed out and will no longer be editable. You will be able to continue to view this response. If you are not ready to close out an initiative, select \\\"Cancel\\\" and you\u2019ll be able to save your draft data.<br><br>This action cannot be undone.<br><br>Are you sure you want to proceed?\",\n                  \"closeOutModalConfirmButtonText\": \"Yes, close out initiative\"\n                },\n                \"enterEntityDetailsButtonText\": \"Edit\",\n                \"readOnlyEntityDetailsButtonText\": \"View\"\n              },\n              \"form\": {\n                \"id\": \"sauxM9MnFZhIn5W44WY3BG\",\n                \"fields\": [\n                  {\n                    \"id\": \"defineInitiative_projectedEndDate_value\",\n                    \"type\": \"date\",\n                    \"validation\": \"textOptional\",\n                    \"props\": {\n                      \"label\": \"Projected end date\",\n                      \"hint\": \"Auto-populates from \u201CI. Define initiative\u201D.\",\n                      \"disabled\": true\n                    }\n                  },\n                  {\n                    \"id\": \"closeOutInformation_actualEndDate\",\n                    \"type\": \"date\",\n                    \"validation\": \"date\",\n                    \"props\": {\n                      \"label\": \"Actual end date\"\n                    }\n                  },\n                  {\n                    \"id\": \"closeOutInformation_initiativeStatus\",\n                    \"type\": \"checkbox\",\n                    \"validation\": \"checkbox\",\n                    \"props\": {\n                      \"label\": \"For initiatives that will no longer be sustained with MFP funding or state or territory-equivalent funding, indicate the status below:\",\n                      \"hint\": \"Select all that apply.\",\n                      \"choices\": [\n                        {\n                          \"id\": \"FhAF0lzeuB4wLalyXv2BeG\",\n                          \"label\": \"Completed initiative\"\n                        },\n                        {\n                          \"id\": \"GUcwKDPBs8K6LY4yT1hPGD\",\n                          \"label\": \"Discontinued initiative\",\n                          \"children\": [\n                            {\n                              \"id\": \"closeOutInformation_initiativeStatus-terminationReason\",\n                              \"props\": {\n                                \"label\": \"Indicate reason for termination\"\n                              },\n                              \"type\": \"textarea\",\n                              \"validation\": {\n                                \"type\": \"text\",\n                                \"nested\": true,\n                                \"parentFieldName\": \"closeOutInformation_initiativeStatus\"\n                              }\n                            }\n                          ]\n                        },\n                        {\n                          \"id\": \"86SG3qhFfsZ0CAu3G4SxM5\",\n                          \"label\": \"Sustaining initiative through a Medicaid authority\",\n                          \"children\": [\n                            {\n                              \"id\": \"closeOutInformation_initiativeStatus-alternateFunding\",\n                              \"props\": {\n                                \"label\": \"Indicate alternative funding source\"\n                              },\n                              \"type\": \"textarea\",\n                              \"validation\": {\n                                \"type\": \"text\",\n                                \"nested\": true,\n                                \"parentFieldName\": \"closeOutInformation_initiativeStatus\"\n                              }\n                            }\n                          ]\n                        }\n                      ]\n                    }\n                  }\n                ]\n              }\n            }\n          ]\n        }\n      ]\n    },\n    {\n      \"name\": \"Review & Submit\",\n      \"path\": \"/wp/review-and-submit\",\n      \"pageType\": \"reviewSubmit\"\n    }\n  ]\n}\n", "{\n  \"type\": \"SAR\",\n  \"name\": \"MFP Semi-Annual Progress Report (SAR)\",\n  \"basePath\": \"/sar\",\n  \"version\": \"SAR_2023-08-21\",\n  \"entities\": {\n    \"\": { \"required\": true }\n  },\n  \"routes\": [\n    {\n      \"name\": \"General Information\",\n      \"path\": \"/sar/general-information\",\n      \"pageType\": \"standard\",\n      \"verbiage\": {\n        \"intro\": {\n          \"section\": \"\",\n          \"subsection\": \"General Information\"\n        },\n        \"praDisclosure\": [\n          {\n            \"type\": \"p\",\n            \"content\": \"<b>PRA Disclosure Statement</b>\"\n          },\n          {\n            \"type\": \"p\",\n            \"content\": \"Under the Privacy Act of 1974 any personally identifying information obtained will be kept private to the extent of the law. According to the Paperwork Reduction Act of 1995, no persons are required to respond to a collection of information unless it displays a valid OMB control number. The valid OMB control number for this information collection is 0938-1053. The time required to complete this information collection is estimated to average 2.5 hours per response, including the time to review instructions, search existing data resources, gather the data needed, and complete and review the information collection. If you have comments concerning the accuracy of the time estimate(s) or suggestions for improving this form, please write to: CMS, 7500 Security Boulevard, Attn: PRA Reports Clearance Officer, Mail Stop C4-26-05, Baltimore, Maryland 21244-1850\"\n          }\n        ]\n      },\n      \"form\": {\n        \"id\": \"ga\",\n        \"fields\": [\n          {\n            \"id\": \"generalInformation_resubmissionInformation\",\n            \"type\": \"textarea\",\n            \"validation\": \"text\",\n            \"props\": {\n              \"label\": \"Briefly describe the questions you plan to revise and the reason(s) for the revision(s):\",\n              \"heading\": \"Resubmission Information\"\n            }\n          },\n          {\n            \"id\": \"generalInformation_MfpOperatingOrganizationName\",\n            \"type\": \"text\",\n            \"validation\": \"text\",\n            \"props\": {\n              \"label\": \"Name of MFP operating organization\",\n              \"heading\": \"Organization Information\"\n            }\n          },\n          {\n            \"id\": \"generalInformation_stateTerritoryMedicaidAgency\",\n            \"type\": \"text\",\n            \"validation\": \"text\",\n            \"props\": {\n              \"label\": \"State or territory Medicaid agency\"\n            }\n          },\n          {\n            \"id\": \"generalInformation_stateTerritoryMedicaidDirector\",\n            \"type\": \"text\",\n            \"validation\": \"text\",\n            \"props\": {\n              \"label\": \"State or territory Medicaid director\"\n            }\n          },\n          {\n            \"id\": \"generalInformation_mfpProgramPublicName\",\n            \"type\": \"text\",\n            \"validation\": \"text\",\n            \"props\": {\n              \"label\": \"MFP program's public name\"\n            }\n          },\n          {\n            \"id\": \"generalInformation_mfpProgramWebsite\",\n            \"type\": \"text\",\n            \"validation\": \"url\",\n            \"props\": {\n              \"label\": \"MFP program's website\"\n            }\n          },\n          {\n            \"id\": \"generalInformation_aorName\",\n            \"type\": \"text\",\n            \"validation\": \"text\",\n            \"props\": {\n              \"label\": \"AOR name\",\n              \"heading\": \"Authorized Organizational Representative (AOR)\"\n            }\n          },\n          {\n            \"id\": \"generalInformation_aorTitleAgency\",\n            \"type\": \"text\",\n            \"validation\": \"text\",\n            \"props\": {\n              \"label\": \"AOR title/agency\"\n            }\n          },\n          {\n            \"id\": \"generalInformation_aorEmail\",\n            \"type\": \"text\",\n            \"validation\": \"email\",\n            \"props\": {\n              \"label\": \"AOR email\"\n            }\n          },\n          {\n            \"id\": \"generalInformation_hasAorChangedSinceLastReport\",\n            \"type\": \"radio\",\n            \"validation\": \"radio\",\n            \"props\": {\n              \"label\": \"Has the AOR changed since last report?\",\n              \"choices\": [\n                {\n                  \"id\": \"2Vff8CQXa1Z82GAXK85KI1nG\",\n                  \"label\": \"Yes\"\n                },\n                {\n                  \"id\": \"2VffASWS2XRfAlc3uLzxCVAC\",\n                  \"label\": \"No\"\n                }\n              ]\n            }\n          },\n          {\n            \"id\": \"generalInformation_projectDirectorName\",\n            \"type\": \"text\",\n            \"validation\": \"text\",\n            \"props\": {\n              \"label\": \"Project director name\",\n              \"heading\": \"Project Director\"\n            }\n          },\n          {\n            \"id\": \"generalInformation_projectDirectorTitle\",\n            \"type\": \"text\",\n            \"validation\": \"text\",\n            \"props\": {\n              \"label\": \"Project director title\"\n            }\n          },\n          {\n            \"id\": \"generalInformation_projectDirectorEmail\",\n            \"type\": \"text\",\n            \"validation\": \"email\",\n            \"props\": {\n              \"label\": \"Project director email\"\n            }\n          },\n          {\n            \"id\": \"generalInformation_cmsProjectOfficerName\",\n            \"type\": \"text\",\n            \"validation\": \"text\",\n            \"props\": {\n              \"label\": \"CMS project officer name\",\n              \"heading\": \"CMS Project Officer\"\n            }\n          }\n        ]\n      }\n    },\n    {\n      \"name\": \"Recruitment, Enrollment, and Transitions\",\n      \"path\": \"/sar/recruitment-enrollment-transitions\",\n      \"verbiage\": {\n        \"intro\": {\n          \"info\": [\n            {\n              \"type\": \"p\",\n              \"content\": \"In this section, please provide information for the specified period. Transition targets are populated from your state\u2019s current MFP Work Plan, where applicable. Blue-shaded cells are auto-calculated.\"\n            }\n          ]\n        }\n      },\n      \"children\": [\n        {\n          \"name\": \"Number of people who signed an MFP informed consent form in the reporting period\",\n          \"path\": \"/sar/recruitment-enrollment-transitions/number-of-people-signed-informed-consent-form\",\n          \"pageType\": \"standard\",\n          \"verbiage\": {\n            \"intro\": {\n              \"section\": \"Recruitment, Enrollment, and Transitions\",\n              \"subsection\": \"Number of people who signed an MFP informed consent form in the reporting period\",\n              \"subsectionTitle\": \"Number of people who signed an MFP informed consent form in the {{reportingPeriod}}\",\n              \"info\": [\n                {\n                  \"type\": \"text\",\n                  \"content\": \"In this section, provide information for the specified period.\"\n                },\n                {\n                  \"type\": \"text\",\n                  \"content\": \"Number of institutional residents who have signed an informed consent form indicating their desire to transition to the community and enroll in the state or territory\u2019s MFP program.\"\n                }\n              ]\n            },\n            \"reviewPdfHint\": \"To view totals, click \\\"Review PDF\\\" and it will open a summary in a new tab.\"\n          },\n          \"form\": {\n            \"id\": \"ret-psmicf\",\n            \"fields\": [\n              {\n                \"id\": \"ret_psmicf_target_populations\",\n                \"type\": \"number\",\n                \"validation\": \"validInteger\",\n                \"props\": {\n                  \"decimalPlacesToRoundTo\": 0\n                },\n                \"transformation\": {\n                  \"rule\": \"targetPopulations\"\n                }\n              }\n            ]\n          }\n        },\n        {\n          \"name\": \"Number of MFP transitions in the reporting period\",\n          \"path\": \"/sar/recruitment-enrollment-transitions/number-of-mfp-transitions\",\n          \"pageType\": \"standard\",\n          \"verbiage\": {\n            \"intro\": {\n              \"section\": \"Recruitment, Enrollment, and Transitions\",\n              \"subsection\": \"Number of MFP transitions in the reporting period\",\n              \"subsectionTitle\": \"Number of MFP transitions in the {{reportingPeriod}}\",\n              \"info\": [\n                {\n                  \"type\": \"text\",\n                  \"content\": \"In this section, provide information for the specified period.\"\n                },\n                {\n                  \"type\": \"text\",\n                  \"content\": \"Number of qualified institutional residents who enrolled in MFP and were discharged from an institution to a qualified residence during the reporting period in the quarter.\"\n                }\n              ]\n            },\n            \"reviewPdfHint\": \"To view totals and transition targets from your associated MFP Work Plan, click \\\"Review PDF\\\" and it will open a summary in a new tab.\"\n          },\n          \"form\": {\n            \"id\": \"ret-mtrp\",\n            \"fields\": [\n              {\n                \"id\": \"ret_mtrp_quarter_header_1\",\n                \"type\": \"sectionHeader\",\n                \"transformation\": {\n                  \"rule\": \"firstQuarterOfThePeriod\"\n                }\n              },\n              {\n                \"id\": \"ret_mtrp_quarter_1_populations\",\n                \"type\": \"number\",\n                \"validation\": \"validInteger\",\n                \"props\": {\n                  \"decimalPlacesToRoundTo\": 0\n                },\n                \"transformation\": {\n                  \"rule\": \"targetPopulations\"\n                }\n              },\n              {\n                \"id\": \"ret_mtrp_quarter_header_2\",\n                \"type\": \"sectionHeader\",\n                \"transformation\": {\n                  \"rule\": \"secondQuarterOfThePeriod\"\n                }\n              },\n              {\n                \"id\": \"ret_mtrp_quarter_2_populations\",\n                \"type\": \"number\",\n                \"validation\": \"validInteger\",\n                \"props\": {\n                  \"decimalPlacesToRoundTo\": 0\n                },\n                \"transformation\": {\n                  \"rule\": \"targetPopulations\"\n                }\n              }\n            ]\n          }\n        },\n        {\n          \"name\": \"Number of MFP transitions from qualified institutions in the reporting period\",\n          \"path\": \"/sar/recruitment-enrollment-transitions/number-of-mfp-transitions-from-qualified-institutions\",\n          \"pageType\": \"standard\",\n          \"verbiage\": {\n            \"intro\": {\n              \"section\": \"Recruitment, Enrollment, and Transitions\",\n              \"subsection\": \"Number of MFP transitions from qualified institutions in the reporting period\",\n              \"subsectionTitle\": \"Number of MFP transitions from qualified institutions in the {{reportingPeriod}}\",\n              \"info\": [\n                {\n                  \"type\": \"text\",\n                  \"content\": \"In this section, provide information for the specified period.\"\n                },\n                {\n                  \"type\": \"text\",\n                  \"content\": \"Of the total transitions reported in \\\"Number of MFP transitions\\\", provide the number of transitions from each qualified inpatient facility type during the reporting period.\"\n                }\n              ]\n            },\n            \"reviewPdfHint\": \"To view totals, click \\\"Review PDF\\\" and it will open a summary in a new tab.\"\n          },\n          \"form\": {\n            \"id\": \"ret-mtfqi\",\n            \"fields\": [\n              {\n                \"id\": \"ret-mtfqi-header-1\",\n                \"type\": \"sectionHeader\",\n                \"props\": {\n                  \"content\": \"Nursing facility\"\n                }\n              },\n              {\n                \"id\": \"ret-mtfqi-1-populations\",\n                \"type\": \"number\",\n                \"validation\": \"validInteger\",\n                \"props\": {\n                  \"decimalPlacesToRoundTo\": 0\n                },\n                \"transformation\": {\n                  \"rule\": \"targetPopulations\"\n                }\n              },\n              {\n                \"id\": \"ret-mtfqi-header-2\",\n                \"type\": \"sectionHeader\",\n                \"props\": {\n                  \"content\": \"Intermediate care facility for individuals with intellectual disabilities (ICF/IID)\"\n                }\n              },\n              {\n                \"id\": \"ret-mtfqi-2-populations\",\n                \"type\": \"number\",\n                \"validation\": \"validInteger\",\n                \"props\": {\n                  \"decimalPlacesToRoundTo\": 0\n                },\n                \"transformation\": {\n                  \"rule\": \"targetPopulations\"\n                }\n              },\n              {\n                \"id\": \"ret-mtfqi-header-3\",\n                \"type\": \"sectionHeader\",\n                \"props\": {\n                  \"content\": \"Institution for mental diseases (IMD)\"\n                }\n              },\n              {\n                \"id\": \"ret-mtfqi-3-populations\",\n                \"type\": \"number\",\n                \"validation\": \"validInteger\",\n                \"props\": {\n                  \"decimalPlacesToRoundTo\": 0\n                },\n                \"transformation\": {\n                  \"rule\": \"targetPopulations\"\n                }\n              },\n              {\n                \"id\": \"ret-mtfqi-header-4\",\n                \"type\": \"sectionHeader\",\n                \"props\": {\n                  \"content\": \"Hospital\"\n                }\n              },\n              {\n                \"id\": \"ret-mtfqi-4-populations\",\n                \"type\": \"number\",\n                \"validation\": \"validInteger\",\n                \"props\": {\n                  \"decimalPlacesToRoundTo\": 0\n                },\n                \"transformation\": {\n                  \"rule\": \"targetPopulations\"\n                }\n              },\n              {\n                \"id\": \"ret-mtfqi-header-5\",\n                \"type\": \"sectionHeader\",\n                \"props\": {\n                  \"content\": \"Other\"\n                }\n              },\n              {\n                \"id\": \"ret-mtfqi-5-populations\",\n                \"type\": \"number\",\n                \"validation\": \"validInteger\",\n                \"props\": {\n                  \"decimalPlacesToRoundTo\": 0\n                },\n                \"transformation\": {\n                  \"rule\": \"targetPopulations\"\n                }\n              }\n            ]\n          }\n        },\n        {\n          \"name\": \"Number of MFP transitions to qualified residences in the reporting period\",\n          \"path\": \"/sar/recruitment-enrollment-transitions/number-of-mfp-transitions-to-qualified-residences\",\n          \"pageType\": \"standard\",\n          \"verbiage\": {\n            \"intro\": {\n              \"section\": \"Recruitment, Enrollment, and Transitions\",\n              \"subsection\": \"Number of MFP transitions to qualified residences in the reporting period\",\n              \"subsectionTitle\": \"Number of MFP transitions to qualified residences in the {{reportingPeriod}}\",\n              \"info\": [\n                {\n                  \"type\": \"text\",\n                  \"content\": \"In this section, provide information for the specified period.\"\n                },\n                {\n                  \"type\": \"text\",\n                  \"content\": \"Of the total transitions reported in \\\"Number of MFP transitions\\\", provide the number of transitions to each qualified residence type during the reporting period.\"\n                }\n              ]\n            },\n            \"reviewPdfHint\": \"To view totals, click \\\"Review PDF\\\" and it will open a summary in a new tab.\"\n          },\n          \"form\": {\n            \"id\": \"ret-mtfqr\",\n            \"fields\": [\n              {\n                \"id\": \"ret-mtfqr-header-1\",\n                \"type\": \"sectionHeader\",\n                \"props\": {\n                  \"content\": \"Home (owned or leased by individual or family)\"\n                }\n              },\n              {\n                \"id\": \"ret-mtfqr-1-populations\",\n                \"type\": \"number\",\n                \"validation\": \"validInteger\",\n                \"props\": {\n                  \"decimalPlacesToRoundTo\": 0\n                },\n                \"transformation\": {\n                  \"rule\": \"targetPopulations\"\n                }\n              },\n              {\n                \"id\": \"ret-mtfqr-header-2\",\n                \"type\": \"sectionHeader\",\n                \"props\": {\n                  \"content\": \"Apartment (individual lease, lockable access, etc.)\"\n                }\n              },\n              {\n                \"id\": \"ret-mtfqr-2-populations\",\n                \"type\": \"number\",\n                \"validation\": \"validInteger\",\n                \"props\": {\n                  \"decimalPlacesToRoundTo\": 0\n                },\n                \"transformation\": {\n                  \"rule\": \"targetPopulations\"\n                }\n              },\n              {\n                \"id\": \"ret-mtfqr-header-3\",\n                \"type\": \"sectionHeader\",\n                \"props\": {\n                  \"content\": \"Group home or other residence in which four or fewer unrelated individuals live\"\n                }\n              },\n              {\n                \"id\": \"ret-mtfqr-3-populations\",\n                \"type\": \"number\",\n                \"validation\": \"validInteger\",\n                \"props\": {\n                  \"decimalPlacesToRoundTo\": 0\n                },\n                \"transformation\": {\n                  \"rule\": \"targetPopulations\"\n                }\n              },\n              {\n                \"id\": \"ret-mtfqr-header-4\",\n                \"type\": \"sectionHeader\",\n                \"props\": {\n                  \"content\": \"Apartment in qualified assisted living\"\n                }\n              },\n              {\n                \"id\": \"ret-mtfqr-4-populations\",\n                \"type\": \"number\",\n                \"validation\": \"validInteger\",\n                \"props\": {\n                  \"decimalPlacesToRoundTo\": 0\n                },\n                \"transformation\": {\n                  \"rule\": \"targetPopulations\"\n                }\n              }\n            ]\n          }\n        },\n        {\n          \"name\": \"Total number of active MFP participants in the reporting period\",\n          \"path\": \"/sar/recruitment-enrollment-transitions/total-number-of-current-mfp-participants\",\n          \"pageType\": \"standard\",\n          \"verbiage\": {\n            \"intro\": {\n              \"section\": \"Recruitment, Enrollment, and Transitions\",\n              \"subsection\": \"Total number of active MFP participants in the reporting period\",\n              \"subsectionTitle\": \"Total number of active MFP participants in the {{reportingPeriod}}\",\n              \"info\": [\n                {\n                  \"type\": \"text\",\n                  \"content\": \"In this section, provide information for the specified period.\"\n                },\n                {\n                  \"type\": \"text\",\n                  \"content\": \"Active MFP participants excludes individuals whose enrollment in the MFP Demonstration ended because they completed their 365 days of MFP eligibility, died before they exhausted their 365-day enrollment period, were institutionalized for 30 days or more and did not subsequently re-enroll in the MFP program, or otherwise disenrolled from the program.\"\n                }\n              ]\n            },\n            \"reviewPdfHint\": \"To view totals, click \\\"Review PDF\\\" and it will open a summary in a new tab.\"\n          },\n          \"form\": {\n            \"id\": \"ret-tnamprp\",\n            \"fields\": [\n              {\n                \"id\": \"ret-tnamprp-1-populations\",\n                \"type\": \"number\",\n                \"validation\": \"validInteger\",\n                \"props\": {\n                  \"decimalPlacesToRoundTo\": 0\n                },\n                \"transformation\": {\n                  \"rule\": \"targetPopulations\"\n                }\n              }\n            ]\n          }\n        },\n        {\n          \"name\": \"Number of MFP participants completing the program in the reporting period\",\n          \"path\": \"/sar/recruitment-enrollment-transitions/number-of-mfp-participants-completing-program\",\n          \"pageType\": \"standard\",\n          \"verbiage\": {\n            \"intro\": {\n              \"section\": \"Recruitment, Enrollment, and Transitions\",\n              \"subsection\": \"Number of MFP participants completing the program in the reporting period\",\n              \"subsectionTitle\": \"Number of MFP participants completing the program in the {{reportingPeriod}}\",\n              \"info\": [\n                {\n                  \"type\": \"text\",\n                  \"content\": \"In this section, provide information for the specified period.\"\n                },\n                {\n                  \"type\": \"text\",\n                  \"content\": \"Number of MFP participants who completed the 365-day enrollment period during the reporting period.\"\n                }\n              ]\n            },\n            \"reviewPdfHint\": \"To view totals, click \\\"Review PDF\\\" and it will open a summary in a new tab.\"\n          },\n          \"form\": {\n            \"id\": \"ret-nmpcprp\",\n            \"fields\": [\n              {\n                \"id\": \"ret-nmpcprp-1-populations\",\n                \"type\": \"number\",\n                \"validation\": \"validInteger\",\n                \"props\": {\n                  \"decimalPlacesToRoundTo\": 0\n                },\n                \"transformation\": {\n                  \"rule\": \"targetPopulations\"\n                }\n              }\n            ]\n          }\n        },\n        {\n          \"name\": \"Number of people re-enrolled in MFP during the reporting period\",\n          \"path\": \"/sar/recruitment-enrollment-transitions/number-of-people-reenrolled-in-mfp\",\n          \"pageType\": \"standard\",\n          \"verbiage\": {\n            \"intro\": {\n              \"section\": \"Recruitment, Enrollment, and Transitions\",\n              \"subsection\": \"Number of people re-enrolled in MFP during the reporting period\",\n              \"subsectionTitle\": \"Number of people re-enrolled in MFP during the {{reportingPeriod}}\",\n              \"info\": [\n                {\n                  \"type\": \"text\",\n                  \"content\": \"In this section, provide information for the specified period.\"\n                },\n                {\n                  \"type\": \"text\",\n                  \"content\": \"Number of people who were disenrolled from the MFP program at any point (during this reporting period or a prior period) and re-enrolled during this reporting period.\"\n                }\n              ]\n            },\n            \"reviewPdfHint\": \"To view totals, click \\\"Review PDF\\\" and it will open a summary in a new tab.\"\n          },\n          \"form\": {\n            \"id\": \"ret-npremrp\",\n            \"fields\": [\n              {\n                \"id\": \"ret-npremrp-1-populations\",\n                \"type\": \"number\",\n                \"validation\": \"validInteger\",\n                \"props\": {\n                  \"decimalPlacesToRoundTo\": 0\n                },\n                \"transformation\": {\n                  \"rule\": \"targetPopulations\"\n                }\n              }\n            ]\n          }\n        },\n        {\n          \"name\": \"Number of MFP participants disenrolled from the program during the reporting period\",\n          \"path\": \"/sar/recruitment-enrollment-transitions/number-of-people-disenrolled-from-program\",\n          \"pageType\": \"standard\",\n          \"verbiage\": {\n            \"intro\": {\n              \"section\": \"Recruitment, Enrollment, and Transitions\",\n              \"subsection\": \"Number of MFP participants disenrolled from the program during the reporting period\",\n              \"subsectionTitle\": \"Number of MFP participants disenrolled from the program during the {{reportingPeriod}}\",\n              \"info\": [\n                {\n                  \"type\": \"text\",\n                  \"content\": \"In this section, provide information for the specified period.\"\n                },\n                {\n                  \"type\": \"text\",\n                  \"content\": \"Provide the number of MFP participants for each target population (if applicable for this reporting period), by reason for disenrollment. If more than one reason applies to an individual\u2019s disenrollment, include the individual in the total for one reason only. Include the individual under the primary reason or, if indeterminate, the first reason listed.\u00A0Enter the number of participants disenrolled for the selected \u201Cother\u201D cause in the new fields. An additional \u201Cother\u201D reason may be specified, if one or more participants disenrolled for reasons other than those listed.\"\n                }\n              ]\n            },\n            \"reviewPdfHint\": \"To view totals, click \\\"Review PDF\\\" and it will open a summary in a new tab.\"\n          },\n          \"form\": {\n            \"id\": \"ret-mpdprp\",\n            \"fields\": [\n              {\n                \"id\": \"ret-mpdprp-header-1\",\n                \"type\": \"sectionHeader\",\n                \"props\": {\n                  \"content\": \"Re-institutionalization\"\n                }\n              },\n              {\n                \"id\": \"ret-mpdprp-1-populations\",\n                \"type\": \"number\",\n                \"validation\": \"validInteger\",\n                \"props\": {\n                  \"decimalPlacesToRoundTo\": 0\n                },\n                \"transformation\": {\n                  \"rule\": \"targetPopulations\"\n                }\n              },\n              {\n                \"id\": \"ret-mpdprp-header-2\",\n                \"type\": \"sectionHeader\",\n                \"props\": {\n                  \"content\": \"Death\"\n                }\n              },\n              {\n                \"id\": \"ret-mpdprp-2-populations\",\n                \"type\": \"number\",\n                \"validation\": \"validInteger\",\n                \"props\": {\n                  \"decimalPlacesToRoundTo\": 0\n                },\n                \"transformation\": {\n                  \"rule\": \"targetPopulations\"\n                }\n              },\n              {\n                \"id\": \"ret-mpdprp-header-3\",\n                \"type\": \"sectionHeader\",\n                \"props\": {\n                  \"content\": \"Voluntary disenrollment\"\n                }\n              },\n              {\n                \"id\": \"ret-mpdprp-3-populations\",\n                \"type\": \"number\",\n                \"validation\": \"validInteger\",\n                \"props\": {\n                  \"decimalPlacesToRoundTo\": 0\n                },\n                \"transformation\": {\n                  \"rule\": \"targetPopulations\"\n                }\n              },\n              {\n                \"id\": \"ret-mpdprp-header-4\",\n                \"type\": \"sectionHeader\",\n                \"props\": {\n                  \"content\": \"Other\"\n                }\n              },\n              {\n                \"id\": \"ret_otherReasons\",\n                \"type\": \"checkbox\",\n                \"validation\": \"checkboxOptional\",\n                \"props\": {\n                  \"label\": \"Other reasons\",\n                  \"hint\": \"Check all that apply.\",\n                  \"choices\": [\n                    {\n                      \"id\": \"2VffASWS2XRfAlc3uLzxCVAC\",\n                      \"label\": \"Moved out of MFP jurisdiction/state/territory\",\n                      \"children\": [\n                        {\n                          \"id\": \"ret-movedout-populations\",\n                          \"type\": \"number\",\n                          \"validation\": {\n                            \"type\": \"validInteger\",\n                            \"nested\": true,\n                            \"parentFieldName\": \"ret_otherReasons\",\n                            \"parentOptionId\": \"2VffASWS2XRfAlc3uLzxCVAC\"\n                          },\n                          \"props\": {\n                            \"decimalPlacesToRoundTo\": 0\n                          },\n                          \"transformation\": {\n                            \"rule\": \"targetPopulations\"\n                          }\n                        }\n                      ]\n                    },\n                    {\n                      \"id\": \"2VfqAGi1lgxX3uXc98cl6q4u\",\n                      \"label\": \"Incarceration\",\n                      \"children\": [\n                        {\n                          \"id\": \"ret-incarceration-populations\",\n                          \"type\": \"number\",\n                          \"validation\": {\n                            \"type\": \"validInteger\",\n                            \"nested\": true,\n                            \"parentFieldName\": \"ret_otherReasons\",\n                            \"parentOptionId\": \"2VfqAGi1lgxX3uXc98cl6q4u\"\n                          },\n                          \"props\": {\n                            \"decimalPlacesToRoundTo\": 0\n                          },\n                          \"transformation\": {\n                            \"rule\": \"targetPopulations\"\n                          }\n                        }\n                      ]\n                    },\n                    {\n                      \"id\": \"2VfqSwWMAr9xOE2zOhpLHtYQ\",\n                      \"label\": \"Move to an unqualified setting\",\n                      \"children\": [\n                        {\n                          \"id\": \"ret-moved-populations\",\n                          \"type\": \"number\",\n                          \"validation\": {\n                            \"type\": \"validInteger\",\n                            \"nested\": true,\n                            \"parentFieldName\": \"ret_otherReasons\",\n                            \"parentOptionId\": \"2VfqSwWMAr9xOE2zOhpLHtYQ\"\n                          },\n                          \"props\": {\n                            \"decimalPlacesToRoundTo\": 0\n                          },\n                          \"transformation\": {\n                            \"rule\": \"targetPopulations\"\n                          }\n                        }\n                      ]\n                    },\n                    {\n                      \"id\": \"2Vfqd9gpWs2J3Wef5CAuqGx9\",\n                      \"label\": \"Other, specify\",\n                      \"children\": [\n                        {\n                          \"id\": \"otherReasons-otherText\",\n                          \"type\": \"text\",\n                          \"validation\": {\n                            \"type\": \"text\",\n                            \"nested\": true,\n                            \"parentFieldName\": \"ret_otherReasons\",\n                            \"parentOptionId\": \"2Vfqd9gpWs2J3Wef5CAuqGx9\"\n                          }\n                        },\n                        {\n                          \"id\": \"ret-other-specify-populations\",\n                          \"type\": \"number\",\n                          \"validation\": {\n                            \"type\": \"validInteger\",\n                            \"nested\": true,\n                            \"parentFieldName\": \"ret_otherReasons\",\n                            \"parentOptionId\": \"2Vfqd9gpWs2J3Wef5CAuqGx9\"\n                          },\n                          \"props\": {\n                            \"decimalPlacesToRoundTo\": 0\n                          },\n                          \"transformation\": {\n                            \"rule\": \"targetPopulations\"\n                          }\n                        }\n                      ]\n                    }\n                  ]\n                }\n              }\n            ]\n          }\n        },\n        {\n          \"name\": \"Number of HCBS participants admitted to a facility from the community, by length of stay and age group\",\n          \"path\": \"/sar/recruitment-enrollment-transitions/number-of-hcbs-participants-admitted-to-facility-from-community\",\n          \"pageType\": \"standard\",\n          \"conditionallyRender\": \"showOnlyInPeriod2\",\n          \"verbiage\": {\n            \"intro\": {\n              \"section\": \"Recruitment, Enrollment, and Transitions\",\n              \"subsection\": \"Number of HCBS participants (including MFP participants) admitted to a facility from the community, by length of stay and age group\",\n              \"info\": [\n                {\n                  \"type\": \"text\",\n                  \"content\": \"In this section, provide information for the specified period.\"\n                },\n                {\n                  \"type\": \"text\",\n                  \"content\": \"Inpatient facilities include hospitals, nursing homes, ICF/IID, or IMD. Provide data for readmissions occurring between July 31 of the current reporting period and August 1 of the prior year.\"\n                }\n              ]\n            },\n            \"reviewPdfHint\": \"To view totals, click \\\"Review PDF\\\" and it will open a summary in a new tab.\"\n          },\n          \"form\": {\n            \"id\": \"ret-hcbs\",\n            \"optional\": true,\n            \"fields\": [\n              {\n                \"id\": \"ret_sectionHeader_shortTermStay\",\n                \"type\": \"sectionHeader\",\n                \"props\": {\n                  \"content\": \"Short-term stay: 1 to 20 days\"\n                }\n              },\n              {\n                \"id\": \"ret_shortTermStayAges18to64\",\n                \"type\": \"number\",\n                \"validation\": \"validIntegerOptional\",\n                \"props\": {\n                  \"label\": \"Ages 18-64\",\n                  \"styleAsOptional\": true,\n                  \"decimalPlacesToRoundTo\": 0\n                }\n              },\n              {\n                \"id\": \"ret_shortTermStayAges65to74\",\n                \"type\": \"number\",\n                \"validation\": \"validIntegerOptional\",\n                \"props\": {\n                  \"label\": \"Ages 65-74\",\n                  \"styleAsOptional\": true,\n                  \"decimalPlacesToRoundTo\": 0\n                }\n              },\n              {\n                \"id\": \"ret_shortTermStayAges75to84\",\n                \"type\": \"number\",\n                \"validation\": \"validIntegerOptional\",\n                \"props\": {\n                  \"label\": \"Ages 75-84\",\n                  \"styleAsOptional\": true,\n                  \"decimalPlacesToRoundTo\": 0\n                }\n              },\n              {\n                \"id\": \"ret_shortTermStayAges85AndOlder\",\n                \"type\": \"number\",\n                \"validation\": \"validIntegerOptional\",\n                \"props\": {\n                  \"label\": \"Ages 85 and older\",\n                  \"styleAsOptional\": true,\n                  \"decimalPlacesToRoundTo\": 0\n                }\n              },\n              {\n                \"id\": \"ret_sectionHeader_mediumTermStay\",\n                \"type\": \"sectionHeader\",\n                \"props\": {\n                  \"content\": \"Medium-term stay: 21-100 days\"\n                }\n              },\n              {\n                \"id\": \"ret_mediumTermStayAges18to64\",\n                \"type\": \"number\",\n                \"validation\": \"validIntegerOptional\",\n                \"props\": {\n                  \"label\": \"Ages 18-64\",\n                  \"styleAsOptional\": true,\n                  \"decimalPlacesToRoundTo\": 0\n                }\n              },\n              {\n                \"id\": \"ret_mediumTermStayAges65to74\",\n                \"type\": \"number\",\n                \"validation\": \"validIntegerOptional\",\n                \"props\": {\n                  \"label\": \"Ages 65-74\",\n                  \"styleAsOptional\": true,\n                  \"decimalPlacesToRoundTo\": 0\n                }\n              },\n              {\n                \"id\": \"ret_mediumTermStayAges75to84\",\n                \"type\": \"number\",\n                \"validation\": \"validIntegerOptional\",\n                \"props\": {\n                  \"label\": \"Ages 75-84\",\n                  \"styleAsOptional\": true,\n                  \"decimalPlacesToRoundTo\": 0\n                }\n              },\n              {\n                \"id\": \"ret_mediumTermStayAges85AndOlder\",\n                \"type\": \"number\",\n                \"validation\": \"validIntegerOptional\",\n                \"props\": {\n                  \"label\": \"Ages 85 and older\",\n                  \"styleAsOptional\": true,\n                  \"decimalPlacesToRoundTo\": 0\n                }\n              },\n              {\n                \"id\": \"ret_sectionHeader_longTermStay\",\n                \"type\": \"sectionHeader\",\n                \"props\": {\n                  \"content\": \"Long-term stay: 101 days or more\"\n                }\n              },\n              {\n                \"id\": \"ret_longTermStayAges18to64\",\n                \"type\": \"number\",\n                \"validation\": \"validIntegerOptional\",\n                \"props\": {\n                  \"label\": \"Ages 18-64\",\n                  \"styleAsOptional\": true,\n                  \"decimalPlacesToRoundTo\": 0\n                }\n              },\n              {\n                \"id\": \"ret_longTermStayAges65to74\",\n                \"type\": \"number\",\n                \"validation\": \"validIntegerOptional\",\n                \"props\": {\n                  \"label\": \"Ages 65-74\",\n                  \"styleAsOptional\": true,\n                  \"decimalPlacesToRoundTo\": 0\n                }\n              },\n              {\n                \"id\": \"ret_longTermStayAges75to84\",\n                \"type\": \"number\",\n                \"validation\": \"validIntegerOptional\",\n                \"props\": {\n                  \"label\": \"Ages 75-84\",\n                  \"styleAsOptional\": true,\n                  \"decimalPlacesToRoundTo\": 0\n                }\n              },\n              {\n                \"id\": \"ret_longTermStayAges85AndOlder\",\n                \"type\": \"number\",\n                \"validation\": \"validIntegerOptional\",\n                \"props\": {\n                  \"label\": \"Ages 85 and older\",\n                  \"styleAsOptional\": true,\n                  \"decimalPlacesToRoundTo\": 0\n                }\n              }\n            ]\n          }\n        }\n      ]\n    },\n    {\n      \"name\": \"State or Territory-Specific Initiatives\",\n      \"path\": \"/sar/state-or-territory-specific-initiatives\",\n      \"pageType\": \"dynamicModalOverlay\",\n      \"entityType\": \"initiative\",\n      \"entityInfo\": [\"initiative_name\", \"initiative_wpTopic\"],\n      \"verbiage\": {\n        \"intro\": {\n          \"section\": \"\",\n          \"subsection\": \"State or Territory-Specific Initiatives\",\n          \"introAccordion\": {\n            \"buttonLabel\": \"Instructions\",\n            \"intro\": [\n              {\n                \"type\": \"html\",\n                \"content\": \"This section requests information on current, new, or expanded initiatives implemented under the MFP demonstration. These initiatives can be funded using one or more of these funding sources:\"\n              }\n            ],\n            \"list\": [\n              \"MFP cooperative agreement funds for:\",\n              [\n                \"Qualified home and community-based services (HCBS) and demonstration services\",\n                \"Supplemental services\",\n                \"Administrative activities\",\n                \"Capacity building initiatives\"\n              ],\n              \"State/Territory equivalent funds attributable to the MFP-enhanced match\"\n            ],\n            \"text\": [\n              {\n                \"type\": \"html\",\n                \"content\": \"State or territory-specific initiatives are a distinct set of activities designed to increase the use of HCBS rather than institutional long-term services and supports (LTSS). These initiatives are specified in your MFP Work Plan and imported into the form below.\"\n              }\n            ]\n          },\n          \"info\": [\n            {\n              \"type\": \"h3\",\n              \"content\": \"Report progress for each initiative\"\n            },\n            {\n              \"type\": \"p\",\n              \"content\": \"Your initiatives are auto-populated from your most recent approved MFP Work Plan.\"\n            },\n            {\n              \"type\": \"p\",\n              \"content\": \"Recipients must report on the progress of initiatives that were ongoing during the current reporting period. For each initiative, enter information on expenditures and activities, whether continuing from prior reporting periods or initiated during this reporting period.\"\n            },\n            {\n              \"type\": \"p\",\n              \"content\": \"For each initiative, recipients must report on the progress toward achieving the objective(s) identified in the initiative\u2019s evaluation plan, as described in the MFP Work Plan. Progress toward these objectives indicates the state or territory\u2019s greater ability to provide HCBS instead of services in institutional settings.\"\n            },\n            {\n              \"type\": \"p\",\n              \"content\": \"If your state or territory has not achieved the targets for performance measures or expected time frames for deliverables set in the initiative\u2019s evaluation plan, use the following questions to explain the barriers or challenges that have hindered progress and describe plans to address them.\"\n            }\n          ]\n        },\n        \"enterEntityDetailsButtonText\": \"Edit\",\n        \"readOnlyEntityDetailsButtonText\": \"View\",\n        \"tableHeader\": \"Initiative name <br/> MFP Work Plan topic\",\n        \"countEntitiesInTitle\": false\n      },\n      \"template\": {\n        \"dashboard\": {\n          \"name\": \"Intitiative Details Dashboard\",\n          \"verbiage\": {\n            \"intro\": {\n              \"section\": \"State or Territory-Specific Initiatives\",\n              \"info\": [\n                {\n                  \"type\": \"span\",\n                  \"content\": \"Report progress on this initiative for this reporting period by completing each of the following 3 sections. Further instructions will be available at the top of each section.\"\n                }\n              ]\n            }\n          }\n        },\n        \"entitySteps\": [\n          {\n            \"name\": \"Objectives progress\",\n            \"pageType\": \"overlayModal\",\n            \"entityType\": \"initiative\",\n            \"stepType\": \"objectiveProgress\",\n            \"stepInfo\": [\"name\", \"hint\"],\n            \"hint\": \"Report progress for each objective\",\n            \"isRequired\": true,\n            \"verbiage\": {\n              \"intro\": {\n                \"section\": \"State or Territory-Specific Initiatives\",\n                \"info\": [\n                  {\n                    \"type\": \"h3\",\n                    \"content\": \"Objectives progress\"\n                  },\n                  {\n                    \"type\": \"p\",\n                    \"content\": \"Report progress for each objective by selecting the button for each.\"\n                  },\n                  {\n                    \"type\": \"p\",\n                    \"content\": \"Objectives are framed as SMART goals and set associated performance measures or indicators to monitor progress toward each objective and evaluate success. Recipients define objectives in your MFP Work Plan\u2019s evaluation plan.\"\n                  }\n                ]\n              },\n              \"accordion\": {\n                \"buttonLabel\": \"About SMART goals\",\n                \"intro\": [\n                  {\n                    \"type\": \"html\",\n                    \"content\": \"The evaluation plan in your MFP Work Plan captured expected results for each state or territory-specific initiative. You identified one or more objectives per initiative and set associated performance measures or indicators to monitor progress toward each objective and evaluate success. In the Semi-Annual Progress Report, you must articulate how you will achieve targets and meet milestones. For more information on objectives and identifying appropriate performance measures, see \"\n                  },\n                  {\n                    \"type\": \"externalLink\",\n                    \"content\": \"\\\"Using Data to Improve Money Follows the Person Program Performance.\\\"\",\n                    \"props\": {\n                      \"href\": \"https://www.medicaid.gov/sites/default/files/2023-01/MFP-Technical-Assistance-Brief.pdf\",\n                      \"target\": \"_blank\",\n                      \"aria-label\": \"Using Data to Improve Money Follows the Person Program Performance (Link opens in new tab)\"\n                    }\n                  },\n                  {\n                    \"type\": \"html\",\n                    \"content\": \"<br/><br/>As a reminder, SMART stands for:\"\n                  }\n                ],\n                \"list\": [\n                  [\n                    \"<b>Specific:</b> Specifies the activities, actors, and beneficiaries\",\n                    \"<b>Measurable:</b> Defines how a change will be measured\",\n                    \"<b>Achievable:</b> Confirms the feasibility of implementing the intervention as planned\",\n                    \"<b>Realistic/relevant</b>: Ensures the intervention relates to the goal\",\n                    \"<b>Time-bound</b>: Specifies when the results are expected\"\n                  ]\n                ]\n              },\n              \"editEntityButtonText\": \"Edit objective progress\",\n              \"reportProgressButtonText\": \"Report objective progress\",\n              \"addEditModalEditTitle\": \"Report progress for \",\n              \"enterEntityDetailsButtonText\": \"Edit\",\n              \"readOnlyEntityDetailsButtonText\": \"View\",\n              \"readOnlyEntityButtonText\": \"View\",\n              \"editEntityHint\": \"Select \\\"Edit\\\" to report the objectives progress.\",\n              \"reviewPdfHint\": \"To view totals against targets from your associated MFP Work Plan, click \\\"Review PDF\\\" and it will open a summary in a new tab.\",\n              \"addEditModalMessage\": \"To view totals and percent target achieved, \\\"Save\\\", close, click \\\"Review PDF\\\" button and it will open a summary in a new tab.\",\n              \"entityUnfinishedMessage\": \"Report objective progress to complete this section.\"\n            },\n            \"transformation\": {\n              \"rule\": \"objectives\"\n            },\n            \"objectiveCardTemplate\": {\n              \"modalForm\": {\n                \"id\": \"stsiop-modal\",\n                \"fields\": [\n                  {\n                    \"id\": \"objectivesProgress_performanceMeasuresIndicators\",\n                    \"type\": \"textarea\",\n                    \"validation\": \"text\",\n                    \"props\": {\n                      \"label\": \"Provide data on performance measures or indicators used for monitoring progress toward the objective during the current reporting period. Include progress toward milestones and key deliverables.\"\n                    }\n                  },\n                  {\n                    \"id\": \"objectivesProgress_quantitative\",\n                    \"transformation\": {\n                      \"rule\": \"quantitativeQuarters\"\n                    }\n                  },\n                  {\n                    \"id\": \"objectivesProgress_deliverablesMet\",\n                    \"type\": \"radio\",\n                    \"validation\": \"radio\",\n                    \"props\": {\n                      \"label\": \"Were targets for performance measures or expected time frames for deliverables met?\",\n                      \"choices\": [\n                        {\n                          \"id\": \"2WaO1Jj3pyUN0j9KjeOqR\",\n                          \"label\": \"Yes\"\n                        },\n                        {\n                          \"id\": \"2WaO1K5umgZcZV4bAW5sPu\",\n                          \"label\": \"No\",\n                          \"children\": [\n                            {\n                              \"id\": \"objectivesProgress_deliverablesMet_otherText\",\n                              \"props\": {\n                                \"label\": \"Describe progress toward reaching the target/milestone during the reporting period. How close are you to meeting the target? How do you plan to address any obstacle(s) to meeting the target?\"\n                              },\n                              \"type\": \"textarea\",\n                              \"validation\": {\n                                \"type\": \"text\",\n                                \"parentFieldName\": \"objectivesProgress_deliverablesMet\",\n                                \"parentOptionId\": \"objectivesProgress_deliverablesMet-2WaO1K5umgZcZV4bAW5sPu\",\n                                \"nested\": true\n                              }\n                            }\n                          ]\n                        }\n                      ]\n                    }\n                  }\n                ]\n              }\n            }\n          },\n          {\n            \"name\": \"Initiative progress\",\n            \"pageType\": \"entityOverlay\",\n            \"entityType\": \"initiative\",\n            \"stepType\": \"initiativeProgress\",\n            \"stepInfo\": [\"name\", \"hint\"],\n            \"hint\": \"Report overall progress for the initiative\",\n            \"isRequired\": true,\n            \"verbiage\": {\n              \"intro\": {\n                \"section\": \"State or Territory-Specific Initiatives\",\n                \"info\": [\n                  {\n                    \"type\": \"h3\",\n                    \"content\": \"Initiative progress\"\n                  },\n                  {\n                    \"type\": \"p\",\n                    \"content\": \"Report progress for this initiative during this reporting period.\"\n                  },\n                  {\n                    \"type\": \"p\",\n                    \"content\": \"Report key accomplishments or challenges for this initiative that are not otherwise mentioned under the objective(s). Recipients can document whether they are considering changes to objective(s) or the initiative based on the developments to date, including collaborations that may be under consideration or occurring with external parties to assist with running the initiative or achieving objectives.\"\n                  }\n                ]\n              },\n              \"enterEntityDetailsButtonText\": \"Edit\",\n              \"readOnlyEntityDetailsButtonText\": \"View\",\n              \"editEntityHint\": \"Select \\\"Edit\\\" to report the overall progress.\",\n              \"reviewPdfHint\": \"To view totals against targets from your associated MFP Work Plan, click \\\"Review PDF\\\" and it will open a summary in a new tab.\"\n            },\n            \"form\": {\n              \"id\": \"stsiip\",\n              \"fields\": [\n                {\n                  \"id\": \"initiativeProgress_describeProgress\",\n                  \"type\": \"textarea\",\n                  \"validation\": \"text\",\n                  \"props\": {\n                    \"label\": \"Describe any progress made under this initiative during the reporting period not otherwise mentioned under the objective(s).\"\n                  }\n                },\n                {\n                  \"id\": \"initiativeProgress_describeIssuesChallenges\",\n                  \"type\": \"textarea\",\n                  \"validation\": \"text\",\n                  \"props\": {\n                    \"label\": \"Describe any issues or challenges that have impacted the development and implementation of the initiative during the reporting period that are not otherwise mentioned under the objective(s).\",\n                    \"hint\": \"Detail what impact such issues may have on the state or territory's ability to provide HCBS rather than institutional services, and describe how you plan to address these issues.\"\n                  }\n                },\n                {\n                  \"id\": \"initiativeProgress_describeCollaborationsWithExternalParties\",\n                  \"type\": \"textarea\",\n                  \"validation\": \"text\",\n                  \"props\": {\n                    \"label\": \"List and describe any collaborations you have with any external parties to run the initiative tasks or to achieve initiative goals.\"\n                  }\n                }\n              ]\n            }\n          },\n          {\n            \"name\": \"Expenditures\",\n            \"pageType\": \"entityOverlay\",\n            \"entityType\": \"initiative\",\n            \"stepType\": \"expenditures\",\n            \"stepInfo\": [\"name\", \"hint\"],\n            \"hint\": \"Report actual quarterly expenditures by funding source\",\n            \"isRequired\": true,\n            \"verbiage\": {\n              \"intro\": {\n                \"section\": \"State or Territory-Specific Initiatives\",\n                \"info\": [\n                  {\n                    \"type\": \"h3\",\n                    \"content\": \"Expenditures\"\n                  },\n                  {\n                    \"type\": \"p\",\n                    \"content\": \"Report initiative expenditures by quarter and funding source. Report actual spending for each quarter for this initiative. Recipients plan quarterly expenditure targets in your MFP Work Plan\u2019s funding sources. Recipients with discrepancies between projected and actual spending due solely to lag time between incurring costs and disbursing funds will have the option to note those cases in this section.\"\n                  },\n                  {\n                    \"type\": \"p\",\n                    \"content\": \"Funding sources and projected spending are auto-populated from your associated MFP Work Plan.\"\n                  }\n                ]\n              },\n              \"enterEntityDetailsButtonText\": \"Edit\",\n              \"readOnlyEntityDetailsButtonText\": \"View\",\n              \"editEntityHint\": \"Select \\\"Edit\\\" to report expenditures.\",\n              \"reviewPdfHint\": \"To view totals against targets from your associated MFP Work Plan, click \\\"Review PDF\\\" and it will open a summary in a new tab.\"\n            },\n            \"form\": {\n              \"id\": \"stsie\",\n              \"fields\": [\n                {\n                  \"id\": \"fundingSources\",\n                  \"type\": \"number\",\n                  \"validation\": \"number\",\n                  \"transformation\": {\n                    \"rule\": \"fundingSources\"\n                  },\n                  \"props\": {\n                    \"mask\": \"currency\"\n                  }\n                },\n                {\n                  \"id\": \"expenditures_onTrackToFullExpendFunds\",\n                  \"type\": \"radio\",\n                  \"validation\": \"radio\",\n                  \"props\": {\n                    \"label\": \"Taking the lag time for reporting expenditures into account, is the state or territory on track to fully expend funds within the projected time frame for this initiative?\",\n                    \"hint\": \"\",\n                    \"choices\": [\n                      {\n                        \"id\": \"2WaUKOjwgUvHO6CMAqE8aOC\",\n                        \"label\": \"Yes\"\n                      },\n                      {\n                        \"id\": \"2WaUKNbCmetqXVFsKnaHqeK\",\n                        \"label\": \"No\",\n                        \"children\": [\n                          {\n                            \"id\": \"expenditures_onTrackToFullExpendFunds-otherText\",\n                            \"type\": \"textarea\",\n                            \"validation\": {\n                              \"type\": \"text\",\n                              \"parentFieldName\": \"expenditures_onTrackToFullExpendFunds\",\n                              \"parentOptionId\": \"2WaUKNbCmetqXVFsKnaHqeK\",\n                              \"nested\": true\n                            },\n                            \"props\": {\n                              \"label\": \"Briefly explain what has contributed to lower than projected expenditures (e.g., challenges with hiring, delays in start-up) and describe your revised time frame for fully expending awarded funds.\"\n                            }\n                          }\n                        ]\n                      }\n                    ]\n                  }\n                }\n              ]\n            }\n          }\n        ]\n      },\n      \"initiatives\": []\n    },\n    {\n      \"name\": \"Organization & Administration\",\n      \"path\": \"/sar/organization-and-administration\",\n      \"pageType\": \"standard\",\n      \"verbiage\": {\n        \"intro\": {\n          \"section\": \"\",\n          \"subsection\": \"Organization & Administration\"\n        }\n      },\n      \"form\": {\n        \"id\": \"oa\",\n        \"fields\": [\n          {\n            \"id\": \"oa_changesOrganizationAdministration\",\n            \"type\": \"radio\",\n            \"validation\": \"radio\",\n            \"props\": {\n              \"label\": \"Were there any changes in the organization or administration of the MFP program during this reporting period? For example, did your Medicaid agency undergo a reorganization that altered the reporting relationship of the MFP Project Director?\",\n              \"choices\": [\n                {\n                  \"id\": \"2VfuG3SVaUXHaKZWzucYU6jt\",\n                  \"label\": \"No\"\n                },\n                {\n                  \"id\": \"2VfuG4GRc4ApcknSSgbMvvHg\",\n                  \"label\": \"Yes\",\n                  \"children\": [\n                    {\n                      \"id\": \"oa_describeOAChanges\",\n                      \"type\": \"textarea\",\n                      \"validation\": {\n                        \"type\": \"text\",\n                        \"nested\": true,\n                        \"parentFieldName\": \"oa_changesOrganizationAdministration\",\n                        \"parentOptionId\": \"2VfuG4GRc4ApcknSSgbMvvHg\"\n                      },\n                      \"props\": {\n                        \"label\": \"Describe the changes.\"\n                      }\n                    }\n                  ]\n                }\n              ]\n            }\n          },\n          {\n            \"id\": \"oa_projectDirectorEmployment\",\n            \"type\": \"radio\",\n            \"validation\": \"radio\",\n            \"props\": {\n              \"label\": \"Is the Project Director an employee of the recipient agency or state/territory Medicaid agency?\",\n              \"choices\": [\n                {\n                  \"id\": \"2VfuG2OUicDROyaE5RmbtqWM\",\n                  \"label\": \"No\",\n                  \"children\": [\n                    {\n                      \"id\": \"oa_provideNameOfEmployerAndReportingRelationship\",\n                      \"type\": \"textarea\",\n                      \"validation\": {\n                        \"type\": \"text\",\n                        \"nested\": true,\n                        \"parentFieldName\": \"oa_projectDirectorEmployment\",\n                        \"parentOptionId\": \"2VfuG2OUicDROyaE5RmbtqWM\"\n                      },\n                      \"props\": {\n                        \"label\": \"Provide the name of the employer and the reporting relationship with the recipient agency.\"\n                      }\n                    }\n                  ]\n                },\n                {\n                  \"id\": \"2VfuG52IEKt1Fft6cz4jgi68\",\n                  \"label\": \"Yes\"\n                }\n              ]\n            }\n          },\n          {\n            \"id\": \"oa_hiringRetentionChallengesMfpStaff\",\n            \"type\": \"radio\",\n            \"validation\": \"radio\",\n            \"props\": {\n              \"label\": \"Are there hiring or retention challenges for MFP staff, including the MFP Project Director and MFP Data and Quality Analyst?\",\n              \"choices\": [\n                {\n                  \"id\": \"2VfuyVPFCc95OFfbF9tEKjzn\",\n                  \"label\": \"No\"\n                },\n                {\n                  \"id\": \"2VfuyVGyPc6ePgAicM69ayDqTpX\",\n                  \"label\": \"Yes\",\n                  \"children\": [\n                    {\n                      \"id\": \"oa_describeHiringRetentionChallenges\",\n                      \"type\": \"textarea\",\n                      \"validation\": {\n                        \"type\": \"text\",\n                        \"nested\": true,\n                        \"parentFieldName\": \"oa_hiringRetentionChallengesMfpStaff\",\n                        \"parentOptionId\": \"2VfuyVGyPc6ePgAicM69ayDqTpX\"\n                      },\n                      \"props\": {\n                        \"label\": \"Describe the challenges.\"\n                      }\n                    }\n                  ]\n                }\n              ]\n            }\n          },\n          {\n            \"id\": \"oa_describeTechnicalAssitanceActivities\",\n            \"type\": \"textarea\",\n            \"validation\": \"text\",\n            \"props\": {\n              \"label\": \"Describe the technical assistance activities MFP staff have engaged in during the reporting period (e.g., participation in a learning collaborative or other training session).\"\n            }\n          },\n          {\n            \"id\": \"oa_additionalTechnicalResourcesSupports\",\n            \"type\": \"radio\",\n            \"validation\": \"radio\",\n            \"props\": {\n              \"label\": \"Are there additional technical assistance resources or supports that your state or territory would benefit from?\",\n              \"choices\": [\n                {\n                  \"id\": \"2VfvDUpKJRmrBrqHpy77oHAi\",\n                  \"label\": \"No\"\n                },\n                {\n                  \"id\": \"2VfvDSkVxhYZwQ8AMXqR5QX8\",\n                  \"label\": \"Yes\",\n                  \"children\": [\n                    {\n                      \"id\": \"oa_describeAdditionalTechnicalResourcesSupports\",\n                      \"type\": \"textarea\",\n                      \"validation\": {\n                        \"type\": \"text\",\n                        \"nested\": true,\n                        \"parentFieldName\": \"oa_additionalTechnicalResourcesSupports\",\n                        \"parentOptionId\": \"2VfvDSkVxhYZwQ8AMXqR5QX8\"\n                      },\n                      \"props\": {\n                        \"label\": \"Describe additional technical resources or supports.\"\n                      }\n                    }\n                  ]\n                }\n              ]\n            }\n          }\n        ]\n      }\n    },\n    {\n      \"name\": \"Additional Achievements\",\n      \"path\": \"/sar/additional-achievements\",\n      \"pageType\": \"standard\",\n      \"verbiage\": {\n        \"intro\": {\n          \"section\": \"\",\n          \"subsection\": \"Additional Achievements\",\n          \"info\": [\n            {\n              \"type\": \"p\",\n              \"content\": \"Use this section to describe any additional achievements or promising practices that have contributed to the effective operation of the demonstration and successful transitions during the reporting period. Achievements or topics discussed in previous sections do not need to be reiterated here. Use the topics below as a guide, but note other important updates.\"\n            },\n            {\n              \"type\": \"ul\",\n              \"props\": {\n                \"style\": {\n                  \"columns\": \"2\",\n                  \"column-gap\": \"2.5rem\",\n                  \"paddingLeft\": \"1rem\",\n                  \"color\": \"#5A5A5A\",\n                  \"li::before\": {\n                    \"color\": \"#5A5A5A\"\n                  }\n                }\n              },\n              \"children\": [\n                {\n                  \"type\": \"li\",\n                  \"content\": \"Person-centered planning and services\"\n                },\n                {\n                  \"type\": \"li\",\n                  \"content\": \"No Wrong Door systems\"\n                },\n                {\n                  \"type\": \"li\",\n                  \"content\": \"Community transition support\"\n                },\n                {\n                  \"type\": \"li\",\n                  \"content\": \"Direct service workforce and caregivers\"\n                },\n                {\n                  \"type\": \"li\",\n                  \"content\": \"Housing to support community-based living options\"\n                },\n                {\n                  \"type\": \"li\",\n                  \"content\": \"Employment support\"\n                },\n                {\n                  \"type\": \"li\",\n                  \"content\": \"Convenient and accessible transportation options\"\n                },\n                {\n                  \"type\": \"li\",\n                  \"content\": \"Data-based decision-making\"\n                },\n                {\n                  \"type\": \"li\",\n                  \"content\": \"Financing approaches\"\n                },\n                {\n                  \"type\": \"li\",\n                  \"content\": \"Stakeholder agreement\"\n                },\n                {\n                  \"type\": \"li\",\n                  \"content\": \"Quality measurement and improvement\"\n                },\n                {\n                  \"type\": \"li\",\n                  \"content\": \"Equity and social determinants of health (SDOH)\"\n                }\n              ]\n            }\n          ]\n        }\n      },\n      \"form\": {\n        \"id\": \"aa\",\n        \"fields\": [\n          {\n            \"id\": \"aa_notableAchievementsPromisingPractices\",\n            \"type\": \"textarea\",\n            \"validation\": \"text\",\n            \"props\": {\n              \"label\": \"Describe any notable achievements and identify any promising practices by your MFP program that have not been captured elsewhere.\"\n            }\n          },\n          {\n            \"id\": \"aa_changesMfpProgramAdministration\",\n            \"type\": \"radio\",\n            \"validation\": \"radio\",\n            \"props\": {\n              \"label\": \"Indicate whether your state or territory has made any changes to operations, objectives, or other aspects of MFP program administration that will require amendments to the Operational Protocol.\",\n              \"choices\": [\n                {\n                  \"id\": \"2Vfw3b0V0urpvwAz9Fnqafb1\",\n                  \"label\": \"No\"\n                },\n                {\n                  \"id\": \"2Vfw3X48qEyKOAxXt7HPlMMg\",\n                  \"label\": \"Yes\",\n                  \"children\": [\n                    {\n                      \"id\": \"oa_describeDevelopmentsChanges\",\n                      \"type\": \"textarea\",\n                      \"validation\": {\n                        \"type\": \"text\",\n                        \"nested\": true,\n                        \"parentFieldName\": \"aa_changesMfpProgramAdministration\",\n                        \"parentOptionId\": \"2Vfw3X48qEyKOAxXt7HPlMMg\"\n                      },\n                      \"props\": {\n                        \"label\": \"Describe the developments or changes.\"\n                      }\n                    }\n                  ]\n                }\n              ]\n            }\n          }\n        ]\n      }\n    },\n    {\n      \"name\": \"Review & Submit\",\n      \"path\": \"/sar/review-and-submit\",\n      \"pageType\": \"reviewSubmit\"\n    }\n  ]\n}\n", "import wpForm from \"../../forms/wp.json\";\nimport sarForm from \"../../forms/sar.json\";\nimport KSUID from \"ksuid\";\nimport {\n  AnyObject,\n  FieldChoice,\n  FormField,\n  FormJson,\n  FormLayoutElement,\n  FormTemplateVersion,\n  ReportJson,\n  ReportRoute,\n  ReportType,\n} from \"../types\";\nimport { createHash } from \"crypto\";\nimport { transformFormTemplate } from \"../transformations/transformations\";\nimport {\n  getReportFormTemplate,\n  putFormTemplateVersion,\n  putReportFormTemplate,\n  queryFormTemplateVersionByHash,\n  queryLatestFormTemplateVersionNumber,\n} from \"../../storage/reports\";\nimport assert from \"node:assert\";\n\nexport const formTemplateForReportType = (reportType: ReportType) => {\n  const map: { [key in ReportType]: ReportJson } = {\n    [ReportType.WP]: wpForm as ReportJson,\n    [ReportType.SAR]: sarForm as ReportJson,\n  };\n  // Clone to prevent accidental changes to the originals\n  return structuredClone(map[reportType]);\n};\n\nexport async function getOrCreateFormTemplate(\n  reportType: ReportType,\n  reportPeriod: number,\n  reportYear: number,\n  workPlanFieldData?: AnyObject\n) {\n  let currentFormTemplate = formTemplateForReportType(reportType);\n\n  if (currentFormTemplate?.routes) {\n    currentFormTemplate = transformFormTemplate(\n      currentFormTemplate,\n      reportPeriod,\n      reportYear,\n      workPlanFieldData\n    );\n  }\n\n  const stringifiedTemplate = JSON.stringify(currentFormTemplate);\n\n  const currentTemplateHash = createHash(\"md5\")\n    .update(stringifiedTemplate)\n    .digest(\"hex\");\n\n  const matchTemplateVersion = await queryFormTemplateVersionByHash(\n    reportType,\n    currentTemplateHash\n  );\n\n  //if a template of this hash already exist\n  if (currentTemplateHash === matchTemplateVersion?.md5Hash) {\n    const matchingTemplate = await getReportFormTemplate({\n      reportType,\n      formTemplateId: matchTemplateVersion.id,\n    });\n    assert(\n      matchingTemplate !== undefined,\n      \"Found version info matching form template hash, but no matching document exists in S3\"\n    );\n    return {\n      formTemplate: matchingTemplate,\n      formTemplateVersion: matchTemplateVersion,\n    };\n  } else {\n    const newFormTemplateId = KSUID.randomSync().string;\n\n    const formTemplateWithValidationJson = {\n      ...currentFormTemplate,\n      validationJson: getValidationFromFormTemplate(currentFormTemplate),\n    };\n\n    await putReportFormTemplate(\n      {\n        reportType,\n        formTemplateId: newFormTemplateId,\n      },\n      formTemplateWithValidationJson\n    );\n\n    const latest = await queryLatestFormTemplateVersionNumber(reportType);\n\n    // If we didn't find any form templates, start version at 1.\n    const newFormTemplateVersionItem: FormTemplateVersion = {\n      versionNumber: latest + 1,\n      md5Hash: currentTemplateHash,\n      id: newFormTemplateId,\n      lastAltered: new Date().toISOString(),\n      reportType,\n    };\n\n    await putFormTemplateVersion(newFormTemplateVersionItem);\n\n    return {\n      formTemplate: formTemplateWithValidationJson,\n      formTemplateVersion: newFormTemplateVersionItem,\n    };\n  }\n}\n\n// returns flattened array of valid routes for given reportJson\nexport const flattenReportRoutesArray = (\n  reportJson: ReportRoute[]\n): ReportRoute[] => {\n  const routesArray: ReportRoute[] = [];\n  const mapRoutesToArray = (reportRoutes: ReportRoute[]) => {\n    reportRoutes.map((route: ReportRoute) => {\n      // if children, recurse; if none, push to routes array\n      if (route?.children) {\n        mapRoutesToArray(route.children);\n      } else {\n        routesArray.push(route);\n      }\n    });\n  };\n  mapRoutesToArray(reportJson);\n  return routesArray;\n};\n\n// returns validation schema object for array of fields\nexport const compileValidationJsonFromFields = (\n  fieldArray: FormField[],\n  parentOption?: any\n): AnyObject => {\n  const validationSchema: AnyObject = {};\n  fieldArray.forEach((field: FormField) => {\n    // if field has a parent option, add option name to validation object\n    if (\n      typeof field.validation === \"object\" &&\n      !field.validation.parentOptionId\n    ) {\n      field.validation.parentOptionId = parentOption?.name;\n    }\n    // compile field's validation schema\n    validationSchema[field.id] = field.validation;\n    // if field has choices/options (ie could have nested children)\n    const fieldChoices = field.props?.choices;\n    if (fieldChoices) {\n      fieldChoices.forEach((choice: FieldChoice) => {\n        // if given field choice has nested children\n        const nestedChildFields = choice.children;\n        if (nestedChildFields) {\n          Object.assign(\n            validationSchema,\n            compileValidationJsonFromFields(nestedChildFields, choice)\n          );\n        }\n      });\n    }\n  });\n  return validationSchema;\n};\n\n// traverse routes and compile all field validation schema into one object\nexport const compileValidationJsonFromRoutes = (\n  routeArray: ReportRoute[]\n): AnyObject => {\n  const validationSchema: AnyObject = {};\n  const addValidationToAccumulator = (form: FormJson) => {\n    Object.assign(\n      validationSchema,\n      compileValidationJsonFromFields(form.fields.filter(isFieldElement))\n    );\n  };\n  routeArray.forEach((route: ReportRoute) => {\n    // check for non-standard needed validation objects\n    if (\n      (route.pageType === \"modalDrawer\" ||\n        route.pageType === \"modalOverlay\" ||\n        route.pageType === \"overlayModal\" ||\n        route.pageType === \"dynamicModalOverlay\") &&\n      route.entityType\n    ) {\n      Object.assign(validationSchema, { [route.entityType]: \"objectArray\" });\n    }\n\n    for (let formType of [\"form\", \"modalForm\", \"drawerForm\"] as const) {\n      if (route[formType]) {\n        addValidationToAccumulator(route[formType]!);\n      }\n    }\n\n    // accumulate entity steps\n    if (route.pageType === \"modalOverlay\") {\n      for (let step of route.entitySteps ?? []) {\n        const stepForm = step.form || step.modalForm;\n        addValidationToAccumulator(stepForm);\n      }\n    }\n\n    if (route.pageType === \"dynamicModalOverlay\") {\n      for (let initiative of route.initiatives ?? []) {\n        for (let step of initiative.entitySteps) {\n          const stepForm = step.form || step.modalForm;\n          if (stepForm) {\n            addValidationToAccumulator(stepForm);\n          } else {\n            if (step.objectiveCards) {\n              for (let objectiveCard of step.objectiveCards) {\n                if (objectiveCard.modalForm) {\n                  addValidationToAccumulator(objectiveCard.modalForm);\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n  });\n  return validationSchema;\n};\n\nexport function isFieldElement(\n  field: FormField | FormLayoutElement\n): field is FormField {\n  /*\n   * This function is duplicated in ui-src/src/types/formFields.ts\n   * If you change it here, change it there!\n   */\n  const formLayoutElementTypes = [\"sectionHeader\", \"sectionContent\"];\n  return !formLayoutElementTypes.includes(field.type);\n}\n\nexport function isLayoutElement(\n  field: FormField | FormLayoutElement\n): field is FormLayoutElement {\n  /*\n   * This function is duplicated in ui-src/src/types/formFields.ts\n   * If you change it here, change it there!\n   */\n  return (field as FormField).validation === undefined;\n}\n\nexport function getValidationFromFormTemplate(reportJson: ReportJson) {\n  return compileValidationJsonFromRoutes(\n    flattenReportRoutesArray(reportJson.routes)\n  );\n}\n\nexport function getPossibleFieldsFromFormTemplate(reportJson: ReportJson) {\n  return Object.keys(getValidationFromFormTemplate(reportJson));\n}\n", "import { randomUUID } from \"crypto\";\nimport {\n  AnyObject,\n  DynamicModalOverlayReportPageShape,\n  EntityDetailsOverlayShape,\n  FieldChoice,\n  FormField,\n  FormJson,\n  FormLayoutElement,\n  OverlayModalPageShape,\n  PageTypes,\n  ReportJson,\n  ReportRoute,\n  SomeRequired,\n  isUsableForTransforms,\n  TargetPopulation,\n  WorkPlanFieldDataForTransforms,\n  FundingSource,\n} from \"../types\";\n\nexport const removeConditionalRoutes = <T extends ReportRoute>(\n  routes: T[],\n  reportPeriod: number\n): T[] => {\n  for (let route of routes) {\n    if (route.children) {\n      route.children = removeConditionalRoutes(route.children, reportPeriod);\n    }\n    if (route.entitySteps) {\n      route.entitySteps = removeConditionalRoutes(\n        route.entitySteps,\n        reportPeriod\n      );\n    }\n  }\n\n  return routes.filter((route) => {\n    switch (route.conditionallyRender) {\n      case undefined:\n        // There is no condition, so this route is always included.\n        return true;\n      case \"showOnlyInPeriod2\":\n        return reportPeriod === 2;\n      default:\n        throw new Error(\n          `Route.conditionallyRender value '${route.conditionallyRender}' is not implemented.`\n        );\n    }\n  });\n};\n\nexport function* iterateAllForms(\n  routes: ReportRoute[]\n): Generator<FormJson, void, unknown> {\n  for (let route of routes) {\n    if (route.form) {\n      yield route.form;\n    }\n    if (route.modalForm) {\n      yield route.modalForm;\n    }\n    if (route.drawerForm) {\n      yield route.drawerForm;\n    }\n    if (route.children) {\n      yield* iterateAllForms(route.children);\n    }\n    if (route.initiatives) {\n      for (let initiative of route.initiatives) {\n        yield* iterateAllForms(initiative.entitySteps);\n      }\n    }\n    if (route.entitySteps) {\n      yield* iterateAllForms(route.entitySteps);\n    }\n    if (route.objectiveCards) {\n      for (let objectiveCard of route.objectiveCards) {\n        if (objectiveCard.modalForm) yield objectiveCard.modalForm;\n      }\n    }\n  }\n}\n\nexport function* iterateChoicesWithChildren(\n  fields: (FormField | FormLayoutElement)[]\n): Generator<SomeRequired<FieldChoice, \"children\">, void, undefined> {\n  const fieldsWithChoices = fields.filter((field) => !!field.props?.choices);\n  for (let field of fieldsWithChoices) {\n    const choicesWithChildren = field.props!.choices.filter(\n      (choice: FieldChoice) => !!choice.children\n    );\n\n    for (let choice of choicesWithChildren) {\n      yield choice;\n      yield* iterateChoicesWithChildren(choice.children);\n    }\n  }\n}\n\n/**\n * Apply all transformations to this list of fields. A \"transformation\" may\n * adjust the content of a field, or create copies of a field,\n * based on data in the report.\n */\nexport const transformFields = (\n  fields: (FormField | FormLayoutElement)[],\n  reportPeriod: number,\n  reportYear: number,\n  workPlanFieldData?: unknown,\n  initiativeId?: string,\n  objectiveId?: string\n): (FormField | FormLayoutElement)[] => {\n  if (!isUsableForTransforms(workPlanFieldData)) {\n    throw new Error(\n      `Work Plan Field Data is not structured as expected for SAR form template transformation`\n    );\n  }\n  return fields?.flatMap((field) => {\n    if (!field.transformation?.rule) {\n      // This field doesn't require any transformation.\n      return field;\n    }\n    switch (field.transformation.rule) {\n      case \"nextTwelveQuarters\":\n        return nextTwelveQuarters(field as FormField, reportYear, reportPeriod);\n      case \"targetPopulations\":\n        // This transformation is only used within the SAR, based on data from its source WP.\n        return targetPopulations(\n          field as FormField,\n          reportPeriod,\n          workPlanFieldData?.targetPopulations\n        );\n      case \"firstQuarterOfThePeriod\":\n        return firstQuarterOfThePeriod(field, reportPeriod);\n      case \"secondQuarterOfThePeriod\":\n        return secondQuarterOfThePeriod(field, reportPeriod);\n      case \"fundingSources\":\n        return fundingSources(\n          field,\n          reportPeriod,\n          reportYear,\n          workPlanFieldData,\n          initiativeId\n        );\n      case \"quantitativeQuarters\":\n        return quantitativeQuarters(\n          field,\n          reportPeriod,\n          reportYear,\n          workPlanFieldData,\n          initiativeId,\n          objectiveId\n        );\n      default:\n        throw new Error(\n          `Field transformation rule ${field.transformation.rule} is not implemented.`\n        );\n    }\n  });\n};\n\nexport const transformFormTemplate = (\n  formTemplate: ReportJson,\n  reportPeriod: number,\n  reportYear: number,\n  workPlanFieldData?: AnyObject\n) => {\n  formTemplate.routes = removeConditionalRoutes(\n    formTemplate.routes,\n    reportPeriod\n  );\n\n  formTemplate.routes = generateSARFormsForInitiatives(\n    formTemplate.routes,\n    workPlanFieldData\n  );\n\n  for (let form of iterateAllForms(formTemplate.routes)) {\n    form.fields = transformFields(\n      form.fields,\n      reportPeriod,\n      reportYear,\n      workPlanFieldData,\n      form.initiativeId,\n      form.objectiveId\n    );\n    for (let choiceWithChildren of iterateChoicesWithChildren(form.fields)) {\n      choiceWithChildren.children = transformFields(\n        choiceWithChildren.children,\n        reportPeriod,\n        reportYear,\n        workPlanFieldData,\n        form.initiativeId,\n        form.objectiveId\n      );\n    }\n  }\n\n  return formTemplate;\n};\n\n/**\n * Given a field and a reporting period, create 12 copies of that field:\n * One copy for each of the next twelve, starting with the first quarter\n * of this period. For example, 2024 period 2 will cover 2024Q3 - 2027Q2.\n */\nconst nextTwelveQuarters = (\n  field: FormField,\n  reportYear: number,\n  reportPeriod: number\n) => {\n  // The first quarter will be Q1 for period 1, or Q3 for period 2.\n  const firstQuarterIndex = reportPeriod === 1 ? 0 : 2;\n\n  // No point in keeping this around in the clones\n  delete field.transformation;\n\n  return [...new Array(12)]\n    .map((_, index) => ({\n      year: reportYear + Math.floor((firstQuarterIndex + index) / 4),\n      quarter: `Q${1 + ((firstQuarterIndex + index) % 4)}`,\n    }))\n    .map(({ year, quarter }) => ({\n      ...field,\n      id: `${field.id}${year}${quarter}`,\n      props: {\n        ...field.props,\n        label: `${year} ${quarter}`,\n      },\n    }));\n};\n\n/**\n * Create copies of the given field, one for each given population.\n * These fields are needed throughout the SAR, using populations from the WP.\n * The list of populations will be stored in a special field in the SAR\n * field data, captured at the moment the SAR is created.\n */\nconst targetPopulations = (\n  field: FormField,\n  reportPeriod: number,\n  targetPopulations?: TargetPopulation[]\n) => {\n  if (!targetPopulations) {\n    throw new Error(\n      \"Field transformation rule 'targetPopulations' requires targetPopulations.\"\n    );\n  }\n\n  // No point keeping this around in the clones\n  delete field.transformation;\n\n  // Exclude populations that were marked in the WP as being non-applicable to MFP\n  const isApplicable = (population: TargetPopulation) =>\n    population.transitionBenchmarks_applicableToMfpDemonstration?.[0]?.value !==\n    \"No\";\n\n  const nameOf = (population: TargetPopulation) =>\n    population.transitionBenchmarks_targetPopulationName;\n\n  const labelOf = (population: TargetPopulation) =>\n    population.isRequired === true\n      ? `Number of ${nameOf(population)}`\n      : `Other: ${nameOf(population)}`;\n\n  return targetPopulations.filter(isApplicable).map((population: any) => ({\n    ...field,\n    id: `${field.id}_Period${reportPeriod}_${nameOf(population)}`,\n    props: {\n      ...field.props,\n      label: labelOf(population),\n    },\n  }));\n};\n\n/** Create a section header with content depending on the report period */\nconst firstQuarterOfThePeriod = (\n  field: FormLayoutElement,\n  reportPeriod: number\n) => {\n  return {\n    id: `${field.id}`,\n    type: `${field.type}`,\n    props: {\n      content:\n        reportPeriod === 1\n          ? \"First quarter (January 1 - March 31)\"\n          : \"Third quarter (July 1 - September 30)\",\n    },\n  };\n};\n\n/** Create a section header with content depending on the report period */\nconst secondQuarterOfThePeriod = (\n  field: FormLayoutElement,\n  reportPeriod: number\n) => {\n  return {\n    id: `${field.id}`,\n    type: `${field.type}`,\n    props: {\n      content:\n        reportPeriod === 1\n          ? \"Second quarter (April 1 - June 30)\"\n          : \"Fourth quarter (October 1 - December 31)\",\n    },\n  };\n};\n\n// Funding sources\nexport const fundingSources = (\n  field: FormField,\n  reportPeriod: number,\n  reportYear: number,\n  workPlanFieldData?: WorkPlanFieldDataForTransforms,\n  initiativeId?: string\n): (FormField | FormLayoutElement)[] => {\n  delete field.transformation;\n\n  const initiativeToUse = workPlanFieldData?.initiative?.find(\n    (initiative: any) => initiative.id === initiativeId\n  );\n\n  if (!initiativeToUse) {\n    throw new Error(\n      `Transformation failed: Could not find initiative with id ${initiativeId}`\n    );\n  }\n\n  const firstPeriodQuarters = [\n    {\n      number: 1,\n      name: \"First\",\n      range: \"January 1 - March 31\",\n    },\n    {\n      number: 2,\n      name: \"Second\",\n      range: \"April 1 - June 30\",\n    },\n  ];\n\n  const secondPeriodQuarters = [\n    {\n      number: 3,\n      name: \"Third\",\n      range: \"July 1 - September 30\",\n    },\n    {\n      number: 4,\n      name: \"Fourth\",\n      range: \"October 1 - December 31\",\n    },\n  ];\n\n  const quarters =\n    reportPeriod === 1 ? firstPeriodQuarters : secondPeriodQuarters;\n\n  const getFundingSourceName = (fundingSource: FundingSource) => {\n    const selectedOption = fundingSource.fundingSources_wpTopic[0].value;\n    if (selectedOption !== \"Other, specify\") {\n      return selectedOption;\n    }\n    return fundingSource.initiative_wp_otherTopic!;\n  };\n\n  return initiativeToUse.fundingSources.flatMap((fundingSource) => [\n    {\n      id: `fundingSourcesHeader_${randomUUID()}`,\n      type: \"sectionHeader\",\n      props: {\n        content: `${getFundingSourceName(fundingSource)}`,\n      },\n    },\n    {\n      id: `fundingSourcesContent_${randomUUID()}`,\n      type: \"sectionContent\",\n      props: {\n        content: \"This funding source auto-populates from MFP Work Plan.\",\n      },\n    },\n    ...quarters.flatMap((quarter) => [\n      {\n        id: `fundingSources_actual_${reportYear}Q${quarter.number}_${fundingSource.id}`,\n        type: \"number\",\n        validation: \"number\",\n        props: {\n          label: `Actual spending (${quarter.name} quarter: ${quarter.range})`,\n          mask: \"currency\",\n        },\n      },\n      {\n        id: `fundingSources_projected_${reportYear}Q${quarter.number}_${fundingSource.id}`,\n        type: \"number\",\n        validation: \"number\",\n        props: {\n          label: `Projected spending (${quarter.name} quarter: ${quarter.range})`,\n          disabled: true,\n          mask: \"currency\",\n        },\n      },\n    ]),\n  ]);\n};\n\nexport const quantitativeQuarters = (\n  fieldToRepeat: FormField,\n  reportPeriod: number,\n  reportYear: number,\n  workPlanFieldData?: AnyObject,\n  initiativeId?: string,\n  objectiveId?: string\n) => {\n  const fieldsToAppend = [];\n\n  const initiativeToUse = workPlanFieldData?.initiative.find(\n    (initiative: any) => initiative.id === initiativeId\n  );\n\n  const objectiveToUse = initiativeToUse.evaluationPlan.find(\n    (objective: any) => objective.id === objectiveId\n  );\n\n  delete fieldToRepeat.transformation;\n\n  if (objectiveToUse?.evaluationPlan_includesTargets?.[0]?.value === \"Yes\") {\n    const headingStringFirstQuarter =\n      reportPeriod === 1\n        ? \"First quarter (January 1 - March 31)\"\n        : \"Third quarter (July 1 - September 30)\";\n\n    const headingStringSecondQuarter =\n      reportPeriod === 1\n        ? \"Second quarter (April 1 - June 30)\"\n        : \"Fourth quarter (October 1 - December 31)\";\n\n    //depending on the period, the quarter starts at  Q1 or Q3\n    const currentQuarter = reportPeriod === 1 ? 1 : 3;\n    // have to loop twice for both periods\n    const quartersInPeriod = reportPeriod === 1 ? [1, 2] : [3, 4];\n    for (let quarterNumber of quartersInPeriod) {\n      const formFieldHeading: FormField = {\n        id: `objectiveTargetsHeader_Q${quarterNumber}_${fieldToRepeat.id}`,\n        type: \"sectionHeader\",\n        props: {\n          content:\n            quarterNumber == currentQuarter\n              ? headingStringFirstQuarter\n              : headingStringSecondQuarter,\n          label:\n            quarterNumber == currentQuarter\n              ? \"Complete the following for quantitative targets:\"\n              : \"\",\n        },\n      };\n      fieldsToAppend.push(formFieldHeading);\n\n      const formFieldActual: FormField = {\n        id: `objectiveTargets_actual_${reportYear}Q${quarterNumber}`,\n        type: \"text\",\n        validation: \"text\",\n        props: {\n          label: \"Actual value\",\n          className: \"number-field\",\n        },\n      };\n      fieldsToAppend.push(formFieldActual);\n\n      const formFieldTarget: FormField = {\n        id: `objectiveTargets_projections_${reportYear}Q${quarterNumber}`,\n        type: \"text\",\n        validation: \"text\",\n        props: {\n          label: \"Target Value\",\n          hint: \"Auto-populates from Work Plan.\",\n          disabled: true,\n          className: \"number-field\",\n        },\n      };\n      fieldsToAppend.push(formFieldTarget);\n    }\n  }\n\n  return fieldsToAppend;\n};\n\nexport const runSARTransformations = (\n  route: DynamicModalOverlayReportPageShape,\n  workPlanFieldData?: WorkPlanFieldDataForTransforms\n): ReportRoute => {\n  if (!workPlanFieldData?.initiative)\n    throw new Error(\n      \"Not implemented yet - Workplan must have initiatives that the SAR can build from\"\n    );\n\n  // At this stage, we know that the route will have a template.\n  const template = route.template!;\n  delete route.template;\n\n  route.initiatives = [];\n  for (let workPlanInitiative of workPlanFieldData.initiative) {\n    let templateEntitySteps = structuredClone(template.entitySteps);\n    for (let step of templateEntitySteps) {\n      for (let formType of [\"form\", \"modalForm\", \"drawerForm\"] as const) {\n        if (step[formType]) {\n          step[formType].initiativeId = workPlanInitiative.id;\n        }\n      }\n      if (step.stepType === \"objectiveProgress\") {\n        step.objectiveCards = [];\n        for (let workPlanObjective of workPlanInitiative.evaluationPlan) {\n          const templateObjectiveCard = structuredClone(\n            step.objectiveCardTemplate\n          );\n          step.objectiveCards.push({\n            ...templateObjectiveCard,\n            modalForm: {\n              ...templateObjectiveCard?.modalForm,\n              initiativeId: workPlanInitiative.id,\n              objectiveId: workPlanObjective.id,\n            },\n          });\n        }\n        delete step.transformation;\n        delete step.objectiveCardTemplate;\n      }\n    }\n\n    route.initiatives.push({\n      initiativeId: workPlanInitiative.id,\n      name: workPlanInitiative.initiative_name,\n      topic: workPlanInitiative.initiative_wpTopic?.[0].value,\n      dashboard: template.dashboard,\n      entitySteps: templateEntitySteps,\n    });\n  }\n\n  return route;\n};\n\nconst generateSARFormsForInitiatives = (\n  reportRoutes: (\n    | ReportRoute\n    | OverlayModalPageShape\n    | EntityDetailsOverlayShape\n  )[],\n  workPlanFieldData?: AnyObject\n) => {\n  for (let route of reportRoutes) {\n    if (route?.pageType === PageTypes.DYNAMIC_MODAL_OVERLAY) {\n      route = runSARTransformations(\n        route as DynamicModalOverlayReportPageShape,\n        workPlanFieldData\n      );\n      delete route.template;\n    }\n  }\n  return reportRoutes;\n};\n\n/**\n * This function acts on the field data.\n * It copies fields that are specific to each funding source,\n * out into their containing initiative. The ID of each field will\n * match up to IDs generated in the form template transformation,\n * so that the frontend hydration code will be able to match up the data.\n */\nexport const extractWorkPlanData = (\n  sarFieldData: AnyObject,\n  reportYear: number,\n  reportPeriod: number\n): void => {\n  const quarters = reportPeriod === 1 ? [1, 2] : [3, 4];\n\n  for (let initiative of sarFieldData.initiative) {\n    for (let fundingSource of initiative.fundingSources) {\n      for (let quarter of quarters) {\n        const wpFieldId = `fundingSources_quarters${reportYear}Q${quarter}`;\n        const sarFieldId = `fundingSources_projected_${reportYear}Q${quarter}_${fundingSource.id}`;\n        initiative[sarFieldId] = fundingSource[wpFieldId];\n      }\n    }\n\n    for (let evaluationPlan of initiative.evaluationPlan) {\n      const objectiveProgress: any = {};\n      //Transfering Blanket Data\n      objectiveProgress[\"id\"] = evaluationPlan[\"id\"];\n      objectiveProgress[\"objectiveProgress_objectiveName\"] =\n        evaluationPlan[\"evaluationPlan_objectiveName\"];\n      objectiveProgress[\"objectiveProgress_description\"] =\n        evaluationPlan[\"evaluationPlan_description\"];\n      objectiveProgress[\"objectiveProgress_targets\"] =\n        evaluationPlan[\"evaluationPlan_targets\"];\n      objectiveProgress[\"objectiveProgress_includesTargets\"] =\n        evaluationPlan[\"evaluationPlan_includesTargets\"];\n      objectiveProgress[\"objectiveProgress_additionalDetails\"] =\n        evaluationPlan[\"evaluationPlan_additionalDetails\"];\n\n      //Transfering Evaluation Plan Quarters Data\n      for (let quarter of quarters) {\n        const wpFieldId = `quarterlyProjections${reportYear}Q${quarter}`;\n        const sarFieldId = `objectiveTargets_projections_${reportYear}Q${quarter}`;\n        objectiveProgress[sarFieldId] = evaluationPlan[wpFieldId];\n      }\n      if (initiative[\"objectiveProgress\"]) {\n        initiative[\"objectiveProgress\"].push(objectiveProgress);\n      } else {\n        initiative[\"objectiveProgress\"] = [objectiveProgress];\n      }\n    }\n  }\n};\n", "import { getReportFieldData } from \"../../storage/reports\";\nimport { getPossibleFieldsFromFormTemplate } from \"../formTemplates/formTemplates\";\nimport { ReportFieldData, ReportJson, State } from \"../types\";\n\n/**\n *\n * @param reportBucket bucket name\n * @param state state\n * @param copyFieldDataSourceId fieldDataId of source report\n * @param formTemplate form template json object\n * @param validatedFieldData validated field data from request\n */\n\n//extra fields that needs to be copied over\nconst additionalFields = [\n  \"id\",\n  \"type\",\n  \"isOtherEntity\",\n  \"isRequired\",\n  \"isCopied\",\n  \"isInitiativeClosed\",\n];\n\nexport async function copyFieldDataFromSource(\n  state: State,\n  copyFieldDataSourceId: string,\n  formTemplate: ReportJson,\n  validatedFieldData: ReportFieldData\n) {\n  const sourceFieldData = await getReportFieldData({\n    reportType: formTemplate.type,\n    state,\n    fieldDataId: copyFieldDataSourceId,\n  });\n\n  if (sourceFieldData) {\n    const possibleFields = getPossibleFieldsFromFormTemplate(formTemplate);\n    Object.keys(sourceFieldData).forEach((key: string) => {\n      // Only iterate through entities, not choice lists\n      if (Array.isArray(sourceFieldData[key])) {\n        pruneEntityData(\n          sourceFieldData,\n          key,\n          sourceFieldData[key] as ReportFieldData[],\n          possibleFields\n        );\n      } else if (!possibleFields.includes(key)) {\n        delete sourceFieldData[key];\n      }\n    });\n\n    Object.assign(validatedFieldData, sourceFieldData);\n  }\n\n  return validatedFieldData;\n}\n\nfunction pruneEntityData(\n  sourceFieldData: ReportFieldData,\n  key: string,\n  entityData: ReportFieldData[],\n  possibleFields: string[]\n) {\n  //adding fields to be copied over from entries\n  const concatEntityFields = possibleFields.concat(additionalFields);\n  entityData.forEach((entity, index) => {\n    // Delete any key existing in the source data not valid in our template, or any entity key that's not a name.\n    if (!concatEntityFields.includes(key)) {\n      delete sourceFieldData[key];\n      return;\n    }\n\n    Object.keys(entity).forEach((entityKey) => {\n      //check to see if the object is an array, this is for capturing substeps in the initiatives\n      if (Array.isArray(entity[entityKey])) {\n        pruneEntityData(\n          sourceFieldData,\n          key,\n          entity[entityKey] as ReportFieldData[],\n          possibleFields\n        );\n      } else if (\n        !concatEntityFields.includes(entityKey) &&\n        !entityKey.includes(\"name\") &&\n        ![\"key\", \"value\"].includes(entityKey)\n      ) {\n        delete entityData[index][entityKey];\n      }\n    });\n\n    if (Object.keys(entity).length === 0) {\n      delete entityData[index];\n    } else {\n      entityData[index][\"isCopied\"] = true;\n    }\n  });\n\n  //filter out any closeout data\n  if (Array.isArray(sourceFieldData[key])) {\n    const filteredData = (sourceFieldData[key] as ReportFieldData[]).filter(\n      (field) => !field[\"isInitiativeClosed\"]\n    );\n    sourceFieldData[key] = filteredData;\n  }\n  // Delete whole key if there's nothing in it.\n  if (entityData.every((e) => e === null)) {\n    delete sourceFieldData[key];\n  }\n}\n"],
./.cdk/cdk.out/asset.7cfe8337b138c5ccdae0d4b62bc998bf0a1129eb31cf6cc5d09089d579d55aba/index.js.map:4: TODO: validate topics not null/empty\n    const fetch = this.lookupRequest(apiKeys.Fetch, requests.Fetch)\n\n    // Shuffle topic-partitions to ensure fair response allocation across partitions (KIP-74)\n    const flattenedTopicPartitions = topics.reduce((topicPartitions, { topic, partitions }) => {\n      partitions.forEach(partition => {\n        topicPartitions.push({ topic, partition })\n      })\n      return topicPartitions\n    }, [])\n\n    const shuffledTopicPartitions = shuffle(flattenedTopicPartitions)\n\n    // Consecutive partitions for the same topic can be combined into a single `topic` entry\n    const consolidatedTopicPartitions = shuffledTopicPartitions.reduce(\n      (topicPartitions, { topic, partition }) => {\n        const last = topicPartitions[topicPartitions.length - 1]\n\n        if (last != null && last.topic === topic) {\n          topicPartitions[topicPartitions.length - 1].partitions.push(partition)\n        } else {\n          topicPartitions.push({ topic, partitions: [partition] })\n        }\n\n        return topicPartitions\n      },\n      []\n    )\n\n    return await this[PRIVATE.SEND_REQUEST](\n      fetch({\n        replicaId,\n        isolationLevel,\n        maxWaitTime,\n        minBytes,\n        maxBytes,\n        topics: consolidatedTopicPartitions,\n        rackId,\n      })\n    )\n  }\n\n  /**\n   * @public\n   * @param {object} request\n   * @param {string} request.groupId The group id\n   * @param {number} request.groupGenerationId The generation of the group\n   * @param {string} request.memberId The member id assigned by the group coordinator\n   * @returns {Promise}\n   */\n  async heartbeat({ groupId, groupGenerationId, memberId }) {\n    const heartbeat = this.lookupRequest(apiKeys.Heartbeat, requests.Heartbeat)\n    return await this[PRIVATE.SEND_REQUEST](heartbeat({ groupId, groupGenerationId, memberId }))\n  }\n\n  /**\n   * @public\n   * @param {object} request\n   * @param {string} request.groupId The unique group id\n   * @param {import(\"../protocol/coordinatorTypes\").CoordinatorType} request.coordinatorType The type of coordinator to find\n   * @returns {Promise}\n   */\n  async findGroupCoordinator({ groupId, coordinatorType }) {\n    // TODO: validate groupId, mandatory\n    const findCoordinator = this.lookupRequest(apiKeys.GroupCoordinator, requests.GroupCoordinator)\n    return await this[PRIVATE.SEND_REQUEST](findCoordinator({ groupId, coordinatorType }))\n  }\n\n  /**\n   * @public\n   * @param {object} request\n   * @param {string} request.groupId The unique group id\n   * @param {number} request.sessionTimeout The coordinator considers the consumer dead if it receives\n   *                                no heartbeat after this timeout in ms\n   * @param {number} request.rebalanceTimeout The maximum time that the coordinator will wait for each member\n   *                                  to rejoin when rebalancing the group\n   * @param {string} [request.memberId=\"\"] The assigned consumer id or an empty string for a new consumer\n   * @param {string} [request.protocolType=\"consumer\"] Unique name for class of protocols implemented by group\n   * @param {Array} request.groupProtocols List of protocols that the member supports (assignment strategy)\n   *                                [{ name: 'AssignerName', metadata: '{\"version\": 1, \"topics\": []}' }]\n   * @returns {Promise}\n   */\n  async joinGroup({\n    groupId,\n    sessionTimeout,\n    rebalanceTimeout,\n    memberId = '',\n    protocolType = 'consumer',\n    groupProtocols,\n  }) {\n    const joinGroup = this.lookupRequest(apiKeys.JoinGroup, requests.JoinGroup)\n    const makeRequest = (assignedMemberId = memberId) =>\n      this[PRIVATE.SEND_REQUEST](\n        joinGroup({\n          groupId,\n          sessionTimeout,\n          rebalanceTimeout,\n          memberId: assignedMemberId,\n          protocolType,\n          groupProtocols,\n        })\n      )\n\n    try {\n      return await makeRequest()\n    } catch (error) {\n      if (error.name === 'KafkaJSMemberIdRequired') {\n        return makeRequest(error.memberId)\n      }\n\n      throw error\n    }\n  }\n\n  /**\n   * @public\n   * @param {object} request\n   * @param {string} request.groupId\n   * @param {string} request.memberId\n   * @returns {Promise}\n   */\n  async leaveGroup({ groupId, memberId }) {\n    const leaveGroup = this.lookupRequest(apiKeys.LeaveGroup, requests.LeaveGroup)\n    return await this[PRIVATE.SEND_REQUEST](leaveGroup({ groupId, memberId }))\n  }\n\n  /**\n   * @public\n   * @param {object} request\n   * @param {string} request.groupId\n   * @param {number} request.generationId\n   * @param {string} request.memberId\n   * @param {object} request.groupAssignment\n   * @returns {Promise}\n   */\n  async syncGroup({ groupId, generationId, memberId, groupAssignment }) {\n    const syncGroup = this.lookupRequest(apiKeys.SyncGroup, requests.SyncGroup)\n    return await this[PRIVATE.SEND_REQUEST](\n      syncGroup({\n        groupId,\n        generationId,\n        memberId,\n        groupAssignment,\n      })\n    )\n  }\n\n  /**\n   * @public\n   * @param {object} request\n   * @param {number} request.replicaId=-1 Broker id of the follower. For normal consumers, use -1\n   * @param {number} request.isolationLevel=1 This setting controls the visibility of transactional records (default READ_COMMITTED, Kafka >0.11 only)\n   * @param {TopicPartitionOffset[]} request.topics e.g:\n   *\n   * @typedef {Object} TopicPartitionOffset\n   * @property {string} topic\n   * @property {PartitionOffset[]} partitions\n   *\n   * @typedef {Object} PartitionOffset\n   * @property {number} partition\n   * @property {number} [timestamp=-1]\n   *\n   *\n   * @returns {Promise}\n   */\n  async listOffsets({ replicaId, isolationLevel, topics }) {\n    const listOffsets = this.lookupRequest(apiKeys.ListOffsets, requests.ListOffsets)\n    const result = await this[PRIVATE.SEND_REQUEST](\n      listOffsets({ replicaId, isolationLevel, topics })\n    )\n\n    // ListOffsets >= v1 will return a single `offset` rather than an array of `offsets` (ListOffsets V0).\n    // Normalize to just return `offset`.\n    for (const response of result.responses) {\n      response.partitions = response.partitions.map(({ offsets, ...partitionData }) => {\n        return offsets ? { ...partitionData, offset: offsets.pop() } : partitionData\n      })\n    }\n\n    return result\n  }\n\n  /**\n   * @public\n   * @param {object} request\n   * @param {string} request.groupId\n   * @param {number} request.groupGenerationId\n   * @param {string} request.memberId\n   * @param {number} [request.retentionTime=-1] -1 signals to the broker that its default configuration\n   *                                    should be used.\n   * @param {object} request.topics Topics to commit offsets, e.g:\n   *                  [\n   *                    {\n   *                      topic: 'topic-name',\n   *                      partitions: [\n   *                        { partition: 0, offset: '11' }\n   *                      ]\n   *                    }\n   *                  ]\n   * @returns {Promise}\n   */\n  async offsetCommit({ groupId, groupGenerationId, memberId, retentionTime, topics }) {\n    const offsetCommit = this.lookupRequest(apiKeys.OffsetCommit, requests.OffsetCommit)\n    return await this[PRIVATE.SEND_REQUEST](\n      offsetCommit({\n        groupId,\n        groupGenerationId,\n        memberId,\n        retentionTime,\n        topics,\n      })\n    )\n  }\n\n  /**\n   * @public\n   * @param {object} request\n   * @param {string} request.groupId\n   * @param {object} request.topics - If the topic array is null fetch offsets for all topics. e.g:\n   *                  [\n   *                    {\n   *                      topic: 'topic-name',\n   *                      partitions: [\n   *                        { partition: 0 }\n   *                      ]\n   *                    }\n   *                  ]\n   * @returns {Promise}\n   */\n  async offsetFetch({ groupId, topics }) {\n    const offsetFetch = this.lookupRequest(apiKeys.OffsetFetch, requests.OffsetFetch)\n    return await this[PRIVATE.SEND_REQUEST](offsetFetch({ groupId, topics }))\n  }\n\n  /**\n   * @public\n   * @param {object} request\n   * @param {Array} request.groupIds\n   * @returns {Promise}\n   */\n  async describeGroups({ groupIds }) {\n    const describeGroups = this.lookupRequest(apiKeys.DescribeGroups, requests.DescribeGroups)\n    return await this[PRIVATE.SEND_REQUEST](describeGroups({ groupIds }))\n  }\n\n  /**\n   * @public\n   * @param {object} request\n   * @param {Array} request.topics e.g:\n   *                 [\n   *                   {\n   *                     topic: 'topic-name',\n   *                     numPartitions: 1,\n   *                     replicationFactor: 1\n   *                   }\n   *                 ]\n   * @param {boolean} [request.validateOnly=false] If this is true, the request will be validated, but the topic\n   *                                       won't be created\n   * @param {number} [request.timeout=5000] The time in ms to wait for a topic to be completely created\n   *                                on the controller node\n   * @returns {Promise}\n   */\n  async createTopics({ topics, validateOnly = false, timeout = 5000 }) {\n    const createTopics = this.lookupRequest(apiKeys.CreateTopics, requests.CreateTopics)\n    return await this[PRIVATE.SEND_REQUEST](createTopics({ topics, validateOnly, timeout }))\n  }\n\n  /**\n   * @public\n   * @param {object} request\n   * @param {Array} request.topicPartitions e.g:\n   *                 [\n   *                   {\n   *                     topic: 'topic-name',\n   *                     count: 3,\n   *                     assignments: []\n   *                   }\n   *                 ]\n   * @param {boolean} [request.validateOnly=false] If this is true, the request will be validated, but the topic\n   *                                       won't be created\n   * @param {number} [request.timeout=5000] The time in ms to wait for a topic to be completely created\n   *                                on the controller node\n   * @returns {Promise<void>}\n   */\n  async createPartitions({ topicPartitions, validateOnly = false, timeout = 5000 }) {\n    const createPartitions = this.lookupRequest(apiKeys.CreatePartitions, requests.CreatePartitions)\n    return await this[PRIVATE.SEND_REQUEST](\n      createPartitions({ topicPartitions, validateOnly, timeout })\n    )\n  }\n\n  /**\n   * @public\n   * @param {object} request\n   * @param {string[]} request.topics An array of topics to be deleted\n   * @param {number} [request.timeout=5000] The time in ms to wait for a topic to be completely deleted on the\n   *                                controller node.\n   * @returns {Promise}\n   */\n  async deleteTopics({ topics, timeout = 5000 }) {\n    const deleteTopics = this.lookupRequest(apiKeys.DeleteTopics, requests.DeleteTopics)\n    return await this[PRIVATE.SEND_REQUEST](deleteTopics({ topics, timeout }))\n  }\n\n  /**\n   * @public\n   * @param {object} request\n   * @param {import(\"../../types\").ResourceConfigQuery[]} request.resources\n   *                                 [{\n   *                                   type: RESOURCE_TYPES.TOPIC,\n   *                                   name: 'topic-name',\n   *                                   configNames: ['compression.type', 'retention.ms']\n   *                                 }]\n   * @param {boolean} [request.includeSynonyms=false]\n   * @returns {Promise}\n   */\n  async describeConfigs({ resources, includeSynonyms = false }) {\n    const describeConfigs = this.lookupRequest(apiKeys.DescribeConfigs, requests.DescribeConfigs)\n    return await this[PRIVATE.SEND_REQUEST](describeConfigs({ resources, includeSynonyms }))\n  }\n\n  /**\n   * @public\n   * @param {object} request\n   * @param {import(\"../../types\").IResourceConfig[]} request.resources\n   *                                 [{\n   *                                  type: RESOURCE_TYPES.TOPIC,\n   *                                  name: 'topic-name',\n   *                                  configEntries: [\n   *                                    {\n   *                                      name: 'cleanup.policy',\n   *                                      value: 'compact'\n   *                                    }\n   *                                  ]\n   *                                 }]\n   * @param {boolean} [request.validateOnly=false]\n   * @returns {Promise}\n   */\n  async alterConfigs({ resources, validateOnly = false }) {\n    const alterConfigs = this.lookupRequest(apiKeys.AlterConfigs, requests.AlterConfigs)\n    return await this[PRIVATE.SEND_REQUEST](alterConfigs({ resources, validateOnly }))\n  }\n\n  /**\n   * Send an `InitProducerId` request to fetch a PID and bump the producer epoch.\n   *\n   * Request should be made to the transaction coordinator.\n   * @public\n   * @param {object} request\n   * @param {number} request.transactionTimeout The time in ms to wait for before aborting idle transactions\n   * @param {number} [request.transactionalId] The transactional id or null if the producer is not transactional\n   * @returns {Promise}\n   */\n  async initProducerId({ transactionalId, transactionTimeout }) {\n    const initProducerId = this.lookupRequest(apiKeys.InitProducerId, requests.InitProducerId)\n    return await this[PRIVATE.SEND_REQUEST](initProducerId({ transactionalId, transactionTimeout }))\n  }\n\n  /**\n   * Send an `AddPartitionsToTxn` request to mark a TopicPartition as participating in the transaction.\n   *\n   * Request should be made to the transaction coordinator.\n   * @public\n   * @param {object} request\n   * @param {string} request.transactionalId The transactional id corresponding to the transaction.\n   * @param {number} request.producerId Current producer id in use by the transactional id.\n   * @param {number} request.producerEpoch Current epoch associated with the producer id.\n   * @param {object[]} request.topics e.g:\n   *                  [\n   *                    {\n   *                      topic: 'topic-name',\n   *                      partitions: [ 0, 1]\n   *                    }\n   *                  ]\n   * @returns {Promise}\n   */\n  async addPartitionsToTxn({ transactionalId, producerId, producerEpoch, topics }) {\n    const addPartitionsToTxn = this.lookupRequest(\n      apiKeys.AddPartitionsToTxn,\n      requests.AddPartitionsToTxn\n    )\n    return await this[PRIVATE.SEND_REQUEST](\n      addPartitionsToTxn({ transactionalId, producerId, producerEpoch, topics })\n    )\n  }\n\n  /**\n   * Send an `AddOffsetsToTxn` request.\n   *\n   * Request should be made to the transaction coordinator.\n   * @public\n   * @param {object} request\n   * @param {string} request.transactionalId The transactional id corresponding to the transaction.\n   * @param {number} request.producerId Current producer id in use by the transactional id.\n   * @param {number} request.producerEpoch Current epoch associated with the producer id.\n   * @param {string} request.groupId The unique group identifier (for the consumer group)\n   * @returns {Promise}\n   */\n  async addOffsetsToTxn({ transactionalId, producerId, producerEpoch, groupId }) {\n    const addOffsetsToTxn = this.lookupRequest(apiKeys.AddOffsetsToTxn, requests.AddOffsetsToTxn)\n    return await this[PRIVATE.SEND_REQUEST](\n      addOffsetsToTxn({ transactionalId, producerId, producerEpoch, groupId })\n    )\n  }\n\n  /**\n   * Send a `TxnOffsetCommit` request to persist the offsets in the `__consumer_offsets` topics.\n   *\n   * Request should be made to the consumer coordinator.\n   * @public\n   * @param {object} request\n   * @param {OffsetCommitTopic[]} request.topics\n   * @param {string} request.transactionalId The transactional id corresponding to the transaction.\n   * @param {string} request.groupId The unique group identifier (for the consumer group)\n   * @param {number} request.producerId Current producer id in use by the transactional id.\n   * @param {number} request.producerEpoch Current epoch associated with the producer id.\n   * @param {OffsetCommitTopic[]} request.topics\n   *\n   * @typedef {Object} OffsetCommitTopic\n   * @property {string} topic\n   * @property {OffsetCommitTopicPartition[]} partitions\n   *\n   * @typedef {Object} OffsetCommitTopicPartition\n   * @property {number} partition\n   * @property {number} offset\n   * @property {string} [metadata]\n   *\n   * @returns {Promise}\n   */\n  async txnOffsetCommit({ transactionalId, groupId, producerId, producerEpoch, topics }) {\n    const txnOffsetCommit = this.lookupRequest(apiKeys.TxnOffsetCommit, requests.TxnOffsetCommit)\n    return await this[PRIVATE.SEND_REQUEST](\n      txnOffsetCommit({ transactionalId, groupId, producerId, producerEpoch, topics })\n    )\n  }\n\n  /**\n   * Send an `EndTxn` request to indicate transaction should be committed or aborted.\n   *\n   * Request should be made to the transaction coordinator.\n   * @public\n   * @param {object} request\n   * @param {string} request.transactionalId The transactional id corresponding to the transaction.\n   * @param {number} request.producerId Current producer id in use by the transactional id.\n   * @param {number} request.producerEpoch Current epoch associated with the producer id.\n   * @param {boolean} request.transactionResult The result of the transaction (false = ABORT, true = COMMIT)\n   * @returns {Promise}\n   */\n  async endTxn({ transactionalId, producerId, producerEpoch, transactionResult }) {\n    const endTxn = this.lookupRequest(apiKeys.EndTxn, requests.EndTxn)\n    return await this[PRIVATE.SEND_REQUEST](\n      endTxn({ transactionalId, producerId, producerEpoch, transactionResult })\n    )\n  }\n\n  /**\n   * Send request for list of groups\n   * @public\n   * @returns {Promise}\n   */\n  async listGroups() {\n    const listGroups = this.lookupRequest(apiKeys.ListGroups, requests.ListGroups)\n    return await this[PRIVATE.SEND_REQUEST](listGroups())\n  }\n\n  /**\n   * Send request to delete groups\n   * @param {string[]} groupIds\n   * @public\n   * @returns {Promise}\n   */\n  async deleteGroups(groupIds) {\n    const deleteGroups = this.lookupRequest(apiKeys.DeleteGroups, requests.DeleteGroups)\n    return await this[PRIVATE.SEND_REQUEST](deleteGroups(groupIds))\n  }\n\n  /**\n   * Send request to delete records\n   * @public\n   * @param {object} request\n   * @param {TopicPartitionRecords[]} request.topics\n   *                          [\n   *                            {\n   *                              topic: 'my-topic-name',\n   *                              partitions: [\n   *                                { partition: 0, offset 2 },\n   *                                { partition: 1, offset 4 },\n   *                              ],\n   *                            }\n   *                          ]\n   * @returns {Promise<Array>} example:\n   *                          {\n   *                            throttleTime: 0\n   *                           [\n   *                              {\n   *                                topic: 'my-topic-name',\n   *                                partitions: [\n   *                                 { partition: 0, lowWatermark: '2n', errorCode: 0 },\n   *                                 { partition: 1, lowWatermark: '4n', errorCode: 0 },\n   *                               ],\n   *                             },\n   *                           ]\n   *                          }\n   *\n   * @typedef {object} TopicPartitionRecords\n   * @property {string} topic\n   * @property {PartitionRecord[]} partitions\n   *\n   * @typedef {object} PartitionRecord\n   * @property {number} partition\n   * @property {number} offset\n   */\n  async deleteRecords({ topics }) {\n    const deleteRecords = this.lookupRequest(apiKeys.DeleteRecords, requests.DeleteRecords)\n    return await this[PRIVATE.SEND_REQUEST](deleteRecords({ topics }))\n  }\n\n  /**\n   * @public\n   * @param {object} request\n   * @param {import(\"../../types\").AclEntry[]} request.acl e.g:\n   *                 [\n   *                   {\n   *                     resourceType: AclResourceTypes.TOPIC,\n   *                     resourceName: 'topic-name',\n   *                     resourcePatternType: ResourcePatternTypes.LITERAL,\n   *                     principal: 'User:bob',\n   *                     host: '*',\n   *                     operation: AclOperationTypes.ALL,\n   *                     permissionType: AclPermissionTypes.DENY,\n   *                   }\n   *                 ]\n   * @returns {Promise<void>}\n   */\n  async createAcls({ acl }) {\n    const createAcls = this.lookupRequest(apiKeys.CreateAcls, requests.CreateAcls)\n    return await this[PRIVATE.SEND_REQUEST](createAcls({ creations: acl }))\n  }\n\n  /**\n   * @public\n   * @param {import(\"../../types\").AclEntry} aclEntry\n   * @returns {Promise<void>}\n   */\n  async describeAcls({\n    resourceType,\n    resourceName,\n    resourcePatternType,\n    principal,\n    host,\n    operation,\n    permissionType,\n  }) {\n    const describeAcls = this.lookupRequest(apiKeys.DescribeAcls, requests.DescribeAcls)\n    return await this[PRIVATE.SEND_REQUEST](\n      describeAcls({\n        resourceType,\n        resourceName,\n        resourcePatternType,\n        principal,\n        host,\n        operation,\n        permissionType,\n      })\n    )\n  }\n\n  /**\n   * @public\n   * @param {Object} request\n   * @param {import(\"../../types\").AclEntry[]} request.filters\n   * @returns {Promise<void>}\n   */\n  async deleteAcls({ filters }) {\n    const deleteAcls = this.lookupRequest(apiKeys.DeleteAcls, requests.DeleteAcls)\n    return await this[PRIVATE.SEND_REQUEST](deleteAcls({ filters }))\n  }\n\n  /**\n   * @public\n   * @param {Object} request\n   * @param {import(\"../../types\").PartitionReassignment[]} request.topics\n   * @param {number} [request.timeout]\n   * @returns {Promise}\n   */\n  async alterPartitionReassignments({ topics, timeout }) {\n    const alterPartitionReassignments = this.lookupRequest(\n      apiKeys.AlterPartitionReassignments,\n      requests.AlterPartitionReassignments\n    )\n    return await this[PRIVATE.SEND_REQUEST](alterPartitionReassignments({ topics, timeout }))\n  }\n\n  /**\n   * @public\n   * @param {Object} request\n   * @param {import(\"../../types\").TopicPartitions[]} request.topics can be null\n   * @param {number} [request.timeout]\n   * @returns {Promise}\n   */\n  async listPartitionReassignments({ topics = null, timeout }) {\n    const listPartitionReassignments = this.lookupRequest(\n      apiKeys.ListPartitionReassignments,\n      requests.ListPartitionReassignments\n    )\n    return await this[PRIVATE.SEND_REQUEST](listPartitionReassignments({ topics, timeout }))\n  }\n\n  /**\n   * @private\n   */\n  async [PRIVATE.SEND_REQUEST](protocolRequest) {\n    try {\n      return await this.connectionPool.send(protocolRequest)\n    } catch (e) {\n      if (e.name === 'KafkaJSConnectionClosedError') {\n        await this.disconnect()\n      }\n\n      throw e\n    }\n  }\n}\n", "module.exports = {\n  maxRetryTime: 1000,\n  initialRetryTime: 50,\n  factor: 0.02, // randomization factor\n  multiplier: 1.5, // exponential factor\n  retries: 15, // max retries\n}\n", "module.exports = {\n  maxRetryTime: 30 * 1000,\n  initialRetryTime: 300,\n  factor: 0.2, // randomization factor\n  multiplier: 2, // exponential factor\n  retries: 5, // max retries\n}\n", "const { KafkaJSNumberOfRetriesExceeded, KafkaJSNonRetriableError } = require('../errors')\n\nconst isTestMode = process.env.NODE_ENV === 'test'\nconst RETRY_DEFAULT = isTestMode ? require('./defaults.test') : require('./defaults')\n\nconst random = (min, max) => {\n  return Math.random() * (max - min) + min\n}\n\nconst randomFromRetryTime = (factor, retryTime) => {\n  const delta = factor * retryTime\n  return Math.ceil(random(retryTime - delta, retryTime + delta))\n}\n\nconst UNRECOVERABLE_ERRORS = ['RangeError', 'ReferenceError', 'SyntaxError', 'TypeError']\nconst isErrorUnrecoverable = e => UNRECOVERABLE_ERRORS.includes(e.name)\nconst isErrorRetriable = error =>\n  (error.retriable || error.retriable !== false) && !isErrorUnrecoverable(error)\n\nconst createRetriable = (configs, resolve, reject, fn) => {\n  let aborted = false\n  const { factor, multiplier, maxRetryTime, retries } = configs\n\n  const bail = error => {\n    aborted = true\n    reject(error || new Error('Aborted'))\n  }\n\n  const calculateExponentialRetryTime = retryTime => {\n    return Math.min(randomFromRetryTime(factor, retryTime) * multiplier, maxRetryTime)\n  }\n\n  const retry = (retryTime, retryCount = 0) => {\n    if (aborted) return\n\n    const nextRetryTime = calculateExponentialRetryTime(retryTime)\n    const shouldRetry = retryCount < retries\n\n    const scheduleRetry = () => {\n      setTimeout(() => retry(nextRetryTime, retryCount + 1), retryTime)\n    }\n\n    fn(bail, retryCount, retryTime)\n      .then(resolve)\n      .catch(e => {\n        if (isErrorRetriable(e)) {\n          if (shouldRetry) {\n            scheduleRetry()\n          } else {\n            reject(\n              new KafkaJSNumberOfRetriesExceeded(e, { retryCount, retryTime, cause: e.cause || e })\n            )\n          }\n        } else {\n          reject(new KafkaJSNonRetriableError(e, { cause: e.cause || e }))\n        }\n      })\n  }\n\n  return retry\n}\n\n/**\n * @typedef {(fn: (bail: (err: Error) => void, retryCount: number, retryTime: number) => any) => Promise<ReturnType<fn>>} Retrier\n */\n\n/**\n * @param {import(\"../../types\").RetryOptions} [opts]\n * @returns {Retrier}\n */\nmodule.exports = (opts = {}) => fn => {\n  return new Promise((resolve, reject) => {\n    const configs = Object.assign({}, RETRY_DEFAULT, opts)\n    const start = createRetriable(configs, resolve, reject, fn)\n    start(randomFromRetryTime(configs.factor, configs.initialRetryTime))\n  })\n}\n", "module.exports = (a, b) => {\n  const result = []\n  const length = a.length\n  let i = 0\n\n  while (i < length) {\n    if (b.indexOf(a[i]) === -1) {\n      result.push(a[i])\n    }\n    i += 1\n  }\n\n  return result\n}\n", "const Broker = require('../broker')\nconst createRetry = require('../retry')\nconst shuffle = require('../utils/shuffle')\nconst arrayDiff = require('../utils/arrayDiff')\nconst { KafkaJSBrokerNotFound, KafkaJSProtocolError } = require('../errors')\n\nconst { keys, assign, values } = Object\nconst hasBrokerBeenReplaced = (broker, { host, port, rack }) =>\n  broker.connectionPool.host !== host ||\n  broker.connectionPool.port !== port ||\n  broker.connectionPool.rack !== rack\n\nmodule.exports = class BrokerPool {\n  /**\n   * @param {object} options\n   * @param {import(\"./connectionPoolBuilder\").ConnectionPoolBuilder} options.connectionPoolBuilder\n   * @param {import(\"../../types\").Logger} options.logger\n   * @param {import(\"../../types\").RetryOptions} [options.retry]\n   * @param {boolean} [options.allowAutoTopicCreation]\n   * @param {number} [options.authenticationTimeout]\n   * @param {number} [options.metadataMaxAge]\n   */\n  constructor({\n    connectionPoolBuilder,\n    logger,\n    retry,\n    allowAutoTopicCreation,\n    authenticationTimeout,\n    metadataMaxAge,\n  }) {\n    this.rootLogger = logger\n    this.connectionPoolBuilder = connectionPoolBuilder\n    this.metadataMaxAge = metadataMaxAge || 0\n    this.logger = logger.namespace('BrokerPool')\n    this.retrier = createRetry(assign({}, retry))\n\n    this.createBroker = options =>\n      new Broker({\n        allowAutoTopicCreation,\n        authenticationTimeout,\n        ...options,\n      })\n\n    this.brokers = {}\n    /** @type {Broker | undefined} */\n    this.seedBroker = undefined\n    /** @type {import(\"../../types\").BrokerMetadata | null} */\n    this.metadata = null\n    this.metadataExpireAt = null\n    this.versions = null\n  }\n\n  /**\n   * @public\n   * @returns {Boolean}\n   */\n  hasConnectedBrokers() {\n    const brokers = values(this.brokers)\n    return (\n      !!brokers.find(broker => broker.isConnected()) ||\n      (this.seedBroker ? this.seedBroker.isConnected() : false)\n    )\n  }\n\n  async createSeedBroker() {\n    if (this.seedBroker) {\n      await this.seedBroker.disconnect()\n    }\n\n    const connectionPool = await this.connectionPoolBuilder.build()\n\n    this.seedBroker = this.createBroker({\n      connectionPool,\n      logger: this.rootLogger,\n    })\n  }\n\n  /**\n   * @public\n   * @returns {Promise<void>}\n   */\n  async connect() {\n    if (this.hasConnectedBrokers()) {\n      return\n    }\n\n    if (!this.seedBroker) {\n      await this.createSeedBroker()\n    }\n\n    return this.retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await this.seedBroker.connect()\n        this.versions = this.seedBroker.versions\n      } catch (e) {\n        if (e.name === 'KafkaJSConnectionError' || e.type === 'ILLEGAL_SASL_STATE') {\n          // Connection builder will always rotate the seed broker\n          await this.createSeedBroker()\n          this.logger.error(\n            `Failed to connect to seed broker, trying another broker from the list: ${e.message}`,\n            { retryCount, retryTime }\n          )\n        } else {\n          this.logger.error(e.message, { retryCount, retryTime })\n        }\n\n        if (e.retriable) throw e\n        bail(e)\n      }\n    })\n  }\n\n  /**\n   * @public\n   * @returns {Promise}\n   */\n  async disconnect() {\n    this.seedBroker && (await this.seedBroker.disconnect())\n    await Promise.all(values(this.brokers).map(broker => broker.disconnect()))\n\n    this.brokers = {}\n    this.metadata = null\n    this.versions = null\n  }\n\n  /**\n   * @public\n   * @param {Object} destination\n   * @param {string} destination.host\n   * @param {number} destination.port\n   */\n  removeBroker({ host, port }) {\n    const removedBroker = values(this.brokers).find(\n      broker => broker.connectionPool.host === host && broker.connectionPool.port === port\n    )\n\n    if (removedBroker) {\n      delete this.brokers[removedBroker.nodeId]\n      this.metadataExpireAt = null\n\n      if (this.seedBroker.nodeId === removedBroker.nodeId) {\n        this.seedBroker = shuffle(values(this.brokers))[0]\n      }\n    }\n  }\n\n  /**\n   * @public\n   * @param {Array<String>} topics\n   * @returns {Promise<null>}\n   */\n  async refreshMetadata(topics) {\n    const broker = await this.findConnectedBroker()\n    const { host: seedHost, port: seedPort } = this.seedBroker.connectionPool\n\n    return this.retrier(async (bail, retryCount, retryTime) => {\n      try {\n        this.metadata = await broker.metadata(topics)\n        this.metadataExpireAt = Date.now() + this.metadataMaxAge\n\n        const replacedBrokers = []\n\n        this.brokers = await this.metadata.brokers.reduce(\n          async (resultPromise, { nodeId, host, port, rack }) => {\n            const result = await resultPromise\n\n            if (result[nodeId]) {\n              if (!hasBrokerBeenReplaced(result[nodeId], { host, port, rack })) {\n                return result\n              }\n\n              replacedBrokers.push(result[nodeId])\n            }\n\n            if (host === seedHost && port === seedPort) {\n              this.seedBroker.nodeId = nodeId\n              this.seedBroker.connectionPool.rack = rack\n              return assign(result, {\n                [nodeId]: this.seedBroker,\n              })\n            }\n\n            return assign(result, {\n              [nodeId]: this.createBroker({\n                logger: this.rootLogger,\n                versions: this.versions,\n                connectionPool: await this.connectionPoolBuilder.build({ host, port, rack }),\n                nodeId,\n              }),\n            })\n          },\n          this.brokers\n        )\n\n        const freshBrokerIds = this.metadata.brokers.map(({ nodeId }) => `${nodeId}`).sort()\n        const currentBrokerIds = keys(this.brokers).sort()\n        const unusedBrokerIds = arrayDiff(currentBrokerIds, freshBrokerIds)\n\n        const brokerDisconnects = unusedBrokerIds.map(nodeId => {\n          const broker = this.brokers[nodeId]\n          return broker.disconnect().then(() => {\n            delete this.brokers[nodeId]\n          })\n        })\n\n        const replacedBrokersDisconnects = replacedBrokers.map(broker => broker.disconnect())\n        await Promise.all([...brokerDisconnects, ...replacedBrokersDisconnects])\n      } catch (e) {\n        if (e.type === 'LEADER_NOT_AVAILABLE') {\n          throw e\n        }\n\n        bail(e)\n      }\n    })\n  }\n\n  /**\n   * Only refreshes metadata if the data is stale according to the `metadataMaxAge` param or does not contain information about the provided topics\n   *\n   * @public\n   * @param {Array<String>} topics\n   * @returns {Promise<null>}\n   */\n  async refreshMetadataIfNecessary(topics) {\n    const shouldRefresh =\n      this.metadata == null ||\n      this.metadataExpireAt == null ||\n      Date.now() > this.metadataExpireAt ||\n      !topics.every(topic =>\n        this.metadata.topicMetadata.some(topicMetadata => topicMetadata.topic === topic)\n      )\n\n    if (shouldRefresh) {\n      return this.refreshMetadata(topics)\n    }\n  }\n\n  /** @type {() => string[]} */\n  getNodeIds() {\n    return keys(this.brokers)\n  }\n\n  /**\n   * @public\n   * @param {object} options\n   * @param {string} options.nodeId\n   * @returns {Promise<Broker>}\n   */\n  async findBroker({ nodeId }) {\n    const broker = this.brokers[nodeId]\n\n    if (!broker) {\n      throw new KafkaJSBrokerNotFound(`Broker ${nodeId} not found in the cached metadata`)\n    }\n\n    await this.connectBroker(broker)\n    return broker\n  }\n\n  /**\n   * @public\n   * @param {(params: { nodeId: string, broker: Broker }) => Promise<T>} callback\n   * @returns {Promise<T>}\n   * @template T\n   */\n  async withBroker(callback) {\n    const brokers = shuffle(keys(this.brokers))\n    if (brokers.length === 0) {\n      throw new KafkaJSBrokerNotFound('No brokers in the broker pool')\n    }\n\n    for (const nodeId of brokers) {\n      const broker = await this.findBroker({ nodeId })\n      try {\n        return await callback({ nodeId, broker })\n      } catch (e) {}\n    }\n\n    return null\n  }\n\n  /**\n   * @public\n   * @returns {Promise<Broker>}\n   */\n  async findConnectedBroker() {\n    const nodeIds = shuffle(keys(this.brokers))\n    const connectedBrokerId = nodeIds.find(nodeId => this.brokers[nodeId].isConnected())\n\n    if (connectedBrokerId) {\n      return await this.findBroker({ nodeId: connectedBrokerId })\n    }\n\n    // Cycle through the nodes until one connects\n    for (const nodeId of nodeIds) {\n      try {\n        return await this.findBroker({ nodeId })\n      } catch (e) {}\n    }\n\n    // Failed to connect to all known brokers, metadata might be old\n    await this.connect()\n    return this.seedBroker\n  }\n\n  /**\n   * @private\n   * @param {Broker} broker\n   * @returns {Promise<null>}\n   */\n  async connectBroker(broker) {\n    if (broker.isConnected()) {\n      return\n    }\n\n    return this.retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await broker.connect()\n      } catch (e) {\n        if (e.name === 'KafkaJSConnectionError' || e.type === 'ILLEGAL_SASL_STATE') {\n          await broker.disconnect()\n        }\n\n        // To avoid reconnecting to an unavailable host, we bail on connection errors\n        // and refresh metadata on a higher level before reconnecting\n        if (e.name === 'KafkaJSConnectionError') {\n          return bail(e)\n        }\n\n        if (e.type === 'ILLEGAL_SASL_STATE') {\n          // Rebuild the connection pool since it can't recover from illegal SASL state\n          broker.connectionPool = await this.connectionPoolBuilder.build({\n            host: broker.connectionPool.host,\n            port: broker.connectionPool.port,\n            rack: broker.connectionPool.rack,\n          })\n\n          this.logger.error(`Failed to connect to broker, reconnecting`, { retryCount, retryTime })\n          throw new KafkaJSProtocolError(e, { retriable: true })\n        }\n\n        if (e.retriable) throw e\n        this.logger.error(e, { retryCount, retryTime, stack: e.stack })\n        bail(e)\n      }\n    })\n  }\n}\n", "/**\n * @template T\n * @param { (...args: any) => Promise<T> } [asyncFunction]\n * Promise returning function that will only ever be invoked sequentially.\n * @returns { (...args: any) => Promise<T> }\n * Function that may invoke asyncFunction if there is not a currently executing invocation.\n * Returns promise from the currently executing invocation.\n */\nmodule.exports = asyncFunction => {\n  let promise = null\n\n  return (...args) => {\n    if (promise == null) {\n      promise = asyncFunction(...args).finally(() => (promise = null))\n    }\n    return promise\n  }\n}\n", "/**\n * @param {Object} options\n * @param {import(\"../../types\").ISocketFactory} options.socketFactory\n * @param {string} options.host\n * @param {number} options.port\n * @param {Object} options.ssl\n * @param {() => void} options.onConnect\n * @param {(data: Buffer) => void} options.onData\n * @param {() => void} options.onEnd\n * @param {(err: Error) => void} options.onError\n * @param {() => void} options.onTimeout\n */\nmodule.exports = ({\n  socketFactory,\n  host,\n  port,\n  ssl,\n  onConnect,\n  onData,\n  onEnd,\n  onError,\n  onTimeout,\n}) => {\n  const socket = socketFactory({ host, port, ssl, onConnect })\n\n  socket.on('data', onData)\n  socket.on('end', onEnd)\n  socket.on('error', onError)\n  socket.on('timeout', onTimeout)\n\n  return socket\n}\n", "const Encoder = require('./encoder')\n\nmodule.exports = async ({ correlationId, clientId, request: { apiKey, apiVersion, encode } }) => {\n  const payload = await encode()\n  const requestPayload = new Encoder()\n    .writeInt16(apiKey)\n    .writeInt16(apiVersion)\n    .writeInt32(correlationId)\n    .writeString(clientId)\n    .writeEncoder(payload)\n\n  return new Encoder().writeInt32(requestPayload.size()).writeEncoder(requestPayload)\n}\n", "const EARLIEST_OFFSET = -2\nconst LATEST_OFFSET = -1\nconst INT_32_MAX_VALUE = Math.pow(2, 31) - 1\n\nmodule.exports = {\n  EARLIEST_OFFSET,\n  LATEST_OFFSET,\n  INT_32_MAX_VALUE,\n}\n", "module.exports = () => ({\n  KAFKAJS_DEBUG_PROTOCOL_BUFFERS: process.env.KAFKAJS_DEBUG_PROTOCOL_BUFFERS,\n  KAFKAJS_DEBUG_EXTENDED_PROTOCOL_BUFFERS: process.env.KAFKAJS_DEBUG_EXTENDED_PROTOCOL_BUFFERS,\n})\n", "/** @type {<T1 extends string>(namespace: T1) => <T2 extends string>(type: T2) => `${T1}.${T2}`} */\nmodule.exports = namespace => type => `${namespace}.${type}`\n", "const InstrumentationEventType = require('../instrumentation/eventType')\nconst eventType = InstrumentationEventType('network')\n\nmodule.exports = {\n  NETWORK_REQUEST: eventType('request'),\n  NETWORK_REQUEST_TIMEOUT: eventType('request_timeout'),\n  NETWORK_REQUEST_QUEUE_SIZE: eventType('request_queue_size'),\n}\n", "const { KafkaJSRequestTimeoutError, KafkaJSNonRetriableError } = require('../../errors')\nconst events = require('../instrumentationEvents')\n\nconst PRIVATE = {\n  STATE: Symbol('private:SocketRequest:state'),\n  EMIT_EVENT: Symbol('private:SocketRequest:emitEvent'),\n}\n\nconst REQUEST_STATE = {\n  PENDING: Symbol('PENDING'),\n  SENT: Symbol('SENT'),\n  COMPLETED: Symbol('COMPLETED'),\n  REJECTED: Symbol('REJECTED'),\n}\n\n/**\n * SocketRequest abstracts the life cycle of a socket request, making it easier to track\n * request durations and to have individual timeouts per request.\n *\n * @typedef {Object} SocketRequest\n * @property {number} createdAt\n * @property {number} sentAt\n * @property {number} pendingDuration\n * @property {number} duration\n * @property {number} requestTimeout\n * @property {string} broker\n * @property {string} clientId\n * @property {RequestEntry} entry\n * @property {boolean} expectResponse\n * @property {Function} send\n * @property {Function} timeout\n *\n * @typedef {Object} RequestEntry\n * @property {string} apiKey\n * @property {string} apiName\n * @property {number} apiVersion\n * @property {number} correlationId\n * @property {Function} resolve\n * @property {Function} reject\n */\nmodule.exports = class SocketRequest {\n  /**\n   * @param {Object} options\n   * @param {number} options.requestTimeout\n   * @param {string} options.broker - e.g: 127.0.0.1:9092\n   * @param {string} options.clientId\n   * @param {RequestEntry} options.entry\n   * @param {boolean} options.expectResponse\n   * @param {Function} options.send\n   * @param {() => void} options.timeout\n   * @param {import(\"../../instrumentation/emitter\")} [options.instrumentationEmitter=null]\n   */\n  constructor({\n    requestTimeout,\n    broker,\n    clientId,\n    entry,\n    expectResponse,\n    send,\n    timeout,\n    instrumentationEmitter = null,\n  }) {\n    this.createdAt = Date.now()\n    this.requestTimeout = requestTimeout\n    this.broker = broker\n    this.clientId = clientId\n    this.entry = entry\n    this.correlationId = entry.correlationId\n    this.expectResponse = expectResponse\n    this.sendRequest = send\n    this.timeoutHandler = timeout\n\n    this.sentAt = null\n    this.duration = null\n    this.pendingDuration = null\n\n    this[PRIVATE.STATE] = REQUEST_STATE.PENDING\n    this[PRIVATE.EMIT_EVENT] = (eventName, payload) =>\n      instrumentationEmitter && instrumentationEmitter.emit(eventName, payload)\n  }\n\n  send() {\n    this.throwIfInvalidState({\n      accepted: [REQUEST_STATE.PENDING],\n      next: REQUEST_STATE.SENT,\n    })\n\n    this.sendRequest()\n    this.sentAt = Date.now()\n    this.pendingDuration = this.sentAt - this.createdAt\n    this[PRIVATE.STATE] = REQUEST_STATE.SENT\n  }\n\n  timeoutRequest() {\n    const { apiName, apiKey, apiVersion } = this.entry\n    const requestInfo = `${apiName}(key: ${apiKey}, version: ${apiVersion})`\n    const eventData = {\n      broker: this.broker,\n      clientId: this.clientId,\n      correlationId: this.correlationId,\n      createdAt: this.createdAt,\n      sentAt: this.sentAt,\n      pendingDuration: this.pendingDuration,\n    }\n\n    this.timeoutHandler()\n    this.rejected(new KafkaJSRequestTimeoutError(`Request ${requestInfo} timed out`, eventData))\n    this[PRIVATE.EMIT_EVENT](events.NETWORK_REQUEST_TIMEOUT, {\n      ...eventData,\n      apiName,\n      apiKey,\n      apiVersion,\n    })\n  }\n\n  completed({ size, payload }) {\n    this.throwIfInvalidState({\n      accepted: [REQUEST_STATE.SENT],\n      next: REQUEST_STATE.COMPLETED,\n    })\n\n    const { entry, correlationId, broker, clientId, createdAt, sentAt, pendingDuration } = this\n\n    this[PRIVATE.STATE] = REQUEST_STATE.COMPLETED\n    this.duration = Date.now() - this.sentAt\n    entry.resolve({ correlationId, entry, size, payload })\n\n    this[PRIVATE.EMIT_EVENT](events.NETWORK_REQUEST, {\n      broker,\n      clientId,\n      correlationId,\n      size,\n      createdAt,\n      sentAt,\n      pendingDuration,\n      duration: this.duration,\n      apiName: entry.apiName,\n      apiKey: entry.apiKey,\n      apiVersion: entry.apiVersion,\n    })\n  }\n\n  rejected(error) {\n    this.throwIfInvalidState({\n      accepted: [REQUEST_STATE.PENDING, REQUEST_STATE.SENT],\n      next: REQUEST_STATE.REJECTED,\n    })\n\n    this[PRIVATE.STATE] = REQUEST_STATE.REJECTED\n    this.duration = Date.now() - this.sentAt\n    this.entry.reject(error)\n  }\n\n  /**\n   * @private\n   */\n  throwIfInvalidState({ accepted, next }) {\n    if (accepted.includes(this[PRIVATE.STATE])) {\n      return\n    }\n\n    const current = this[PRIVATE.STATE].toString()\n\n    throw new KafkaJSNonRetriableError(\n      `Invalid state, can't transition from ${current} to ${next.toString()}`\n    )\n  }\n}\n", "const { EventEmitter } = require('events')\nconst SocketRequest = require('./socketRequest')\nconst events = require('../instrumentationEvents')\nconst { KafkaJSInvariantViolation } = require('../../errors')\n\nconst PRIVATE = {\n  EMIT_QUEUE_SIZE_EVENT: Symbol('private:RequestQueue:emitQueueSizeEvent'),\n  EMIT_REQUEST_QUEUE_EMPTY: Symbol('private:RequestQueue:emitQueueEmpty'),\n}\n\nconst REQUEST_QUEUE_EMPTY = 'requestQueueEmpty'\nconst CHECK_PENDING_REQUESTS_INTERVAL = 10\n\nmodule.exports = class RequestQueue extends EventEmitter {\n  /**\n   * @param {Object} options\n   * @param {number} options.maxInFlightRequests\n   * @param {number} options.requestTimeout\n   * @param {boolean} options.enforceRequestTimeout\n   * @param {string} options.clientId\n   * @param {string} options.broker\n   * @param {import(\"../../../types\").Logger} options.logger\n   * @param {import(\"../../instrumentation/emitter\")} [options.instrumentationEmitter=null]\n   * @param {() => boolean} [options.isConnected]\n   */\n  constructor({\n    instrumentationEmitter = null,\n    maxInFlightRequests,\n    requestTimeout,\n    enforceRequestTimeout,\n    clientId,\n    broker,\n    logger,\n    isConnected = () => true,\n  }) {\n    super()\n    this.instrumentationEmitter = instrumentationEmitter\n    this.maxInFlightRequests = maxInFlightRequests\n    this.requestTimeout = requestTimeout\n    this.enforceRequestTimeout = enforceRequestTimeout\n    this.clientId = clientId\n    this.broker = broker\n    this.logger = logger\n    this.isConnected = isConnected\n\n    this.inflight = new Map()\n    this.pending = []\n\n    /**\n     * Until when this request queue is throttled and shouldn't send requests\n     *\n     * The value represents the timestamp of the end of the throttling in ms-since-epoch. If the value\n     * is smaller than the current timestamp no throttling is active.\n     *\n     * @type {number}\n     */\n    this.throttledUntil = -1\n\n    /**\n     * Timeout id if we have scheduled a check for pending requests due to client-side throttling\n     *\n     * @type {null|NodeJS.Timeout}\n     */\n    this.throttleCheckTimeoutId = null\n\n    this[PRIVATE.EMIT_REQUEST_QUEUE_EMPTY] = () => {\n      if (this.pending.length === 0 && this.inflight.size === 0) {\n        this.emit(REQUEST_QUEUE_EMPTY)\n      }\n    }\n\n    this[PRIVATE.EMIT_QUEUE_SIZE_EVENT] = () => {\n      instrumentationEmitter &&\n        instrumentationEmitter.emit(events.NETWORK_REQUEST_QUEUE_SIZE, {\n          broker: this.broker,\n          clientId: this.clientId,\n          queueSize: this.pending.length,\n        })\n\n      this[PRIVATE.EMIT_REQUEST_QUEUE_EMPTY]()\n    }\n  }\n\n  /**\n   * @public\n   */\n  scheduleRequestTimeoutCheck() {\n    if (this.enforceRequestTimeout) {\n      this.destroy()\n\n      this.requestTimeoutIntervalId = setInterval(() => {\n        this.inflight.forEach(request => {\n          if (Date.now() - request.sentAt > request.requestTimeout) {\n            request.timeoutRequest()\n          }\n        })\n\n        if (!this.isConnected()) {\n          this.destroy()\n        }\n      }, Math.min(this.requestTimeout, 100))\n    }\n  }\n\n  maybeThrottle(clientSideThrottleTime) {\n    if (clientSideThrottleTime !== null && clientSideThrottleTime > 0) {\n      this.logger.debug(`Client side throttling in effect for ${clientSideThrottleTime}ms`)\n      const minimumThrottledUntil = Date.now() + clientSideThrottleTime\n      this.throttledUntil = Math.max(minimumThrottledUntil, this.throttledUntil)\n    }\n  }\n\n  createSocketRequest(pushedRequest) {\n    const { correlationId } = pushedRequest.entry\n    const defaultRequestTimeout = this.requestTimeout\n    const customRequestTimeout = pushedRequest.requestTimeout\n\n    // Some protocol requests have custom request timeouts (e.g JoinGroup, Fetch, etc). The custom\n    // timeouts are influenced by user configurations, which can be lower than the default requestTimeout\n    const requestTimeout = Math.max(defaultRequestTimeout, customRequestTimeout || 0)\n\n    const socketRequest = new SocketRequest({\n      entry: pushedRequest.entry,\n      expectResponse: pushedRequest.expectResponse,\n      broker: this.broker,\n      clientId: this.clientId,\n      instrumentationEmitter: this.instrumentationEmitter,\n      requestTimeout,\n      send: () => {\n        if (this.inflight.has(correlationId)) {\n          throw new KafkaJSInvariantViolation('Correlation id already exists')\n        }\n        this.inflight.set(correlationId, socketRequest)\n        pushedRequest.sendRequest()\n      },\n      timeout: () => {\n        this.inflight.delete(correlationId)\n        this.checkPendingRequests()\n        // Try to emit REQUEST_QUEUE_EMPTY. Otherwise, waitForPendingRequests may stuck forever\n        this[PRIVATE.EMIT_REQUEST_QUEUE_EMPTY]()\n      },\n    })\n\n    return socketRequest\n  }\n\n  /**\n   * @typedef {Object} PushedRequest\n   * @property {import(\"./socketRequest\").RequestEntry} entry\n   * @property {boolean} expectResponse\n   * @property {Function} sendRequest\n   * @property {number} [requestTimeout]\n   *\n   * @public\n   * @param {PushedRequest} pushedRequest\n   */\n  push(pushedRequest) {\n    const { correlationId } = pushedRequest.entry\n    const socketRequest = this.createSocketRequest(pushedRequest)\n\n    if (this.canSendSocketRequestImmediately()) {\n      this.sendSocketRequest(socketRequest)\n      return\n    }\n\n    this.pending.push(socketRequest)\n    this.scheduleCheckPendingRequests()\n\n    this.logger.debug(`Request enqueued`, {\n      clientId: this.clientId,\n      broker: this.broker,\n      correlationId,\n    })\n\n    this[PRIVATE.EMIT_QUEUE_SIZE_EVENT]()\n  }\n\n  /**\n   * @param {SocketRequest} socketRequest\n   */\n  sendSocketRequest(socketRequest) {\n    socketRequest.send()\n\n    if (!socketRequest.expectResponse) {\n      this.logger.debug(`Request does not expect a response, resolving immediately`, {\n        clientId: this.clientId,\n        broker: this.broker,\n        correlationId: socketRequest.correlationId,\n      })\n\n      this.inflight.delete(socketRequest.correlationId)\n      socketRequest.completed({ size: 0, payload: null })\n    }\n  }\n\n  /**\n   * @public\n   * @param {object} response\n   * @param {number} response.correlationId\n   * @param {Buffer} response.payload\n   * @param {number} response.size\n   */\n  fulfillRequest({ correlationId, payload, size }) {\n    const socketRequest = this.inflight.get(correlationId)\n    this.inflight.delete(correlationId)\n    this.checkPendingRequests()\n\n    if (socketRequest) {\n      socketRequest.completed({ size, payload })\n    } else {\n      this.logger.warn(`Response without match`, {\n        clientId: this.clientId,\n        broker: this.broker,\n        correlationId,\n      })\n    }\n\n    this[PRIVATE.EMIT_REQUEST_QUEUE_EMPTY]()\n  }\n\n  /**\n   * @public\n   * @param {Error} error\n   */\n  rejectAll(error) {\n    const requests = [...this.inflight.values(), ...this.pending]\n\n    for (const socketRequest of requests) {\n      socketRequest.rejected(error)\n      this.inflight.delete(socketRequest.correlationId)\n    }\n\n    this.pending = []\n    this.inflight.clear()\n    this[PRIVATE.EMIT_QUEUE_SIZE_EVENT]()\n  }\n\n  /**\n   * @public\n   */\n  waitForPendingRequests() {\n    return new Promise(resolve => {\n      if (this.pending.length === 0 && this.inflight.size === 0) {\n        return resolve()\n      }\n\n      this.logger.debug('Waiting for pending requests', {\n        clientId: this.clientId,\n        broker: this.broker,\n        currentInflightRequests: this.inflight.size,\n        currentPendingQueueSize: this.pending.length,\n      })\n\n      this.once(REQUEST_QUEUE_EMPTY, () => resolve())\n    })\n  }\n\n  /**\n   * @public\n   */\n  destroy() {\n    clearInterval(this.requestTimeoutIntervalId)\n    clearTimeout(this.throttleCheckTimeoutId)\n    this.throttleCheckTimeoutId = null\n  }\n\n  canSendSocketRequestImmediately() {\n    const shouldEnqueue =\n      (this.maxInFlightRequests != null && this.inflight.size >= this.maxInFlightRequests) ||\n      this.throttledUntil > Date.now()\n\n    return !shouldEnqueue\n  }\n\n  /**\n   * Check and process pending requests either now or in the future\n   *\n   * This function will send out as many pending requests as possible taking throttling and\n   * in-flight limits into account.\n   */\n  checkPendingRequests() {\n    while (this.pending.length > 0 && this.canSendSocketRequestImmediately()) {\n      const pendingRequest = this.pending.shift() // first in first out\n      this.sendSocketRequest(pendingRequest)\n\n      this.logger.debug(`Consumed pending request`, {\n        clientId: this.clientId,\n        broker: this.broker,\n        correlationId: pendingRequest.correlationId,\n        pendingDuration: pendingRequest.pendingDuration,\n        currentPendingQueueSize: this.pending.length,\n      })\n\n      this[PRIVATE.EMIT_QUEUE_SIZE_EVENT]()\n    }\n\n    this.scheduleCheckPendingRequests()\n  }\n\n  /**\n   * Ensure that pending requests will be checked in the future\n   *\n   * If there is a client-side throttling in place this will ensure that we will check\n   * the pending request queue eventually.\n   */\n  scheduleCheckPendingRequests() {\n    // If we're throttled: Schedule checkPendingRequests when the throttle\n    // should be resolved. If there is already something scheduled we assume that that\n    // will be fine, and potentially fix up a new timeout if needed at that time.\n    // Note that if we're merely \"overloaded\" by having too many inflight requests\n    // we will anyways check the queue when one of them gets fulfilled.\n    let scheduleAt = this.throttledUntil - Date.now()\n    if (!this.throttleCheckTimeoutId) {\n      if (this.pending.length > 0) {\n        scheduleAt = scheduleAt > 0 ? scheduleAt : CHECK_PENDING_REQUESTS_INTERVAL\n      }\n      this.throttleCheckTimeoutId = setTimeout(() => {\n        this.throttleCheckTimeoutId = null\n        this.checkPendingRequests()\n      }, scheduleAt)\n    }\n  }\n}\n", "const CONNECTION_STATUS = {\n  CONNECTED: 'connected',\n  DISCONNECTING: 'disconnecting',\n  DISCONNECTED: 'disconnected',\n}\n\nconst CONNECTED_STATUS = [CONNECTION_STATUS.CONNECTED, CONNECTION_STATUS.DISCONNECTING]\n\nmodule.exports = {\n  CONNECTION_STATUS,\n  CONNECTED_STATUS,\n}\n", "/**\n * http://www.ietf.org/rfc/rfc2595.txt\n *\n * The mechanism consists of a single message from the client to the\n * server.  The client sends the authorization identity (identity to\n * login as), followed by a US-ASCII NUL character, followed by the\n * authentication identity (identity whose password will be used),\n * followed by a US-ASCII NUL character, followed by the clear-text\n * password.  The client may leave the authorization identity empty to\n * indicate that it is the same as the authentication identity.\n *\n * The server will verify the authentication identity and password with\n * the system authentication database and verify that the authentication\n * credentials permit the client to login as the authorization identity.\n * If both steps succeed, the user is logged in.\n */\n\nconst Encoder = require('../../encoder')\n\nconst US_ASCII_NULL_CHAR = '\\u0000'\n\nmodule.exports = ({ authorizationIdentity = null, username, password }) => ({\n  encode: async () => {\n    return new Encoder().writeBytes(\n      [authorizationIdentity, username, password].join(US_ASCII_NULL_CHAR)\n    ).buffer\n  },\n})\n", "module.exports = {\n  decode: async () => true,\n  parse: async () => true,\n}\n", "module.exports = {\n  request: require('./request'),\n  response: require('./response'),\n}\n", "const { request, response } = require('../../protocol/sasl/plain')\nconst { KafkaJSSASLAuthenticationError } = require('../../errors')\n\nconst plainAuthenticatorProvider = sasl => ({ host, port, logger, saslAuthenticate }) => {\n  return {\n    authenticate: async () => {\n      if (sasl.username == null || sasl.password == null) {\n        throw new KafkaJSSASLAuthenticationError('SASL Plain: Invalid username or password')\n      }\n\n      const broker = `${host}:${port}`\n\n      try {\n        logger.debug('Authenticate with SASL PLAIN', { broker })\n        await saslAuthenticate({ request: request(sasl), response })\n        logger.debug('SASL PLAIN authentication successful', { broker })\n      } catch (e) {\n        const error = new KafkaJSSASLAuthenticationError(\n          `SASL PLAIN authentication failed: ${e.message}`\n        )\n        logger.error(error.message, { broker })\n        throw error\n      }\n    },\n  }\n}\n\nmodule.exports = plainAuthenticatorProvider\n", "/**\n * https://tools.ietf.org/html/rfc5802\n *\n * First, the client sends the \"client-first-message\" containing:\n *\n *  -> a GS2 header consisting of a flag indicating whether channel\n * binding is supported-but-not-used, not supported, or used, and an\n * optional SASL authorization identity;\n *\n *  -> SCRAM username and a random, unique nonce attributes.\n *\n * Note that the client's first message will always start with \"n\", \"y\",\n * or \"p\"; otherwise, the message is invalid and authentication MUST\n * fail.  This is important, as it allows for GS2 extensibility (e.g.,\n * to add support for security layers).\n */\n\nconst Encoder = require('../../../encoder')\n\nmodule.exports = ({ clientFirstMessage }) => ({\n  encode: async () => new Encoder().writeBytes(clientFirstMessage).buffer,\n})\n", "/* eslint no-unused-vars: [\"error\", { \"varsIgnorePattern\": \"_\" }] */\n\nconst Decoder = require('../../../decoder')\n\nconst ENTRY_REGEX = /^([rsiev])=(.*)$/\n\nmodule.exports = {\n  decode: async rawData => {\n    return new Decoder(rawData).readBytes()\n  },\n  parse: async data => {\n    const processed = data\n      .toString()\n      .split(',')\n      .map(str => {\n        const [_, key, value] = str.match(ENTRY_REGEX)\n        return [key, value]\n      })\n      .reduce((obj, entry) => ({ ...obj, [entry[0]]: entry[1] }), {})\n\n    return { original: data.toString(), ...processed }\n  },\n}\n", "const Encoder = require('../../../encoder')\n\nmodule.exports = ({ finalMessage }) => ({\n  encode: async () => new Encoder().writeBytes(finalMessage).buffer,\n})\n", "module.exports = require('../firstMessage/response')\n", "module.exports = {\n  firstMessage: {\n    request: require('./firstMessage/request'),\n    response: require('./firstMessage/response'),\n  },\n  finalMessage: {\n    request: require('./finalMessage/request'),\n    response: require('./finalMessage/response'),\n  },\n}\n", "const crypto = require('crypto')\nconst scram = require('../../protocol/sasl/scram')\nconst { KafkaJSSASLAuthenticationError, KafkaJSNonRetriableError } = require('../../errors')\n\nconst GS2_HEADER = 'n,,'\n\nconst EQUAL_SIGN_REGEX = /=/g\nconst COMMA_SIGN_REGEX = /,/g\n\nconst URLSAFE_BASE64_PLUS_REGEX = /\\+/g\nconst URLSAFE_BASE64_SLASH_REGEX = /\\//g\nconst URLSAFE_BASE64_TRAILING_EQUAL_REGEX = /=+$/\n\nconst HMAC_CLIENT_KEY = 'Client Key'\nconst HMAC_SERVER_KEY = 'Server Key'\n\nconst DIGESTS = {\n  SHA256: {\n    length: 32,\n    type: 'sha256',\n    minIterations: 4096,\n  },\n  SHA512: {\n    length: 64,\n    type: 'sha512',\n    minIterations: 4096,\n  },\n}\n\nconst encode64 = str => Buffer.from(str).toString('base64')\n\nclass SCRAM {\n  /**\n   * From https://tools.ietf.org/html/rfc5802#section-5.1\n   *\n   * The characters ',' or '=' in usernames are sent as '=2C' and\n   * '=3D' respectively.  If the server receives a username that\n   * contains '=' not followed by either '2C' or '3D', then the\n   * server MUST fail the authentication.\n   *\n   * @returns {String}\n   */\n  static sanitizeString(str) {\n    return str.replace(EQUAL_SIGN_REGEX, '=3D').replace(COMMA_SIGN_REGEX, '=2C')\n  }\n\n  /**\n   * In cryptography, a nonce is an arbitrary number that can be used just once.\n   * It is similar in spirit to a nonce * word, hence the name. It is often a random or pseudo-random\n   * number issued in an authentication protocol to * ensure that old communications cannot be reused\n   * in replay attacks.\n   *\n   * @returns {String}\n   */\n  static nonce() {\n    return crypto\n      .randomBytes(16)\n      .toString('base64')\n      .replace(URLSAFE_BASE64_PLUS_REGEX, '-') // make it url safe\n      .replace(URLSAFE_BASE64_SLASH_REGEX, '_')\n      .replace(URLSAFE_BASE64_TRAILING_EQUAL_REGEX, '')\n      .toString('ascii')\n  }\n\n  /**\n   * Hi() is, essentially, PBKDF2 [RFC2898] with HMAC() as the\n   * pseudorandom function (PRF) and with dkLen == output length of\n   * HMAC() == output length of H()\n   *\n   * @returns {Promise<Buffer>}\n   */\n  static hi(password, salt, iterations, digestDefinition) {\n    return new Promise((resolve, reject) => {\n      crypto.pbkdf2(\n        password,\n        salt,\n        iterations,\n        digestDefinition.length,\n        digestDefinition.type,\n        (err, derivedKey) => (err ? reject(err) : resolve(derivedKey))\n      )\n    })\n  }\n\n  /**\n   * Apply the exclusive-or operation to combine the octet string\n   * on the left of this operator with the octet string on the right of\n   * this operator.  The length of the output and each of the two\n   * inputs will be the same for this use\n   *\n   * @returns {Buffer}\n   */\n  static xor(left, right) {\n    const bufferA = Buffer.from(left)\n    const bufferB = Buffer.from(right)\n    const length = Buffer.byteLength(bufferA)\n\n    if (length !== Buffer.byteLength(bufferB)) {\n      throw new KafkaJSNonRetriableError('Buffers must be of the same length')\n    }\n\n    const result = []\n    for (let i = 0; i < length; i++) {\n      result.push(bufferA[i] ^ bufferB[i])\n    }\n\n    return Buffer.from(result)\n  }\n\n  /**\n   * @param {SASLOptions} sasl\n   * @param {Logger} logger\n   * @param {Function} saslAuthenticate\n   * @param {DigestDefinition} digestDefinition\n   */\n  constructor(sasl, host, port, logger, saslAuthenticate, digestDefinition) {\n    this.sasl = sasl\n    this.host = host\n    this.port = port\n    this.logger = logger\n    this.saslAuthenticate = saslAuthenticate\n    this.digestDefinition = digestDefinition\n\n    const digestType = digestDefinition.type.toUpperCase()\n    this.PREFIX = `SASL SCRAM ${digestType} authentication`\n\n    this.currentNonce = SCRAM.nonce()\n  }\n\n  async authenticate() {\n    const { PREFIX } = this\n    const broker = `${this.host}:${this.port}`\n\n    if (this.sasl.username == null || this.sasl.password == null) {\n      throw new KafkaJSSASLAuthenticationError(`${this.PREFIX}: Invalid username or password`)\n    }\n\n    try {\n      this.logger.debug('Exchanging first client message', { broker })\n      const clientMessageResponse = await this.sendClientFirstMessage()\n\n      this.logger.debug('Sending final message', { broker })\n      const finalResponse = await this.sendClientFinalMessage(clientMessageResponse)\n\n      if (finalResponse.e) {\n        throw new Error(finalResponse.e)\n      }\n\n      const serverKey = await this.serverKey(clientMessageResponse)\n      const serverSignature = this.serverSignature(serverKey, clientMessageResponse)\n\n      if (finalResponse.v !== serverSignature) {\n        throw new Error('Invalid server signature in server final message')\n      }\n\n      this.logger.debug(`${PREFIX} successful`, { broker })\n    } catch (e) {\n      const error = new KafkaJSSASLAuthenticationError(`${PREFIX} failed: ${e.message}`)\n      this.logger.error(error.message, { broker })\n      throw error\n    }\n  }\n\n  /**\n   * @private\n   */\n  async sendClientFirstMessage() {\n    const clientFirstMessage = `${GS2_HEADER}${this.firstMessageBare()}`\n    const request = scram.firstMessage.request({ clientFirstMessage })\n    const response = scram.firstMessage.response\n\n    return this.saslAuthenticate({\n      request,\n      response,\n    })\n  }\n\n  /**\n   * @private\n   */\n  async sendClientFinalMessage(clientMessageResponse) {\n    const { PREFIX } = this\n    const iterations = parseInt(clientMessageResponse.i, 10)\n    const { minIterations } = this.digestDefinition\n\n    if (!clientMessageResponse.r.startsWith(this.currentNonce)) {\n      throw new KafkaJSSASLAuthenticationError(\n        `${PREFIX} failed: Invalid server nonce, it does not start with the client nonce`\n      )\n    }\n\n    if (iterations < minIterations) {\n      throw new KafkaJSSASLAuthenticationError(\n        `${PREFIX} failed: Requested iterations ${iterations} is less than the minimum ${minIterations}`\n      )\n    }\n\n    const finalMessageWithoutProof = this.finalMessageWithoutProof(clientMessageResponse)\n    const clientProof = await this.clientProof(clientMessageResponse)\n    const finalMessage = `${finalMessageWithoutProof},p=${clientProof}`\n    const request = scram.finalMessage.request({ finalMessage })\n    const response = scram.finalMessage.response\n\n    return this.saslAuthenticate({\n      request,\n      response,\n    })\n  }\n\n  /**\n   * @private\n   */\n  async clientProof(clientMessageResponse) {\n    const clientKey = await this.clientKey(clientMessageResponse)\n    const storedKey = this.H(clientKey)\n    const clientSignature = this.clientSignature(storedKey, clientMessageResponse)\n    return encode64(SCRAM.xor(clientKey, clientSignature))\n  }\n\n  /**\n   * @private\n   */\n  async clientKey(clientMessageResponse) {\n    const saltedPassword = await this.saltPassword(clientMessageResponse)\n    return this.HMAC(saltedPassword, HMAC_CLIENT_KEY)\n  }\n\n  /**\n   * @private\n   */\n  async serverKey(clientMessageResponse) {\n    const saltedPassword = await this.saltPassword(clientMessageResponse)\n    return this.HMAC(saltedPassword, HMAC_SERVER_KEY)\n  }\n\n  /**\n   * @private\n   */\n  clientSignature(storedKey, clientMessageResponse) {\n    return this.HMAC(storedKey, this.authMessage(clientMessageResponse))\n  }\n\n  /**\n   * @private\n   */\n  serverSignature(serverKey, clientMessageResponse) {\n    return encode64(this.HMAC(serverKey, this.authMessage(clientMessageResponse)))\n  }\n\n  /**\n   * @private\n   */\n  authMessage(clientMessageResponse) {\n    return [\n      this.firstMessageBare(),\n      clientMessageResponse.original,\n      this.finalMessageWithoutProof(clientMessageResponse),\n    ].join(',')\n  }\n\n  /**\n   * @private\n   */\n  async saltPassword(clientMessageResponse) {\n    const salt = Buffer.from(clientMessageResponse.s, 'base64')\n    const iterations = parseInt(clientMessageResponse.i, 10)\n    return SCRAM.hi(this.encodedPassword(), salt, iterations, this.digestDefinition)\n  }\n\n  /**\n   * @private\n   */\n  firstMessageBare() {\n    return `n=${this.encodedUsername()},r=${this.currentNonce}`\n  }\n\n  /**\n   * @private\n   */\n  finalMessageWithoutProof(clientMessageResponse) {\n    const rnonce = clientMessageResponse.r\n    return `c=${encode64(GS2_HEADER)},r=${rnonce}`\n  }\n\n  /**\n   * @private\n   */\n  encodedUsername() {\n    const { username } = this.sasl\n    return SCRAM.sanitizeString(username).toString('utf-8')\n  }\n\n  /**\n   * @private\n   */\n  encodedPassword() {\n    const { password } = this.sasl\n    return password.toString('utf-8')\n  }\n\n  /**\n   * @private\n   */\n  H(data) {\n    return crypto\n      .createHash(this.digestDefinition.type)\n      .update(data)\n      .digest()\n  }\n\n  /**\n   * @private\n   */\n  HMAC(key, data) {\n    return crypto\n      .createHmac(this.digestDefinition.type, key)\n      .update(data)\n      .digest()\n  }\n}\n\nmodule.exports = {\n  DIGESTS,\n  SCRAM,\n}\n", "const { SCRAM, DIGESTS } = require('./scram')\n\nconst scram256AuthenticatorProvider = sasl => ({ host, port, logger, saslAuthenticate }) => {\n  const scram = new SCRAM(sasl, host, port, logger, saslAuthenticate, DIGESTS.SHA256)\n  return {\n    authenticate: async () => await scram.authenticate(),\n  }\n}\n\nmodule.exports = scram256AuthenticatorProvider\n", "const { SCRAM, DIGESTS } = require('./scram')\n\nconst scram512AuthenticatorProvider = sasl => ({ host, port, logger, saslAuthenticate }) => {\n  const scram = new SCRAM(sasl, host, port, logger, saslAuthenticate, DIGESTS.SHA512)\n  return {\n    authenticate: async () => await scram.authenticate(),\n  }\n}\n\nmodule.exports = scram512AuthenticatorProvider\n", "const Encoder = require('../../encoder')\n\nconst US_ASCII_NULL_CHAR = '\\u0000'\n\nmodule.exports = ({ authorizationIdentity, accessKeyId, secretAccessKey, sessionToken = '' }) => ({\n  encode: async () => {\n    return new Encoder().writeBytes(\n      [authorizationIdentity, accessKeyId, secretAccessKey, sessionToken].join(US_ASCII_NULL_CHAR)\n    ).buffer\n  },\n})\n", "module.exports = {\n  decode: async () => true,\n  parse: async () => true,\n}\n", "module.exports = {\n  request: require('./request'),\n  response: require('./response'),\n}\n", "const { request, response } = require('../../protocol/sasl/awsIam')\nconst { KafkaJSSASLAuthenticationError } = require('../../errors')\n\nconst awsIAMAuthenticatorProvider = sasl => ({ host, port, logger, saslAuthenticate }) => {\n  return {\n    authenticate: async () => {\n      if (!sasl.authorizationIdentity) {\n        throw new KafkaJSSASLAuthenticationError('SASL AWS-IAM: Missing authorizationIdentity')\n      }\n      if (!sasl.accessKeyId) {\n        throw new KafkaJSSASLAuthenticationError('SASL AWS-IAM: Missing accessKeyId')\n      }\n      if (!sasl.secretAccessKey) {\n        throw new KafkaJSSASLAuthenticationError('SASL AWS-IAM: Missing secretAccessKey')\n      }\n      if (!sasl.sessionToken) {\n        sasl.sessionToken = ''\n      }\n\n      const broker = `${host}:${port}`\n\n      try {\n        logger.debug('Authenticate with SASL AWS-IAM', { broker })\n        await saslAuthenticate({ request: request(sasl), response })\n        logger.debug('SASL AWS-IAM authentication successful', { broker })\n      } catch (e) {\n        const error = new KafkaJSSASLAuthenticationError(\n          `SASL AWS-IAM authentication failed: ${e.message}`\n        )\n        logger.error(error.message, { broker })\n        throw error\n      }\n    },\n  }\n}\n\nmodule.exports = awsIAMAuthenticatorProvider\n", "/**\n * http://www.ietf.org/rfc/rfc5801.txt\n *\n * See org.apache.kafka.common.security.oauthbearer.internals.OAuthBearerClientInitialResponse\n * for official Java client implementation.\n *\n * The mechanism consists of a message from the client to the server.\n * The client sends the \"n,\"\" GS header, followed by the authorizationIdentitty\n * prefixed by \"a=\" (if present), followed by \",\", followed by a US-ASCII SOH\n * character, followed by \"auth=Bearer \", followed by the token value, followed\n * by US-ASCII SOH character, followed by SASL extensions in OAuth \"friendly\"\n * format and then closed by two additionals US-ASCII SOH characters.\n *\n * SASL extensions are optional an must be expressed as key-value pairs in an\n * object. Each expression is converted as, the extension entry key, followed\n * by \"=\", followed by extension entry value. Each extension is separated by a\n * US-ASCII SOH character. If extensions are not present, their relative part\n * in the message, including the US-ASCII SOH character, is omitted.\n *\n * The client may leave the authorization identity empty to\n * indicate that it is the same as the authentication identity.\n *\n * The server will verify the authentication token and verify that the\n * authentication credentials permit the client to login as the authorization\n * identity. If both steps succeed, the user is logged in.\n */\n\nconst Encoder = require('../../encoder')\n\nconst SEPARATOR = '\\u0001' // SOH - Start Of Header ASCII\n\nfunction formatExtensions(extensions) {\n  let msg = ''\n\n  if (extensions == null) {\n    return msg\n  }\n\n  let prefix = ''\n  for (const k in extensions) {\n    msg += `${prefix}${k}=${extensions[k]}`\n    prefix = SEPARATOR\n  }\n\n  return msg\n}\n\nmodule.exports = async ({ authorizationIdentity = null }, oauthBearerToken) => {\n  const authzid = authorizationIdentity == null ? '' : `\"a=${authorizationIdentity}`\n  let ext = formatExtensions(oauthBearerToken.extensions)\n  if (ext.length > 0) {\n    ext = `${SEPARATOR}${ext}`\n  }\n\n  const oauthMsg = `n,${authzid},${SEPARATOR}auth=Bearer ${oauthBearerToken.value}${ext}${SEPARATOR}${SEPARATOR}`\n\n  return {\n    encode: async () => {\n      return new Encoder().writeBytes(Buffer.from(oauthMsg)).buffer\n    },\n  }\n}\n", "module.exports = {\n  decode: async () => true,\n  parse: async () => true,\n}\n", "module.exports = {\n  request: require('./request'),\n  response: require('./response'),\n}\n", "/**\n * The sasl object must include a property named oauthBearerProvider, an\n * async function that is used to return the OAuth bearer token.\n *\n * The OAuth bearer token must be an object with properties value and\n * (optionally) extensions, that will be sent during the SASL/OAUTHBEARER\n * request.\n *\n * The implementation of the oauthBearerProvider must take care that tokens are\n * reused and refreshed when appropriate.\n */\n\nconst { request } = require('../../protocol/sasl/oauthBearer')\nconst { KafkaJSSASLAuthenticationError } = require('../../errors')\n\nconst oauthBearerAuthenticatorProvider = sasl => ({ host, port, logger, saslAuthenticate }) => {\n  return {\n    authenticate: async () => {\n      const { oauthBearerProvider } = sasl\n\n      if (oauthBearerProvider == null) {\n        throw new KafkaJSSASLAuthenticationError(\n          'SASL OAUTHBEARER: Missing OAuth bearer token provider'\n        )\n      }\n\n      const oauthBearerToken = await oauthBearerProvider()\n\n      if (oauthBearerToken.value == null) {\n        throw new KafkaJSSASLAuthenticationError('SASL OAUTHBEARER: Invalid OAuth bearer token')\n      }\n\n      const broker = `${host}:${port}`\n\n      try {\n        logger.debug('Authenticate with SASL OAUTHBEARER', { broker })\n        await saslAuthenticate({ request: await request(sasl, oauthBearerToken) })\n        logger.debug('SASL OAUTHBEARER authentication successful', { broker })\n      } catch (e) {\n        const error = new KafkaJSSASLAuthenticationError(\n          `SASL OAUTHBEARER authentication failed: ${e.message}`\n        )\n        logger.error(error.message, { broker })\n        throw error\n      }\n    },\n  }\n}\n\nmodule.exports = oauthBearerAuthenticatorProvider\n", "const { requests, lookup } = require('../../protocol/requests')\nconst apiKeys = require('../../protocol/requests/apiKeys')\nconst plainAuthenticatorProvider = require('./plain')\nconst scram256AuthenticatorProvider = require('./scram256')\nconst scram512AuthenticatorProvider = require('./scram512')\nconst awsIAMAuthenticatorProvider = require('./awsIam')\nconst oauthBearerAuthenticatorProvider = require('./oauthBearer')\nconst { KafkaJSSASLAuthenticationError } = require('../../errors')\n\nconst BUILT_IN_AUTHENTICATION_PROVIDERS = {\n  AWS: awsIAMAuthenticatorProvider,\n  PLAIN: plainAuthenticatorProvider,\n  OAUTHBEARER: oauthBearerAuthenticatorProvider,\n  'SCRAM-SHA-256': scram256AuthenticatorProvider,\n  'SCRAM-SHA-512': scram512AuthenticatorProvider,\n}\n\nconst UNLIMITED_SESSION_LIFETIME = '0'\n\nmodule.exports = class SASLAuthenticator {\n  constructor(connection, logger, versions, supportAuthenticationProtocol) {\n    this.connection = connection\n    this.logger = logger\n    this.sessionLifetime = UNLIMITED_SESSION_LIFETIME\n\n    const lookupRequest = lookup(versions)\n    this.saslHandshake = lookupRequest(apiKeys.SaslHandshake, requests.SaslHandshake)\n    this.protocolAuthentication = supportAuthenticationProtocol\n      ? lookupRequest(apiKeys.SaslAuthenticate, requests.SaslAuthenticate)\n      : null\n  }\n\n  async authenticate() {\n    const mechanism = this.connection.sasl.mechanism.toUpperCase()\n    const handshake = await this.connection.send(this.saslHandshake({ mechanism }))\n    if (!handshake.enabledMechanisms.includes(mechanism)) {\n      throw new KafkaJSSASLAuthenticationError(\n        `SASL ${mechanism} mechanism is not supported by the server`\n      )\n    }\n\n    const saslAuthenticate = async ({ request, response }) => {\n      if (this.protocolAuthentication) {\n        const requestAuthBytes = await request.encode()\n        const authResponse = await this.connection.send(\n          this.protocolAuthentication({ authBytes: requestAuthBytes })\n        )\n\n        // `0` is a string because `sessionLifetimeMs` is an int64 encoded as string.\n        // This is not present in SaslAuthenticateV0, so we default to `\"0\"`\n        this.sessionLifetime = authResponse.sessionLifetimeMs || UNLIMITED_SESSION_LIFETIME\n\n        if (!response) {\n          return\n        }\n\n        const { authBytes: responseAuthBytes } = authResponse\n        const payloadDecoded = await response.decode(responseAuthBytes)\n        return response.parse(payloadDecoded)\n      }\n\n      return this.connection.sendAuthRequest({ request, response })\n    }\n\n    if (\n      !this.connection.sasl.authenticationProvider &&\n      Object.keys(BUILT_IN_AUTHENTICATION_PROVIDERS).includes(mechanism)\n    ) {\n      this.connection.sasl.authenticationProvider = BUILT_IN_AUTHENTICATION_PROVIDERS[mechanism](\n        this.connection.sasl\n      )\n    }\n    await this.connection.sasl\n      .authenticationProvider({\n        host: this.connection.host,\n        port: this.connection.port,\n        logger: this.logger.namespace(`SaslAuthenticator-${mechanism}`),\n        saslAuthenticate,\n      })\n      .authenticate()\n  }\n}\n", "const createSocket = require('./socket')\nconst createRequest = require('../protocol/request')\nconst Decoder = require('../protocol/decoder')\nconst { KafkaJSConnectionError, KafkaJSConnectionClosedError } = require('../errors')\nconst { INT_32_MAX_VALUE } = require('../constants')\nconst getEnv = require('../env')\nconst RequestQueue = require('./requestQueue')\nconst { CONNECTION_STATUS, CONNECTED_STATUS } = require('./connectionStatus')\nconst sharedPromiseTo = require('../utils/sharedPromiseTo')\nconst Long = require('../utils/long')\nconst SASLAuthenticator = require('../broker/saslAuthenticator')\nconst apiKeys = require('../protocol/requests/apiKeys')\n\nconst requestInfo = ({ apiName, apiKey, apiVersion }) =>\n  `${apiName}(key: ${apiKey}, version: ${apiVersion})`\n\n/**\n * @param request - request from protocol\n * @returns {boolean}\n */\nconst isAuthenticatedRequest = request => {\n  return ![apiKeys.ApiVersions, apiKeys.SaslHandshake, apiKeys.SaslAuthenticate].includes(\n    request.apiKey\n  )\n}\n\nconst PRIVATE = {\n  SHOULD_REAUTHENTICATE: Symbol('private:Connection:shouldReauthenticate'),\n  AUTHENTICATE: Symbol('private:Connection:authenticate'),\n}\n\nmodule.exports = class Connection {\n  /**\n   * @param {Object} options\n   * @param {string} options.host\n   * @param {number} options.port\n   * @param {import(\"../../types\").Logger} options.logger\n   * @param {import(\"../../types\").ISocketFactory} options.socketFactory\n   * @param {string} [options.clientId='kafkajs']\n   * @param {number} options.requestTimeout The maximum amount of time the client will wait for the response of a request,\n   *                                in milliseconds\n   * @param {string} [options.rack=null]\n   * @param {Object} [options.ssl=null] Options for the TLS Secure Context. It accepts all options,\n   *                            usually \"cert\", \"key\" and \"ca\". More information at\n   *                            https://nodejs.org/api/tls.html#tls_tls_createsecurecontext_options\n   * @param {Object} [options.sasl=null] Attributes used for SASL authentication. Options based on the\n   *                             key \"mechanism\". Connection is not actively using the SASL attributes\n   *                             but acting as a data object for this information\n   * @param {number} [options.reauthenticationThreshold=10000]\n   * @param {number} options.connectionTimeout The connection timeout, in milliseconds\n   * @param {boolean} [options.enforceRequestTimeout]\n   * @param {number} [options.maxInFlightRequests=null] The maximum number of unacknowledged requests on a connection before\n   *                                            enqueuing\n   * @param {import(\"../instrumentation/emitter\")} [options.instrumentationEmitter=null]\n   */\n  constructor({\n    host,\n    port,\n    logger,\n    socketFactory,\n    requestTimeout,\n    reauthenticationThreshold = 10000,\n    rack = null,\n    ssl = null,\n    sasl = null,\n    clientId = 'kafkajs',\n    connectionTimeout,\n    enforceRequestTimeout = true,\n    maxInFlightRequests = null,\n    instrumentationEmitter = null,\n  }) {\n    this.host = host\n    this.port = port\n    this.rack = rack\n    this.clientId = clientId\n    this.broker = `${this.host}:${this.port}`\n    this.logger = logger.namespace('Connection')\n\n    this.socketFactory = socketFactory\n    this.ssl = ssl\n    this.sasl = sasl\n\n    this.requestTimeout = requestTimeout\n    this.connectionTimeout = connectionTimeout\n    this.reauthenticationThreshold = reauthenticationThreshold\n\n    this.bytesBuffered = 0\n    this.bytesNeeded = Decoder.int32Size()\n    this.chunks = []\n\n    this.connectionStatus = CONNECTION_STATUS.DISCONNECTED\n    this.correlationId = 0\n    this.requestQueue = new RequestQueue({\n      instrumentationEmitter,\n      maxInFlightRequests,\n      requestTimeout,\n      enforceRequestTimeout,\n      clientId,\n      broker: this.broker,\n      logger: logger.namespace('RequestQueue'),\n      isConnected: () => this.isConnected(),\n    })\n\n    this.versions = null\n\n    this.authHandlers = null\n    this.authExpectResponse = false\n\n    const log = level => (message, extra = {}) => {\n      const logFn = this.logger[level]\n      logFn(message, { broker: this.broker, clientId, ...extra })\n    }\n\n    this.logDebug = log('debug')\n    this.logError = log('error')\n\n    const env = getEnv()\n    this.shouldLogBuffers = env.KAFKAJS_DEBUG_PROTOCOL_BUFFERS === '1'\n    this.shouldLogFetchBuffer =\n      this.shouldLogBuffers && env.KAFKAJS_DEBUG_EXTENDED_PROTOCOL_BUFFERS === '1'\n\n    this.authenticatedAt = null\n    this.sessionLifetime = Long.ZERO\n    this.supportAuthenticationProtocol = null\n\n    /**\n     * @private\n     * @returns {Promise}\n     */\n    this[PRIVATE.AUTHENTICATE] = sharedPromiseTo(async () => {\n      if (this.sasl && !this.isAuthenticated()) {\n        const authenticator = new SASLAuthenticator(\n          this,\n          this.logger,\n          this.versions,\n          this.supportAuthenticationProtocol\n        )\n\n        await authenticator.authenticate()\n        this.authenticatedAt = process.hrtime()\n        this.sessionLifetime = Long.fromValue(authenticator.sessionLifetime)\n      }\n    })\n  }\n\n  getSupportAuthenticationProtocol() {\n    return this.supportAuthenticationProtocol\n  }\n\n  setSupportAuthenticationProtocol(isSupported) {\n    this.supportAuthenticationProtocol = isSupported\n  }\n\n  setVersions(versions) {\n    this.versions = versions\n  }\n\n  isConnected() {\n    return CONNECTED_STATUS.includes(this.connectionStatus)\n  }\n\n  /**\n   * @public\n   * @returns {Promise}\n   */\n  connect() {\n    return new Promise((resolve, reject) => {\n      if (this.isConnected()) {\n        return resolve(true)\n      }\n\n      this.authenticatedAt = null\n\n      let timeoutId\n\n      const onConnect = () => {\n        clearTimeout(timeoutId)\n        this.connectionStatus = CONNECTION_STATUS.CONNECTED\n        this.requestQueue.scheduleRequestTimeoutCheck()\n        resolve(true)\n      }\n\n      const onData = data => {\n        this.processData(data)\n      }\n\n      const onEnd = async () => {\n        clearTimeout(timeoutId)\n\n        const wasConnected = this.isConnected()\n\n        if (this.authHandlers) {\n          this.authHandlers.onError()\n        } else if (wasConnected) {\n          this.logDebug('Kafka server has closed connection')\n          this.rejectRequests(\n            new KafkaJSConnectionClosedError('Closed connection', {\n              host: this.host,\n              port: this.port,\n            })\n          )\n        }\n\n        await this.disconnect()\n      }\n\n      const onError = async e => {\n        clearTimeout(timeoutId)\n\n        const error = new KafkaJSConnectionError(`Connection error: ${e.message}`, {\n          broker: `${this.host}:${this.port}`,\n          code: e.code,\n        })\n\n        this.logError(error.message, { stack: e.stack })\n        this.rejectRequests(error)\n        await this.disconnect()\n\n        reject(error)\n      }\n\n      const onTimeout = async () => {\n        const error = new KafkaJSConnectionError('Connection timeout', {\n          broker: `${this.host}:${this.port}`,\n        })\n\n        this.logError(error.message)\n        this.rejectRequests(error)\n        await this.disconnect()\n        reject(error)\n      }\n\n      this.logDebug(`Connecting`, {\n        ssl: !!this.ssl,\n        sasl: !!this.sasl,\n      })\n\n      try {\n        timeoutId = setTimeout(onTimeout, this.connectionTimeout)\n        this.socket = createSocket({\n          socketFactory: this.socketFactory,\n          host: this.host,\n          port: this.port,\n          ssl: this.ssl,\n          onConnect,\n          onData,\n          onEnd,\n          onError,\n          onTimeout,\n        })\n      } catch (e) {\n        clearTimeout(timeoutId)\n        reject(\n          new KafkaJSConnectionError(`Failed to connect: ${e.message}`, {\n            broker: `${this.host}:${this.port}`,\n          })\n        )\n      }\n    })\n  }\n\n  /**\n   * @public\n   * @returns {Promise}\n   */\n  async disconnect() {\n    this.authenticatedAt = null\n    this.connectionStatus = CONNECTION_STATUS.DISCONNECTING\n    this.logDebug('disconnecting...')\n\n    await this.requestQueue.waitForPendingRequests()\n    this.requestQueue.destroy()\n\n    if (this.socket) {\n      this.socket.end()\n      this.socket.unref()\n    }\n\n    this.connectionStatus = CONNECTION_STATUS.DISCONNECTED\n    this.logDebug('disconnected')\n    return true\n  }\n\n  /**\n   * @public\n   * @returns {boolean}\n   */\n  isAuthenticated() {\n    return this.authenticatedAt != null && !this[PRIVATE.SHOULD_REAUTHENTICATE]()\n  }\n\n  /***\n   * @private\n   */\n  [PRIVATE.SHOULD_REAUTHENTICATE]() {\n    if (this.sessionLifetime.equals(Long.ZERO)) {\n      return false\n    }\n\n    if (this.authenticatedAt == null) {\n      return true\n    }\n\n    const [secondsSince, remainingNanosSince] = process.hrtime(this.authenticatedAt)\n    const millisSince = Long.fromValue(secondsSince)\n      .multiply(1000)\n      .add(Long.fromValue(remainingNanosSince).divide(1000000))\n\n    const reauthenticateAt = millisSince.add(this.reauthenticationThreshold)\n    return reauthenticateAt.greaterThanOrEqual(this.sessionLifetime)\n  }\n\n  /** @public */\n  async authenticate() {\n    await this[PRIVATE.AUTHENTICATE]()\n  }\n\n  /**\n   * @public\n   * @returns {Promise}\n   */\n  sendAuthRequest({ request, response }) {\n    this.authExpectResponse = !!response\n\n    /**\n     * TODO: rewrite removing the async promise executor\n     */\n\n    /* eslint-disable no-async-promise-executor */\n    return new Promise(async (resolve, reject) => {\n      this.authHandlers = {\n        onSuccess: rawData => {\n          this.authHandlers = null\n          this.authExpectResponse = false\n\n          response\n            .decode(rawData)\n            .then(data => response.parse(data))\n            .then(resolve)\n            .catch(reject)\n        },\n        onError: () => {\n          this.authHandlers = null\n          this.authExpectResponse = false\n\n          reject(\n            new KafkaJSConnectionError('Connection closed by the server', {\n              broker: `${this.host}:${this.port}`,\n            })\n          )\n        },\n      }\n\n      try {\n        const requestPayload = await request.encode()\n\n        this.failIfNotConnected()\n        this.socket.write(requestPayload, 'binary')\n      } catch (e) {\n        reject(e)\n      }\n    })\n  }\n\n  /**\n   * @public\n   * @param {object} protocol\n   * @param {object} protocol.request It is defined by the protocol and consists of an object with \"apiKey\",\n   *                         \"apiVersion\", \"apiName\" and an \"encode\" function. The encode function\n   *                         must return an instance of Encoder\n   *\n   * @param {object} protocol.response It is defined by the protocol and consists of an object with two functions:\n   *                          \"decode\" and \"parse\"\n   *\n   * @param {number} [protocol.requestTimeout=null] Override for the default requestTimeout\n   * @param {boolean} [protocol.logResponseError=true] Whether to log errors\n   * @returns {Promise<data>} where data is the return of \"response#parse\"\n   */\n  async send({ request, response, requestTimeout = null, logResponseError = true }) {\n    if (!this.isAuthenticated() && isAuthenticatedRequest(request)) {\n      await this[PRIVATE.AUTHENTICATE]()\n    }\n\n    this.failIfNotConnected()\n\n    const expectResponse = !request.expectResponse || request.expectResponse()\n    const sendRequest = async () => {\n      const { clientId } = this\n      const correlationId = this.nextCorrelationId()\n\n      const requestPayload = await createRequest({ request, correlationId, clientId })\n      const { apiKey, apiName, apiVersion } = request\n      this.logDebug(`Request ${requestInfo(request)}`, {\n        correlationId,\n        expectResponse,\n        size: Buffer.byteLength(requestPayload.buffer),\n      })\n\n      return new Promise((resolve, reject) => {\n        try {\n          this.failIfNotConnected()\n          const entry = { apiKey, apiName, apiVersion, correlationId, resolve, reject }\n\n          this.requestQueue.push({\n            entry,\n            expectResponse,\n            requestTimeout,\n            sendRequest: () => {\n              this.socket.write(requestPayload.buffer, 'binary')\n            },\n          })\n        } catch (e) {\n          reject(e)\n        }\n      })\n    }\n\n    const { correlationId, size, entry, payload } = await sendRequest()\n\n    if (!expectResponse) {\n      return\n    }\n\n    try {\n      const payloadDecoded = await response.decode(payload)\n\n      /**\n       * @see KIP-219\n       * If the response indicates that the client-side needs to throttle, do that.\n       */\n      this.requestQueue.maybeThrottle(payloadDecoded.clientSideThrottleTime)\n\n      const data = await response.parse(payloadDecoded)\n      const isFetchApi = entry.apiName === 'Fetch'\n      this.logDebug(`Response ${requestInfo(entry)}`, {\n        correlationId,\n        size,\n        data: isFetchApi && !this.shouldLogFetchBuffer ? '[filtered]' : data,\n      })\n\n      return data\n    } catch (e) {\n      if (logResponseError) {\n        this.logError(`Response ${requestInfo(entry)}`, {\n          error: e.message,\n          correlationId,\n          size,\n        })\n      }\n\n      const isBuffer = Buffer.isBuffer(payload)\n      this.logDebug(`Response ${requestInfo(entry)}`, {\n        error: e.message,\n        correlationId,\n        payload:\n          isBuffer && !this.shouldLogBuffers ? { type: 'Buffer', data: '[filtered]' } : payload,\n      })\n\n      throw e\n    }\n  }\n\n  /**\n   * @private\n   */\n  failIfNotConnected() {\n    if (!this.isConnected()) {\n      throw new KafkaJSConnectionError('Not connected', {\n        broker: `${this.host}:${this.port}`,\n      })\n    }\n  }\n\n  /**\n   * @private\n   */\n  nextCorrelationId() {\n    if (this.correlationId >= INT_32_MAX_VALUE) {\n      this.correlationId = 0\n    }\n\n    return this.correlationId++\n  }\n\n  /**\n   * @private\n   */\n  processData(rawData) {\n    if (this.authHandlers && !this.authExpectResponse) {\n      return this.authHandlers.onSuccess(rawData)\n    }\n\n    // Accumulate the new chunk\n    this.chunks.push(rawData)\n    this.bytesBuffered += Buffer.byteLength(rawData)\n\n    // Process data if there are enough bytes to read the expected response size,\n    // otherwise keep buffering\n    while (this.bytesNeeded <= this.bytesBuffered) {\n      const buffer = this.chunks.length > 1 ? Buffer.concat(this.chunks) : this.chunks[0]\n      const decoder = new Decoder(buffer)\n      const expectedResponseSize = decoder.readInt32()\n\n      // Return early if not enough bytes to read the full response\n      if (!decoder.canReadBytes(expectedResponseSize)) {\n        this.chunks = [buffer]\n        this.bytesBuffered = Buffer.byteLength(buffer)\n        this.bytesNeeded = Decoder.int32Size() + expectedResponseSize\n        return\n      }\n\n      const response = new Decoder(decoder.readBytes(expectedResponseSize))\n\n      // Reset the buffered chunks as the rest of the bytes\n      const remainderBuffer = decoder.readAll()\n      this.chunks = [remainderBuffer]\n      this.bytesBuffered = Buffer.byteLength(remainderBuffer)\n      this.bytesNeeded = Decoder.int32Size()\n\n      if (this.authHandlers) {\n        const rawResponseSize = Decoder.int32Size() + expectedResponseSize\n        const rawResponseBuffer = buffer.slice(0, rawResponseSize)\n        return this.authHandlers.onSuccess(rawResponseBuffer)\n      }\n\n      const correlationId = response.readInt32()\n      const payload = response.readAll()\n\n      this.requestQueue.fulfillRequest({\n        size: expectedResponseSize,\n        correlationId,\n        payload,\n      })\n    }\n  }\n\n  /**\n   * @private\n   */\n  rejectRequests(error) {\n    this.requestQueue.rejectAll(error)\n  }\n}\n", "const apiKeys = require('../protocol/requests/apiKeys')\nconst Connection = require('./connection')\n\nmodule.exports = class ConnectionPool {\n  /**\n   * @param {ConstructorParameters<typeof Connection>[0]} options\n   */\n  constructor(options) {\n    this.logger = options.logger.namespace('ConnectionPool')\n    this.connectionTimeout = options.connectionTimeout\n    this.host = options.host\n    this.port = options.port\n    this.rack = options.rack\n    this.ssl = options.ssl\n    this.sasl = options.sasl\n    this.clientId = options.clientId\n    this.socketFactory = options.socketFactory\n\n    this.pool = new Array(2).fill().map(() => new Connection(options))\n  }\n\n  isConnected() {\n    return this.pool.some(c => c.isConnected())\n  }\n\n  isAuthenticated() {\n    return this.pool.some(c => c.isAuthenticated())\n  }\n\n  setSupportAuthenticationProtocol(isSupported) {\n    this.map(c => c.setSupportAuthenticationProtocol(isSupported))\n  }\n\n  setVersions(versions) {\n    this.map(c => c.setVersions(versions))\n  }\n\n  map(callback) {\n    return this.pool.map(c => callback(c))\n  }\n\n  async send(protocolRequest) {\n    const connection = await this.getConnectionByRequest(protocolRequest)\n    return connection.send(protocolRequest)\n  }\n\n  getConnectionByRequest({ request: { apiKey } }) {\n    const index = { [apiKeys.Fetch]: 1 }[apiKey] || 0\n    return this.getConnection(index)\n  }\n\n  async getConnection(index = 0) {\n    const connection = this.pool[index]\n\n    if (!connection.isConnected()) {\n      await connection.connect()\n    }\n\n    return connection\n  }\n\n  async destroy() {\n    await Promise.all(this.map(c => c.disconnect()))\n  }\n}\n", "const { KafkaJSConnectionError, KafkaJSNonRetriableError } = require('../errors')\nconst ConnectionPool = require('../network/connectionPool')\n\n/**\n * @typedef {Object} ConnectionPoolBuilder\n * @property {(destination?: { host?: string, port?: number, rack?: string }) => Promise<ConnectionPool>} build\n */\n\n/**\n * @param {Object} options\n * @param {import(\"../../types\").ISocketFactory} [options.socketFactory]\n * @param {string[]|(() => string[])} options.brokers\n * @param {Object} [options.ssl]\n * @param {Object} [options.sasl]\n * @param {string} options.clientId\n * @param {number} options.requestTimeout\n * @param {boolean} [options.enforceRequestTimeout]\n * @param {number} [options.connectionTimeout]\n * @param {number} [options.maxInFlightRequests]\n * @param {import(\"../../types\").RetryOptions} [options.retry]\n * @param {import(\"../../types\").Logger} options.logger\n * @param {import(\"../instrumentation/emitter\")} [options.instrumentationEmitter]\n * @param {number} [options.reauthenticationThreshold]\n * @returns {ConnectionPoolBuilder}\n */\nmodule.exports = ({\n  socketFactory,\n  brokers,\n  ssl,\n  sasl,\n  clientId,\n  requestTimeout,\n  enforceRequestTimeout,\n  connectionTimeout,\n  maxInFlightRequests,\n  logger,\n  instrumentationEmitter = null,\n  reauthenticationThreshold,\n}) => {\n  let index = 0\n\n  const isValidBroker = broker => {\n    return broker && typeof broker === 'string' && broker.length > 0\n  }\n\n  const validateBrokers = brokers => {\n    if (!brokers) {\n      throw new KafkaJSNonRetriableError(`Failed to connect: brokers should not be null`)\n    }\n\n    if (Array.isArray(brokers)) {\n      if (!brokers.length) {\n        throw new KafkaJSNonRetriableError(`Failed to connect: brokers array is empty`)\n      }\n\n      brokers.forEach((broker, index) => {\n        if (!isValidBroker(broker)) {\n          throw new KafkaJSNonRetriableError(\n            `Failed to connect: broker at index ${index} is invalid \"${typeof broker}\"`\n          )\n        }\n      })\n    }\n  }\n\n  const getBrokers = async () => {\n    let list\n\n    if (typeof brokers === 'function') {\n      try {\n        list = await brokers()\n      } catch (e) {\n        const wrappedError = new KafkaJSConnectionError(\n          `Failed to connect: \"config.brokers\" threw: ${e.message}`\n        )\n        wrappedError.stack = `${wrappedError.name}\\n  Caused by: ${e.stack}`\n        throw wrappedError\n      }\n    } else {\n      list = brokers\n    }\n\n    validateBrokers(list)\n\n    return list\n  }\n\n  return {\n    build: async ({ host, port, rack } = {}) => {\n      if (!host) {\n        const list = await getBrokers()\n\n        const randomBroker = list[index++ % list.length]\n\n        host = randomBroker.split(':')[0]\n        port = Number(randomBroker.split(':')[1])\n      }\n\n      return new ConnectionPool({\n        host,\n        port,\n        rack,\n        sasl,\n        ssl,\n        clientId,\n        socketFactory,\n        connectionTimeout,\n        requestTimeout,\n        enforceRequestTimeout,\n        maxInFlightRequests,\n        instrumentationEmitter,\n        logger,\n        reauthenticationThreshold,\n      })\n    },\n  }\n}\n", "const BrokerPool = require('./brokerPool')\nconst Lock = require('../utils/lock')\nconst sharedPromiseTo = require('../utils/sharedPromiseTo')\nconst createRetry = require('../retry')\nconst connectionPoolBuilder = require('./connectionPoolBuilder')\nconst { EARLIEST_OFFSET, LATEST_OFFSET } = require('../constants')\nconst {\n  KafkaJSError,\n  KafkaJSBrokerNotFound,\n  KafkaJSMetadataNotLoaded,\n  KafkaJSTopicMetadataNotLoaded,\n  KafkaJSGroupCoordinatorNotFound,\n} = require('../errors')\nconst COORDINATOR_TYPES = require('../protocol/coordinatorTypes')\n\nconst { keys } = Object\n\nconst mergeTopics = (obj, { topic, partitions }) => ({\n  ...obj,\n  [topic]: [...(obj[topic] || []), ...partitions],\n})\n\nconst PRIVATE = {\n  CONNECT: Symbol('private:Cluster:connect'),\n  REFRESH_METADATA: Symbol('private:Cluster:refreshMetadata'),\n  REFRESH_METADATA_IF_NECESSARY: Symbol('private:Cluster:refreshMetadataIfNecessary'),\n  FIND_CONTROLLER_BROKER: Symbol('private:Cluster:findControllerBroker'),\n}\n\nmodule.exports = class Cluster {\n  /**\n   * @param {Object} options\n   * @param {Array<string>} options.brokers example: ['127.0.0.1:9092', '127.0.0.1:9094']\n   * @param {Object} options.ssl\n   * @param {Object} options.sasl\n   * @param {string} options.clientId\n   * @param {number} options.connectionTimeout - in milliseconds\n   * @param {number} options.authenticationTimeout - in milliseconds\n   * @param {number} options.reauthenticationThreshold - in milliseconds\n   * @param {number} [options.requestTimeout=30000] - in milliseconds\n   * @param {boolean} [options.enforceRequestTimeout]\n   * @param {number} options.metadataMaxAge - in milliseconds\n   * @param {boolean} options.allowAutoTopicCreation\n   * @param {number} options.maxInFlightRequests\n   * @param {number} options.isolationLevel\n   * @param {import(\"../../types\").RetryOptions} options.retry\n   * @param {import(\"../../types\").Logger} options.logger\n   * @param {import(\"../../types\").ISocketFactory} options.socketFactory\n   * @param {Map} [options.offsets]\n   * @param {import(\"../instrumentation/emitter\")} [options.instrumentationEmitter=null]\n   */\n  constructor({\n    logger: rootLogger,\n    socketFactory,\n    brokers,\n    ssl,\n    sasl,\n    clientId,\n    connectionTimeout,\n    authenticationTimeout,\n    reauthenticationThreshold,\n    requestTimeout = 30000,\n    enforceRequestTimeout,\n    metadataMaxAge,\n    retry,\n    allowAutoTopicCreation,\n    maxInFlightRequests,\n    isolationLevel,\n    instrumentationEmitter = null,\n    offsets = new Map(),\n  }) {\n    this.rootLogger = rootLogger\n    this.logger = rootLogger.namespace('Cluster')\n    this.retrier = createRetry(retry)\n    this.connectionPoolBuilder = connectionPoolBuilder({\n      logger: rootLogger,\n      instrumentationEmitter,\n      socketFactory,\n      brokers,\n      ssl,\n      sasl,\n      clientId,\n      connectionTimeout,\n      requestTimeout,\n      enforceRequestTimeout,\n      maxInFlightRequests,\n      reauthenticationThreshold,\n    })\n\n    this.targetTopics = new Set()\n    this.mutatingTargetTopics = new Lock({\n      description: `updating target topics`,\n      timeout: requestTimeout,\n    })\n    this.isolationLevel = isolationLevel\n    this.brokerPool = new BrokerPool({\n      connectionPoolBuilder: this.connectionPoolBuilder,\n      logger: this.rootLogger,\n      retry,\n      allowAutoTopicCreation,\n      authenticationTimeout,\n      metadataMaxAge,\n    })\n    this.committedOffsetsByGroup = offsets\n\n    this[PRIVATE.CONNECT] = sharedPromiseTo(async () => {\n      return await this.brokerPool.connect()\n    })\n\n    this[PRIVATE.REFRESH_METADATA] = sharedPromiseTo(async () => {\n      return await this.brokerPool.refreshMetadata(Array.from(this.targetTopics))\n    })\n\n    this[PRIVATE.REFRESH_METADATA_IF_NECESSARY] = sharedPromiseTo(async () => {\n      return await this.brokerPool.refreshMetadataIfNecessary(Array.from(this.targetTopics))\n    })\n\n    this[PRIVATE.FIND_CONTROLLER_BROKER] = sharedPromiseTo(async () => {\n      const { metadata } = this.brokerPool\n\n      if (!metadata || metadata.controllerId == null) {\n        throw new KafkaJSMetadataNotLoaded('Topic metadata not loaded')\n      }\n\n      const broker = await this.findBroker({ nodeId: metadata.controllerId })\n\n      if (!broker) {\n        throw new KafkaJSBrokerNotFound(\n          `Controller broker with id ${metadata.controllerId} not found in the cached metadata`\n        )\n      }\n\n      return broker\n    })\n  }\n\n  isConnected() {\n    return this.brokerPool.hasConnectedBrokers()\n  }\n\n  /**\n   * @public\n   * @returns {Promise<void>}\n   */\n  async connect() {\n    await this[PRIVATE.CONNECT]()\n  }\n\n  /**\n   * @public\n   * @returns {Promise<void>}\n   */\n  async disconnect() {\n    await this.brokerPool.disconnect()\n  }\n\n  /**\n   * @public\n   * @param {object} destination\n   * @param {String} destination.host\n   * @param {Number} destination.port\n   */\n  removeBroker({ host, port }) {\n    this.brokerPool.removeBroker({ host, port })\n  }\n\n  /**\n   * @public\n   * @returns {Promise<void>}\n   */\n  async refreshMetadata() {\n    await this[PRIVATE.REFRESH_METADATA]()\n  }\n\n  /**\n   * @public\n   * @returns {Promise<void>}\n   */\n  async refreshMetadataIfNecessary() {\n    await this[PRIVATE.REFRESH_METADATA_IF_NECESSARY]()\n  }\n\n  /**\n   * @public\n   * @returns {Promise<import(\"../../types\").BrokerMetadata>}\n   */\n  async metadata({ topics = [] } = {}) {\n    return this.retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await this.brokerPool.refreshMetadataIfNecessary(topics)\n        return this.brokerPool.withBroker(async ({ broker }) => broker.metadata(topics))\n      } catch (e) {\n        if (e.type === 'LEADER_NOT_AVAILABLE') {\n          throw e\n        }\n\n        bail(e)\n      }\n    })\n  }\n\n  /**\n   * @public\n   * @param {string} topic\n   * @return {Promise}\n   */\n  async addTargetTopic(topic) {\n    return this.addMultipleTargetTopics([topic])\n  }\n\n  /**\n   * @public\n   * @param {string[]} topics\n   * @return {Promise}\n   */\n  async addMultipleTargetTopics(topics) {\n    await this.mutatingTargetTopics.acquire()\n\n    try {\n      const previousSize = this.targetTopics.size\n      const previousTopics = new Set(this.targetTopics)\n      for (const topic of topics) {\n        this.targetTopics.add(topic)\n      }\n\n      const hasChanged = previousSize !== this.targetTopics.size || !this.brokerPool.metadata\n\n      if (hasChanged) {\n        try {\n          await this.refreshMetadata()\n        } catch (e) {\n          if (\n            e.type === 'INVALID_TOPIC_EXCEPTION' ||\n            e.type === 'UNKNOWN_TOPIC_OR_PARTITION' ||\n            e.type === 'TOPIC_AUTHORIZATION_FAILED'\n          ) {\n            this.targetTopics = previousTopics\n          }\n\n          throw e\n        }\n      }\n    } finally {\n      await this.mutatingTargetTopics.release()\n    }\n  }\n\n  /** @type {() => string[]} */\n  getNodeIds() {\n    return this.brokerPool.getNodeIds()\n  }\n\n  /**\n   * @public\n   * @param {object} options\n   * @param {string} options.nodeId\n   * @returns {Promise<import(\"../../types\").Broker>}\n   */\n  async findBroker({ nodeId }) {\n    try {\n      return await this.brokerPool.findBroker({ nodeId })\n    } catch (e) {\n      // The client probably has stale metadata\n      if (\n        e.name === 'KafkaJSBrokerNotFound' ||\n        e.name === 'KafkaJSLockTimeout' ||\n        e.name === 'KafkaJSConnectionError'\n      ) {\n        await this.refreshMetadata()\n      }\n\n      throw e\n    }\n  }\n\n  /**\n   * @public\n   * @returns {Promise<import(\"../../types\").Broker>}\n   */\n  async findControllerBroker() {\n    return await this[PRIVATE.FIND_CONTROLLER_BROKER]()\n  }\n\n  /**\n   * @public\n   * @param {string} topic\n   * @returns {import(\"../../types\").PartitionMetadata[]} Example:\n   *                   [{\n   *                     isr: [2],\n   *                     leader: 2,\n   *                     partitionErrorCode: 0,\n   *                     partitionId: 0,\n   *                     replicas: [2],\n   *                   }]\n   */\n  findTopicPartitionMetadata(topic) {\n    const { metadata } = this.brokerPool\n    if (!metadata || !metadata.topicMetadata) {\n      throw new KafkaJSTopicMetadataNotLoaded('Topic metadata not loaded', { topic })\n    }\n\n    const topicMetadata = metadata.topicMetadata.find(t => t.topic === topic)\n    return topicMetadata ? topicMetadata.partitionMetadata : []\n  }\n\n  /**\n   * @public\n   * @param {string} topic\n   * @param {(number|string)[]} partitions\n   * @returns {Object} Object with leader and partitions. For partitions 0 and 5\n   *                   the result could be:\n   *                     { '0': [0], '2': [5] }\n   *\n   *                   where the key is the nodeId.\n   */\n  findLeaderForPartitions(topic, partitions) {\n    const partitionMetadata = this.findTopicPartitionMetadata(topic)\n    return partitions.reduce((result, id) => {\n      const partitionId = parseInt(id, 10)\n      const metadata = partitionMetadata.find(p => p.partitionId === partitionId)\n\n      if (!metadata) {\n        return result\n      }\n\n      if (metadata.leader === null || metadata.leader === undefined) {\n        throw new KafkaJSError('Invalid partition metadata', { topic, partitionId, metadata })\n      }\n\n      const { leader } = metadata\n      const current = result[leader] || []\n      return { ...result, [leader]: [...current, partitionId] }\n    }, {})\n  }\n\n  /**\n   * @public\n   * @param {object} params\n   * @param {string} params.groupId\n   * @param {import(\"../protocol/coordinatorTypes\").CoordinatorType} [params.coordinatorType=0]\n   * @returns {Promise<import(\"../../types\").Broker>}\n   */\n  async findGroupCoordinator({ groupId, coordinatorType = COORDINATOR_TYPES.GROUP }) {\n    return this.retrier(async (bail, retryCount, retryTime) => {\n      try {\n        const { coordinator } = await this.findGroupCoordinatorMetadata({\n          groupId,\n          coordinatorType,\n        })\n        return await this.findBroker({ nodeId: coordinator.nodeId })\n      } catch (e) {\n        // A new broker can join the cluster before we have the chance\n        // to refresh metadata\n        if (e.name === 'KafkaJSBrokerNotFound' || e.type === 'GROUP_COORDINATOR_NOT_AVAILABLE') {\n          this.logger.debug(`${e.message}, refreshing metadata and trying again...`, {\n            groupId,\n            retryCount,\n            retryTime,\n          })\n\n          await this.refreshMetadata()\n          throw e\n        }\n\n        if (e.code === 'ECONNREFUSED') {\n          // During maintenance the current coordinator can go down; findBroker will\n          // refresh metadata and re-throw the error. findGroupCoordinator has to re-throw\n          // the error to go through the retry cycle.\n          throw e\n        }\n\n        bail(e)\n      }\n    })\n  }\n\n  /**\n   * @public\n   * @param {object} params\n   * @param {string} params.groupId\n   * @param {import(\"../protocol/coordinatorTypes\").CoordinatorType} [params.coordinatorType=0]\n   * @returns {Promise<Object>}\n   */\n  async findGroupCoordinatorMetadata({ groupId, coordinatorType }) {\n    const brokerMetadata = await this.brokerPool.withBroker(async ({ nodeId, broker }) => {\n      return await this.retrier(async (bail, retryCount, retryTime) => {\n        try {\n          const brokerMetadata = await broker.findGroupCoordinator({ groupId, coordinatorType })\n          this.logger.debug('Found group coordinator', {\n            broker: brokerMetadata.host,\n            nodeId: brokerMetadata.coordinator.nodeId,\n          })\n          return brokerMetadata\n        } catch (e) {\n          this.logger.debug('Tried to find group coordinator', {\n            nodeId,\n            groupId,\n            error: e,\n          })\n\n          if (e.type === 'GROUP_COORDINATOR_NOT_AVAILABLE') {\n            this.logger.debug('Group coordinator not available, retrying...', {\n              nodeId,\n              retryCount,\n              retryTime,\n            })\n\n            throw e\n          }\n\n          bail(e)\n        }\n      })\n    })\n\n    if (brokerMetadata) {\n      return brokerMetadata\n    }\n\n    throw new KafkaJSGroupCoordinatorNotFound('Failed to find group coordinator')\n  }\n\n  /**\n   * @param {object} topicConfiguration\n   * @returns {number}\n   */\n  defaultOffset({ fromBeginning }) {\n    return fromBeginning ? EARLIEST_OFFSET : LATEST_OFFSET\n  }\n\n  /**\n   * @public\n   * @param {Array<Object>} topics\n   *                          [\n   *                            {\n   *                              topic: 'my-topic-name',\n   *                              partitions: [{ partition: 0 }],\n   *                              fromBeginning: false\n   *                            }\n   *                          ]\n   * @returns {Promise<import(\"../../types\").TopicOffsets[]>} example:\n   *                          [\n   *                            {\n   *                              topic: 'my-topic-name',\n   *                              partitions: [\n   *                                { partition: 0, offset: '1' },\n   *                                { partition: 1, offset: '2' },\n   *                                { partition: 2, offset: '1' },\n   *                              ],\n   *                            },\n   *                          ]\n   */\n  async fetchTopicsOffset(topics) {\n    const partitionsPerBroker = {}\n    const topicConfigurations = {}\n\n    const addDefaultOffset = topic => partition => {\n      const { timestamp } = topicConfigurations[topic]\n      return { ...partition, timestamp }\n    }\n\n    // Index all topics and partitions per leader (nodeId)\n    for (const topicData of topics) {\n      const { topic, partitions, fromBeginning, fromTimestamp } = topicData\n      const partitionsPerLeader = this.findLeaderForPartitions(\n        topic,\n        partitions.map(p => p.partition)\n      )\n      const timestamp =\n        fromTimestamp != null ? fromTimestamp : this.defaultOffset({ fromBeginning })\n\n      topicConfigurations[topic] = { timestamp }\n\n      keys(partitionsPerLeader).forEach(nodeId => {\n        partitionsPerBroker[nodeId] = partitionsPerBroker[nodeId] || {}\n        partitionsPerBroker[nodeId][topic] = partitions.filter(p =>\n          partitionsPerLeader[nodeId].includes(p.partition)\n        )\n      })\n    }\n\n    // Create a list of requests to fetch the offset of all partitions\n    const requests = keys(partitionsPerBroker).map(async nodeId => {\n      const broker = await this.findBroker({ nodeId })\n      const partitions = partitionsPerBroker[nodeId]\n\n      const { responses: topicOffsets } = await broker.listOffsets({\n        isolationLevel: this.isolationLevel,\n        topics: keys(partitions).map(topic => ({\n          topic,\n          partitions: partitions[topic].map(addDefaultOffset(topic)),\n        })),\n      })\n\n      return topicOffsets\n    })\n\n    // Execute all requests, merge and normalize the responses\n    const responses = await Promise.all(requests)\n    const partitionsPerTopic = responses.flat().reduce(mergeTopics, {})\n\n    return keys(partitionsPerTopic).map(topic => ({\n      topic,\n      partitions: partitionsPerTopic[topic].map(({ partition, offset }) => ({\n        partition,\n        offset,\n      })),\n    }))\n  }\n\n  /**\n   * Retrieve the object mapping for committed offsets for a single consumer group\n   * @param {object} options\n   * @param {string} options.groupId\n   * @returns {Object}\n   */\n  committedOffsets({ groupId }) {\n    if (!this.committedOffsetsByGroup.has(groupId)) {\n      this.committedOffsetsByGroup.set(groupId, {})\n    }\n\n    return this.committedOffsetsByGroup.get(groupId)\n  }\n\n  /**\n   * Mark offset as committed for a single consumer group's topic-partition\n   * @param {object} options\n   * @param {string} options.groupId\n   * @param {string} options.topic\n   * @param {string|number} options.partition\n   * @param {string} options.offset\n   */\n  markOffsetAsCommitted({ groupId, topic, partition, offset }) {\n    const committedOffsets = this.committedOffsets({ groupId })\n\n    committedOffsets[topic] = committedOffsets[topic] || {}\n    committedOffsets[topic][partition] = offset\n  }\n}\n", "/* eslint-disable */\nconst Long = require('../../../utils/long')\n\n// Based on the kafka client 0.10.2 murmur2 implementation\n// https://github.com/apache/kafka/blob/0.10.2/clients/src/main/java/org/apache/kafka/common/utils/Utils.java#L364\n\nconst SEED = Long.fromValue(0x9747b28c)\n\n// 'm' and 'r' are mixing constants generated offline.\n// They're not really 'magic', they just happen to work well.\nconst M = Long.fromValue(0x5bd1e995)\nconst R = Long.fromValue(24)\n\nmodule.exports = key => {\n  const data = Buffer.isBuffer(key) ? key : Buffer.from(String(key))\n  const length = data.length\n\n  // Initialize the hash to a random value\n  let h = Long.fromValue(SEED.xor(length))\n  let length4 = Math.floor(length / 4)\n\n  for (let i = 0; i < length4; i++) {\n    const i4 = i * 4\n    let k =\n      (data[i4 + 0] & 0xff) +\n      ((data[i4 + 1] & 0xff) << 8) +\n      ((data[i4 + 2] & 0xff) << 16) +\n      ((data[i4 + 3] & 0xff) << 24)\n    k = Long.fromValue(k)\n    k = k.multiply(M)\n    k = k.xor(k.toInt() >>> R)\n    k = Long.fromValue(k).multiply(M)\n    h = h.multiply(M)\n    h = h.xor(k)\n  }\n\n  // Handle the last few bytes of the input array\n  switch (length % 4) {\n    case 3:\n      h = h.xor((data[(length & ~3) + 2] & 0xff) << 16)\n    case 2:\n      h = h.xor((data[(length & ~3) + 1] & 0xff) << 8)\n    case 1:\n      h = h.xor(data[length & ~3] & 0xff)\n      h = h.multiply(M)\n  }\n\n  h = h.xor(h.toInt() >>> 13)\n  h = h.multiply(M)\n  h = h.xor(h.toInt() >>> 15)\n\n  return h.toInt()\n}\n", "const { KafkaJSNonRetriableError } = require('../../../errors')\n\nconst toNodeCompatible = crypto => ({\n  randomBytes: size => crypto.getRandomValues(Buffer.allocUnsafe(size)),\n})\n\nlet cryptoImplementation = null\nif (global && global.crypto) {\n  cryptoImplementation =\n    global.crypto.randomBytes === undefined ? toNodeCompatible(global.crypto) : global.crypto\n} else if (global && global.msCrypto) {\n  cryptoImplementation = toNodeCompatible(global.msCrypto)\n} else if (global && !global.crypto) {\n  cryptoImplementation = require('crypto')\n}\n\nconst MAX_BYTES = 65536\n\nmodule.exports = size => {\n  if (size > MAX_BYTES) {\n    throw new KafkaJSNonRetriableError(\n      `Byte length (${size}) exceeds the max number of bytes of entropy available (${MAX_BYTES})`\n    )\n  }\n\n  if (!cryptoImplementation) {\n    throw new KafkaJSNonRetriableError('No available crypto implementation')\n  }\n\n  return cryptoImplementation.randomBytes(size)\n}\n", "const randomBytes = require('./randomBytes')\n\n// Based on the java client 0.10.2\n// https://github.com/apache/kafka/blob/0.10.2/clients/src/main/java/org/apache/kafka/clients/producer/internals/DefaultPartitioner.java\n\n/**\n * A cheap way to deterministically convert a number to a positive value. When the input is\n * positive, the original value is returned. When the input number is negative, the returned\n * positive value is the original value bit AND against 0x7fffffff which is not its absolutely\n * value.\n */\nconst toPositive = x => x & 0x7fffffff\n\n/**\n * The default partitioning strategy:\n *  - If a partition is specified in the message, use it\n *  - If no partition is specified but a key is present choose a partition based on a hash of the key\n *  - If no partition or key is present choose a partition in a round-robin fashion\n */\nmodule.exports = murmur2 => () => {\n  const counters = {}\n\n  return ({ topic, partitionMetadata, message }) => {\n    if (!(topic in counters)) {\n      counters[topic] = randomBytes(32).readUInt32BE(0)\n    }\n    const numPartitions = partitionMetadata.length\n    const availablePartitions = partitionMetadata.filter(p => p.leader >= 0)\n    const numAvailablePartitions = availablePartitions.length\n\n    if (message.partition !== null && message.partition !== undefined) {\n      return message.partition\n    }\n\n    if (message.key !== null && message.key !== undefined) {\n      return toPositive(murmur2(message.key)) % numPartitions\n    }\n\n    if (numAvailablePartitions > 0) {\n      const i = toPositive(++counters[topic]) % numAvailablePartitions\n      return availablePartitions[i].partitionId\n    }\n\n    // no partitions are available, give a non-available partition\n    return toPositive(++counters[topic]) % numPartitions\n  }\n}\n", "const murmur2 = require('./murmur2')\nconst createDefaultPartitioner = require('../legacy/partitioner')\n\nmodule.exports = createDefaultPartitioner(murmur2)\n", "/* eslint-disable */\n\n// Based on the kafka client 0.10.2 murmur2 implementation\n// https://github.com/apache/kafka/blob/0.10.2/clients/src/main/java/org/apache/kafka/common/utils/Utils.java#L364\n\nconst SEED = 0x9747b28c\n\n// 'm' and 'r' are mixing constants generated offline.\n// They're not really 'magic', they just happen to work well.\nconst M = 0x5bd1e995\nconst R = 24\n\nmodule.exports = key => {\n  const data = Buffer.isBuffer(key) ? key : Buffer.from(String(key))\n  const length = data.length\n\n  // Initialize the hash to a random value\n  let h = SEED ^ length\n  let length4 = length / 4\n\n  for (let i = 0; i < length4; i++) {\n    const i4 = i * 4\n    let k =\n      (data[i4 + 0] & 0xff) +\n      ((data[i4 + 1] & 0xff) << 8) +\n      ((data[i4 + 2] & 0xff) << 16) +\n      ((data[i4 + 3] & 0xff) << 24)\n    k *= M\n    k ^= k >>> R\n    k *= M\n    h *= M\n    h ^= k\n  }\n\n  // Handle the last few bytes of the input array\n  switch (length % 4) {\n    case 3:\n      h ^= (data[(length & ~3) + 2] & 0xff) << 16\n    case 2:\n      h ^= (data[(length & ~3) + 1] & 0xff) << 8\n    case 1:\n      h ^= data[length & ~3] & 0xff\n      h *= M\n  }\n\n  h ^= h >>> 13\n  h *= M\n  h ^= h >>> 15\n\n  return h\n}\n", "const murmur2 = require('./murmur2')\nconst createLegacyPartitioner = require('./partitioner')\n\nmodule.exports = createLegacyPartitioner(murmur2)\n", "const DefaultPartitioner = require('./default')\nconst LegacyPartitioner = require('./legacy')\n\nmodule.exports = {\n  DefaultPartitioner,\n  LegacyPartitioner,\n  /**\n   * @deprecated Use DefaultPartitioner instead\n   *\n   * The JavaCompatiblePartitioner was renamed DefaultPartitioner\n   * and made to be the default in 2.0.0.\n   */\n  JavaCompatiblePartitioner: DefaultPartitioner,\n}\n", "module.exports = {\n  UNINITIALIZED: 'UNINITIALIZED',\n  READY: 'READY',\n  TRANSACTING: 'TRANSACTING',\n  COMMITTING: 'COMMITTING',\n  ABORTING: 'ABORTING',\n}\n", "const { EventEmitter } = require('events')\nconst { KafkaJSNonRetriableError } = require('../../errors')\nconst STATES = require('./transactionStates')\n\nconst VALID_STATE_TRANSITIONS = {\n  [STATES.UNINITIALIZED]: [STATES.READY],\n  [STATES.READY]: [STATES.READY, STATES.TRANSACTING],\n  [STATES.TRANSACTING]: [STATES.COMMITTING, STATES.ABORTING],\n  [STATES.COMMITTING]: [STATES.READY],\n  [STATES.ABORTING]: [STATES.READY],\n}\n\nmodule.exports = ({ logger, initialState = STATES.UNINITIALIZED }) => {\n  let currentState = initialState\n\n  const guard = (object, method, { legalStates, async: isAsync = true }) => {\n    if (!object[method]) {\n      throw new KafkaJSNonRetriableError(`Cannot add guard on missing method \"${method}\"`)\n    }\n\n    return (...args) => {\n      const fn = object[method]\n\n      if (!legalStates.includes(currentState)) {\n        const error = new KafkaJSNonRetriableError(\n          `Transaction state exception: Cannot call \"${method}\" in state \"${currentState}\"`\n        )\n\n        if (isAsync) {\n          return Promise.reject(error)\n        } else {\n          throw error\n        }\n      }\n\n      return fn.apply(object, args)\n    }\n  }\n\n  const stateMachine = Object.assign(new EventEmitter(), {\n    /**\n     * Create a clone of \"object\" where we ensure state machine is in correct state\n     * prior to calling any of the configured methods\n     * @param {Object} object The object whose methods we will guard\n     * @param {Object} methodStateMapping Keys are method names on \"object\"\n     * @param {string[]} methodStateMapping.legalStates Legal states for this method\n     * @param {boolean=true} methodStateMapping.async Whether this method is async (throw vs reject)\n     */\n    createGuarded(object, methodStateMapping) {\n      const guardedMethods = Object.keys(methodStateMapping).reduce((guards, method) => {\n        guards[method] = guard(object, method, methodStateMapping[method])\n        return guards\n      }, {})\n\n      return { ...object, ...guardedMethods }\n    },\n    /**\n     * Transition safely to a new state\n     */\n    transitionTo(state) {\n      logger.debug(`Transaction state transition ${currentState} --> ${state}`)\n\n      if (!VALID_STATE_TRANSITIONS[currentState].includes(state)) {\n        throw new KafkaJSNonRetriableError(\n          `Transaction state exception: Invalid transition ${currentState} --> ${state}`\n        )\n      }\n\n      stateMachine.emit('transition', { to: state, from: currentState })\n      currentState = state\n    },\n\n    state() {\n      return currentState\n    },\n  })\n\n  return stateMachine\n}\n", "const createRetry = require('../../retry')\nconst Lock = require('../../utils/lock')\nconst { KafkaJSNonRetriableError } = require('../../errors')\nconst COORDINATOR_TYPES = require('../../protocol/coordinatorTypes')\nconst createStateMachine = require('./transactionStateMachine')\nconst { INT_32_MAX_VALUE } = require('../../constants')\nconst assert = require('assert')\n\nconst STATES = require('./transactionStates')\nconst NO_PRODUCER_ID = -1\nconst SEQUENCE_START = 0\nconst INIT_PRODUCER_RETRIABLE_PROTOCOL_ERRORS = [\n  'NOT_COORDINATOR_FOR_GROUP',\n  'GROUP_COORDINATOR_NOT_AVAILABLE',\n  'GROUP_LOAD_IN_PROGRESS',\n  /**\n   * The producer might have crashed and never committed the transaction; retry the\n   * request so Kafka can abort the current transaction\n   * @see https://github.com/apache/kafka/blob/201da0542726472d954080d54bc585b111aaf86f/clients/src/main/java/org/apache/kafka/clients/producer/internals/TransactionManager.java#L1001-L1002\n   */\n  'CONCURRENT_TRANSACTIONS',\n]\nconst COMMIT_RETRIABLE_PROTOCOL_ERRORS = [\n  'UNKNOWN_TOPIC_OR_PARTITION',\n  'COORDINATOR_LOAD_IN_PROGRESS',\n]\nconst COMMIT_STALE_COORDINATOR_PROTOCOL_ERRORS = ['COORDINATOR_NOT_AVAILABLE', 'NOT_COORDINATOR']\n\n/**\n * @typedef {Object} EosManager\n */\n\n/**\n * Manage behavior for an idempotent producer and transactions.\n *\n * @returns {EosManager}\n */\nmodule.exports = ({\n  logger,\n  cluster,\n  transactionTimeout = 60000,\n  transactional,\n  transactionalId,\n}) => {\n  if (transactional && !transactionalId) {\n    throw new KafkaJSNonRetriableError('Cannot manage transactions without a transactionalId')\n  }\n\n  const retrier = createRetry(cluster.retry)\n\n  /**\n   * Current producer ID\n   */\n  let producerId = NO_PRODUCER_ID\n\n  /**\n   * Current producer epoch\n   */\n  let producerEpoch = 0\n\n  /**\n   * Idempotent production requires that the producer track the sequence number of messages.\n   *\n   * Sequences are sent with every Record Batch and tracked per Topic-Partition\n   */\n  let producerSequence = {}\n\n  /**\n   * Idempotent production requires a mutex lock per broker to serialize requests with sequence number handling\n   */\n  let brokerMutexLocks = {}\n\n  /**\n   * Topic partitions already participating in the transaction\n   */\n  let transactionTopicPartitions = {}\n\n  /**\n   * Offsets have been added to the transaction\n   */\n  let hasOffsetsAddedToTransaction = false\n\n  const stateMachine = createStateMachine({ logger })\n  stateMachine.on('transition', ({ to }) => {\n    if (to === STATES.READY) {\n      transactionTopicPartitions = {}\n      hasOffsetsAddedToTransaction = false\n    }\n  })\n\n  const findTransactionCoordinator = () => {\n    return cluster.findGroupCoordinator({\n      groupId: transactionalId,\n      coordinatorType: COORDINATOR_TYPES.TRANSACTION,\n    })\n  }\n\n  const transactionalGuard = () => {\n    if (!transactional) {\n      throw new KafkaJSNonRetriableError('Method unavailable if non-transactional')\n    }\n  }\n\n  /**\n   * A transaction is ongoing when offsets or partitions added to it\n   *\n   * @returns {boolean}\n   */\n  const isOngoing = () => {\n    return (\n      hasOffsetsAddedToTransaction ||\n      Object.entries(transactionTopicPartitions).some(([, partitions]) => {\n        return Object.entries(partitions).some(\n          ([, isPartitionAddedToTransaction]) => isPartitionAddedToTransaction\n        )\n      })\n    )\n  }\n\n  const eosManager = stateMachine.createGuarded(\n    {\n      /**\n       * Get the current producer id\n       * @returns {number}\n       */\n      getProducerId() {\n        return producerId\n      },\n\n      /**\n       * Get the current producer epoch\n       * @returns {number}\n       */\n      getProducerEpoch() {\n        return producerEpoch\n      },\n\n      getTransactionalId() {\n        return transactionalId\n      },\n\n      /**\n       * Initialize the idempotent producer by making an `InitProducerId` request.\n       * Overwrites any existing state in this transaction manager\n       */\n      async initProducerId() {\n        return retrier(async (bail, retryCount, retryTime) => {\n          try {\n            await cluster.refreshMetadataIfNecessary()\n\n            // If non-transactional we can request the PID from any broker\n            const broker = await (transactional\n              ? findTransactionCoordinator()\n              : cluster.findControllerBroker())\n\n            const result = await broker.initProducerId({\n              transactionalId: transactional ? transactionalId : undefined,\n              transactionTimeout,\n            })\n\n            stateMachine.transitionTo(STATES.READY)\n            producerId = result.producerId\n            producerEpoch = result.producerEpoch\n            producerSequence = {}\n            brokerMutexLocks = {}\n\n            logger.debug('Initialized producer id & epoch', { producerId, producerEpoch })\n          } catch (e) {\n            if (INIT_PRODUCER_RETRIABLE_PROTOCOL_ERRORS.includes(e.type)) {\n              if (e.type === 'CONCURRENT_TRANSACTIONS') {\n                logger.debug('There is an ongoing transaction on this transactionId, retrying', {\n                  error: e.message,\n                  stack: e.stack,\n                  transactionalId,\n                  retryCount,\n                  retryTime,\n                })\n              }\n\n              throw e\n            }\n\n            bail(e)\n          }\n        })\n      },\n\n      /**\n       * Get the current sequence for a given Topic-Partition. Defaults to 0.\n       *\n       * @param {string} topic\n       * @param {string} partition\n       * @returns {number}\n       */\n      getSequence(topic, partition) {\n        if (!eosManager.isInitialized()) {\n          return SEQUENCE_START\n        }\n\n        producerSequence[topic] = producerSequence[topic] || {}\n        producerSequence[topic][partition] = producerSequence[topic][partition] || SEQUENCE_START\n\n        return producerSequence[topic][partition]\n      },\n\n      /**\n       * Update the sequence for a given Topic-Partition.\n       *\n       * Do nothing if not yet initialized (not idempotent)\n       * @param {string} topic\n       * @param {string} partition\n       * @param {number} increment\n       */\n      updateSequence(topic, partition, increment) {\n        if (!eosManager.isInitialized()) {\n          return\n        }\n\n        const previous = eosManager.getSequence(topic, partition)\n        let sequence = previous + increment\n\n        // Sequence is defined as Int32 in the Record Batch,\n        // so theoretically should need to rotate here\n        if (sequence >= INT_32_MAX_VALUE) {\n          logger.debug(\n            `Sequence for ${topic} ${partition} exceeds max value (${sequence}). Rotating to 0.`\n          )\n          sequence = 0\n        }\n\n        producerSequence[topic][partition] = sequence\n      },\n\n      /**\n       * Begin a transaction\n       */\n      beginTransaction() {\n        transactionalGuard()\n        stateMachine.transitionTo(STATES.TRANSACTING)\n      },\n\n      /**\n       * Add partitions to a transaction if they are not already marked as participating.\n       *\n       * Should be called prior to sending any messages during a transaction\n       * @param {TopicData[]} topicData\n       *\n       * @typedef {Object} TopicData\n       * @property {string} topic\n       * @property {object[]} partitions\n       * @property {number} partitions[].partition\n       */\n      async addPartitionsToTransaction(topicData) {\n        transactionalGuard()\n        const newTopicPartitions = {}\n\n        topicData.forEach(({ topic, partitions }) => {\n          transactionTopicPartitions[topic] = transactionTopicPartitions[topic] || {}\n\n          partitions.forEach(({ partition }) => {\n            if (!transactionTopicPartitions[topic][partition]) {\n              newTopicPartitions[topic] = newTopicPartitions[topic] || []\n              newTopicPartitions[topic].push(partition)\n            }\n          })\n        })\n\n        const topics = Object.keys(newTopicPartitions).map(topic => ({\n          topic,\n          partitions: newTopicPartitions[topic],\n        }))\n\n        if (topics.length) {\n          const broker = await findTransactionCoordinator()\n          await broker.addPartitionsToTxn({ transactionalId, producerId, producerEpoch, topics })\n        }\n\n        topics.forEach(({ topic, partitions }) => {\n          partitions.forEach(partition => {\n            transactionTopicPartitions[topic][partition] = true\n          })\n        })\n      },\n\n      /**\n       * Commit the ongoing transaction\n       */\n      async commit() {\n        transactionalGuard()\n        stateMachine.transitionTo(STATES.COMMITTING)\n\n        if (!isOngoing()) {\n          logger.debug('No partitions or offsets registered, not sending EndTxn')\n\n          stateMachine.transitionTo(STATES.READY)\n          return\n        }\n\n        const broker = await findTransactionCoordinator()\n        await broker.endTxn({\n          producerId,\n          producerEpoch,\n          transactionalId,\n          transactionResult: true,\n        })\n\n        stateMachine.transitionTo(STATES.READY)\n      },\n\n      /**\n       * Abort the ongoing transaction\n       */\n      async abort() {\n        transactionalGuard()\n        stateMachine.transitionTo(STATES.ABORTING)\n\n        if (!isOngoing()) {\n          logger.debug('No partitions or offsets registered, not sending EndTxn')\n\n          stateMachine.transitionTo(STATES.READY)\n          return\n        }\n\n        const broker = await findTransactionCoordinator()\n        await broker.endTxn({\n          producerId,\n          producerEpoch,\n          transactionalId,\n          transactionResult: false,\n        })\n\n        stateMachine.transitionTo(STATES.READY)\n      },\n\n      /**\n       * Whether the producer id has already been initialized\n       */\n      isInitialized() {\n        return producerId !== NO_PRODUCER_ID\n      },\n\n      isTransactional() {\n        return transactional\n      },\n\n      isInTransaction() {\n        return stateMachine.state() === STATES.TRANSACTING\n      },\n\n      async acquireBrokerLock(broker) {\n        if (this.isInitialized()) {\n          brokerMutexLocks[broker.nodeId] =\n            brokerMutexLocks[broker.nodeId] || new Lock({ timeout: 0xffff })\n          await brokerMutexLocks[broker.nodeId].acquire()\n        }\n      },\n\n      releaseBrokerLock(broker) {\n        if (this.isInitialized()) brokerMutexLocks[broker.nodeId].release()\n      },\n\n      /**\n       * Mark the provided offsets as participating in the transaction for the given consumer group.\n       *\n       * This allows us to commit an offset as consumed only if the transaction passes.\n       * @param {string} consumerGroupId The unique group identifier\n       * @param {OffsetCommitTopic[]} topics The unique group identifier\n       * @returns {Promise}\n       *\n       * @typedef {Object} OffsetCommitTopic\n       * @property {string} topic\n       * @property {OffsetCommitTopicPartition[]} partitions\n       *\n       * @typedef {Object} OffsetCommitTopicPartition\n       * @property {number} partition\n       * @property {number} offset\n       */\n      async sendOffsets({ consumerGroupId, topics }) {\n        assert(consumerGroupId, 'Missing consumerGroupId')\n        assert(topics, 'Missing offset topics')\n\n        const transactionCoordinator = await findTransactionCoordinator()\n\n        // Do we need to add offsets if we've already done so for this consumer group?\n        await transactionCoordinator.addOffsetsToTxn({\n          transactionalId,\n          producerId,\n          producerEpoch,\n          groupId: consumerGroupId,\n        })\n\n        hasOffsetsAddedToTransaction = true\n\n        let groupCoordinator = await cluster.findGroupCoordinator({\n          groupId: consumerGroupId,\n          coordinatorType: COORDINATOR_TYPES.GROUP,\n        })\n\n        return retrier(async (bail, retryCount, retryTime) => {\n          try {\n            await groupCoordinator.txnOffsetCommit({\n              transactionalId,\n              producerId,\n              producerEpoch,\n              groupId: consumerGroupId,\n              topics,\n            })\n          } catch (e) {\n            if (COMMIT_RETRIABLE_PROTOCOL_ERRORS.includes(e.type)) {\n              logger.debug('Group coordinator is not ready yet, retrying', {\n                error: e.message,\n                stack: e.stack,\n                transactionalId,\n                retryCount,\n                retryTime,\n              })\n\n              throw e\n            }\n\n            if (\n              COMMIT_STALE_COORDINATOR_PROTOCOL_ERRORS.includes(e.type) ||\n              e.code === 'ECONNREFUSED'\n            ) {\n              logger.debug(\n                'Invalid group coordinator, finding new group coordinator and retrying',\n                {\n                  error: e.message,\n                  stack: e.stack,\n                  transactionalId,\n                  retryCount,\n                  retryTime,\n                }\n              )\n\n              groupCoordinator = await cluster.findGroupCoordinator({\n                groupId: consumerGroupId,\n                coordinatorType: COORDINATOR_TYPES.GROUP,\n              })\n\n              throw e\n            }\n\n            bail(e)\n          }\n        })\n      },\n    },\n\n    /**\n     * Transaction state guards\n     */\n    {\n      initProducerId: { legalStates: [STATES.UNINITIALIZED, STATES.READY] },\n      beginTransaction: { legalStates: [STATES.READY], async: false },\n      addPartitionsToTransaction: { legalStates: [STATES.TRANSACTING] },\n      sendOffsets: { legalStates: [STATES.TRANSACTING] },\n      commit: { legalStates: [STATES.TRANSACTING] },\n      abort: { legalStates: [STATES.TRANSACTING] },\n    }\n  )\n\n  return eosManager\n}\n", "module.exports = ({ topic, partitionMetadata, messages, partitioner }) => {\n  if (partitionMetadata.length === 0) {\n    return {}\n  }\n\n  return messages.reduce((result, message) => {\n    const partition = partitioner({ topic, partitionMetadata, message })\n    const current = result[partition] || []\n    return Object.assign(result, { [partition]: [...current, message] })\n  }, {})\n}\n", "module.exports = topicDataForBroker => {\n  return topicDataForBroker.map(\n    ({ topic, partitions, messagesPerPartition, sequencePerPartition }) => ({\n      topic,\n      partitions: partitions.map(partition => ({\n        partition,\n        messages: messagesPerPartition[partition],\n      })),\n    })\n  )\n}\n", "module.exports = ({ topics }) =>\n  topics.flatMap(({ topicName, partitions }) =>\n    partitions.map(partition => ({ topicName, ...partition }))\n  )\n", "const { KafkaJSMetadataNotLoaded } = require('../errors')\nconst { staleMetadata } = require('../protocol/error')\nconst groupMessagesPerPartition = require('./groupMessagesPerPartition')\nconst createTopicData = require('./createTopicData')\nconst responseSerializer = require('./responseSerializer')\n\nconst { keys } = Object\n\n/**\n * @param {Object} options\n * @param {import(\"../../types\").Logger} options.logger\n * @param {import(\"../../types\").Cluster} options.cluster\n * @param {ReturnType<import(\"../../types\").ICustomPartitioner>} options.partitioner\n * @param {import(\"./eosManager\").EosManager} options.eosManager\n * @param {import(\"../retry\").Retrier} options.retrier\n */\nmodule.exports = ({ logger, cluster, partitioner, eosManager, retrier }) => {\n  return async ({ acks, timeout, compression, topicMessages }) => {\n    /** @type {Map<import(\"../../types\").Broker, any[]>} */\n    const responsePerBroker = new Map()\n\n    /** @param {Map<import(\"../../types\").Broker, any[]>} responsePerBroker */\n    const createProducerRequests = async responsePerBroker => {\n      const topicMetadata = new Map()\n\n      await cluster.refreshMetadataIfNecessary()\n\n      for (const { topic, messages } of topicMessages) {\n        const partitionMetadata = cluster.findTopicPartitionMetadata(topic)\n\n        if (partitionMetadata.length === 0) {\n          logger.debug('Producing to topic without metadata', {\n            topic,\n            targetTopics: Array.from(cluster.targetTopics),\n          })\n\n          throw new KafkaJSMetadataNotLoaded('Producing to topic without metadata')\n        }\n\n        const messagesPerPartition = groupMessagesPerPartition({\n          topic,\n          partitionMetadata,\n          messages,\n          partitioner,\n        })\n\n        const partitions = keys(messagesPerPartition)\n        const partitionsPerLeader = cluster.findLeaderForPartitions(topic, partitions)\n        const leaders = keys(partitionsPerLeader)\n\n        topicMetadata.set(topic, {\n          partitionsPerLeader,\n          messagesPerPartition,\n        })\n\n        for (const nodeId of leaders) {\n          const broker = await cluster.findBroker({ nodeId })\n          if (!responsePerBroker.has(broker)) {\n            responsePerBroker.set(broker, null)\n          }\n        }\n      }\n\n      const brokers = Array.from(responsePerBroker.keys())\n      const brokersWithoutResponse = brokers.filter(broker => !responsePerBroker.get(broker))\n\n      return brokersWithoutResponse.map(async broker => {\n        const entries = Array.from(topicMetadata.entries())\n        const topicDataForBroker = entries\n          .filter(([_, { partitionsPerLeader }]) => !!partitionsPerLeader[broker.nodeId])\n          .map(([topic, { partitionsPerLeader, messagesPerPartition, sequencePerPartition }]) => ({\n            topic,\n            partitions: partitionsPerLeader[broker.nodeId],\n            messagesPerPartition,\n          }))\n\n        const topicData = createTopicData(topicDataForBroker)\n\n        await eosManager.acquireBrokerLock(broker)\n        try {\n          if (eosManager.isTransactional()) {\n            await eosManager.addPartitionsToTransaction(topicData)\n          }\n\n          topicData.forEach(({ topic, partitions }) => {\n            partitions.forEach(entry => {\n              entry['firstSequence'] = eosManager.getSequence(topic, entry.partition)\n              eosManager.updateSequence(topic, entry.partition, entry.messages.length)\n            })\n          })\n\n          let response\n          try {\n            response = await broker.produce({\n              transactionalId: eosManager.isTransactional()\n                ? eosManager.getTransactionalId()\n                : undefined,\n              producerId: eosManager.getProducerId(),\n              producerEpoch: eosManager.getProducerEpoch(),\n              acks,\n              timeout,\n              compression,\n              topicData,\n            })\n          } catch (e) {\n            topicData.forEach(({ topic, partitions }) => {\n              partitions.forEach(entry => {\n                eosManager.updateSequence(topic, entry.partition, -entry.messages.length)\n              })\n            })\n            throw e\n          }\n\n          const expectResponse = acks !== 0\n          const formattedResponse = expectResponse ? responseSerializer(response) : []\n\n          responsePerBroker.set(broker, formattedResponse)\n        } catch (e) {\n          responsePerBroker.delete(broker)\n          throw e\n        } finally {\n          await eosManager.releaseBrokerLock(broker)\n        }\n      })\n    }\n\n    return retrier(async (bail, retryCount, retryTime) => {\n      const topics = topicMessages.map(({ topic }) => topic)\n      await cluster.addMultipleTargetTopics(topics)\n\n      try {\n        const requests = await createProducerRequests(responsePerBroker)\n        await Promise.all(requests)\n        return Array.from(responsePerBroker.values()).flat()\n      } catch (e) {\n        if (e.name === 'KafkaJSConnectionClosedError') {\n          cluster.removeBroker({ host: e.host, port: e.port })\n        }\n\n        if (!cluster.isConnected()) {\n          logger.debug(`Cluster has disconnected, reconnecting: ${e.message}`, {\n            retryCount,\n            retryTime,\n          })\n          await cluster.connect()\n          await cluster.refreshMetadata()\n          throw e\n        }\n\n        // This is necessary in case the metadata is stale and the number of partitions\n        // for this topic has increased in the meantime\n        if (\n          staleMetadata(e) ||\n          e.name === 'KafkaJSMetadataNotLoaded' ||\n          e.name === 'KafkaJSConnectionError' ||\n          e.name === 'KafkaJSConnectionClosedError' ||\n          (e.name === 'KafkaJSProtocolError' && e.retriable)\n        ) {\n          logger.error(`Failed to send messages: ${e.message}`, { retryCount, retryTime })\n          await cluster.refreshMetadata()\n          throw e\n        }\n\n        logger.error(`${e.message}`, { retryCount, retryTime })\n        if (e.retriable) throw e\n        bail(e)\n      }\n    })\n  }\n}\n", "const createSendMessages = require('./sendMessages')\nconst { KafkaJSError, KafkaJSNonRetriableError } = require('../errors')\nconst { CONNECTION_STATUS } = require('../network/connectionStatus')\n\nmodule.exports = ({\n  logger,\n  cluster,\n  partitioner,\n  eosManager,\n  idempotent,\n  retrier,\n  getConnectionStatus,\n}) => {\n  const sendMessages = createSendMessages({\n    logger,\n    cluster,\n    retrier,\n    partitioner,\n    eosManager,\n  })\n\n  const validateConnectionStatus = () => {\n    const connectionStatus = getConnectionStatus()\n\n    switch (connectionStatus) {\n      case CONNECTION_STATUS.DISCONNECTING:\n        throw new KafkaJSNonRetriableError(\n          `The producer is disconnecting; therefore, it can't safely accept messages anymore`\n        )\n      case CONNECTION_STATUS.DISCONNECTED:\n        throw new KafkaJSError('The producer is disconnected')\n    }\n  }\n\n  /**\n   * @typedef {Object} TopicMessages\n   * @property {string} topic\n   * @property {Array} messages An array of objects with \"key\" and \"value\", example:\n   *                         [{ key: 'my-key', value: 'my-value'}]\n   *\n   * @typedef {Object} SendBatchRequest\n   * @property {Array<TopicMessages>} topicMessages\n   * @property {number} [acks=-1] Control the number of required acks.\n   *                           -1 = all replicas must acknowledge\n   *                            0 = no acknowledgments\n   *                            1 = only waits for the leader to acknowledge\n   *\n   * @property {number} [timeout=30000] The time to await a response in ms\n   * @property {Compression.Types} [compression=Compression.Types.None] Compression codec\n   *\n   * @param {SendBatchRequest}\n   * @returns {Promise}\n   */\n  const sendBatch = async ({ acks = -1, timeout, compression, topicMessages = [] }) => {\n    if (topicMessages.some(({ topic }) => !topic)) {\n      throw new KafkaJSNonRetriableError(`Invalid topic`)\n    }\n\n    if (idempotent && acks !== -1) {\n      throw new KafkaJSNonRetriableError(\n        `Not requiring ack for all messages invalidates the idempotent producer's EoS guarantees`\n      )\n    }\n\n    for (const { topic, messages } of topicMessages) {\n      if (!messages) {\n        throw new KafkaJSNonRetriableError(\n          `Invalid messages array [${messages}] for topic \"${topic}\"`\n        )\n      }\n\n      const messageWithoutValue = messages.find(message => message.value === undefined)\n      if (messageWithoutValue) {\n        throw new KafkaJSNonRetriableError(\n          `Invalid message without value for topic \"${topic}\": ${JSON.stringify(\n            messageWithoutValue\n          )}`\n        )\n      }\n    }\n\n    validateConnectionStatus()\n    const mergedTopicMessages = topicMessages.reduce((merged, { topic, messages }) => {\n      const index = merged.findIndex(({ topic: mergedTopic }) => topic === mergedTopic)\n\n      if (index === -1) {\n        merged.push({ topic, messages })\n      } else {\n        merged[index].messages = [...merged[index].messages, ...messages]\n      }\n\n      return merged\n    }, [])\n\n    return await sendMessages({\n      acks,\n      timeout,\n      compression,\n      topicMessages: mergedTopicMessages,\n    })\n  }\n\n  /**\n   * @param {ProduceRequest} ProduceRequest\n   * @returns {Promise}\n   *\n   * @typedef {Object} ProduceRequest\n   * @property {string} topic\n   * @property {Array} messages An array of objects with \"key\" and \"value\", example:\n   *                         [{ key: 'my-key', value: 'my-value'}]\n   * @property {number} [acks=-1] Control the number of required acks.\n   *                           -1 = all replicas must acknowledge\n   *                            0 = no acknowledgments\n   *                            1 = only waits for the leader to acknowledge\n   * @property {number} [timeout=30000] The time to await a response in ms\n   * @property {Compression.Types} [compression=Compression.Types.None] Compression codec\n   */\n  const send = async ({ acks, timeout, compression, topic, messages }) => {\n    const topicMessage = { topic, messages }\n    return sendBatch({\n      acks,\n      timeout,\n      compression,\n      topicMessages: [topicMessage],\n    })\n  }\n\n  return {\n    send,\n    sendBatch,\n  }\n}\n", "const { keys } = Object\nmodule.exports = object =>\n  keys(object).reduce((result, key) => ({ ...result, [object[key]]: key }), {})\n", "const swapObject = require('../utils/swapObject')\nconst networkEvents = require('../network/instrumentationEvents')\nconst InstrumentationEventType = require('../instrumentation/eventType')\nconst producerType = InstrumentationEventType('producer')\n\nconst events = {\n  CONNECT: producerType('connect'),\n  DISCONNECT: producerType('disconnect'),\n  REQUEST: producerType(networkEvents.NETWORK_REQUEST),\n  REQUEST_TIMEOUT: producerType(networkEvents.NETWORK_REQUEST_TIMEOUT),\n  REQUEST_QUEUE_SIZE: producerType(networkEvents.NETWORK_REQUEST_QUEUE_SIZE),\n}\n\nconst wrappedEvents = {\n  [events.REQUEST]: networkEvents.NETWORK_REQUEST,\n  [events.REQUEST_TIMEOUT]: networkEvents.NETWORK_REQUEST_TIMEOUT,\n  [events.REQUEST_QUEUE_SIZE]: networkEvents.NETWORK_REQUEST_QUEUE_SIZE,\n}\n\nconst reversedWrappedEvents = swapObject(wrappedEvents)\nconst unwrap = eventName => wrappedEvents[eventName] || eventName\nconst wrap = eventName => reversedWrappedEvents[eventName] || eventName\n\nmodule.exports = {\n  events,\n  wrap,\n  unwrap,\n}\n", "const createRetry = require('../retry')\nconst { CONNECTION_STATUS } = require('../network/connectionStatus')\nconst { DefaultPartitioner } = require('./partitioners/')\nconst InstrumentationEventEmitter = require('../instrumentation/emitter')\nconst createEosManager = require('./eosManager')\nconst createMessageProducer = require('./messageProducer')\nconst { events, wrap: wrapEvent, unwrap: unwrapEvent } = require('./instrumentationEvents')\nconst { KafkaJSNonRetriableError } = require('../errors')\n\nconst { values, keys } = Object\nconst eventNames = values(events)\nconst eventKeys = keys(events)\n  .map(key => `producer.events.${key}`)\n  .join(', ')\n\nconst { CONNECT, DISCONNECT } = events\n\n/**\n *\n * @param {Object} params\n * @param {import('../../types').Cluster} params.cluster\n * @param {import('../../types').Logger} params.logger\n * @param {import('../../types').ICustomPartitioner} [params.createPartitioner]\n * @param {import('../../types').RetryOptions} [params.retry]\n * @param {boolean} [params.idempotent]\n * @param {string} [params.transactionalId]\n * @param {number} [params.transactionTimeout]\n * @param {InstrumentationEventEmitter} [params.instrumentationEmitter]\n *\n * @returns {import('../../types').Producer}\n */\nmodule.exports = ({\n  cluster,\n  logger: rootLogger,\n  createPartitioner = DefaultPartitioner,\n  retry,\n  idempotent = false,\n  transactionalId,\n  transactionTimeout,\n  instrumentationEmitter: rootInstrumentationEmitter,\n}) => {\n  let connectionStatus = CONNECTION_STATUS.DISCONNECTED\n  retry = retry || { retries: idempotent ? Number.MAX_SAFE_INTEGER : 5 }\n\n  if (idempotent && retry.retries < 1) {\n    throw new KafkaJSNonRetriableError(\n      'Idempotent producer must allow retries to protect against transient errors'\n    )\n  }\n\n  const logger = rootLogger.namespace('Producer')\n\n  if (idempotent && retry.retries < Number.MAX_SAFE_INTEGER) {\n    logger.warn('Limiting retries for the idempotent producer may invalidate EoS guarantees')\n  }\n\n  const partitioner = createPartitioner()\n  const retrier = createRetry(Object.assign({}, cluster.retry, retry))\n  const instrumentationEmitter = rootInstrumentationEmitter || new InstrumentationEventEmitter()\n  const idempotentEosManager = createEosManager({\n    logger,\n    cluster,\n    transactionTimeout,\n    transactional: false,\n    transactionalId,\n  })\n\n  const { send, sendBatch } = createMessageProducer({\n    logger,\n    cluster,\n    partitioner,\n    eosManager: idempotentEosManager,\n    idempotent,\n    retrier,\n    getConnectionStatus: () => connectionStatus,\n  })\n\n  let transactionalEosManager\n\n  /** @type {import(\"../../types\").Producer[\"on\"]} */\n  const on = (eventName, listener) => {\n    if (!eventNames.includes(eventName)) {\n      throw new KafkaJSNonRetriableError(`Event name should be one of ${eventKeys}`)\n    }\n\n    return instrumentationEmitter.addListener(unwrapEvent(eventName), event => {\n      event.type = wrapEvent(event.type)\n      Promise.resolve(listener(event)).catch(e => {\n        logger.error(`Failed to execute listener: ${e.message}`, {\n          eventName,\n          stack: e.stack,\n        })\n      })\n    })\n  }\n\n  /**\n   * Begin a transaction. The returned object contains methods to send messages\n   * to the transaction and end the transaction by committing or aborting.\n   *\n   * Only messages sent on the transaction object will participate in the transaction.\n   *\n   * Calling any of the transactional methods after the transaction has ended\n   * will raise an exception (use `isActive` to ascertain if ended).\n   * @returns {Promise<Transaction>}\n   *\n   * @typedef {Object} Transaction\n   * @property {Function} send  Identical to the producer \"send\" method\n   * @property {Function} sendBatch Identical to the producer \"sendBatch\" method\n   * @property {Function} abort Abort the transaction\n   * @property {Function} commit  Commit the transaction\n   * @property {Function} isActive  Whether the transaction is active\n   */\n  const transaction = async () => {\n    if (!transactionalId) {\n      throw new KafkaJSNonRetriableError('Must provide transactional id for transactional producer')\n    }\n\n    let transactionDidEnd = false\n    transactionalEosManager =\n      transactionalEosManager ||\n      createEosManager({\n        logger,\n        cluster,\n        transactionTimeout,\n        transactional: true,\n        transactionalId,\n      })\n\n    if (transactionalEosManager.isInTransaction()) {\n      throw new KafkaJSNonRetriableError(\n        'There is already an ongoing transaction for this producer. Please end the transaction before beginning another.'\n      )\n    }\n\n    // We only initialize the producer id once\n    if (!transactionalEosManager.isInitialized()) {\n      await transactionalEosManager.initProducerId()\n    }\n    transactionalEosManager.beginTransaction()\n\n    const { send: sendTxn, sendBatch: sendBatchTxn } = createMessageProducer({\n      logger,\n      cluster,\n      partitioner,\n      retrier,\n      eosManager: transactionalEosManager,\n      idempotent: true,\n      getConnectionStatus: () => connectionStatus,\n    })\n\n    const isActive = () => transactionalEosManager.isInTransaction() && !transactionDidEnd\n\n    const transactionGuard = fn => (...args) => {\n      if (!isActive()) {\n        return Promise.reject(\n          new KafkaJSNonRetriableError('Cannot continue to use transaction once ended')\n        )\n      }\n\n      return fn(...args)\n    }\n\n    return {\n      sendBatch: transactionGuard(sendBatchTxn),\n      send: transactionGuard(sendTxn),\n      /**\n       * Abort the ongoing transaction.\n       *\n       * @throws {KafkaJSNonRetriableError} If transaction has ended\n       */\n      abort: transactionGuard(async () => {\n        await transactionalEosManager.abort()\n        transactionDidEnd = true\n      }),\n      /**\n       * Commit the ongoing transaction.\n       *\n       * @throws {KafkaJSNonRetriableError} If transaction has ended\n       */\n      commit: transactionGuard(async () => {\n        await transactionalEosManager.commit()\n        transactionDidEnd = true\n      }),\n      /**\n       * Sends a list of specified offsets to the consumer group coordinator, and also marks those offsets as part of the current transaction.\n       *\n       * @throws {KafkaJSNonRetriableError} If transaction has ended\n       */\n      sendOffsets: transactionGuard(async ({ consumerGroupId, topics }) => {\n        await transactionalEosManager.sendOffsets({ consumerGroupId, topics })\n\n        for (const topicOffsets of topics) {\n          const { topic, partitions } = topicOffsets\n          for (const { partition, offset } of partitions) {\n            cluster.markOffsetAsCommitted({\n              groupId: consumerGroupId,\n              topic,\n              partition,\n              offset,\n            })\n          }\n        }\n      }),\n      isActive,\n    }\n  }\n\n  /**\n   * @returns {Object} logger\n   */\n  const getLogger = () => logger\n\n  return {\n    /**\n     * @returns {Promise}\n     */\n    connect: async () => {\n      await cluster.connect()\n      connectionStatus = CONNECTION_STATUS.CONNECTED\n      instrumentationEmitter.emit(CONNECT)\n\n      if (idempotent && !idempotentEosManager.isInitialized()) {\n        await idempotentEosManager.initProducerId()\n      }\n    },\n    /**\n     * @return {Promise}\n     */\n    disconnect: async () => {\n      connectionStatus = CONNECTION_STATUS.DISCONNECTING\n      await cluster.disconnect()\n      connectionStatus = CONNECTION_STATUS.DISCONNECTED\n      instrumentationEmitter.emit(DISCONNECT)\n    },\n    isIdempotent: () => {\n      return idempotent\n    },\n    events,\n    on,\n    send,\n    sendBatch,\n    transaction,\n    logger: getLogger,\n  }\n}\n", "module.exports = timeInMs =>\n  new Promise(resolve => {\n    setTimeout(resolve, timeInMs)\n  })\n", "const Long = require('../../utils/long')\n\nmodule.exports = offset => (!offset && offset !== 0) || Long.fromValue(offset).isNegative()\n", "const isInvalidOffset = require('./isInvalidOffset')\nconst { keys, assign } = Object\n\nconst indexPartitions = (obj, { partition, offset }) => assign(obj, { [partition]: offset })\nconst indexTopics = (obj, { topic, partitions }) =>\n  assign(obj, { [topic]: partitions.reduce(indexPartitions, {}) })\n\nmodule.exports = (consumerOffsets, topicOffsets) => {\n  const indexedConsumerOffsets = consumerOffsets.reduce(indexTopics, {})\n  const indexedTopicOffsets = topicOffsets.reduce(indexTopics, {})\n\n  return keys(indexedConsumerOffsets).map(topic => {\n    const partitions = indexedConsumerOffsets[topic]\n    return {\n      topic,\n      partitions: keys(partitions).map(partition => {\n        const offset = partitions[partition]\n        const resolvedOffset = isInvalidOffset(offset)\n          ? indexedTopicOffsets[topic][partition]\n          : offset\n\n        return { partition: Number(partition), offset: resolvedOffset }\n      }),\n    }\n  })\n}\n", "const swapObject = require('../utils/swapObject')\nconst InstrumentationEventType = require('../instrumentation/eventType')\nconst networkEvents = require('../network/instrumentationEvents')\nconst consumerType = InstrumentationEventType('consumer')\n\n/** @type {import('types').ConsumerEvents} */\nconst events = {\n  HEARTBEAT: consumerType('heartbeat'),\n  COMMIT_OFFSETS: consumerType('commit_offsets'),\n  GROUP_JOIN: consumerType('group_join'),\n  FETCH: consumerType('fetch'),\n  FETCH_START: consumerType('fetch_start'),\n  START_BATCH_PROCESS: consumerType('start_batch_process'),\n  END_BATCH_PROCESS: consumerType('end_batch_process'),\n  CONNECT: consumerType('connect'),\n  DISCONNECT: consumerType('disconnect'),\n  STOP: consumerType('stop'),\n  CRASH: consumerType('crash'),\n  REBALANCING: consumerType('rebalancing'),\n  RECEIVED_UNSUBSCRIBED_TOPICS: consumerType('received_unsubscribed_topics'),\n  REQUEST: consumerType(networkEvents.NETWORK_REQUEST),\n  REQUEST_TIMEOUT: consumerType(networkEvents.NETWORK_REQUEST_TIMEOUT),\n  REQUEST_QUEUE_SIZE: consumerType(networkEvents.NETWORK_REQUEST_QUEUE_SIZE),\n}\n\nconst wrappedEvents = {\n  [events.REQUEST]: networkEvents.NETWORK_REQUEST,\n  [events.REQUEST_TIMEOUT]: networkEvents.NETWORK_REQUEST_TIMEOUT,\n  [events.REQUEST_QUEUE_SIZE]: networkEvents.NETWORK_REQUEST_QUEUE_SIZE,\n}\n\nconst reversedWrappedEvents = swapObject(wrappedEvents)\nconst unwrap = eventName => wrappedEvents[eventName] || eventName\nconst wrap = eventName => reversedWrappedEvents[eventName] || eventName\n\nmodule.exports = {\n  events,\n  wrap,\n  unwrap,\n}\n", "const Long = require('../../utils/long')\nconst isInvalidOffset = require('./isInvalidOffset')\nconst initializeConsumerOffsets = require('./initializeConsumerOffsets')\nconst {\n  events: { COMMIT_OFFSETS },\n} = require('../instrumentationEvents')\n\nconst { keys, assign } = Object\nconst indexTopics = topics => topics.reduce((obj, topic) => assign(obj, { [topic]: {} }), {})\n\nconst PRIVATE = {\n  COMMITTED_OFFSETS: Symbol('private:OffsetManager:committedOffsets'),\n}\nmodule.exports = class OffsetManager {\n  /**\n   * @param {Object} options\n   * @param {import(\"../../../types\").Cluster} options.cluster\n   * @param {import(\"../../../types\").Broker} options.coordinator\n   * @param {import(\"../../../types\").IMemberAssignment} options.memberAssignment\n   * @param {boolean} options.autoCommit\n   * @param {number | null} options.autoCommitInterval\n   * @param {number | null} options.autoCommitThreshold\n   * @param {{[topic: string]: { fromBeginning: boolean }}} options.topicConfigurations\n   * @param {import(\"../../instrumentation/emitter\")} options.instrumentationEmitter\n   * @param {string} options.groupId\n   * @param {number} options.generationId\n   * @param {string} options.memberId\n   */\n  constructor({\n    cluster,\n    coordinator,\n    memberAssignment,\n    autoCommit,\n    autoCommitInterval,\n    autoCommitThreshold,\n    topicConfigurations,\n    instrumentationEmitter,\n    groupId,\n    generationId,\n    memberId,\n  }) {\n    this.cluster = cluster\n    this.coordinator = coordinator\n\n    // memberAssignment format:\n    // {\n    //   'topic1': [0, 1, 2, 3],\n    //   'topic2': [0, 1, 2, 3, 4, 5],\n    // }\n    this.memberAssignment = memberAssignment\n\n    this.topicConfigurations = topicConfigurations\n    this.instrumentationEmitter = instrumentationEmitter\n    this.groupId = groupId\n    this.generationId = generationId\n    this.memberId = memberId\n\n    this.autoCommit = autoCommit\n    this.autoCommitInterval = autoCommitInterval\n    this.autoCommitThreshold = autoCommitThreshold\n    this.lastCommit = Date.now()\n\n    this.topics = keys(memberAssignment)\n    this.clearAllOffsets()\n  }\n\n  /**\n   * @param {string} topic\n   * @param {number} partition\n   * @returns {Long}\n   */\n  nextOffset(topic, partition) {\n    if (!this.resolvedOffsets[topic][partition]) {\n      this.resolvedOffsets[topic][partition] = this.committedOffsets()[topic][partition]\n    }\n\n    let offset = this.resolvedOffsets[topic][partition]\n    if (isInvalidOffset(offset)) {\n      offset = '0'\n    }\n\n    return Long.fromValue(offset)\n  }\n\n  /**\n   * @returns {Promise<import(\"../../../types\").Broker>}\n   */\n  async getCoordinator() {\n    if (!this.coordinator.isConnected()) {\n      this.coordinator = await this.cluster.findBroker(this.coordinator)\n    }\n\n    return this.coordinator\n  }\n\n  /**\n   * @param {import(\"../../../types\").TopicPartition} topicPartition\n   */\n  resetOffset({ topic, partition }) {\n    this.resolvedOffsets[topic][partition] = this.committedOffsets()[topic][partition]\n  }\n\n  /**\n   * @param {import(\"../../../types\").TopicPartitionOffset} topicPartitionOffset\n   */\n  resolveOffset({ topic, partition, offset }) {\n    this.resolvedOffsets[topic][partition] = Long.fromValue(offset)\n      .add(1)\n      .toString()\n  }\n\n  /**\n   * @returns {Long}\n   */\n  countResolvedOffsets() {\n    const committedOffsets = this.committedOffsets()\n\n    const subtractOffsets = (resolvedOffset, committedOffset) => {\n      const resolvedOffsetLong = Long.fromValue(resolvedOffset)\n      return isInvalidOffset(committedOffset)\n        ? resolvedOffsetLong\n        : resolvedOffsetLong.subtract(Long.fromValue(committedOffset))\n    }\n\n    const subtractPartitionOffsets = (resolvedTopicOffsets, committedTopicOffsets) =>\n      keys(resolvedTopicOffsets).map(partition =>\n        subtractOffsets(resolvedTopicOffsets[partition], committedTopicOffsets[partition])\n      )\n\n    const subtractTopicOffsets = topic =>\n      subtractPartitionOffsets(this.resolvedOffsets[topic], committedOffsets[topic])\n\n    const offsetsDiff = this.topics.flatMap(subtractTopicOffsets)\n    return offsetsDiff.reduce((sum, offset) => sum.add(offset), Long.fromValue(0))\n  }\n\n  /**\n   * @param {import(\"../../../types\").TopicPartition} topicPartition\n   */\n  async setDefaultOffset({ topic, partition }) {\n    const { groupId, generationId, memberId } = this\n    const defaultOffset = this.cluster.defaultOffset(this.topicConfigurations[topic])\n    const coordinator = await this.getCoordinator()\n\n    await coordinator.offsetCommit({\n      groupId,\n      memberId,\n      groupGenerationId: generationId,\n      topics: [\n        {\n          topic,\n          partitions: [{ partition, offset: defaultOffset }],\n        },\n      ],\n    })\n\n    this.clearOffsets({ topic, partition })\n  }\n\n  /**\n   * Commit the given offset to the topic/partition. If the consumer isn't assigned to the given\n   * topic/partition this method will be a NO-OP.\n   *\n   * @param {import(\"../../../types\").TopicPartitionOffset} topicPartitionOffset\n   */\n  async seek({ topic, partition, offset }) {\n    if (!this.memberAssignment[topic] || !this.memberAssignment[topic].includes(partition)) {\n      return\n    }\n\n    if (!this.autoCommit) {\n      this.resolveOffset({\n        topic,\n        partition,\n        offset: Long.fromValue(offset)\n          .subtract(1)\n          .toString(),\n      })\n      return\n    }\n\n    const { groupId, generationId, memberId } = this\n    const coordinator = await this.getCoordinator()\n\n    await coordinator.offsetCommit({\n      groupId,\n      memberId,\n      groupGenerationId: generationId,\n      topics: [\n        {\n          topic,\n          partitions: [{ partition, offset }],\n        },\n      ],\n    })\n\n    this.clearOffsets({ topic, partition })\n  }\n\n  async commitOffsetsIfNecessary() {\n    const now = Date.now()\n\n    const timeoutReached =\n      this.autoCommitInterval != null && now >= this.lastCommit + this.autoCommitInterval\n\n    const thresholdReached =\n      this.autoCommitThreshold != null &&\n      this.countResolvedOffsets().gte(Long.fromValue(this.autoCommitThreshold))\n\n    if (timeoutReached || thresholdReached) {\n      return this.commitOffsets()\n    }\n  }\n\n  /**\n   * Return all locally resolved offsets which are not marked as committed, by topic-partition.\n   * @returns {import('../../../types').OffsetsByTopicPartition}\n   */\n  uncommittedOffsets() {\n    const offsets = topic => keys(this.resolvedOffsets[topic])\n    const emptyPartitions = ({ partitions }) => partitions.length > 0\n    const toPartitions = topic => partition => ({\n      partition,\n      offset: this.resolvedOffsets[topic][partition],\n    })\n    const changedOffsets = topic => ({ partition, offset }) => {\n      return (\n        offset !== this.committedOffsets()[topic][partition] &&\n        Long.fromValue(offset).greaterThanOrEqual(0)\n      )\n    }\n\n    // Select and format updated partitions\n    const topicsWithPartitionsToCommit = this.topics\n      .map(topic => ({\n        topic,\n        partitions: offsets(topic)\n          .map(toPartitions(topic))\n          .filter(changedOffsets(topic)),\n      }))\n      .filter(emptyPartitions)\n\n    return { topics: topicsWithPartitionsToCommit }\n  }\n\n  async commitOffsets(offsets = {}) {\n    const { groupId, generationId, memberId } = this\n    const { topics = this.uncommittedOffsets().topics } = offsets\n\n    if (topics.length === 0) {\n      this.lastCommit = Date.now()\n      return\n    }\n\n    const payload = {\n      groupId,\n      memberId,\n      groupGenerationId: generationId,\n      topics,\n    }\n\n    try {\n      const coordinator = await this.getCoordinator()\n      await coordinator.offsetCommit(payload)\n      this.instrumentationEmitter.emit(COMMIT_OFFSETS, payload)\n\n      // Update local reference of committed offsets\n      topics.forEach(({ topic, partitions }) => {\n        const updatedOffsets = partitions.reduce(\n          (obj, { partition, offset }) => assign(obj, { [partition]: offset }),\n          {}\n        )\n\n        this[PRIVATE.COMMITTED_OFFSETS][topic] = assign(\n          {},\n          this.committedOffsets()[topic],\n          updatedOffsets\n        )\n      })\n\n      this.lastCommit = Date.now()\n    } catch (e) {\n      // metadata is stale, the coordinator has changed due to a restart or\n      // broker reassignment\n      if (e.type === 'NOT_COORDINATOR_FOR_GROUP') {\n        await this.cluster.refreshMetadata()\n      }\n\n      throw e\n    }\n  }\n\n  async resolveOffsets() {\n    const { groupId } = this\n    const invalidOffset = topic => partition => {\n      return isInvalidOffset(this.committedOffsets()[topic][partition])\n    }\n\n    const pendingPartitions = this.topics\n      .map(topic => ({\n        topic,\n        partitions: this.memberAssignment[topic]\n          .filter(invalidOffset(topic))\n          .map(partition => ({ partition })),\n      }))\n      .filter(t => t.partitions.length > 0)\n\n    if (pendingPartitions.length === 0) {\n      return\n    }\n\n    const coordinator = await this.getCoordinator()\n    const { responses: consumerOffsets } = await coordinator.offsetFetch({\n      groupId,\n      topics: pendingPartitions,\n    })\n\n    const unresolvedPartitions = consumerOffsets.map(({ topic, partitions }) =>\n      assign(\n        {\n          topic,\n          partitions: partitions\n            .filter(({ offset }) => isInvalidOffset(offset))\n            .map(({ partition }) => assign({ partition })),\n        },\n        this.topicConfigurations[topic]\n      )\n    )\n\n    const indexPartitions = (obj, { partition, offset }) => {\n      return assign(obj, { [partition]: offset })\n    }\n\n    const hasUnresolvedPartitions = () => unresolvedPartitions.some(t => t.partitions.length > 0)\n\n    let offsets = consumerOffsets\n    if (hasUnresolvedPartitions()) {\n      const topicOffsets = await this.cluster.fetchTopicsOffset(unresolvedPartitions)\n      offsets = initializeConsumerOffsets(consumerOffsets, topicOffsets)\n    }\n\n    offsets.forEach(({ topic, partitions }) => {\n      this.committedOffsets()[topic] = partitions.reduce(indexPartitions, {\n        ...this.committedOffsets()[topic],\n      })\n    })\n  }\n\n  /**\n   * @private\n   * @param {import(\"../../../types\").TopicPartition} topicPartition\n   */\n  clearOffsets({ topic, partition }) {\n    delete this.committedOffsets()[topic][partition]\n    delete this.resolvedOffsets[topic][partition]\n  }\n\n  /**\n   * @private\n   */\n  clearAllOffsets() {\n    const committedOffsets = this.committedOffsets()\n\n    for (const topic in committedOffsets) {\n      delete committedOffsets[topic]\n    }\n\n    for (const topic of this.topics) {\n      committedOffsets[topic] = {}\n    }\n\n    this.resolvedOffsets = indexTopics(this.topics)\n  }\n\n  committedOffsets() {\n    if (!this[PRIVATE.COMMITTED_OFFSETS]) {\n      this[PRIVATE.COMMITTED_OFFSETS] = this.groupId\n        ? this.cluster.committedOffsets({ groupId: this.groupId })\n        : {}\n    }\n\n    return this[PRIVATE.COMMITTED_OFFSETS]\n  }\n}\n", "const Long = require('../utils/long')\nconst ABORTED_MESSAGE_KEY = Buffer.from([0, 0, 0, 0])\n\nconst isAbortMarker = ({ key }) => {\n  // Handle null/undefined keys.\n  if (!key) return false\n  // Cast key to buffer defensively\n  return Buffer.from(key).equals(ABORTED_MESSAGE_KEY)\n}\n\n/**\n * Remove messages marked as aborted according to the aborted transactions list.\n *\n * Start of an aborted transaction is determined by message offset.\n * End of an aborted transaction is determined by control messages.\n * @param {Message[]} messages\n * @param {Transaction[]} [abortedTransactions]\n * @returns {Message[]} Messages which did not participate in an aborted transaction\n *\n * @typedef {object} Message\n * @param {Buffer} key\n * @param {lastOffset} key  Int64\n * @param {RecordBatch}  batchContext\n *\n * @typedef {object} Transaction\n * @param {string} firstOffset  Int64\n * @param {string} producerId  Int64\n *\n * @typedef {object} RecordBatch\n * @param {string}  producerId  Int64\n * @param {boolean}  inTransaction\n */\nmodule.exports = ({ messages, abortedTransactions }) => {\n  const currentAbortedTransactions = new Map()\n\n  if (!abortedTransactions || !abortedTransactions.length) {\n    return messages\n  }\n\n  const remainingAbortedTransactions = [...abortedTransactions]\n\n  return messages.filter(message => {\n    // If the message offset is GTE the first offset of the next aborted transaction\n    // then we have stepped into an aborted transaction.\n    if (\n      remainingAbortedTransactions.length &&\n      Long.fromValue(message.offset).gte(remainingAbortedTransactions[0].firstOffset)\n    ) {\n      const { producerId } = remainingAbortedTransactions.shift()\n      currentAbortedTransactions.set(producerId, true)\n    }\n\n    const { producerId, inTransaction } = message.batchContext\n\n    if (isAbortMarker(message)) {\n      // Transaction is over, we no longer need to ignore messages from this producer\n      currentAbortedTransactions.delete(producerId)\n    } else if (currentAbortedTransactions.has(producerId) && inTransaction) {\n      return false\n    }\n\n    return true\n  })\n}\n", "const Long = require('../utils/long')\nconst filterAbortedMessages = require('./filterAbortedMessages')\n\n/**\n * A batch collects messages returned from a single fetch call.\n *\n * A batch could contain _multiple_ Kafka RecordBatches.\n */\nmodule.exports = class Batch {\n  constructor(topic, fetchedOffset, partitionData) {\n    this.fetchedOffset = fetchedOffset\n    const longFetchedOffset = Long.fromValue(this.fetchedOffset)\n    const { abortedTransactions, messages } = partitionData\n\n    this.topic = topic\n    this.partition = partitionData.partition\n    this.highWatermark = partitionData.highWatermark\n\n    this.rawMessages = messages\n    // Apparently fetch can return different offsets than the target offset provided to the fetch API.\n    // Discard messages that are not in the requested offset\n    // https://github.com/apache/kafka/blob/bf237fa7c576bd141d78fdea9f17f65ea269c290/clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java#L912\n    this.messagesWithinOffset = this.rawMessages.filter(message =>\n      Long.fromValue(message.offset).gte(longFetchedOffset)\n    )\n\n    // 1. Don't expose aborted messages\n    // 2. Don't expose control records\n    // @see https://kafka.apache.org/documentation/#controlbatch\n    this.messages = filterAbortedMessages({\n      messages: this.messagesWithinOffset,\n      abortedTransactions,\n    }).filter(message => !message.isControlRecord)\n  }\n\n  isEmpty() {\n    return this.messages.length === 0\n  }\n\n  isEmptyIncludingFiltered() {\n    return this.messagesWithinOffset.length === 0\n  }\n\n  /**\n   * If the batch contained raw messages (i.e was not truly empty) but all messages were filtered out due to\n   * log compaction, control records or other reasons\n   */\n  isEmptyDueToFiltering() {\n    return this.isEmpty() && this.rawMessages.length > 0\n  }\n\n  isEmptyControlRecord() {\n    return (\n      this.isEmpty() && this.messagesWithinOffset.some(({ isControlRecord }) => isControlRecord)\n    )\n  }\n\n  /**\n   * With compressed messages, it's possible for the returned messages to have offsets smaller than the starting offset.\n   * These messages will be filtered out (i.e. they are not even included in this.messagesWithinOffset)\n   * If these are the only messages, the batch will appear as an empty batch.\n   *\n   * isEmpty() and isEmptyIncludingFiltered() will always return true if the batch is empty,\n   * but this method will only return true if the batch is empty due to log compacted messages.\n   *\n   * @returns boolean True if the batch is empty, because of log compacted messages in the partition.\n   */\n  isEmptyDueToLogCompactedMessages() {\n    const hasMessages = this.rawMessages.length > 0\n    return hasMessages && this.isEmptyIncludingFiltered()\n  }\n\n  firstOffset() {\n    return this.isEmptyIncludingFiltered() ? null : this.messagesWithinOffset[0].offset\n  }\n\n  lastOffset() {\n    if (this.isEmptyDueToLogCompactedMessages()) {\n      return this.fetchedOffset\n    }\n\n    if (this.isEmptyIncludingFiltered()) {\n      return Long.fromValue(this.highWatermark)\n        .add(-1)\n        .toString()\n    }\n\n    return this.messagesWithinOffset[this.messagesWithinOffset.length - 1].offset\n  }\n\n  /**\n   * Returns the lag based on the last offset in the batch (also known as \"high\")\n   */\n  offsetLag() {\n    const lastOffsetOfPartition = Long.fromValue(this.highWatermark).add(-1)\n    const lastConsumedOffset = Long.fromValue(this.lastOffset())\n    return lastOffsetOfPartition.add(lastConsumedOffset.multiply(-1)).toString()\n  }\n\n  /**\n   * Returns the lag based on the first offset in the batch\n   */\n  offsetLagLow() {\n    if (this.isEmptyIncludingFiltered()) {\n      return '0'\n    }\n\n    const lastOffsetOfPartition = Long.fromValue(this.highWatermark).add(-1)\n    const firstConsumedOffset = Long.fromValue(this.firstOffset())\n    return lastOffsetOfPartition.add(firstConsumedOffset.multiply(-1)).toString()\n  }\n}\n", "module.exports = class SeekOffsets extends Map {\n  getKey(topic, partition) {\n    return JSON.stringify([topic, partition])\n  }\n\n  set(topic, partition, offset) {\n    const key = this.getKey(topic, partition)\n    super.set(key, offset)\n  }\n\n  has(topic, partition) {\n    const key = this.getKey(topic, partition)\n    return super.has(key)\n  }\n\n  pop(topic, partition) {\n    if (this.size === 0 || !this.has(topic, partition)) {\n      return\n    }\n\n    const key = this.getKey(topic, partition)\n    const offset = this.get(key)\n\n    this.delete(key)\n    return { topic, partition, offset }\n  }\n}\n", "const createState = topic => ({\n  topic,\n  paused: new Set(),\n  pauseAll: false,\n  resumed: new Set(),\n})\n\nmodule.exports = class SubscriptionState {\n  constructor() {\n    this.assignedPartitionsByTopic = {}\n    this.subscriptionStatesByTopic = {}\n  }\n\n  /**\n   * Replace the current assignment with a new set of assignments\n   *\n   * @param {Array<TopicPartitions>} topicPartitions Example: [{ topic: 'topic-name', partitions: [1, 2] }]\n   */\n  assign(topicPartitions = []) {\n    this.assignedPartitionsByTopic = topicPartitions.reduce(\n      (assigned, { topic, partitions = [] }) => {\n        return { ...assigned, [topic]: { topic, partitions } }\n      },\n      {}\n    )\n  }\n\n  /**\n   * @param {Array<TopicPartitions>} topicPartitions Example: [{ topic: 'topic-name', partitions: [1, 2] }]\n   */\n  pause(topicPartitions = []) {\n    topicPartitions.forEach(({ topic, partitions }) => {\n      const state = this.subscriptionStatesByTopic[topic] || createState(topic)\n\n      if (typeof partitions === 'undefined') {\n        state.paused.clear()\n        state.resumed.clear()\n        state.pauseAll = true\n      } else if (Array.isArray(partitions)) {\n        partitions.forEach(partition => {\n          state.paused.add(partition)\n          state.resumed.delete(partition)\n        })\n        state.pauseAll = false\n      }\n\n      this.subscriptionStatesByTopic[topic] = state\n    })\n  }\n\n  /**\n   * @param {Array<TopicPartitions>} topicPartitions Example: [{ topic: 'topic-name', partitions: [1, 2] }]\n   */\n  resume(topicPartitions = []) {\n    topicPartitions.forEach(({ topic, partitions }) => {\n      const state = this.subscriptionStatesByTopic[topic] || createState(topic)\n\n      if (typeof partitions === 'undefined') {\n        state.paused.clear()\n        state.resumed.clear()\n        state.pauseAll = false\n      } else if (Array.isArray(partitions)) {\n        partitions.forEach(partition => {\n          state.paused.delete(partition)\n\n          if (state.pauseAll) {\n            state.resumed.add(partition)\n          }\n        })\n      }\n\n      this.subscriptionStatesByTopic[topic] = state\n    })\n  }\n\n  /**\n   * @returns {Array<import(\"../../types\").TopicPartitions>} topicPartitions\n   * Example: [{ topic: 'topic-name', partitions: [1, 2] }]\n   */\n  assigned() {\n    return Object.values(this.assignedPartitionsByTopic).map(({ topic, partitions }) => ({\n      topic,\n      partitions: partitions.sort(),\n    }))\n  }\n\n  /**\n   * @returns {Array<import(\"../../types\").TopicPartitions>} topicPartitions\n   * Example: [{ topic: 'topic-name', partitions: [1, 2] }]\n   */\n  active() {\n    return Object.values(this.assignedPartitionsByTopic).map(({ topic, partitions }) => ({\n      topic,\n      partitions: partitions.filter(partition => !this.isPaused(topic, partition)).sort(),\n    }))\n  }\n\n  /**\n   * @returns {Array<import(\"../../types\").TopicPartitions>} topicPartitions\n   * Example: [{ topic: 'topic-name', partitions: [1, 2] }]\n   */\n  paused() {\n    return Object.values(this.assignedPartitionsByTopic)\n      .map(({ topic, partitions }) => ({\n        topic,\n        partitions: partitions.filter(partition => this.isPaused(topic, partition)).sort(),\n      }))\n      .filter(({ partitions }) => partitions.length !== 0)\n  }\n\n  isPaused(topic, partition) {\n    const state = this.subscriptionStatesByTopic[topic]\n\n    if (!state) {\n      return false\n    }\n\n    const partitionResumed = state.resumed.has(partition)\n    const partitionPaused = state.paused.has(partition)\n\n    return (state.pauseAll && !partitionResumed) || partitionPaused\n  }\n}\n", "const Encoder = require('../protocol/encoder')\nconst Decoder = require('../protocol/decoder')\n\nconst MemberMetadata = {\n  /**\n   * @param {Object} metadata\n   * @param {number} metadata.version\n   * @param {Array<string>} metadata.topics\n   * @param {Buffer} [metadata.userData=Buffer.alloc(0)]\n   *\n   * @returns Buffer\n   */\n  encode({ version, topics, userData = Buffer.alloc(0) }) {\n    return new Encoder()\n      .writeInt16(version)\n      .writeArray(topics)\n      .writeBytes(userData).buffer\n  },\n\n  /**\n   * @param {Buffer} buffer\n   * @returns {Object}\n   */\n  decode(buffer) {\n    const decoder = new Decoder(buffer)\n    return {\n      version: decoder.readInt16(),\n      topics: decoder.readArray(d => d.readString()),\n      userData: decoder.readBytes(),\n    }\n  },\n}\n\nconst MemberAssignment = {\n  /**\n   * @param {object} options\n   * @param {number} options.version\n   * @param {Object<String,Array>} options.assignment, example:\n   *                               {\n   *                                 'topic-A': [0, 2, 4, 6],\n   *                                 'topic-B': [0, 2],\n   *                               }\n   * @param {Buffer} [options.userData=Buffer.alloc(0)]\n   *\n   * @returns Buffer\n   */\n  encode({ version, assignment, userData = Buffer.alloc(0) }) {\n    return new Encoder()\n      .writeInt16(version)\n      .writeArray(\n        Object.keys(assignment).map(topic =>\n          new Encoder().writeString(topic).writeArray(assignment[topic])\n        )\n      )\n      .writeBytes(userData).buffer\n  },\n\n  /**\n   * @param {Buffer} buffer\n   * @returns {Object|null}\n   */\n  decode(buffer) {\n    const decoder = new Decoder(buffer)\n    const decodePartitions = d => d.readInt32()\n    const decodeAssignment = d => ({\n      topic: d.readString(),\n      partitions: d.readArray(decodePartitions),\n    })\n    const indexAssignment = (obj, { topic, partitions }) =>\n      Object.assign(obj, { [topic]: partitions })\n\n    if (!decoder.canReadInt16()) {\n      return null\n    }\n\n    return {\n      version: decoder.readInt16(),\n      assignment: decoder.readArray(decodeAssignment).reduce(indexAssignment, {}),\n      userData: decoder.readBytes(),\n    }\n  },\n}\n\nmodule.exports = {\n  MemberMetadata,\n  MemberAssignment,\n}\n", "const sleep = require('../utils/sleep')\nconst websiteUrl = require('../utils/websiteUrl')\nconst arrayDiff = require('../utils/arrayDiff')\nconst createRetry = require('../retry')\nconst sharedPromiseTo = require('../utils/sharedPromiseTo')\n\nconst OffsetManager = require('./offsetManager')\nconst Batch = require('./batch')\nconst SeekOffsets = require('./seekOffsets')\nconst SubscriptionState = require('./subscriptionState')\nconst {\n  events: { GROUP_JOIN, HEARTBEAT, CONNECT, RECEIVED_UNSUBSCRIBED_TOPICS },\n} = require('./instrumentationEvents')\nconst { MemberAssignment } = require('./assignerProtocol')\nconst {\n  KafkaJSError,\n  KafkaJSNonRetriableError,\n  KafkaJSStaleTopicMetadataAssignment,\n  isRebalancing,\n} = require('../errors')\n\nconst { keys } = Object\n\nconst STALE_METADATA_ERRORS = [\n  'LEADER_NOT_AVAILABLE',\n  // Fetch before v9 uses NOT_LEADER_FOR_PARTITION\n  'NOT_LEADER_FOR_PARTITION',\n  // Fetch after v9 uses {FENCED,UNKNOWN}_LEADER_EPOCH\n  'FENCED_LEADER_EPOCH',\n  'UNKNOWN_LEADER_EPOCH',\n  'UNKNOWN_TOPIC_OR_PARTITION',\n]\n\nconst PRIVATE = {\n  JOIN: Symbol('private:ConsumerGroup:join'),\n  SYNC: Symbol('private:ConsumerGroup:sync'),\n  SHARED_HEARTBEAT: Symbol('private:ConsumerGroup:sharedHeartbeat'),\n}\n\nmodule.exports = class ConsumerGroup {\n  /**\n   * @param {object} options\n   * @param {import('../../types').RetryOptions} options.retry\n   * @param {import('../../types').Cluster} options.cluster\n   * @param {string} options.groupId\n   * @param {string[]} options.topics\n   * @param {Record<string, { fromBeginning?: boolean }>} options.topicConfigurations\n   * @param {import('../../types').Logger} options.logger\n   * @param {import('../instrumentation/emitter')} options.instrumentationEmitter\n   * @param {import('../../types').Assigner[]} options.assigners\n   * @param {number} options.sessionTimeout\n   * @param {number} options.rebalanceTimeout\n   * @param {number} options.maxBytesPerPartition\n   * @param {number} options.minBytes\n   * @param {number} options.maxBytes\n   * @param {number} options.maxWaitTimeInMs\n   * @param {boolean} options.autoCommit\n   * @param {number} options.autoCommitInterval\n   * @param {number} options.autoCommitThreshold\n   * @param {number} options.isolationLevel\n   * @param {string} options.rackId\n   * @param {number} options.metadataMaxAge\n   */\n  constructor({\n    retry,\n    cluster,\n    groupId,\n    topics,\n    topicConfigurations,\n    logger,\n    instrumentationEmitter,\n    assigners,\n    sessionTimeout,\n    rebalanceTimeout,\n    maxBytesPerPartition,\n    minBytes,\n    maxBytes,\n    maxWaitTimeInMs,\n    autoCommit,\n    autoCommitInterval,\n    autoCommitThreshold,\n    isolationLevel,\n    rackId,\n    metadataMaxAge,\n  }) {\n    /** @type {import(\"../../types\").Cluster} */\n    this.cluster = cluster\n    this.groupId = groupId\n    this.topics = topics\n    this.topicsSubscribed = topics\n    this.topicConfigurations = topicConfigurations\n    this.logger = logger.namespace('ConsumerGroup')\n    this.instrumentationEmitter = instrumentationEmitter\n    this.retrier = createRetry(Object.assign({}, retry))\n    this.assigners = assigners\n    this.sessionTimeout = sessionTimeout\n    this.rebalanceTimeout = rebalanceTimeout\n    this.maxBytesPerPartition = maxBytesPerPartition\n    this.minBytes = minBytes\n    this.maxBytes = maxBytes\n    this.maxWaitTime = maxWaitTimeInMs\n    this.autoCommit = autoCommit\n    this.autoCommitInterval = autoCommitInterval\n    this.autoCommitThreshold = autoCommitThreshold\n    this.isolationLevel = isolationLevel\n    this.rackId = rackId\n    this.metadataMaxAge = metadataMaxAge\n\n    this.seekOffset = new SeekOffsets()\n    this.coordinator = null\n    this.generationId = null\n    this.leaderId = null\n    this.memberId = null\n    this.members = null\n    this.groupProtocol = null\n\n    this.partitionsPerSubscribedTopic = null\n    /**\n     * Preferred read replica per topic and partition\n     *\n     * Each of the partitions tracks the preferred read replica (`nodeId`) and a timestamp\n     * until when that preference is valid.\n     *\n     * @type {{[topicName: string]: {[partition: number]: {nodeId: number, expireAt: number}}}}\n     */\n    this.preferredReadReplicasPerTopicPartition = {}\n    this.offsetManager = null\n    this.subscriptionState = new SubscriptionState()\n\n    this.lastRequest = Date.now()\n\n    this[PRIVATE.SHARED_HEARTBEAT] = sharedPromiseTo(async ({ interval }) => {\n      const { groupId, generationId, memberId } = this\n      const now = Date.now()\n\n      if (memberId && now >= this.lastRequest + interval) {\n        const payload = {\n          groupId,\n          memberId,\n          groupGenerationId: generationId,\n        }\n\n        await this.coordinator.heartbeat(payload)\n        this.instrumentationEmitter.emit(HEARTBEAT, payload)\n        this.lastRequest = Date.now()\n      }\n    })\n  }\n\n  isLeader() {\n    return this.leaderId && this.memberId === this.leaderId\n  }\n\n  getNodeIds() {\n    return this.cluster.getNodeIds()\n  }\n\n  async connect() {\n    await this.cluster.connect()\n    this.instrumentationEmitter.emit(CONNECT)\n    await this.cluster.refreshMetadataIfNecessary()\n  }\n\n  async [PRIVATE.JOIN]() {\n    const { groupId, sessionTimeout, rebalanceTimeout } = this\n\n    this.coordinator = await this.cluster.findGroupCoordinator({ groupId })\n\n    const groupData = await this.coordinator.joinGroup({\n      groupId,\n      sessionTimeout,\n      rebalanceTimeout,\n      memberId: this.memberId || '',\n      groupProtocols: this.assigners.map(assigner =>\n        assigner.protocol({\n          topics: this.topicsSubscribed,\n        })\n      ),\n    })\n\n    this.generationId = groupData.generationId\n    this.leaderId = groupData.leaderId\n    this.memberId = groupData.memberId\n    this.members = groupData.members\n    this.groupProtocol = groupData.groupProtocol\n  }\n\n  async leave() {\n    const { groupId, memberId } = this\n    if (memberId) {\n      await this.coordinator.leaveGroup({ groupId, memberId })\n      this.memberId = null\n    }\n  }\n\n  async [PRIVATE.SYNC]() {\n    let assignment = []\n    const {\n      groupId,\n      generationId,\n      memberId,\n      members,\n      groupProtocol,\n      topics,\n      topicsSubscribed,\n      coordinator,\n    } = this\n\n    if (this.isLeader()) {\n      this.logger.debug('Chosen as group leader', { groupId, generationId, memberId, topics })\n      const assigner = this.assigners.find(({ name }) => name === groupProtocol)\n\n      if (!assigner) {\n        throw new KafkaJSNonRetriableError(\n          `Unsupported partition assigner \"${groupProtocol}\", the assigner wasn't found in the assigners list`\n        )\n      }\n\n      await this.cluster.refreshMetadata()\n      assignment = await assigner.assign({ members, topics: topicsSubscribed })\n\n      this.logger.debug('Group assignment', {\n        groupId,\n        generationId,\n        groupProtocol,\n        assignment,\n        topics: topicsSubscribed,\n      })\n    }\n\n    // Keep track of the partitions for the subscribed topics\n    this.partitionsPerSubscribedTopic = this.generatePartitionsPerSubscribedTopic()\n    const { memberAssignment } = await this.coordinator.syncGroup({\n      groupId,\n      generationId,\n      memberId,\n      groupAssignment: assignment,\n    })\n\n    const decodedMemberAssignment = MemberAssignment.decode(memberAssignment)\n    const decodedAssignment =\n      decodedMemberAssignment != null ? decodedMemberAssignment.assignment : {}\n\n    this.logger.debug('Received assignment', {\n      groupId,\n      generationId,\n      memberId,\n      memberAssignment: decodedAssignment,\n    })\n\n    const assignedTopics = keys(decodedAssignment)\n    const topicsNotSubscribed = arrayDiff(assignedTopics, topicsSubscribed)\n\n    if (topicsNotSubscribed.length > 0) {\n      const payload = {\n        groupId,\n        generationId,\n        memberId,\n        assignedTopics,\n        topicsSubscribed,\n        topicsNotSubscribed,\n      }\n\n      this.instrumentationEmitter.emit(RECEIVED_UNSUBSCRIBED_TOPICS, payload)\n      this.logger.warn('Consumer group received unsubscribed topics', {\n        ...payload,\n        helpUrl: websiteUrl(\n          'docs/faq',\n          'why-am-i-receiving-messages-for-topics-i-m-not-subscribed-to'\n        ),\n      })\n    }\n\n    // Remove unsubscribed topics from the list\n    const safeAssignment = arrayDiff(assignedTopics, topicsNotSubscribed)\n    const currentMemberAssignment = safeAssignment.map(topic => ({\n      topic,\n      partitions: decodedAssignment[topic],\n    }))\n\n    // Check if the consumer is aware of all assigned partitions\n    for (const assignment of currentMemberAssignment) {\n      const { topic, partitions: assignedPartitions } = assignment\n      const knownPartitions = this.partitionsPerSubscribedTopic.get(topic)\n      const isAwareOfAllAssignedPartitions = assignedPartitions.every(partition =>\n        knownPartitions.includes(partition)\n      )\n\n      if (!isAwareOfAllAssignedPartitions) {\n        this.logger.warn('Consumer is not aware of all assigned partitions, refreshing metadata', {\n          groupId,\n          generationId,\n          memberId,\n          topic,\n          knownPartitions,\n          assignedPartitions,\n        })\n\n        // If the consumer is not aware of all assigned partitions, refresh metadata\n        // and update the list of partitions per subscribed topic. It's enough to perform\n        // this operation once since refresh metadata will update metadata for all topics\n        await this.cluster.refreshMetadata()\n        this.partitionsPerSubscribedTopic = this.generatePartitionsPerSubscribedTopic()\n        break\n      }\n    }\n\n    this.topics = currentMemberAssignment.map(({ topic }) => topic)\n    this.subscriptionState.assign(currentMemberAssignment)\n    this.offsetManager = new OffsetManager({\n      cluster: this.cluster,\n      topicConfigurations: this.topicConfigurations,\n      instrumentationEmitter: this.instrumentationEmitter,\n      memberAssignment: currentMemberAssignment.reduce(\n        (partitionsByTopic, { topic, partitions }) => ({\n          ...partitionsByTopic,\n          [topic]: partitions,\n        }),\n        {}\n      ),\n      autoCommit: this.autoCommit,\n      autoCommitInterval: this.autoCommitInterval,\n      autoCommitThreshold: this.autoCommitThreshold,\n      coordinator,\n      groupId,\n      generationId,\n      memberId,\n    })\n  }\n\n  joinAndSync() {\n    const startJoin = Date.now()\n    return this.retrier(async bail => {\n      try {\n        await this[PRIVATE.JOIN]()\n        await this[PRIVATE.SYNC]()\n\n        const memberAssignment = this.assigned().reduce(\n          (result, { topic, partitions }) => ({ ...result, [topic]: partitions }),\n          {}\n        )\n\n        const payload = {\n          groupId: this.groupId,\n          memberId: this.memberId,\n          leaderId: this.leaderId,\n          isLeader: this.isLeader(),\n          memberAssignment,\n          groupProtocol: this.groupProtocol,\n          duration: Date.now() - startJoin,\n        }\n\n        this.instrumentationEmitter.emit(GROUP_JOIN, payload)\n        this.logger.info('Consumer has joined the group', payload)\n      } catch (e) {\n        if (isRebalancing(e)) {\n          // Rebalance in progress isn't a retriable protocol error since the consumer\n          // has to go through find coordinator and join again before it can\n          // actually retry the operation. We wrap the original error in a retriable error\n          // here instead in order to restart the join + sync sequence using the retrier.\n          throw new KafkaJSError(e)\n        }\n\n        if (e.type === 'UNKNOWN_MEMBER_ID') {\n          this.memberId = null\n          throw new KafkaJSError(e)\n        }\n\n        bail(e)\n      }\n    })\n  }\n\n  /**\n   * @param {import(\"../../types\").TopicPartition} topicPartition\n   */\n  resetOffset({ topic, partition }) {\n    this.offsetManager.resetOffset({ topic, partition })\n  }\n\n  /**\n   * @param {import(\"../../types\").TopicPartitionOffset} topicPartitionOffset\n   */\n  resolveOffset({ topic, partition, offset }) {\n    this.offsetManager.resolveOffset({ topic, partition, offset })\n  }\n\n  /**\n   * Update the consumer offset for the given topic/partition. This will be used\n   * on the next fetch. If this API is invoked for the same topic/partition more\n   * than once, the latest offset will be used on the next fetch.\n   *\n   * @param {import(\"../../types\").TopicPartitionOffset} topicPartitionOffset\n   */\n  seek({ topic, partition, offset }) {\n    this.seekOffset.set(topic, partition, offset)\n  }\n\n  pause(topicPartitions) {\n    this.logger.info(`Pausing fetching from ${topicPartitions.length} topics`, {\n      topicPartitions,\n    })\n    this.subscriptionState.pause(topicPartitions)\n  }\n\n  resume(topicPartitions) {\n    this.logger.info(`Resuming fetching from ${topicPartitions.length} topics`, {\n      topicPartitions,\n    })\n    this.subscriptionState.resume(topicPartitions)\n  }\n\n  assigned() {\n    return this.subscriptionState.assigned()\n  }\n\n  paused() {\n    return this.subscriptionState.paused()\n  }\n\n  /**\n   * @param {string} topic\n   * @param {string} partition\n   * @returns {boolean} whether the specified topic-partition are paused or not\n   */\n  isPaused(topic, partition) {\n    return this.subscriptionState.isPaused(topic, partition)\n  }\n\n  async commitOffsetsIfNecessary() {\n    await this.offsetManager.commitOffsetsIfNecessary()\n  }\n\n  async commitOffsets(offsets) {\n    await this.offsetManager.commitOffsets(offsets)\n  }\n\n  uncommittedOffsets() {\n    return this.offsetManager.uncommittedOffsets()\n  }\n\n  async heartbeat({ interval }) {\n    return this[PRIVATE.SHARED_HEARTBEAT]({ interval })\n  }\n\n  async fetch(nodeId) {\n    try {\n      await this.cluster.refreshMetadataIfNecessary()\n      this.checkForStaleAssignment()\n\n      let topicPartitions = this.subscriptionState.assigned()\n      topicPartitions = this.filterPartitionsByNode(nodeId, topicPartitions)\n\n      await this.seekOffsets(topicPartitions)\n\n      const committedOffsets = this.offsetManager.committedOffsets()\n      const activeTopicPartitions = this.getActiveTopicPartitions()\n\n      const requests = topicPartitions\n        .map(({ topic, partitions }) => ({\n          topic,\n          partitions: partitions\n            .filter(\n              partition =>\n                /**\n                 * When recovering from OffsetOutOfRange, each partition can recover\n                 * concurrently, which invalidates resolved and committed offsets as part\n                 * of the recovery mechanism (see OffsetManager.clearOffsets). In concurrent\n                 * scenarios this can initiate a new fetch with invalid offsets.\n                 *\n                 * This was further highlighted by https://github.com/tulios/kafkajs/pull/570,\n                 * which increased concurrency, making this more likely to happen.\n                 *\n                 * This is solved by only making requests for partitions with initialized offsets.\n                 *\n                 * See the following pull request which explains the context of the problem:\n                 * @issue https://github.com/tulios/kafkajs/pull/578\n                 */\n                committedOffsets[topic][partition] != null &&\n                activeTopicPartitions[topic].has(partition)\n            )\n            .map(partition => ({\n              partition,\n              fetchOffset: this.offsetManager.nextOffset(topic, partition).toString(),\n              maxBytes: this.maxBytesPerPartition,\n            })),\n        }))\n        .filter(({ partitions }) => partitions.length)\n\n      if (!requests.length) {\n        await sleep(this.maxWaitTime)\n        return []\n      }\n\n      const broker = await this.cluster.findBroker({ nodeId })\n\n      const { responses } = await broker.fetch({\n        maxWaitTime: this.maxWaitTime,\n        minBytes: this.minBytes,\n        maxBytes: this.maxBytes,\n        isolationLevel: this.isolationLevel,\n        topics: requests,\n        rackId: this.rackId,\n      })\n\n      return responses.flatMap(({ topicName, partitions }) => {\n        const topicRequestData = requests.find(({ topic }) => topic === topicName)\n\n        let preferredReadReplicas = this.preferredReadReplicasPerTopicPartition[topicName]\n        if (!preferredReadReplicas) {\n          this.preferredReadReplicasPerTopicPartition[topicName] = preferredReadReplicas = {}\n        }\n\n        return partitions\n          .filter(\n            ({ partition }) =>\n              !this.seekOffset.has(topicName, partition) &&\n              !this.subscriptionState.isPaused(topicName, partition)\n          )\n          .map(partitionData => {\n            const { partition, preferredReadReplica } = partitionData\n\n            if (preferredReadReplica != null && preferredReadReplica !== -1) {\n              const { nodeId: currentPreferredReadReplica } = preferredReadReplicas[partition] || {}\n              if (currentPreferredReadReplica !== preferredReadReplica) {\n                this.logger.info(`Preferred read replica is now ${preferredReadReplica}`, {\n                  groupId: this.groupId,\n                  memberId: this.memberId,\n                  topic: topicName,\n                  partition,\n                })\n              }\n              preferredReadReplicas[partition] = {\n                nodeId: preferredReadReplica,\n                expireAt: Date.now() + this.metadataMaxAge,\n              }\n            }\n\n            const partitionRequestData = topicRequestData.partitions.find(\n              ({ partition }) => partition === partitionData.partition\n            )\n\n            const fetchedOffset = partitionRequestData.fetchOffset\n            return new Batch(topicName, fetchedOffset, partitionData)\n          })\n      })\n    } catch (e) {\n      await this.recoverFromFetch(e)\n      return []\n    }\n  }\n\n  async recoverFromFetch(e) {\n    if (STALE_METADATA_ERRORS.includes(e.type) || e.name === 'KafkaJSTopicMetadataNotLoaded') {\n      this.logger.debug('Stale cluster metadata, refreshing...', {\n        groupId: this.groupId,\n        memberId: this.memberId,\n        error: e.message,\n      })\n\n      await this.cluster.refreshMetadata()\n      await this.joinAndSync()\n      return\n    }\n\n    if (e.name === 'KafkaJSStaleTopicMetadataAssignment') {\n      this.logger.warn(`${e.message}, resync group`, {\n        groupId: this.groupId,\n        memberId: this.memberId,\n        topic: e.topic,\n        unknownPartitions: e.unknownPartitions,\n      })\n\n      await this.joinAndSync()\n      return\n    }\n\n    if (e.name === 'KafkaJSOffsetOutOfRange') {\n      await this.recoverFromOffsetOutOfRange(e)\n      return\n    }\n\n    if (e.name === 'KafkaJSConnectionClosedError') {\n      this.cluster.removeBroker({ host: e.host, port: e.port })\n      return\n    }\n\n    if (e.name === 'KafkaJSBrokerNotFound' || e.name === 'KafkaJSConnectionClosedError') {\n      this.logger.debug(`${e.message}, refreshing metadata and retrying...`)\n      await this.cluster.refreshMetadata()\n      return\n    }\n\n    throw e\n  }\n\n  async recoverFromOffsetOutOfRange(e) {\n    // If we are fetching from a follower try with the leader before resetting offsets\n    const preferredReadReplicas = this.preferredReadReplicasPerTopicPartition[e.topic]\n    if (preferredReadReplicas && typeof preferredReadReplicas[e.partition] === 'number') {\n      this.logger.info('Offset out of range while fetching from follower, retrying with leader', {\n        topic: e.topic,\n        partition: e.partition,\n        groupId: this.groupId,\n        memberId: this.memberId,\n      })\n      delete preferredReadReplicas[e.partition]\n    } else {\n      this.logger.error('Offset out of range, resetting to default offset', {\n        topic: e.topic,\n        partition: e.partition,\n        groupId: this.groupId,\n        memberId: this.memberId,\n      })\n\n      await this.offsetManager.setDefaultOffset({\n        topic: e.topic,\n        partition: e.partition,\n      })\n    }\n  }\n\n  generatePartitionsPerSubscribedTopic() {\n    const map = new Map()\n\n    for (const topic of this.topicsSubscribed) {\n      const partitions = this.cluster\n        .findTopicPartitionMetadata(topic)\n        .map(m => m.partitionId)\n        .sort()\n\n      map.set(topic, partitions)\n    }\n\n    return map\n  }\n\n  checkForStaleAssignment() {\n    if (!this.partitionsPerSubscribedTopic) {\n      return\n    }\n\n    const newPartitionsPerSubscribedTopic = this.generatePartitionsPerSubscribedTopic()\n\n    for (const [topic, partitions] of newPartitionsPerSubscribedTopic) {\n      const diff = arrayDiff(partitions, this.partitionsPerSubscribedTopic.get(topic))\n\n      if (diff.length > 0) {\n        throw new KafkaJSStaleTopicMetadataAssignment('Topic has been updated', {\n          topic,\n          unknownPartitions: diff,\n        })\n      }\n    }\n  }\n\n  async seekOffsets(topicPartitions) {\n    for (const { topic, partitions } of topicPartitions) {\n      for (const partition of partitions) {\n        const seekEntry = this.seekOffset.pop(topic, partition)\n        if (!seekEntry) {\n          continue\n        }\n\n        this.logger.debug('Seek offset', {\n          groupId: this.groupId,\n          memberId: this.memberId,\n          seek: seekEntry,\n        })\n        await this.offsetManager.seek(seekEntry)\n      }\n    }\n\n    await this.offsetManager.resolveOffsets()\n  }\n\n  hasSeekOffset({ topic, partition }) {\n    return this.seekOffset.has(topic, partition)\n  }\n\n  /**\n   * For each of the partitions find the best nodeId to read it from\n   *\n   * @param {string} topic\n   * @param {number[]} partitions\n   * @returns {{[nodeId: number]: number[]}} per-node assignment of partitions\n   * @see Cluster~findLeaderForPartitions\n   */\n  // Invariant: The resulting object has each partition referenced exactly once\n  findReadReplicaForPartitions(topic, partitions) {\n    const partitionMetadata = this.cluster.findTopicPartitionMetadata(topic)\n    const preferredReadReplicas = this.preferredReadReplicasPerTopicPartition[topic]\n    return partitions.reduce((result, id) => {\n      const partitionId = parseInt(id, 10)\n      const metadata = partitionMetadata.find(p => p.partitionId === partitionId)\n      if (!metadata) {\n        return result\n      }\n\n      if (metadata.leader == null) {\n        throw new KafkaJSError('Invalid partition metadata', { topic, partitionId, metadata })\n      }\n\n      // Pick the preferred replica if there is one, and it isn't known to be offline, otherwise the leader.\n      let nodeId = metadata.leader\n      if (preferredReadReplicas) {\n        const { nodeId: preferredReadReplica, expireAt } = preferredReadReplicas[partitionId] || {}\n        if (Date.now() >= expireAt) {\n          this.logger.debug('Preferred read replica information has expired, using leader', {\n            topic,\n            partitionId,\n            groupId: this.groupId,\n            memberId: this.memberId,\n            preferredReadReplica,\n            leader: metadata.leader,\n          })\n          // Drop the entry\n          delete preferredReadReplicas[partitionId]\n        } else if (preferredReadReplica != null) {\n          // Valid entry, check whether it is not offline\n          // Note that we don't delete the preference here, and rather hope that eventually that replica comes online again\n          const offlineReplicas = metadata.offlineReplicas\n          if (Array.isArray(offlineReplicas) && offlineReplicas.includes(nodeId)) {\n            this.logger.debug('Preferred read replica is offline, using leader', {\n              topic,\n              partitionId,\n              groupId: this.groupId,\n              memberId: this.memberId,\n              preferredReadReplica,\n              leader: metadata.leader,\n            })\n          } else {\n            nodeId = preferredReadReplica\n          }\n        }\n      }\n      const current = result[nodeId] || []\n      return { ...result, [nodeId]: [...current, partitionId] }\n    }, {})\n  }\n\n  filterPartitionsByNode(nodeId, topicPartitions) {\n    return topicPartitions.map(({ topic, partitions }) => ({\n      topic,\n      partitions: this.findReadReplicaForPartitions(topic, partitions)[nodeId] || [],\n    }))\n  }\n\n  getActiveTopicPartitions() {\n    const activeSubscriptionState = this.subscriptionState.active()\n\n    const activeTopicPartitions = {}\n    activeSubscriptionState.forEach(({ topic, partitions }) => {\n      activeTopicPartitions[topic] = new Set(partitions)\n    })\n\n    return activeTopicPartitions\n  }\n}\n", "/**\n * @param {number} count\n * @param {(index: number) => T} [callback]\n * @template T\n */\nconst seq = (count, callback = x => x) =>\n  new Array(count).fill(0).map((_, index) => callback(index))\n\nmodule.exports = seq\n", "const EventEmitter = require('events')\n\n/**\n * Fetches data from all assigned nodes, waits for workerQueue to drain and repeats.\n *\n * @param {object} options\n * @param {number} options.nodeId\n * @param {import('./workerQueue').WorkerQueue} options.workerQueue\n * @param {Map<string, string[]>} options.partitionAssignments\n * @param {(nodeId: number) => Promise<T[]>} options.fetch\n * @param {import('../../types').Logger} options.logger\n * @template T\n */\nconst createFetcher = ({\n  nodeId,\n  workerQueue,\n  partitionAssignments,\n  fetch,\n  logger: rootLogger,\n}) => {\n  const logger = rootLogger.namespace(`Fetcher ${nodeId}`)\n  const emitter = new EventEmitter()\n  let isRunning = false\n\n  const getWorkerQueue = () => workerQueue\n  const assignmentKey = ({ topic, partition }) => `${topic}|${partition}`\n  const getAssignedFetcher = batch => partitionAssignments.get(assignmentKey(batch))\n  const assignTopicPartition = batch => partitionAssignments.set(assignmentKey(batch), nodeId)\n  const unassignTopicPartition = batch => partitionAssignments.delete(assignmentKey(batch))\n  const filterUnassignedBatches = batches =>\n    batches.filter(batch => {\n      const assignedFetcher = getAssignedFetcher(batch)\n      if (assignedFetcher != null && assignedFetcher !== nodeId) {\n        logger.info(\n          'Filtering out batch due to partition already being processed by another fetcher',\n          {\n            topic: batch.topic,\n            partition: batch.partition,\n            assignedFetcher: assignedFetcher,\n            fetcher: nodeId,\n          }\n        )\n        return false\n      }\n\n      return true\n    })\n\n  const start = async () => {\n    if (isRunning) return\n    isRunning = true\n\n    while (isRunning) {\n      try {\n        const batches = await fetch(nodeId)\n        if (isRunning) {\n          const availableBatches = filterUnassignedBatches(batches)\n\n          if (availableBatches.length > 0) {\n            availableBatches.forEach(assignTopicPartition)\n            try {\n              await workerQueue.push(...availableBatches)\n            } finally {\n              availableBatches.forEach(unassignTopicPartition)\n            }\n          }\n        }\n      } catch (error) {\n        isRunning = false\n        emitter.emit('end')\n        throw error\n      }\n    }\n    emitter.emit('end')\n  }\n\n  const stop = async () => {\n    if (!isRunning) return\n    isRunning = false\n    await new Promise(resolve => emitter.once('end', () => resolve()))\n  }\n\n  return { start, stop, getWorkerQueue }\n}\n\nmodule.exports = createFetcher\n", "/**\n * @typedef {(batch: T, metadata: { workerId: number }) => Promise<void>} Handler\n * @template T\n *\n * @typedef {ReturnType<typeof createWorker>} Worker\n */\n\nconst sharedPromiseTo = require('../utils/sharedPromiseTo')\n\n/**\n * @param {{ handler: Handler<T>, workerId: number }} options\n * @template T\n */\nconst createWorker = ({ handler, workerId }) => {\n  /**\n   * Takes batches from next() until it returns undefined.\n   *\n   * @param {{ next: () => { batch: T, resolve: () => void, reject: (e: Error) => void } | undefined }} param0\n   * @returns {Promise<void>}\n   */\n  const run = sharedPromiseTo(async ({ next }) => {\n    while (true) {\n      const item = next()\n      if (!item) break\n\n      const { batch, resolve, reject } = item\n\n      try {\n        await handler(batch, { workerId })\n        resolve()\n      } catch (error) {\n        reject(error)\n      }\n    }\n  })\n\n  return { run }\n}\n\nmodule.exports = createWorker\n", "/**\n * @typedef {ReturnType<typeof createWorkerQueue>} WorkerQueue\n */\n\n/**\n * @param {object} options\n * @param {import('./worker').Worker<T>[]} options.workers\n * @template T\n */\nconst createWorkerQueue = ({ workers }) => {\n  /** @type {{ batch: T, resolve: (value?: any) => void, reject: (e: Error) => void}[]} */\n  const queue = []\n\n  const getWorkers = () => workers\n\n  /**\n   * Waits until workers have processed all batches in the queue.\n   *\n   * @param {...T} batches\n   * @returns {Promise<void>}\n   */\n  const push = async (...batches) => {\n    const promises = batches.map(\n      batch => new Promise((resolve, reject) => queue.push({ batch, resolve, reject }))\n    )\n\n    workers.forEach(worker => worker.run({ next: () => queue.shift() }))\n\n    const results = await Promise.allSettled(promises)\n    const rejected = results.find(result => result.status === 'rejected')\n    if (rejected) {\n      // @ts-ignore\n      throw rejected.reason\n    }\n  }\n\n  return { push, getWorkers }\n}\n\nmodule.exports = createWorkerQueue\n", "const seq = require('../utils/seq')\nconst createFetcher = require('./fetcher')\nconst createWorker = require('./worker')\nconst createWorkerQueue = require('./workerQueue')\nconst { KafkaJSFetcherRebalanceError, KafkaJSNoBrokerAvailableError } = require('../errors')\n\n/** @typedef {ReturnType<typeof createFetchManager>} FetchManager */\n\n/**\n * @param {object} options\n * @param {import('../../types').Logger} options.logger\n * @param {() => number[]} options.getNodeIds\n * @param {(nodeId: number) => Promise<import('../../types').Batch[]>} options.fetch\n * @param {import('./worker').Handler<T>} options.handler\n * @param {number} [options.concurrency]\n * @template T\n */\nconst createFetchManager = ({\n  logger: rootLogger,\n  getNodeIds,\n  fetch,\n  handler,\n  concurrency = 1,\n}) => {\n  const logger = rootLogger.namespace('FetchManager')\n  const workers = seq(concurrency, workerId => createWorker({ handler, workerId }))\n  const workerQueue = createWorkerQueue({ workers })\n\n  let fetchers = []\n\n  const getFetchers = () => fetchers\n\n  const createFetchers = () => {\n    const nodeIds = getNodeIds()\n    const partitionAssignments = new Map()\n\n    if (nodeIds.length === 0) {\n      throw new KafkaJSNoBrokerAvailableError()\n    }\n\n    const validateShouldRebalance = () => {\n      const current = getNodeIds()\n      const hasChanged =\n        nodeIds.length !== current.length || nodeIds.some(nodeId => !current.includes(nodeId))\n      if (hasChanged && current.length !== 0) {\n        throw new KafkaJSFetcherRebalanceError()\n      }\n    }\n\n    const fetchers = nodeIds.map(nodeId =>\n      createFetcher({\n        nodeId,\n        workerQueue,\n        partitionAssignments,\n        fetch: async nodeId => {\n          validateShouldRebalance()\n          return fetch(nodeId)\n        },\n        logger,\n      })\n    )\n\n    logger.debug(`Created ${fetchers.length} fetchers`, { nodeIds, concurrency })\n    return fetchers\n  }\n\n  const start = async () => {\n    logger.debug('Starting...')\n\n    while (true) {\n      fetchers = createFetchers()\n\n      try {\n        await Promise.all(fetchers.map(fetcher => fetcher.start()))\n      } catch (error) {\n        await stop()\n\n        if (error instanceof KafkaJSFetcherRebalanceError) {\n          logger.debug('Rebalancing fetchers...')\n          continue\n        }\n\n        throw error\n      }\n\n      break\n    }\n  }\n\n  const stop = async () => {\n    logger.debug('Stopping fetchers...')\n    await Promise.all(fetchers.map(fetcher => fetcher.stop()))\n    logger.debug('Stopped fetchers')\n  }\n\n  return { start, stop, getFetchers }\n}\n\nmodule.exports = createFetchManager\n", "const { EventEmitter } = require('events')\nconst Long = require('../utils/long')\nconst createRetry = require('../retry')\nconst { isKafkaJSError, isRebalancing } = require('../errors')\n\nconst {\n  events: { FETCH, FETCH_START, START_BATCH_PROCESS, END_BATCH_PROCESS, REBALANCING },\n} = require('./instrumentationEvents')\nconst createFetchManager = require('./fetchManager')\n\nconst isSameOffset = (offsetA, offsetB) => Long.fromValue(offsetA).equals(Long.fromValue(offsetB))\nconst CONSUMING_START = 'consuming-start'\nconst CONSUMING_STOP = 'consuming-stop'\n\nmodule.exports = class Runner extends EventEmitter {\n  /**\n   * @param {object} options\n   * @param {import(\"../../types\").Logger} options.logger\n   * @param {import(\"./consumerGroup\")} options.consumerGroup\n   * @param {import(\"../instrumentation/emitter\")} options.instrumentationEmitter\n   * @param {boolean} [options.eachBatchAutoResolve=true]\n   * @param {number} options.concurrency\n   * @param {(payload: import(\"../../types\").EachBatchPayload) => Promise<void>} [options.eachBatch]\n   * @param {(payload: import(\"../../types\").EachMessagePayload) => Promise<void>} [options.eachMessage]\n   * @param {number} [options.heartbeatInterval]\n   * @param {(reason: Error) => void} options.onCrash\n   * @param {import(\"../../types\").RetryOptions} [options.retry]\n   * @param {boolean} [options.autoCommit=true]\n   */\n  constructor({\n    logger,\n    consumerGroup,\n    instrumentationEmitter,\n    eachBatchAutoResolve = true,\n    concurrency,\n    eachBatch,\n    eachMessage,\n    heartbeatInterval,\n    onCrash,\n    retry,\n    autoCommit = true,\n  }) {\n    super()\n    this.logger = logger.namespace('Runner')\n    this.consumerGroup = consumerGroup\n    this.instrumentationEmitter = instrumentationEmitter\n    this.eachBatchAutoResolve = eachBatchAutoResolve\n    this.eachBatch = eachBatch\n    this.eachMessage = eachMessage\n    this.heartbeatInterval = heartbeatInterval\n    this.retrier = createRetry(Object.assign({}, retry))\n    this.onCrash = onCrash\n    this.autoCommit = autoCommit\n    this.fetchManager = createFetchManager({\n      logger: this.logger,\n      getNodeIds: () => this.consumerGroup.getNodeIds(),\n      fetch: nodeId => this.fetch(nodeId),\n      handler: batch => this.handleBatch(batch),\n      concurrency,\n    })\n\n    this.running = false\n    this.consuming = false\n  }\n\n  get consuming() {\n    return this._consuming\n  }\n\n  set consuming(value) {\n    if (this._consuming !== value) {\n      this._consuming = value\n      this.emit(value ? CONSUMING_START : CONSUMING_STOP)\n    }\n  }\n\n  async start() {\n    if (this.running) {\n      return\n    }\n\n    try {\n      await this.consumerGroup.connect()\n      await this.consumerGroup.joinAndSync()\n    } catch (e) {\n      return this.onCrash(e)\n    }\n\n    this.running = true\n    this.scheduleFetchManager()\n  }\n\n  scheduleFetchManager() {\n    if (!this.running) {\n      this.consuming = false\n\n      this.logger.info('consumer not running, exiting', {\n        groupId: this.consumerGroup.groupId,\n        memberId: this.consumerGroup.memberId,\n      })\n\n      return\n    }\n\n    this.consuming = true\n\n    this.retrier(async (bail, retryCount, retryTime) => {\n      if (!this.running) {\n        return\n      }\n\n      try {\n        await this.fetchManager.start()\n      } catch (e) {\n        if (isRebalancing(e)) {\n          this.logger.warn('The group is rebalancing, re-joining', {\n            groupId: this.consumerGroup.groupId,\n            memberId: this.consumerGroup.memberId,\n            error: e.message,\n          })\n\n          this.instrumentationEmitter.emit(REBALANCING, {\n            groupId: this.consumerGroup.groupId,\n            memberId: this.consumerGroup.memberId,\n          })\n\n          await this.consumerGroup.joinAndSync()\n          return\n        }\n\n        if (e.type === 'UNKNOWN_MEMBER_ID') {\n          this.logger.error('The coordinator is not aware of this member, re-joining the group', {\n            groupId: this.consumerGroup.groupId,\n            memberId: this.consumerGroup.memberId,\n            error: e.message,\n          })\n\n          this.consumerGroup.memberId = null\n          await this.consumerGroup.joinAndSync()\n          return\n        }\n\n        if (e.name === 'KafkaJSNotImplemented') {\n          return bail(e)\n        }\n\n        if (e.name === 'KafkaJSNoBrokerAvailableError') {\n          return bail(e)\n        }\n\n        this.logger.debug('Error while scheduling fetch manager, trying again...', {\n          groupId: this.consumerGroup.groupId,\n          memberId: this.consumerGroup.memberId,\n          error: e.message,\n          stack: e.stack,\n          retryCount,\n          retryTime,\n        })\n\n        throw e\n      }\n    })\n      .then(() => {\n        this.scheduleFetchManager()\n      })\n      .catch(e => {\n        this.onCrash(e)\n        this.consuming = false\n        this.running = false\n      })\n  }\n\n  async stop() {\n    if (!this.running) {\n      return\n    }\n\n    this.logger.debug('stop consumer group', {\n      groupId: this.consumerGroup.groupId,\n      memberId: this.consumerGroup.memberId,\n    })\n\n    this.running = false\n\n    try {\n      await this.fetchManager.stop()\n      await this.waitForConsumer()\n      await this.consumerGroup.leave()\n    } catch (e) {}\n  }\n\n  waitForConsumer() {\n    return new Promise(resolve => {\n      if (!this.consuming) {\n        return resolve()\n      }\n\n      this.logger.debug('waiting for consumer to finish...', {\n        groupId: this.consumerGroup.groupId,\n        memberId: this.consumerGroup.memberId,\n      })\n\n      this.once(CONSUMING_STOP, () => resolve())\n    })\n  }\n\n  async heartbeat() {\n    try {\n      await this.consumerGroup.heartbeat({ interval: this.heartbeatInterval })\n    } catch (e) {\n      if (isRebalancing(e)) {\n        await this.autoCommitOffsets()\n      }\n      throw e\n    }\n  }\n\n  async processEachMessage(batch) {\n    const { topic, partition } = batch\n\n    const pause = () => {\n      this.consumerGroup.pause([{ topic, partitions: [partition] }])\n      return () => this.consumerGroup.resume([{ topic, partitions: [partition] }])\n    }\n    for (const message of batch.messages) {\n      if (!this.running || this.consumerGroup.hasSeekOffset({ topic, partition })) {\n        break\n      }\n\n      try {\n        await this.eachMessage({\n          topic,\n          partition,\n          message,\n          heartbeat: () => this.heartbeat(),\n          pause,\n        })\n      } catch (e) {\n        if (!isKafkaJSError(e)) {\n          this.logger.error(`Error when calling eachMessage`, {\n            topic,\n            partition,\n            offset: message.offset,\n            stack: e.stack,\n            error: e,\n          })\n        }\n\n        // In case of errors, commit the previously consumed offsets unless autoCommit is disabled\n        await this.autoCommitOffsets()\n        throw e\n      }\n\n      this.consumerGroup.resolveOffset({ topic, partition, offset: message.offset })\n      await this.heartbeat()\n      await this.autoCommitOffsetsIfNecessary()\n\n      if (this.consumerGroup.isPaused(topic, partition)) {\n        break\n      }\n    }\n  }\n\n  async processEachBatch(batch) {\n    const { topic, partition } = batch\n    const lastFilteredMessage = batch.messages[batch.messages.length - 1]\n\n    const pause = () => {\n      this.consumerGroup.pause([{ topic, partitions: [partition] }])\n      return () => this.consumerGroup.resume([{ topic, partitions: [partition] }])\n    }\n\n    try {\n      await this.eachBatch({\n        batch,\n        resolveOffset: offset => {\n          /**\n           * The transactional producer generates a control record after committing the transaction.\n           * The control record is the last record on the RecordBatch, and it is filtered before it\n           * reaches the eachBatch callback. When disabling auto-resolve, the user-land code won't\n           * be able to resolve the control record offset, since it never reaches the callback,\n           * causing stuck consumers as the consumer will never move the offset marker.\n           *\n           * When the last offset of the batch is resolved, we should automatically resolve\n           * the control record offset as this entry doesn't have any meaning to the user-land code,\n           * and won't interfere with the stream processing.\n           *\n           * @see https://github.com/apache/kafka/blob/9aa660786e46c1efbf5605a6a69136a1dac6edb9/clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java#L1499-L1505\n           */\n          const offsetToResolve =\n            lastFilteredMessage && isSameOffset(offset, lastFilteredMessage.offset)\n              ? batch.lastOffset()\n              : offset\n\n          this.consumerGroup.resolveOffset({ topic, partition, offset: offsetToResolve })\n        },\n        heartbeat: () => this.heartbeat(),\n        /**\n         * Pause consumption for the current topic-partition being processed\n         */\n        pause,\n        /**\n         * Commit offsets if provided. Otherwise commit most recent resolved offsets\n         * if the autoCommit conditions are met.\n         *\n         * @param {import('../../types').OffsetsByTopicPartition} [offsets] Optional.\n         */\n        commitOffsetsIfNecessary: async offsets => {\n          return offsets\n            ? this.consumerGroup.commitOffsets(offsets)\n            : this.consumerGroup.commitOffsetsIfNecessary()\n        },\n        uncommittedOffsets: () => this.consumerGroup.uncommittedOffsets(),\n        isRunning: () => this.running,\n        isStale: () => this.consumerGroup.hasSeekOffset({ topic, partition }),\n      })\n    } catch (e) {\n      if (!isKafkaJSError(e)) {\n        this.logger.error(`Error when calling eachBatch`, {\n          topic,\n          partition,\n          offset: batch.firstOffset(),\n          stack: e.stack,\n          error: e,\n        })\n      }\n\n      // eachBatch has a special resolveOffset which can be used\n      // to keep track of the messages\n      await this.autoCommitOffsets()\n      throw e\n    }\n\n    // resolveOffset for the last offset can be disabled to allow the users of eachBatch to\n    // stop their consumers without resolving unprocessed offsets (issues/18)\n    if (this.eachBatchAutoResolve) {\n      this.consumerGroup.resolveOffset({ topic, partition, offset: batch.lastOffset() })\n    }\n  }\n\n  async fetch(nodeId) {\n    if (!this.running) {\n      this.logger.debug('consumer not running, exiting', {\n        groupId: this.consumerGroup.groupId,\n        memberId: this.consumerGroup.memberId,\n      })\n\n      return []\n    }\n\n    const startFetch = Date.now()\n\n    this.instrumentationEmitter.emit(FETCH_START, { nodeId })\n\n    const batches = await this.consumerGroup.fetch(nodeId)\n\n    this.instrumentationEmitter.emit(FETCH, {\n      /**\n       * PR #570 removed support for the number of batches in this instrumentation event;\n       * The new implementation uses an async generation to deliver the batches, which makes\n       * this number impossible to get. The number is set to 0 to keep the event backward\n       * compatible until we bump KafkaJS to version 2, following the end of node 8 LTS.\n       *\n       * @since 2019-11-29\n       */\n      numberOfBatches: 0,\n      duration: Date.now() - startFetch,\n      nodeId,\n    })\n\n    if (batches.length === 0) {\n      await this.heartbeat()\n    }\n\n    return batches\n  }\n\n  async handleBatch(batch) {\n    if (!this.running) {\n      this.logger.debug('consumer not running, exiting', {\n        groupId: this.consumerGroup.groupId,\n        memberId: this.consumerGroup.memberId,\n      })\n\n      return\n    }\n\n    /** @param {import('./batch')} batch */\n    const onBatch = async batch => {\n      const startBatchProcess = Date.now()\n      const payload = {\n        topic: batch.topic,\n        partition: batch.partition,\n        highWatermark: batch.highWatermark,\n        offsetLag: batch.offsetLag(),\n        /**\n         * @since 2019-06-24 (>= 1.8.0)\n         *\n         * offsetLag returns the lag based on the latest offset in the batch, to\n         * keep the event backward compatible we just introduced \"offsetLagLow\"\n         * which calculates the lag based on the first offset in the batch\n         */\n        offsetLagLow: batch.offsetLagLow(),\n        batchSize: batch.messages.length,\n        firstOffset: batch.firstOffset(),\n        lastOffset: batch.lastOffset(),\n      }\n\n      /**\n       * If the batch contained only control records or only aborted messages then we still\n       * need to resolve and auto-commit to ensure the consumer can move forward.\n       *\n       * We also need to emit batch instrumentation events to allow any listeners keeping\n       * track of offsets to know about the latest point of consumption.\n       *\n       * Added in #1256\n       *\n       * @see https://github.com/apache/kafka/blob/9aa660786e46c1efbf5605a6a69136a1dac6edb9/clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java#L1499-L1505\n       */\n      if (batch.isEmptyDueToFiltering()) {\n        this.instrumentationEmitter.emit(START_BATCH_PROCESS, payload)\n\n        this.consumerGroup.resolveOffset({\n          topic: batch.topic,\n          partition: batch.partition,\n          offset: batch.lastOffset(),\n        })\n        await this.autoCommitOffsetsIfNecessary()\n\n        this.instrumentationEmitter.emit(END_BATCH_PROCESS, {\n          ...payload,\n          duration: Date.now() - startBatchProcess,\n        })\n\n        await this.heartbeat()\n        return\n      }\n\n      if (batch.isEmpty()) {\n        await this.heartbeat()\n        return\n      }\n\n      this.instrumentationEmitter.emit(START_BATCH_PROCESS, payload)\n\n      if (this.eachMessage) {\n        await this.processEachMessage(batch)\n      } else if (this.eachBatch) {\n        await this.processEachBatch(batch)\n      }\n\n      this.instrumentationEmitter.emit(END_BATCH_PROCESS, {\n        ...payload,\n        duration: Date.now() - startBatchProcess,\n      })\n\n      await this.autoCommitOffsets()\n      await this.heartbeat()\n    }\n\n    await onBatch(batch)\n  }\n\n  autoCommitOffsets() {\n    if (this.autoCommit) {\n      return this.consumerGroup.commitOffsets()\n    }\n  }\n\n  autoCommitOffsetsIfNecessary() {\n    if (this.autoCommit) {\n      return this.consumerGroup.commitOffsetsIfNecessary()\n    }\n  }\n\n  commitOffsets(offsets) {\n    if (!this.running) {\n      this.logger.debug('consumer not running, exiting', {\n        groupId: this.consumerGroup.groupId,\n        memberId: this.consumerGroup.memberId,\n        offsets,\n      })\n      return\n    }\n\n    return this.retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await this.consumerGroup.commitOffsets(offsets)\n      } catch (e) {\n        if (!this.running) {\n          this.logger.debug('consumer not running, exiting', {\n            error: e.message,\n            groupId: this.consumerGroup.groupId,\n            memberId: this.consumerGroup.memberId,\n            offsets,\n          })\n          return\n        }\n\n        if (e.name === 'KafkaJSNotImplemented') {\n          return bail(e)\n        }\n\n        this.logger.debug('Error while committing offsets, trying again...', {\n          groupId: this.consumerGroup.groupId,\n          memberId: this.consumerGroup.memberId,\n          error: e.message,\n          stack: e.stack,\n          retryCount,\n          retryTime,\n          offsets,\n        })\n\n        throw e\n      }\n    })\n  }\n}\n", "const { MemberMetadata, MemberAssignment } = require('../../assignerProtocol')\n\n/**\n * RoundRobinAssigner\n * @type {import('types').PartitionAssigner}\n */\nmodule.exports = ({ cluster }) => ({\n  name: 'RoundRobinAssigner',\n  version: 0,\n\n  /**\n   * Assign the topics to the provided members.\n   *\n   * The members array contains information about each member, `memberMetadata` is the result of the\n   * `protocol` operation.\n   *\n   * @param {object} group\n   * @param {import('types').GroupMember[]} group.members array of members, e.g:\n                              [{ memberId: 'test-5f93f5a3', memberMetadata: Buffer }]\n   * @param {string[]} group.topics\n   * @returns {Promise<import('types').GroupMemberAssignment[]>} object partitions per topic per member, e.g:\n   *                   [\n   *                     {\n   *                       memberId: 'test-5f93f5a3',\n   *                       memberAssignment: {\n   *                         'topic-A': [0, 2, 4, 6],\n   *                         'topic-B': [1],\n   *                       },\n   *                     },\n   *                     {\n   *                       memberId: 'test-3d3d5341',\n   *                       memberAssignment: {\n   *                         'topic-A': [1, 3, 5],\n   *                         'topic-B': [0, 2],\n   *                       },\n   *                     }\n   *                   ]\n   */\n  async assign({ members, topics }) {\n    const membersCount = members.length\n    const sortedMembers = members.map(({ memberId }) => memberId).sort()\n    const assignment = {}\n\n    const topicsPartitions = topics.flatMap(topic => {\n      const partitionMetadata = cluster.findTopicPartitionMetadata(topic)\n      return partitionMetadata.map(m => ({ topic: topic, partitionId: m.partitionId }))\n    })\n\n    topicsPartitions.forEach((topicPartition, i) => {\n      const assignee = sortedMembers[i % membersCount]\n\n      if (!assignment[assignee]) {\n        assignment[assignee] = Object.create(null)\n      }\n\n      if (!assignment[assignee][topicPartition.topic]) {\n        assignment[assignee][topicPartition.topic] = []\n      }\n\n      assignment[assignee][topicPartition.topic].push(topicPartition.partitionId)\n    })\n\n    return Object.keys(assignment).map(memberId => ({\n      memberId,\n      memberAssignment: MemberAssignment.encode({\n        version: this.version,\n        assignment: assignment[memberId],\n      }),\n    }))\n  },\n\n  protocol({ topics }) {\n    return {\n      name: this.name,\n      metadata: MemberMetadata.encode({\n        version: this.version,\n        topics,\n      }),\n    }\n  },\n})\n", "const roundRobin = require('./roundRobinAssigner')\n\nmodule.exports = {\n  roundRobin,\n}\n", "const Long = require('../utils/long')\nconst createRetry = require('../retry')\nconst { initialRetryTime } = require('../retry/defaults')\nconst ConsumerGroup = require('./consumerGroup')\nconst Runner = require('./runner')\nconst { events, wrap: wrapEvent, unwrap: unwrapEvent } = require('./instrumentationEvents')\nconst InstrumentationEventEmitter = require('../instrumentation/emitter')\nconst { KafkaJSNonRetriableError } = require('../errors')\nconst { roundRobin } = require('./assigners')\nconst { EARLIEST_OFFSET, LATEST_OFFSET } = require('../constants')\nconst ISOLATION_LEVEL = require('../protocol/isolationLevel')\nconst sharedPromiseTo = require('../utils/sharedPromiseTo')\n\nconst { keys, values } = Object\nconst { CONNECT, DISCONNECT, STOP, CRASH } = events\n\nconst eventNames = values(events)\nconst eventKeys = keys(events)\n  .map(key => `consumer.events.${key}`)\n  .join(', ')\n\nconst specialOffsets = [\n  Long.fromValue(EARLIEST_OFFSET).toString(),\n  Long.fromValue(LATEST_OFFSET).toString(),\n]\n\n/**\n * @param {Object} params\n * @param {import(\"../../types\").Cluster} params.cluster\n * @param {String} params.groupId\n * @param {import('../../types').RetryOptions} [params.retry]\n * @param {import('../../types').Logger} params.logger\n * @param {import('../../types').PartitionAssigner[]} [params.partitionAssigners]\n * @param {number} [params.sessionTimeout]\n * @param {number} [params.rebalanceTimeout]\n * @param {number} [params.heartbeatInterval]\n * @param {number} [params.maxBytesPerPartition]\n * @param {number} [params.minBytes]\n * @param {number} [params.maxBytes]\n * @param {number} [params.maxWaitTimeInMs]\n * @param {number} [params.isolationLevel]\n * @param {string} [params.rackId]\n * @param {InstrumentationEventEmitter} [params.instrumentationEmitter]\n * @param {number} params.metadataMaxAge\n *\n * @returns {import(\"../../types\").Consumer}\n */\nmodule.exports = ({\n  cluster,\n  groupId,\n  retry,\n  logger: rootLogger,\n  partitionAssigners = [roundRobin],\n  sessionTimeout = 30000,\n  rebalanceTimeout = 60000,\n  heartbeatInterval = 3000,\n  maxBytesPerPartition = 1048576, // 1MB\n  minBytes = 1,\n  maxBytes = 10485760, // 10MB\n  maxWaitTimeInMs = 5000,\n  isolationLevel = ISOLATION_LEVEL.READ_COMMITTED,\n  rackId = '',\n  instrumentationEmitter: rootInstrumentationEmitter,\n  metadataMaxAge,\n}) => {\n  if (!groupId) {\n    throw new KafkaJSNonRetriableError('Consumer groupId must be a non-empty string.')\n  }\n\n  const logger = rootLogger.namespace('Consumer')\n  const instrumentationEmitter = rootInstrumentationEmitter || new InstrumentationEventEmitter()\n  const assigners = partitionAssigners.map(createAssigner =>\n    createAssigner({ groupId, logger, cluster })\n  )\n\n  /** @type {Record<string, { fromBeginning?: boolean }>} */\n  const topics = {}\n  let runner = null\n  /** @type {ConsumerGroup} */\n  let consumerGroup = null\n  let restartTimeout = null\n\n  if (heartbeatInterval >= sessionTimeout) {\n    throw new KafkaJSNonRetriableError(\n      `Consumer heartbeatInterval (${heartbeatInterval}) must be lower than sessionTimeout (${sessionTimeout}). It is recommended to set heartbeatInterval to approximately a third of the sessionTimeout.`\n    )\n  }\n\n  /** @type {import(\"../../types\").Consumer[\"connect\"]} */\n  const connect = async () => {\n    await cluster.connect()\n    instrumentationEmitter.emit(CONNECT)\n  }\n\n  /** @type {import(\"../../types\").Consumer[\"disconnect\"]} */\n  const disconnect = async () => {\n    try {\n      await stop()\n      logger.debug('consumer has stopped, disconnecting', { groupId })\n      await cluster.disconnect()\n      instrumentationEmitter.emit(DISCONNECT)\n    } catch (e) {\n      logger.error(`Caught error when disconnecting the consumer: ${e.message}`, {\n        stack: e.stack,\n        groupId,\n      })\n      throw e\n    }\n  }\n\n  /** @type {import(\"../../types\").Consumer[\"stop\"]} */\n  const stop = sharedPromiseTo(async () => {\n    try {\n      if (runner) {\n        await runner.stop()\n        runner = null\n        consumerGroup = null\n        instrumentationEmitter.emit(STOP)\n      }\n\n      clearTimeout(restartTimeout)\n      logger.info('Stopped', { groupId })\n    } catch (e) {\n      logger.error(`Caught error when stopping the consumer: ${e.message}`, {\n        stack: e.stack,\n        groupId,\n      })\n\n      throw e\n    }\n  })\n\n  /** @type {import(\"../../types\").Consumer[\"subscribe\"]} */\n  const subscribe = async ({ topic, topics: subscriptionTopics, fromBeginning = false }) => {\n    if (consumerGroup) {\n      throw new KafkaJSNonRetriableError('Cannot subscribe to topic while consumer is running')\n    }\n\n    if (!topic && !subscriptionTopics) {\n      throw new KafkaJSNonRetriableError('Missing required argument \"topics\"')\n    }\n\n    if (subscriptionTopics != null && !Array.isArray(subscriptionTopics)) {\n      throw new KafkaJSNonRetriableError('Argument \"topics\" must be an array')\n    }\n\n    const subscriptions = subscriptionTopics || [topic]\n\n    for (const subscription of subscriptions) {\n      if (typeof subscription !== 'string' && !(subscription instanceof RegExp)) {\n        throw new KafkaJSNonRetriableError(\n          `Invalid topic ${subscription} (${typeof subscription}), the topic name has to be a String or a RegExp`\n        )\n      }\n    }\n\n    const hasRegexSubscriptions = subscriptions.some(subscription => subscription instanceof RegExp)\n    const metadata = hasRegexSubscriptions ? await cluster.metadata() : undefined\n\n    const topicsToSubscribe = []\n    for (const subscription of subscriptions) {\n      const isRegExp = subscription instanceof RegExp\n      if (isRegExp) {\n        const topicRegExp = subscription\n        const matchedTopics = metadata.topicMetadata\n          .map(({ topic: topicName }) => topicName)\n          .filter(topicName => topicRegExp.test(topicName))\n\n        logger.debug('Subscription based on RegExp', {\n          groupId,\n          topicRegExp: topicRegExp.toString(),\n          matchedTopics,\n        })\n\n        topicsToSubscribe.push(...matchedTopics)\n      } else {\n        topicsToSubscribe.push(subscription)\n      }\n    }\n\n    for (const t of topicsToSubscribe) {\n      topics[t] = { fromBeginning }\n    }\n\n    await cluster.addMultipleTargetTopics(topicsToSubscribe)\n  }\n\n  /** @type {import(\"../../types\").Consumer[\"run\"]} */\n  const run = async ({\n    autoCommit = true,\n    autoCommitInterval = null,\n    autoCommitThreshold = null,\n    eachBatchAutoResolve = true,\n    partitionsConsumedConcurrently: concurrency = 1,\n    eachBatch = null,\n    eachMessage = null,\n  } = {}) => {\n    if (consumerGroup) {\n      logger.warn('consumer#run was called, but the consumer is already running', { groupId })\n      return\n    }\n\n    const start = async onCrash => {\n      logger.info('Starting', { groupId })\n\n      consumerGroup = new ConsumerGroup({\n        logger: rootLogger,\n        topics: keys(topics),\n        topicConfigurations: topics,\n        retry,\n        cluster,\n        groupId,\n        assigners,\n        sessionTimeout,\n        rebalanceTimeout,\n        maxBytesPerPartition,\n        minBytes,\n        maxBytes,\n        maxWaitTimeInMs,\n        instrumentationEmitter,\n        isolationLevel,\n        rackId,\n        metadataMaxAge,\n        autoCommit,\n        autoCommitInterval,\n        autoCommitThreshold,\n      })\n\n      runner = new Runner({\n        logger: rootLogger,\n        consumerGroup,\n        instrumentationEmitter,\n        heartbeatInterval,\n        retry,\n        autoCommit,\n        eachBatchAutoResolve,\n        eachBatch,\n        eachMessage,\n        onCrash,\n        concurrency,\n      })\n\n      await runner.start()\n    }\n\n    const onCrash = async e => {\n      logger.error(`Crash: ${e.name}: ${e.message}`, {\n        groupId,\n        retryCount: e.retryCount,\n        stack: e.stack,\n      })\n\n      if (e.name === 'KafkaJSConnectionClosedError') {\n        cluster.removeBroker({ host: e.host, port: e.port })\n      }\n\n      await disconnect()\n\n      const getOriginalCause = error => {\n        if (error.cause) {\n          return getOriginalCause(error.cause)\n        }\n\n        return error\n      }\n\n      const isErrorRetriable =\n        e.name === 'KafkaJSNumberOfRetriesExceeded' || getOriginalCause(e).retriable === true\n      const shouldRestart =\n        isErrorRetriable &&\n        (!retry ||\n          !retry.restartOnFailure ||\n          (await retry.restartOnFailure(e).catch(error => {\n            logger.error(\n              'Caught error when invoking user-provided \"restartOnFailure\" callback. Defaulting to restarting.',\n              {\n                error: error.message || error,\n                cause: e.message || e,\n                groupId,\n              }\n            )\n\n            return true\n          })))\n\n      instrumentationEmitter.emit(CRASH, {\n        error: e,\n        groupId,\n        restart: shouldRestart,\n      })\n\n      if (shouldRestart) {\n        const retryTime = e.retryTime || (retry && retry.initialRetryTime) || initialRetryTime\n        logger.error(`Restarting the consumer in ${retryTime}ms`, {\n          retryCount: e.retryCount,\n          retryTime,\n          groupId,\n        })\n\n        restartTimeout = setTimeout(() => start(onCrash), retryTime)\n      }\n    }\n\n    await start(onCrash)\n  }\n\n  /** @type {import(\"../../types\").Consumer[\"on\"]} */\n  const on = (eventName, listener) => {\n    if (!eventNames.includes(eventName)) {\n      throw new KafkaJSNonRetriableError(`Event name should be one of ${eventKeys}`)\n    }\n\n    return instrumentationEmitter.addListener(unwrapEvent(eventName), event => {\n      event.type = wrapEvent(event.type)\n      Promise.resolve(listener(event)).catch(e => {\n        logger.error(`Failed to execute listener: ${e.message}`, {\n          eventName,\n          stack: e.stack,\n        })\n      })\n    })\n  }\n\n  /**\n   * @type {import(\"../../types\").Consumer[\"commitOffsets\"]}\n   * @param topicPartitions\n   *   Example: [{ topic: 'topic-name', partition: 0, offset: '1', metadata: 'event-id-3' }]\n   */\n  const commitOffsets = async (topicPartitions = []) => {\n    const commitsByTopic = topicPartitions.reduce(\n      (payload, { topic, partition, offset, metadata = null }) => {\n        if (!topic) {\n          throw new KafkaJSNonRetriableError(`Invalid topic ${topic}`)\n        }\n\n        if (isNaN(partition)) {\n          throw new KafkaJSNonRetriableError(\n            `Invalid partition, expected a number received ${partition}`\n          )\n        }\n\n        let commitOffset\n        try {\n          commitOffset = Long.fromValue(offset)\n        } catch (_) {\n          throw new KafkaJSNonRetriableError(`Invalid offset, expected a long received ${offset}`)\n        }\n\n        if (commitOffset.lessThan(0)) {\n          throw new KafkaJSNonRetriableError('Offset must not be a negative number')\n        }\n\n        if (metadata !== null && typeof metadata !== 'string') {\n          throw new KafkaJSNonRetriableError(\n            `Invalid offset metadata, expected string or null, received ${metadata}`\n          )\n        }\n\n        const topicCommits = payload[topic] || []\n\n        topicCommits.push({ partition, offset: commitOffset, metadata })\n\n        return { ...payload, [topic]: topicCommits }\n      },\n      {}\n    )\n\n    if (!consumerGroup) {\n      throw new KafkaJSNonRetriableError(\n        'Consumer group was not initialized, consumer#run must be called first'\n      )\n    }\n\n    const topics = Object.keys(commitsByTopic)\n\n    return runner.commitOffsets({\n      topics: topics.map(topic => {\n        return {\n          topic,\n          partitions: commitsByTopic[topic],\n        }\n      }),\n    })\n  }\n\n  /** @type {import(\"../../types\").Consumer[\"seek\"]} */\n  const seek = ({ topic, partition, offset }) => {\n    if (!topic) {\n      throw new KafkaJSNonRetriableError(`Invalid topic ${topic}`)\n    }\n\n    if (isNaN(partition)) {\n      throw new KafkaJSNonRetriableError(\n        `Invalid partition, expected a number received ${partition}`\n      )\n    }\n\n    let seekOffset\n    try {\n      seekOffset = Long.fromValue(offset)\n    } catch (_) {\n      throw new KafkaJSNonRetriableError(`Invalid offset, expected a long received ${offset}`)\n    }\n\n    if (seekOffset.lessThan(0) && !specialOffsets.includes(seekOffset.toString())) {\n      throw new KafkaJSNonRetriableError('Offset must not be a negative number')\n    }\n\n    if (!consumerGroup) {\n      throw new KafkaJSNonRetriableError(\n        'Consumer group was not initialized, consumer#run must be called first'\n      )\n    }\n\n    consumerGroup.seek({ topic, partition, offset: seekOffset.toString() })\n  }\n\n  /** @type {import(\"../../types\").Consumer[\"describeGroup\"]} */\n  const describeGroup = async () => {\n    const coordinator = await cluster.findGroupCoordinator({ groupId })\n    const retrier = createRetry(retry)\n    return retrier(async () => {\n      const { groups } = await coordinator.describeGroups({ groupIds: [groupId] })\n      return groups.find(group => group.groupId === groupId)\n    })\n  }\n\n  /**\n   * @type {import(\"../../types\").Consumer[\"pause\"]}\n   * @param topicPartitions\n   *   Example: [{ topic: 'topic-name', partitions: [1, 2] }]\n   */\n  const pause = (topicPartitions = []) => {\n    for (const topicPartition of topicPartitions) {\n      if (!topicPartition || !topicPartition.topic) {\n        throw new KafkaJSNonRetriableError(\n          `Invalid topic ${(topicPartition && topicPartition.topic) || topicPartition}`\n        )\n      } else if (\n        typeof topicPartition.partitions !== 'undefined' &&\n        (!Array.isArray(topicPartition.partitions) || topicPartition.partitions.some(isNaN))\n      ) {\n        throw new KafkaJSNonRetriableError(\n          `Array of valid partitions required to pause specific partitions instead of ${topicPartition.partitions}`\n        )\n      }\n    }\n\n    if (!consumerGroup) {\n      throw new KafkaJSNonRetriableError(\n        'Consumer group was not initialized, consumer#run must be called first'\n      )\n    }\n\n    consumerGroup.pause(topicPartitions)\n  }\n\n  /**\n   * Returns the list of topic partitions paused on this consumer\n   *\n   * @type {import(\"../../types\").Consumer[\"paused\"]}\n   */\n  const paused = () => {\n    if (!consumerGroup) {\n      return []\n    }\n\n    return consumerGroup.paused()\n  }\n\n  /**\n   * @type {import(\"../../types\").Consumer[\"resume\"]}\n   * @param topicPartitions\n   *  Example: [{ topic: 'topic-name', partitions: [1, 2] }]\n   */\n  const resume = (topicPartitions = []) => {\n    for (const topicPartition of topicPartitions) {\n      if (!topicPartition || !topicPartition.topic) {\n        throw new KafkaJSNonRetriableError(\n          `Invalid topic ${(topicPartition && topicPartition.topic) || topicPartition}`\n        )\n      } else if (\n        typeof topicPartition.partitions !== 'undefined' &&\n        (!Array.isArray(topicPartition.partitions) || topicPartition.partitions.some(isNaN))\n      ) {\n        throw new KafkaJSNonRetriableError(\n          `Array of valid partitions required to resume specific partitions instead of ${topicPartition.partitions}`\n        )\n      }\n    }\n\n    if (!consumerGroup) {\n      throw new KafkaJSNonRetriableError(\n        'Consumer group was not initialized, consumer#run must be called first'\n      )\n    }\n\n    consumerGroup.resume(topicPartitions)\n  }\n\n  /**\n   * @return {Object} logger\n   */\n  const getLogger = () => logger\n\n  return {\n    connect,\n    disconnect,\n    subscribe,\n    stop,\n    run,\n    commitOffsets,\n    seek,\n    describeGroup,\n    pause,\n    paused,\n    resume,\n    on,\n    events,\n    logger: getLogger,\n  }\n}\n", "const sleep = require('./sleep')\nconst { KafkaJSTimeout } = require('../errors')\n\nmodule.exports = (\n  fn,\n  { delay = 50, maxWait = 10000, timeoutMessage = 'Timeout', ignoreTimeout = false } = {}\n) => {\n  let timeoutId\n  let totalWait = 0\n  let fulfilled = false\n\n  const checkCondition = async (resolve, reject) => {\n    totalWait += delay\n    if (fulfilled) {\n      return\n    }\n\n    await sleep(delay)\n\n    try {\n      const result = await fn(totalWait)\n      if (result) {\n        fulfilled = true\n        clearTimeout(timeoutId)\n        return resolve(result)\n      }\n\n      checkCondition(resolve, reject)\n    } catch (e) {\n      fulfilled = true\n      clearTimeout(timeoutId)\n      reject(e)\n    }\n  }\n\n  return new Promise((resolve, reject) => {\n    checkCondition(resolve, reject)\n\n    if (ignoreTimeout) {\n      return\n    }\n\n    timeoutId = setTimeout(() => {\n      if (!fulfilled) {\n        fulfilled = true\n        return reject(new KafkaJSTimeout(timeoutMessage))\n      }\n    }, maxWait)\n  })\n}\n", "module.exports = async (array, groupFn) => {\n  const result = new Map()\n\n  for (const item of array) {\n    const group = await Promise.resolve(groupFn(item))\n    result.set(group, result.has(group) ? [...result.get(group), item] : [item])\n  }\n\n  return result\n}\n", "const swapObject = require('../utils/swapObject')\nconst networkEvents = require('../network/instrumentationEvents')\nconst InstrumentationEventType = require('../instrumentation/eventType')\nconst adminType = InstrumentationEventType('admin')\n\nconst events = {\n  CONNECT: adminType('connect'),\n  DISCONNECT: adminType('disconnect'),\n  REQUEST: adminType(networkEvents.NETWORK_REQUEST),\n  REQUEST_TIMEOUT: adminType(networkEvents.NETWORK_REQUEST_TIMEOUT),\n  REQUEST_QUEUE_SIZE: adminType(networkEvents.NETWORK_REQUEST_QUEUE_SIZE),\n}\n\nconst wrappedEvents = {\n  [events.REQUEST]: networkEvents.NETWORK_REQUEST,\n  [events.REQUEST_TIMEOUT]: networkEvents.NETWORK_REQUEST_TIMEOUT,\n  [events.REQUEST_QUEUE_SIZE]: networkEvents.NETWORK_REQUEST_QUEUE_SIZE,\n}\n\nconst reversedWrappedEvents = swapObject(wrappedEvents)\nconst unwrap = eventName => wrappedEvents[eventName] || eventName\nconst wrap = eventName => reversedWrappedEvents[eventName] || eventName\n\nmodule.exports = {\n  events,\n  wrap,\n  unwrap,\n}\n", "/**\n * @see https://github.com/apache/kafka/blob/a15387f34d142684859c2a57fcbef25edcdce25a/clients/src/main/java/org/apache/kafka/common/resource/ResourceType.java#L25-L31\n * @typedef {number} ACLResourceTypes\n *\n * Enum for ACL Resource Types\n * @readonly\n * @enum {ACLResourceTypes}\n */\n\nmodule.exports = {\n  /**\n   * Represents any ResourceType which this client cannot understand,\n   * perhaps because this client is too old.\n   */\n  UNKNOWN: 0,\n  /**\n   * In a filter, matches any ResourceType.\n   */\n  ANY: 1,\n  /**\n   * A Kafka topic.\n   * @see http://kafka.apache.org/documentation/#topicconfigs\n   */\n  TOPIC: 2,\n  /**\n   * A consumer group.\n   * @see http://kafka.apache.org/documentation/#consumerconfigs\n   */\n  GROUP: 3,\n  /**\n   * The cluster as a whole.\n   */\n  CLUSTER: 4,\n  /**\n   * A transactional ID.\n   */\n  TRANSACTIONAL_ID: 5,\n  /**\n   * A token ID.\n   */\n  DELEGATION_TOKEN: 6,\n}\n", "// From:\n// https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/common/acl/AclOperation.java#L44\n\n/**\n * @typedef {number} ACLOperationTypes\n *\n * Enum for ACL Operations Types\n * @readonly\n * @enum {ACLOperationTypes}\n */\nmodule.exports = {\n  /**\n   * Represents any AclOperation which this client cannot understand, perhaps because this\n   * client is too old.\n   */\n  UNKNOWN: 0,\n  /**\n   * In a filter, matches any AclOperation.\n   */\n  ANY: 1,\n  /**\n   * ALL operation.\n   */\n  ALL: 2,\n  /**\n   * READ operation.\n   */\n  READ: 3,\n  /**\n   * WRITE operation.\n   */\n  WRITE: 4,\n  /**\n   * CREATE operation.\n   */\n  CREATE: 5,\n  /**\n   * DELETE operation.\n   */\n  DELETE: 6,\n  /**\n   * ALTER operation.\n   */\n  ALTER: 7,\n  /**\n   * DESCRIBE operation.\n   */\n  DESCRIBE: 8,\n  /**\n   * CLUSTER_ACTION operation.\n   */\n  CLUSTER_ACTION: 9,\n  /**\n   * DESCRIBE_CONFIGS operation.\n   */\n  DESCRIBE_CONFIGS: 10,\n  /**\n   * ALTER_CONFIGS operation.\n   */\n  ALTER_CONFIGS: 11,\n  /**\n   * IDEMPOTENT_WRITE operation.\n   */\n  IDEMPOTENT_WRITE: 12,\n}\n", "// From:\n// https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/common/acl/AclPermissionType.java/#L31\n\n/**\n * @typedef {number} ACLPermissionTypes\n *\n * Enum for Permission Types\n * @readonly\n * @enum {ACLPermissionTypes}\n */\nmodule.exports = {\n  /**\n   * Represents any AclPermissionType which this client cannot understand,\n   * perhaps because this client is too old.\n   */\n  UNKNOWN: 0,\n  /**\n   * In a filter, matches any AclPermissionType.\n   */\n  ANY: 1,\n  /**\n   * Disallows access.\n   */\n  DENY: 2,\n  /**\n   * Grants access.\n   */\n  ALLOW: 3,\n}\n", "// From:\n// https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/common/resource/PatternType.java#L32\n\n/**\n * @typedef {number} ACLResourcePatternTypes\n *\n * Enum for ACL Resource Pattern Type\n * @readonly\n * @enum {ACLResourcePatternTypes}\n */\nmodule.exports = {\n  /**\n   * Represents any PatternType which this client cannot understand, perhaps because this client is too old.\n   */\n  UNKNOWN: 0,\n  /**\n   * In a filter, matches any resource pattern type.\n   */\n  ANY: 1,\n  /**\n   * In a filter, will perform pattern matching.\n   *\n   * e.g. Given a filter of {@code ResourcePatternFilter(TOPIC, \"payments.received\", MATCH)`}, the filter match\n   * any {@link ResourcePattern} that matches topic 'payments.received'. This might include:\n   * <ul>\n   *     <li>A Literal pattern with the same type and name, e.g. {@code ResourcePattern(TOPIC, \"payments.received\", LITERAL)}</li>\n   *     <li>A Wildcard pattern with the same type, e.g. {@code ResourcePattern(TOPIC, \"*\", LITERAL)}</li>\n   *     <li>A Prefixed pattern with the same type and where the name is a matching prefix, e.g. {@code ResourcePattern(TOPIC, \"payments.\", PREFIXED)}</li>\n   * </ul>\n   */\n  MATCH: 2,\n  /**\n   * A literal resource name.\n   *\n   * A literal name defines the full name of a resource, e.g. topic with name 'foo', or group with name 'bob'.\n   *\n   * The special wildcard character {@code *} can be used to represent a resource with any name.\n   */\n  LITERAL: 3,\n  /**\n   * A prefixed resource name.\n   *\n   * A prefixed name defines a prefix for a resource, e.g. topics with names that start with 'foo'.\n   */\n  PREFIXED: 4,\n}\n", "const createRetry = require('../retry')\nconst waitFor = require('../utils/waitFor')\nconst groupBy = require('../utils/groupBy')\nconst createConsumer = require('../consumer')\nconst InstrumentationEventEmitter = require('../instrumentation/emitter')\nconst { events, wrap: wrapEvent, unwrap: unwrapEvent } = require('./instrumentationEvents')\nconst { LEVELS } = require('../loggers')\nconst {\n  KafkaJSNonRetriableError,\n  KafkaJSDeleteGroupsError,\n  KafkaJSBrokerNotFound,\n  KafkaJSDeleteTopicRecordsError,\n  KafkaJSAggregateError,\n} = require('../errors')\nconst { staleMetadata } = require('../protocol/error')\nconst CONFIG_RESOURCE_TYPES = require('../protocol/configResourceTypes')\nconst ACL_RESOURCE_TYPES = require('../protocol/aclResourceTypes')\nconst ACL_OPERATION_TYPES = require('../protocol/aclOperationTypes')\nconst ACL_PERMISSION_TYPES = require('../protocol/aclPermissionTypes')\nconst RESOURCE_PATTERN_TYPES = require('../protocol/resourcePatternTypes')\nconst { EARLIEST_OFFSET, LATEST_OFFSET } = require('../constants')\n\nconst { CONNECT, DISCONNECT } = events\n\nconst NO_CONTROLLER_ID = -1\n\nconst { values, keys, entries } = Object\nconst eventNames = values(events)\nconst eventKeys = keys(events)\n  .map(key => `admin.events.${key}`)\n  .join(', ')\n\nconst retryOnLeaderNotAvailable = (fn, opts = {}) => {\n  const callback = async () => {\n    try {\n      return await fn()\n    } catch (e) {\n      if (e.type !== 'LEADER_NOT_AVAILABLE') {\n        throw e\n      }\n      return false\n    }\n  }\n\n  return waitFor(callback, opts)\n}\n\nconst isConsumerGroupRunning = description => ['Empty', 'Dead'].includes(description.state)\nconst findTopicPartitions = async (cluster, topic) => {\n  await cluster.addTargetTopic(topic)\n  await cluster.refreshMetadataIfNecessary()\n\n  return cluster\n    .findTopicPartitionMetadata(topic)\n    .map(({ partitionId }) => partitionId)\n    .sort()\n}\nconst indexByPartition = array =>\n  array.reduce(\n    (obj, { partition, ...props }) => Object.assign(obj, { [partition]: { ...props } }),\n    {}\n  )\n\n/**\n *\n * @param {Object} params\n * @param {import(\"../../types\").Logger} params.logger\n * @param {InstrumentationEventEmitter} [params.instrumentationEmitter]\n * @param {import('../../types').RetryOptions} params.retry\n * @param {import(\"../../types\").Cluster} params.cluster\n *\n * @returns {import(\"../../types\").Admin}\n */\nmodule.exports = ({\n  logger: rootLogger,\n  instrumentationEmitter: rootInstrumentationEmitter,\n  retry,\n  cluster,\n}) => {\n  const logger = rootLogger.namespace('Admin')\n  const instrumentationEmitter = rootInstrumentationEmitter || new InstrumentationEventEmitter()\n\n  /**\n   * @returns {Promise}\n   */\n  const connect = async () => {\n    await cluster.connect()\n    instrumentationEmitter.emit(CONNECT)\n  }\n\n  /**\n   * @return {Promise}\n   */\n  const disconnect = async () => {\n    await cluster.disconnect()\n    instrumentationEmitter.emit(DISCONNECT)\n  }\n\n  /**\n   * @return {Promise}\n   */\n  const listTopics = async () => {\n    const { topicMetadata } = await cluster.metadata()\n    const topics = topicMetadata.map(t => t.topic)\n    return topics\n  }\n\n  /**\n   * @param {Object} request\n   * @param {array} request.topics\n   * @param {boolean} [request.validateOnly=false]\n   * @param {number} [request.timeout=5000]\n   * @param {boolean} [request.waitForLeaders=true]\n   * @return {Promise}\n   */\n  const createTopics = async ({ topics, validateOnly, timeout, waitForLeaders = true }) => {\n    if (!topics || !Array.isArray(topics)) {\n      throw new KafkaJSNonRetriableError(`Invalid topics array ${topics}`)\n    }\n\n    if (topics.filter(({ topic }) => typeof topic !== 'string').length > 0) {\n      throw new KafkaJSNonRetriableError(\n        'Invalid topics array, the topic names have to be a valid string'\n      )\n    }\n\n    const topicNames = new Set(topics.map(({ topic }) => topic))\n    if (topicNames.size < topics.length) {\n      throw new KafkaJSNonRetriableError(\n        'Invalid topics array, it cannot have multiple entries for the same topic'\n      )\n    }\n\n    for (const { topic, configEntries } of topics) {\n      if (configEntries == null) {\n        continue\n      }\n\n      if (!Array.isArray(configEntries)) {\n        throw new KafkaJSNonRetriableError(\n          `Invalid configEntries for topic \"${topic}\", must be an array`\n        )\n      }\n\n      configEntries.forEach((entry, index) => {\n        if (typeof entry !== 'object' || entry == null) {\n          throw new KafkaJSNonRetriableError(\n            `Invalid configEntries for topic \"${topic}\". Entry ${index} must be an object`\n          )\n        }\n\n        for (const requiredProperty of ['name', 'value']) {\n          if (\n            !Object.prototype.hasOwnProperty.call(entry, requiredProperty) ||\n            typeof entry[requiredProperty] !== 'string'\n          ) {\n            throw new KafkaJSNonRetriableError(\n              `Invalid configEntries for topic \"${topic}\". Entry ${index} must have a valid \"${requiredProperty}\" property`\n            )\n          }\n        }\n      })\n    }\n\n    const retrier = createRetry(retry)\n\n    return retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await cluster.refreshMetadata()\n        const broker = await cluster.findControllerBroker()\n        await broker.createTopics({ topics, validateOnly, timeout })\n\n        if (waitForLeaders) {\n          const topicNamesArray = Array.from(topicNames.values())\n          await retryOnLeaderNotAvailable(async () => await broker.metadata(topicNamesArray), {\n            delay: 100,\n            maxWait: timeout,\n            timeoutMessage: 'Timed out while waiting for topic leaders',\n          })\n        }\n\n        return true\n      } catch (e) {\n        if (e.type === 'NOT_CONTROLLER') {\n          logger.warn('Could not create topics', { error: e.message, retryCount, retryTime })\n          throw e\n        }\n\n        if (e instanceof KafkaJSAggregateError) {\n          if (e.errors.every(error => error.type === 'TOPIC_ALREADY_EXISTS')) {\n            return false\n          }\n        }\n\n        bail(e)\n      }\n    })\n  }\n  /**\n   * @param {array} topicPartitions\n   * @param {boolean} [validateOnly=false]\n   * @param {number} [timeout=5000]\n   * @return {Promise<void>}\n   */\n  const createPartitions = async ({ topicPartitions, validateOnly, timeout }) => {\n    if (!topicPartitions || !Array.isArray(topicPartitions)) {\n      throw new KafkaJSNonRetriableError(`Invalid topic partitions array ${topicPartitions}`)\n    }\n    if (topicPartitions.length === 0) {\n      throw new KafkaJSNonRetriableError(`Empty topic partitions array`)\n    }\n\n    if (topicPartitions.filter(({ topic }) => typeof topic !== 'string').length > 0) {\n      throw new KafkaJSNonRetriableError(\n        'Invalid topic partitions array, the topic names have to be a valid string'\n      )\n    }\n\n    const topicNames = new Set(topicPartitions.map(({ topic }) => topic))\n    if (topicNames.size < topicPartitions.length) {\n      throw new KafkaJSNonRetriableError(\n        'Invalid topic partitions array, it cannot have multiple entries for the same topic'\n      )\n    }\n\n    const retrier = createRetry(retry)\n\n    return retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await cluster.refreshMetadata()\n        const broker = await cluster.findControllerBroker()\n        await broker.createPartitions({ topicPartitions, validateOnly, timeout })\n      } catch (e) {\n        if (e.type === 'NOT_CONTROLLER') {\n          logger.warn('Could not create topics', { error: e.message, retryCount, retryTime })\n          throw e\n        }\n\n        bail(e)\n      }\n    })\n  }\n\n  /**\n   * @param {string[]} topics\n   * @param {number} [timeout=5000]\n   * @return {Promise}\n   */\n  const deleteTopics = async ({ topics, timeout }) => {\n    if (!topics || !Array.isArray(topics)) {\n      throw new KafkaJSNonRetriableError(`Invalid topics array ${topics}`)\n    }\n\n    if (topics.filter(topic => typeof topic !== 'string').length > 0) {\n      throw new KafkaJSNonRetriableError('Invalid topics array, the names must be a valid string')\n    }\n\n    const retrier = createRetry(retry)\n\n    return retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await cluster.refreshMetadata()\n        const broker = await cluster.findControllerBroker()\n        await broker.deleteTopics({ topics, timeout })\n\n        // Remove deleted topics\n        for (const topic of topics) {\n          cluster.targetTopics.delete(topic)\n        }\n\n        await cluster.refreshMetadata()\n      } catch (e) {\n        if (['NOT_CONTROLLER', 'UNKNOWN_TOPIC_OR_PARTITION'].includes(e.type)) {\n          logger.warn('Could not delete topics', { error: e.message, retryCount, retryTime })\n          throw e\n        }\n\n        if (e.type === 'REQUEST_TIMED_OUT') {\n          logger.error(\n            'Could not delete topics, check if \"delete.topic.enable\" is set to \"true\" (the default value is \"false\") or increase the timeout',\n            {\n              error: e.message,\n              retryCount,\n              retryTime,\n            }\n          )\n        }\n\n        bail(e)\n      }\n    })\n  }\n\n  /**\n   * @param {string} topic\n   */\n\n  const fetchTopicOffsets = async topic => {\n    if (!topic || typeof topic !== 'string') {\n      throw new KafkaJSNonRetriableError(`Invalid topic ${topic}`)\n    }\n\n    const retrier = createRetry(retry)\n\n    return retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await cluster.addTargetTopic(topic)\n        await cluster.refreshMetadataIfNecessary()\n\n        const metadata = cluster.findTopicPartitionMetadata(topic)\n        const high = await cluster.fetchTopicsOffset([\n          {\n            topic,\n            fromBeginning: false,\n            partitions: metadata.map(p => ({ partition: p.partitionId })),\n          },\n        ])\n\n        const low = await cluster.fetchTopicsOffset([\n          {\n            topic,\n            fromBeginning: true,\n            partitions: metadata.map(p => ({ partition: p.partitionId })),\n          },\n        ])\n\n        const { partitions: highPartitions } = high.pop()\n        const { partitions: lowPartitions } = low.pop()\n        return highPartitions.map(({ partition, offset }) => ({\n          partition,\n          offset,\n          high: offset,\n          low: lowPartitions.find(({ partition: lowPartition }) => lowPartition === partition)\n            .offset,\n        }))\n      } catch (e) {\n        if (e.type === 'UNKNOWN_TOPIC_OR_PARTITION') {\n          await cluster.refreshMetadata()\n          throw e\n        }\n\n        bail(e)\n      }\n    })\n  }\n\n  /**\n   * @param {string} topic\n   * @param {number} [timestamp]\n   */\n\n  const fetchTopicOffsetsByTimestamp = async (topic, timestamp) => {\n    if (!topic || typeof topic !== 'string') {\n      throw new KafkaJSNonRetriableError(`Invalid topic ${topic}`)\n    }\n\n    const retrier = createRetry(retry)\n\n    return retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await cluster.addTargetTopic(topic)\n        await cluster.refreshMetadataIfNecessary()\n\n        const metadata = cluster.findTopicPartitionMetadata(topic)\n        const partitions = metadata.map(p => ({ partition: p.partitionId }))\n\n        const high = await cluster.fetchTopicsOffset([\n          {\n            topic,\n            fromBeginning: false,\n            partitions,\n          },\n        ])\n        const { partitions: highPartitions } = high.pop()\n\n        const offsets = await cluster.fetchTopicsOffset([\n          {\n            topic,\n            fromTimestamp: timestamp,\n            partitions,\n          },\n        ])\n        const { partitions: lowPartitions } = offsets.pop()\n\n        return lowPartitions.map(({ partition, offset }) => ({\n          partition,\n          offset:\n            parseInt(offset, 10) >= 0\n              ? offset\n              : highPartitions.find(({ partition: highPartition }) => highPartition === partition)\n                  .offset,\n        }))\n      } catch (e) {\n        if (e.type === 'UNKNOWN_TOPIC_OR_PARTITION') {\n          await cluster.refreshMetadata()\n          throw e\n        }\n\n        bail(e)\n      }\n    })\n  }\n\n  /**\n   * Fetch offsets for a topic or multiple topics\n   *\n   * Note: set either topic or topics but not both.\n   *\n   * @param {string} groupId\n   * @param {string[]} topics - list of topics to fetch offsets for, defaults to `[]` which fetches all topics for `groupId`.\n   * @param {boolean} [resolveOffsets=false]\n   * @return {Promise}\n   */\n  const fetchOffsets = async ({ groupId, topics, resolveOffsets = false }) => {\n    if (!groupId) {\n      throw new KafkaJSNonRetriableError(`Invalid groupId ${groupId}`)\n    }\n\n    if (!topics) {\n      topics = []\n    }\n\n    if (!Array.isArray(topics)) {\n      throw new KafkaJSNonRetriableError('Expected topics array to be set')\n    }\n\n    const coordinator = await cluster.findGroupCoordinator({ groupId })\n    const topicsToFetch = await Promise.all(\n      topics.map(async topic => {\n        const partitions = await findTopicPartitions(cluster, topic)\n        const partitionsToFetch = partitions.map(partition => ({ partition }))\n        return { topic, partitions: partitionsToFetch }\n      })\n    )\n    let { responses: consumerOffsets } = await coordinator.offsetFetch({\n      groupId,\n      topics: topicsToFetch,\n    })\n\n    if (resolveOffsets) {\n      consumerOffsets = await Promise.all(\n        consumerOffsets.map(async ({ topic, partitions }) => {\n          const indexedOffsets = indexByPartition(await fetchTopicOffsets(topic))\n          const recalculatedPartitions = partitions.map(({ offset, partition, ...props }) => {\n            let resolvedOffset = offset\n            if (Number(offset) === EARLIEST_OFFSET) {\n              resolvedOffset = indexedOffsets[partition].low\n            }\n            if (Number(offset) === LATEST_OFFSET) {\n              resolvedOffset = indexedOffsets[partition].high\n            }\n            return {\n              partition,\n              offset: resolvedOffset,\n              ...props,\n            }\n          })\n\n          await setOffsets({ groupId, topic, partitions: recalculatedPartitions })\n\n          return {\n            topic,\n            partitions: recalculatedPartitions,\n          }\n        })\n      )\n    }\n\n    return consumerOffsets.map(({ topic, partitions }) => {\n      const completePartitions = partitions.map(({ partition, offset, metadata }) => ({\n        partition,\n        offset,\n        metadata: metadata || null,\n      }))\n\n      return { topic, partitions: completePartitions }\n    })\n  }\n\n  /**\n   * @param {string} groupId\n   * @param {string} topic\n   * @param {boolean} [earliest=false]\n   * @return {Promise}\n   */\n  const resetOffsets = async ({ groupId, topic, earliest = false }) => {\n    if (!groupId) {\n      throw new KafkaJSNonRetriableError(`Invalid groupId ${groupId}`)\n    }\n\n    if (!topic) {\n      throw new KafkaJSNonRetriableError(`Invalid topic ${topic}`)\n    }\n\n    const partitions = await findTopicPartitions(cluster, topic)\n    const partitionsToSeek = partitions.map(partition => ({\n      partition,\n      offset: cluster.defaultOffset({ fromBeginning: earliest }),\n    }))\n\n    return setOffsets({ groupId, topic, partitions: partitionsToSeek })\n  }\n\n  /**\n   * @param {string} groupId\n   * @param {string} topic\n   * @param {Array<SeekEntry>} partitions\n   * @return {Promise}\n   *\n   * @typedef {Object} SeekEntry\n   * @property {number} partition\n   * @property {string} offset\n   */\n  const setOffsets = async ({ groupId, topic, partitions }) => {\n    if (!groupId) {\n      throw new KafkaJSNonRetriableError(`Invalid groupId ${groupId}`)\n    }\n\n    if (!topic) {\n      throw new KafkaJSNonRetriableError(`Invalid topic ${topic}`)\n    }\n\n    if (!partitions || partitions.length === 0) {\n      throw new KafkaJSNonRetriableError(`Invalid partitions`)\n    }\n\n    const consumer = createConsumer({\n      logger: rootLogger.namespace('Admin', LEVELS.NOTHING),\n      cluster,\n      groupId,\n    })\n\n    await consumer.subscribe({ topic, fromBeginning: true })\n    const description = await consumer.describeGroup()\n\n    if (!isConsumerGroupRunning(description)) {\n      throw new KafkaJSNonRetriableError(\n        `The consumer group must have no running instances, current state: ${description.state}`\n      )\n    }\n\n    return new Promise((resolve, reject) => {\n      consumer.on(consumer.events.FETCH, async () =>\n        consumer\n          .stop()\n          .then(resolve)\n          .catch(reject)\n      )\n\n      consumer\n        .run({\n          eachBatchAutoResolve: false,\n          eachBatch: async () => true,\n        })\n        .catch(reject)\n\n      // This consumer doesn't need to consume any data\n      consumer.pause([{ topic }])\n\n      for (const seekData of partitions) {\n        consumer.seek({ topic, ...seekData })\n      }\n    })\n  }\n\n  const isBrokerConfig = type =>\n    [CONFIG_RESOURCE_TYPES.BROKER, CONFIG_RESOURCE_TYPES.BROKER_LOGGER].includes(type)\n\n  /**\n   * Broker configs can only be returned by the target broker\n   *\n   * @see\n   * https://github.com/apache/kafka/blob/821c1ac6641845aeca96a43bc2b946ecec5cba4f/clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java#L3783\n   * https://github.com/apache/kafka/blob/821c1ac6641845aeca96a43bc2b946ecec5cba4f/clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java#L2027\n   *\n   * @param {Broker} defaultBroker. Broker used in case the configuration is not a broker config\n   */\n  const groupResourcesByBroker = ({ resources, defaultBroker }) =>\n    groupBy(resources, async ({ type, name: nodeId }) => {\n      return isBrokerConfig(type)\n        ? await cluster.findBroker({ nodeId: String(nodeId) })\n        : defaultBroker\n    })\n\n  /**\n   * @param {Array<ResourceConfigQuery>} resources\n   * @param {boolean} [includeSynonyms=false]\n   * @return {Promise}\n   *\n   * @typedef {Object} ResourceConfigQuery\n   * @property {ConfigResourceType} type\n   * @property {string} name\n   * @property {Array<String>} [configNames=[]]\n   */\n  const describeConfigs = async ({ resources, includeSynonyms }) => {\n    if (!resources || !Array.isArray(resources)) {\n      throw new KafkaJSNonRetriableError(`Invalid resources array ${resources}`)\n    }\n\n    if (resources.length === 0) {\n      throw new KafkaJSNonRetriableError('Resources array cannot be empty')\n    }\n\n    const validResourceTypes = Object.values(CONFIG_RESOURCE_TYPES)\n    const invalidType = resources.find(r => !validResourceTypes.includes(r.type))\n\n    if (invalidType) {\n      throw new KafkaJSNonRetriableError(\n        `Invalid resource type ${invalidType.type}: ${JSON.stringify(invalidType)}`\n      )\n    }\n\n    const invalidName = resources.find(r => !r.name || typeof r.name !== 'string')\n\n    if (invalidName) {\n      throw new KafkaJSNonRetriableError(\n        `Invalid resource name ${invalidName.name}: ${JSON.stringify(invalidName)}`\n      )\n    }\n\n    const invalidConfigs = resources.find(\n      r => !Array.isArray(r.configNames) && r.configNames != null\n    )\n\n    if (invalidConfigs) {\n      const { configNames } = invalidConfigs\n      throw new KafkaJSNonRetriableError(\n        `Invalid resource configNames ${configNames}: ${JSON.stringify(invalidConfigs)}`\n      )\n    }\n\n    const retrier = createRetry(retry)\n\n    return retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await cluster.refreshMetadata()\n        const controller = await cluster.findControllerBroker()\n        const resourcerByBroker = await groupResourcesByBroker({\n          resources,\n          defaultBroker: controller,\n        })\n\n        const describeConfigsAction = async broker => {\n          const targetBroker = broker || controller\n          return targetBroker.describeConfigs({\n            resources: resourcerByBroker.get(targetBroker),\n            includeSynonyms,\n          })\n        }\n\n        const brokers = Array.from(resourcerByBroker.keys())\n        const responses = await Promise.all(brokers.map(describeConfigsAction))\n        const responseResources = responses.reduce(\n          (result, { resources }) => [...result, ...resources],\n          []\n        )\n\n        return { resources: responseResources }\n      } catch (e) {\n        if (e.type === 'NOT_CONTROLLER') {\n          logger.warn('Could not describe configs', { error: e.message, retryCount, retryTime })\n          throw e\n        }\n\n        bail(e)\n      }\n    })\n  }\n\n  /**\n   * @param {Array<ResourceConfig>} resources\n   * @param {boolean} [validateOnly=false]\n   * @return {Promise}\n   *\n   * @typedef {Object} ResourceConfig\n   * @property {ConfigResourceType} type\n   * @property {string} name\n   * @property {Array<ResourceConfigEntry>} configEntries\n   *\n   * @typedef {Object} ResourceConfigEntry\n   * @property {string} name\n   * @property {string} value\n   */\n  const alterConfigs = async ({ resources, validateOnly }) => {\n    if (!resources || !Array.isArray(resources)) {\n      throw new KafkaJSNonRetriableError(`Invalid resources array ${resources}`)\n    }\n\n    if (resources.length === 0) {\n      throw new KafkaJSNonRetriableError('Resources array cannot be empty')\n    }\n\n    const validResourceTypes = Object.values(CONFIG_RESOURCE_TYPES)\n    const invalidType = resources.find(r => !validResourceTypes.includes(r.type))\n\n    if (invalidType) {\n      throw new KafkaJSNonRetriableError(\n        `Invalid resource type ${invalidType.type}: ${JSON.stringify(invalidType)}`\n      )\n    }\n\n    const invalidName = resources.find(r => !r.name || typeof r.name !== 'string')\n\n    if (invalidName) {\n      throw new KafkaJSNonRetriableError(\n        `Invalid resource name ${invalidName.name}: ${JSON.stringify(invalidName)}`\n      )\n    }\n\n    const invalidConfigs = resources.find(r => !Array.isArray(r.configEntries))\n\n    if (invalidConfigs) {\n      const { configEntries } = invalidConfigs\n      throw new KafkaJSNonRetriableError(\n        `Invalid resource configEntries ${configEntries}: ${JSON.stringify(invalidConfigs)}`\n      )\n    }\n\n    const invalidConfigValue = resources.find(r =>\n      r.configEntries.some(e => typeof e.name !== 'string' || typeof e.value !== 'string')\n    )\n\n    if (invalidConfigValue) {\n      throw new KafkaJSNonRetriableError(\n        `Invalid resource config value: ${JSON.stringify(invalidConfigValue)}`\n      )\n    }\n\n    const retrier = createRetry(retry)\n\n    return retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await cluster.refreshMetadata()\n        const controller = await cluster.findControllerBroker()\n        const resourcerByBroker = await groupResourcesByBroker({\n          resources,\n          defaultBroker: controller,\n        })\n\n        const alterConfigsAction = async broker => {\n          const targetBroker = broker || controller\n          return targetBroker.alterConfigs({\n            resources: resourcerByBroker.get(targetBroker),\n            validateOnly: !!validateOnly,\n          })\n        }\n\n        const brokers = Array.from(resourcerByBroker.keys())\n        const responses = await Promise.all(brokers.map(alterConfigsAction))\n        const responseResources = responses.reduce(\n          (result, { resources }) => [...result, ...resources],\n          []\n        )\n\n        return { resources: responseResources }\n      } catch (e) {\n        if (e.type === 'NOT_CONTROLLER') {\n          logger.warn('Could not alter configs', { error: e.message, retryCount, retryTime })\n          throw e\n        }\n\n        bail(e)\n      }\n    })\n  }\n\n  /**\n   * Fetch metadata for provided topics.\n   *\n   * If no topics are provided fetch metadata for all topics.\n   * @see https://kafka.apache.org/protocol#The_Messages_Metadata\n   *\n   * @param {Object} [options]\n   * @param {string[]} [options.topics]\n   * @return {Promise<TopicsMetadata>}\n   *\n   * @typedef {Object} TopicsMetadata\n   * @property {Array<TopicMetadata>} topics\n   *\n   * @typedef {Object} TopicMetadata\n   * @property {String} name\n   * @property {Array<PartitionMetadata>} partitions\n   *\n   * @typedef {Object} PartitionMetadata\n   * @property {number} partitionErrorCode Response error code\n   * @property {number} partitionId Topic partition id\n   * @property {number} leader  The id of the broker acting as leader for this partition.\n   * @property {Array<number>} replicas The set of all nodes that host this partition.\n   * @property {Array<number>} isr The set of nodes that are in sync with the leader for this partition.\n   */\n  const fetchTopicMetadata = async ({ topics = [] } = {}) => {\n    if (topics) {\n      topics.forEach(topic => {\n        if (!topic || typeof topic !== 'string') {\n          throw new KafkaJSNonRetriableError(`Invalid topic ${topic}`)\n        }\n      })\n    }\n\n    const metadata = await cluster.metadata({ topics })\n\n    return {\n      topics: metadata.topicMetadata.map(topicMetadata => ({\n        name: topicMetadata.topic,\n        partitions: topicMetadata.partitionMetadata,\n      })),\n    }\n  }\n\n  /**\n   * Describe cluster\n   *\n   * @return {Promise<ClusterMetadata>}\n   *\n   * @typedef {Object} ClusterMetadata\n   * @property {Array<Broker>} brokers\n   * @property {Number} controller Current controller id. Returns null if unknown.\n   * @property {String} clusterId\n   *\n   * @typedef {Object} Broker\n   * @property {Number} nodeId\n   * @property {String} host\n   * @property {Number} port\n   */\n  const describeCluster = async () => {\n    const { brokers: nodes, clusterId, controllerId } = await cluster.metadata({ topics: [] })\n    const brokers = nodes.map(({ nodeId, host, port }) => ({\n      nodeId,\n      host,\n      port,\n    }))\n    const controller =\n      controllerId == null || controllerId === NO_CONTROLLER_ID ? null : controllerId\n\n    return {\n      brokers,\n      controller,\n      clusterId,\n    }\n  }\n\n  /**\n   * List groups in a broker\n   *\n   * @return {Promise<ListGroups>}\n   *\n   * @typedef {Object} ListGroups\n   * @property {Array<ListGroup>} groups\n   *\n   * @typedef {Object} ListGroup\n   * @property {string} groupId\n   * @property {string} protocolType\n   */\n  const listGroups = async () => {\n    await cluster.refreshMetadata()\n    let groups = []\n    for (var nodeId in cluster.brokerPool.brokers) {\n      const broker = await cluster.findBroker({ nodeId })\n      const response = await broker.listGroups()\n      groups = groups.concat(response.groups)\n    }\n\n    return { groups }\n  }\n\n  /**\n   * Describe groups by group ids\n   * @param {Array<string>} groupIds\n   *\n   * @typedef {Object} GroupDescriptions\n   * @property {Array<GroupDescription>} groups\n   *\n   * @return {Promise<GroupDescriptions>}\n   */\n  const describeGroups = async groupIds => {\n    const coordinatorsForGroup = await Promise.all(\n      groupIds.map(async groupId => {\n        const coordinator = await cluster.findGroupCoordinator({ groupId })\n        return {\n          coordinator,\n          groupId,\n        }\n      })\n    )\n\n    const groupsByCoordinator = Object.values(\n      coordinatorsForGroup.reduce((coordinators, { coordinator, groupId }) => {\n        const group = coordinators[coordinator.nodeId]\n\n        if (group) {\n          coordinators[coordinator.nodeId] = {\n            ...group,\n            groupIds: [...group.groupIds, groupId],\n          }\n        } else {\n          coordinators[coordinator.nodeId] = { coordinator, groupIds: [groupId] }\n        }\n        return coordinators\n      }, {})\n    )\n\n    const responses = await Promise.all(\n      groupsByCoordinator.map(async ({ coordinator, groupIds }) => {\n        const retrier = createRetry(retry)\n        const { groups } = await retrier(() => coordinator.describeGroups({ groupIds }))\n        return groups\n      })\n    )\n\n    const groups = [].concat.apply([], responses)\n\n    return { groups }\n  }\n\n  /**\n   * Delete groups in a broker\n   *\n   * @param {string[]} [groupIds]\n   * @return {Promise<DeleteGroups>}\n   *\n   * @typedef {Array} DeleteGroups\n   * @property {string} groupId\n   * @property {number} errorCode\n   */\n  const deleteGroups = async groupIds => {\n    if (!groupIds || !Array.isArray(groupIds)) {\n      throw new KafkaJSNonRetriableError(`Invalid groupIds array ${groupIds}`)\n    }\n\n    const invalidGroupId = groupIds.some(g => typeof g !== 'string')\n\n    if (invalidGroupId) {\n      throw new KafkaJSNonRetriableError(`Invalid groupId name: ${JSON.stringify(invalidGroupId)}`)\n    }\n\n    const retrier = createRetry(retry)\n\n    let results = []\n\n    let clonedGroupIds = groupIds.slice()\n\n    return retrier(async (bail, retryCount, retryTime) => {\n      try {\n        if (clonedGroupIds.length === 0) return []\n\n        await cluster.refreshMetadata()\n\n        const brokersPerGroups = {}\n        const brokersPerNode = {}\n        for (const groupId of clonedGroupIds) {\n          const broker = await cluster.findGroupCoordinator({ groupId })\n          if (brokersPerGroups[broker.nodeId] === undefined) brokersPerGroups[broker.nodeId] = []\n          brokersPerGroups[broker.nodeId].push(groupId)\n          brokersPerNode[broker.nodeId] = broker\n        }\n\n        const res = await Promise.all(\n          Object.keys(brokersPerNode).map(\n            async nodeId => await brokersPerNode[nodeId].deleteGroups(brokersPerGroups[nodeId])\n          )\n        )\n\n        const errors = res\n          .flatMap(({ results }) =>\n            results.map(({ groupId, errorCode, error }) => {\n              return { groupId, errorCode, error }\n            })\n          )\n          .filter(({ errorCode }) => errorCode !== 0)\n\n        clonedGroupIds = errors.map(({ groupId }) => groupId)\n\n        if (errors.length > 0) throw new KafkaJSDeleteGroupsError('Error in DeleteGroups', errors)\n\n        results = res.flatMap(({ results }) => results)\n\n        return results\n      } catch (e) {\n        if (e.type === 'NOT_CONTROLLER' || e.type === 'COORDINATOR_NOT_AVAILABLE') {\n          logger.warn('Could not delete groups', { error: e.message, retryCount, retryTime })\n          throw e\n        }\n\n        bail(e)\n      }\n    })\n  }\n\n  /**\n   * Delete topic records up to the selected partition offsets\n   *\n   * @param {string} topic\n   * @param {Array<SeekEntry>} partitions\n   * @return {Promise}\n   *\n   * @typedef {Object} SeekEntry\n   * @property {number} partition\n   * @property {string} offset\n   */\n  const deleteTopicRecords = async ({ topic, partitions }) => {\n    if (!topic || typeof topic !== 'string') {\n      throw new KafkaJSNonRetriableError(`Invalid topic \"${topic}\"`)\n    }\n\n    if (!partitions || partitions.length === 0) {\n      throw new KafkaJSNonRetriableError(`Invalid partitions`)\n    }\n\n    const partitionsByBroker = cluster.findLeaderForPartitions(\n      topic,\n      partitions.map(p => p.partition)\n    )\n\n    const partitionsFound = values(partitionsByBroker).flat()\n    const topicOffsets = await fetchTopicOffsets(topic)\n\n    const leaderNotFoundErrors = []\n    partitions.forEach(({ partition, offset }) => {\n      // throw if no leader found for partition\n      if (!partitionsFound.includes(partition)) {\n        leaderNotFoundErrors.push({\n          partition,\n          offset,\n          error: new KafkaJSBrokerNotFound('Could not find the leader for the partition', {\n            retriable: false,\n          }),\n        })\n        return\n      }\n      const { low } = topicOffsets.find(p => p.partition === partition) || {\n        high: undefined,\n        low: undefined,\n      }\n      // warn in case of offset below low watermark\n      if (parseInt(offset) < parseInt(low) && parseInt(offset) !== -1) {\n        logger.warn(\n          'The requested offset is before the earliest offset maintained on the partition - no records will be deleted from this partition',\n          {\n            topic,\n            partition,\n            offset,\n          }\n        )\n      }\n    })\n\n    if (leaderNotFoundErrors.length > 0) {\n      throw new KafkaJSDeleteTopicRecordsError({ topic, partitions: leaderNotFoundErrors })\n    }\n\n    const seekEntriesByBroker = entries(partitionsByBroker).reduce(\n      (obj, [nodeId, nodePartitions]) => {\n        obj[nodeId] = {\n          topic,\n          partitions: partitions.filter(p => nodePartitions.includes(p.partition)),\n        }\n        return obj\n      },\n      {}\n    )\n\n    const retrier = createRetry(retry)\n    return retrier(async bail => {\n      try {\n        const partitionErrors = []\n\n        const brokerRequests = entries(seekEntriesByBroker).map(\n          ([nodeId, { topic, partitions }]) => async () => {\n            const broker = await cluster.findBroker({ nodeId })\n            await broker.deleteRecords({ topics: [{ topic, partitions }] })\n            // remove successful entry so it's ignored on retry\n            delete seekEntriesByBroker[nodeId]\n          }\n        )\n\n        await Promise.all(\n          brokerRequests.map(request =>\n            request().catch(e => {\n              if (e.name === 'KafkaJSDeleteTopicRecordsError') {\n                e.partitions.forEach(({ partition, offset, error }) => {\n                  partitionErrors.push({\n                    partition,\n                    offset,\n                    error,\n                  })\n                })\n              } else {\n                // then it's an unknown error, not from the broker response\n                throw e\n              }\n            })\n          )\n        )\n\n        if (partitionErrors.length > 0) {\n          throw new KafkaJSDeleteTopicRecordsError({\n            topic,\n            partitions: partitionErrors,\n          })\n        }\n      } catch (e) {\n        if (\n          e.retriable &&\n          e.partitions.some(\n            ({ error }) => staleMetadata(error) || error.name === 'KafkaJSMetadataNotLoaded'\n          )\n        ) {\n          await cluster.refreshMetadata()\n        }\n        throw e\n      }\n    })\n  }\n\n  /**\n   * @param {Array<ACLEntry>} acl\n   * @return {Promise<void>}\n   *\n   * @typedef {Object} ACLEntry\n   */\n  const createAcls = async ({ acl }) => {\n    if (!acl || !Array.isArray(acl)) {\n      throw new KafkaJSNonRetriableError(`Invalid ACL array ${acl}`)\n    }\n    if (acl.length === 0) {\n      throw new KafkaJSNonRetriableError('Empty ACL array')\n    }\n\n    // Validate principal\n    if (acl.some(({ principal }) => typeof principal !== 'string')) {\n      throw new KafkaJSNonRetriableError(\n        'Invalid ACL array, the principals have to be a valid string'\n      )\n    }\n\n    // Validate host\n    if (acl.some(({ host }) => typeof host !== 'string')) {\n      throw new KafkaJSNonRetriableError('Invalid ACL array, the hosts have to be a valid string')\n    }\n\n    // Validate resourceName\n    if (acl.some(({ resourceName }) => typeof resourceName !== 'string')) {\n      throw new KafkaJSNonRetriableError(\n        'Invalid ACL array, the resourceNames have to be a valid string'\n      )\n    }\n\n    let invalidType\n    // Validate operation\n    const validOperationTypes = Object.values(ACL_OPERATION_TYPES)\n    invalidType = acl.find(i => !validOperationTypes.includes(i.operation))\n\n    if (invalidType) {\n      throw new KafkaJSNonRetriableError(\n        `Invalid operation type ${invalidType.operation}: ${JSON.stringify(invalidType)}`\n      )\n    }\n\n    // Validate resourcePatternTypes\n    const validResourcePatternTypes = Object.values(RESOURCE_PATTERN_TYPES)\n    invalidType = acl.find(i => !validResourcePatternTypes.includes(i.resourcePatternType))\n\n    if (invalidType) {\n      throw new KafkaJSNonRetriableError(\n        `Invalid resource pattern type ${invalidType.resourcePatternType}: ${JSON.stringify(\n          invalidType\n        )}`\n      )\n    }\n\n    // Validate permissionTypes\n    const validPermissionTypes = Object.values(ACL_PERMISSION_TYPES)\n    invalidType = acl.find(i => !validPermissionTypes.includes(i.permissionType))\n\n    if (invalidType) {\n      throw new KafkaJSNonRetriableError(\n        `Invalid permission type ${invalidType.permissionType}: ${JSON.stringify(invalidType)}`\n      )\n    }\n\n    // Validate resourceTypes\n    const validResourceTypes = Object.values(ACL_RESOURCE_TYPES)\n    invalidType = acl.find(i => !validResourceTypes.includes(i.resourceType))\n\n    if (invalidType) {\n      throw new KafkaJSNonRetriableError(\n        `Invalid resource type ${invalidType.resourceType}: ${JSON.stringify(invalidType)}`\n      )\n    }\n\n    const retrier = createRetry(retry)\n\n    return retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await cluster.refreshMetadata()\n        const broker = await cluster.findControllerBroker()\n        await broker.createAcls({ acl })\n\n        return true\n      } catch (e) {\n        if (e.type === 'NOT_CONTROLLER') {\n          logger.warn('Could not create ACL', { error: e.message, retryCount, retryTime })\n          throw e\n        }\n\n        bail(e)\n      }\n    })\n  }\n\n  /**\n   * @param {ACLResourceTypes} resourceType The type of resource\n   * @param {string} resourceName The name of the resource\n   * @param {ACLResourcePatternTypes} resourcePatternType The resource pattern type filter\n   * @param {string} principal The principal name\n   * @param {string} host The hostname\n   * @param {ACLOperationTypes} operation The type of operation\n   * @param {ACLPermissionTypes} permissionType The type of permission\n   * @return {Promise<void>}\n   *\n   * @typedef {number} ACLResourceTypes\n   * @typedef {number} ACLResourcePatternTypes\n   * @typedef {number} ACLOperationTypes\n   * @typedef {number} ACLPermissionTypes\n   */\n  const describeAcls = async ({\n    resourceType,\n    resourceName,\n    resourcePatternType,\n    principal,\n    host,\n    operation,\n    permissionType,\n  }) => {\n    // Validate principal\n    if (typeof principal !== 'string' && typeof principal !== 'undefined') {\n      throw new KafkaJSNonRetriableError(\n        'Invalid principal, the principal have to be a valid string'\n      )\n    }\n\n    // Validate host\n    if (typeof host !== 'string' && typeof host !== 'undefined') {\n      throw new KafkaJSNonRetriableError('Invalid host, the host have to be a valid string')\n    }\n\n    // Validate resourceName\n    if (typeof resourceName !== 'string' && typeof resourceName !== 'undefined') {\n      throw new KafkaJSNonRetriableError(\n        'Invalid resourceName, the resourceName have to be a valid string'\n      )\n    }\n\n    // Validate operation\n    const validOperationTypes = Object.values(ACL_OPERATION_TYPES)\n    if (!validOperationTypes.includes(operation)) {\n      throw new KafkaJSNonRetriableError(`Invalid operation type ${operation}`)\n    }\n\n    // Validate resourcePatternType\n    const validResourcePatternTypes = Object.values(RESOURCE_PATTERN_TYPES)\n    if (!validResourcePatternTypes.includes(resourcePatternType)) {\n      throw new KafkaJSNonRetriableError(\n        `Invalid resource pattern filter type ${resourcePatternType}`\n      )\n    }\n\n    // Validate permissionType\n    const validPermissionTypes = Object.values(ACL_PERMISSION_TYPES)\n    if (!validPermissionTypes.includes(permissionType)) {\n      throw new KafkaJSNonRetriableError(`Invalid permission type ${permissionType}`)\n    }\n\n    // Validate resourceType\n    const validResourceTypes = Object.values(ACL_RESOURCE_TYPES)\n    if (!validResourceTypes.includes(resourceType)) {\n      throw new KafkaJSNonRetriableError(`Invalid resource type ${resourceType}`)\n    }\n\n    const retrier = createRetry(retry)\n\n    return retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await cluster.refreshMetadata()\n        const broker = await cluster.findControllerBroker()\n        const { resources } = await broker.describeAcls({\n          resourceType,\n          resourceName,\n          resourcePatternType,\n          principal,\n          host,\n          operation,\n          permissionType,\n        })\n        return { resources }\n      } catch (e) {\n        if (e.type === 'NOT_CONTROLLER') {\n          logger.warn('Could not describe ACL', { error: e.message, retryCount, retryTime })\n          throw e\n        }\n\n        bail(e)\n      }\n    })\n  }\n\n  /**\n   * @param {Array<ACLFilter>} filters\n   * @return {Promise<void>}\n   *\n   * @typedef {Object} ACLFilter\n   */\n  const deleteAcls = async ({ filters }) => {\n    if (!filters || !Array.isArray(filters)) {\n      throw new KafkaJSNonRetriableError(`Invalid ACL Filter array ${filters}`)\n    }\n\n    if (filters.length === 0) {\n      throw new KafkaJSNonRetriableError('Empty ACL Filter array')\n    }\n\n    // Validate principal\n    if (\n      filters.some(\n        ({ principal }) => typeof principal !== 'string' && typeof principal !== 'undefined'\n      )\n    ) {\n      throw new KafkaJSNonRetriableError(\n        'Invalid ACL Filter array, the principals have to be a valid string'\n      )\n    }\n\n    // Validate host\n    if (filters.some(({ host }) => typeof host !== 'string' && typeof host !== 'undefined')) {\n      throw new KafkaJSNonRetriableError(\n        'Invalid ACL Filter array, the hosts have to be a valid string'\n      )\n    }\n\n    // Validate resourceName\n    if (\n      filters.some(\n        ({ resourceName }) =>\n          typeof resourceName !== 'string' && typeof resourceName !== 'undefined'\n      )\n    ) {\n      throw new KafkaJSNonRetriableError(\n        'Invalid ACL Filter array, the resourceNames have to be a valid string'\n      )\n    }\n\n    let invalidType\n    // Validate operation\n    const validOperationTypes = Object.values(ACL_OPERATION_TYPES)\n    invalidType = filters.find(i => !validOperationTypes.includes(i.operation))\n\n    if (invalidType) {\n      throw new KafkaJSNonRetriableError(\n        `Invalid operation type ${invalidType.operation}: ${JSON.stringify(invalidType)}`\n      )\n    }\n\n    // Validate resourcePatternTypes\n    const validResourcePatternTypes = Object.values(RESOURCE_PATTERN_TYPES)\n    invalidType = filters.find(i => !validResourcePatternTypes.includes(i.resourcePatternType))\n\n    if (invalidType) {\n      throw new KafkaJSNonRetriableError(\n        `Invalid resource pattern type ${invalidType.resourcePatternType}: ${JSON.stringify(\n          invalidType\n        )}`\n      )\n    }\n\n    // Validate permissionTypes\n    const validPermissionTypes = Object.values(ACL_PERMISSION_TYPES)\n    invalidType = filters.find(i => !validPermissionTypes.includes(i.permissionType))\n\n    if (invalidType) {\n      throw new KafkaJSNonRetriableError(\n        `Invalid permission type ${invalidType.permissionType}: ${JSON.stringify(invalidType)}`\n      )\n    }\n\n    // Validate resourceTypes\n    const validResourceTypes = Object.values(ACL_RESOURCE_TYPES)\n    invalidType = filters.find(i => !validResourceTypes.includes(i.resourceType))\n\n    if (invalidType) {\n      throw new KafkaJSNonRetriableError(\n        `Invalid resource type ${invalidType.resourceType}: ${JSON.stringify(invalidType)}`\n      )\n    }\n\n    const retrier = createRetry(retry)\n\n    return retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await cluster.refreshMetadata()\n        const broker = await cluster.findControllerBroker()\n        const { filterResponses } = await broker.deleteAcls({ filters })\n        return { filterResponses }\n      } catch (e) {\n        if (e.type === 'NOT_CONTROLLER') {\n          logger.warn('Could not delete ACL', { error: e.message, retryCount, retryTime })\n          throw e\n        }\n\n        bail(e)\n      }\n    })\n  }\n\n  /**\n   * Alter the replicas partitions are assigned to for a topic\n   * @param {Object} request\n   * @param {import(\"../../types\").IPartitionReassignment[]} request.topics topics and the paritions to be reassigned\n   * @param {number} [request.timeout]\n   * @returns {Promise}\n   */\n  const alterPartitionReassignments = async ({ topics, timeout }) => {\n    if (!topics || !Array.isArray(topics)) {\n      throw new KafkaJSNonRetriableError(`Invalid topics array ${topics}`)\n    }\n\n    if (topics.filter(({ topic }) => typeof topic !== 'string').length > 0) {\n      throw new KafkaJSNonRetriableError(\n        'Invalid topics array, the topic names have to be a valid string'\n      )\n    }\n\n    const topicNames = new Set(topics.map(({ topic }) => topic))\n    if (topicNames.size < topics.length) {\n      throw new KafkaJSNonRetriableError(\n        'Invalid topics array, it cannot have multiple entries for the same topic'\n      )\n    }\n\n    for (const { topic, partitionAssignment } of topics) {\n      if (!partitionAssignment || !Array.isArray(partitionAssignment)) {\n        throw new KafkaJSNonRetriableError(\n          `Invalid partitions array: ${partitionAssignment} for topic: ${topic}`\n        )\n      }\n\n      for (const { partition, replicas } of partitionAssignment) {\n        if (\n          partition === null ||\n          partition === undefined ||\n          typeof partition !== 'number' ||\n          partition < 0\n        ) {\n          throw new KafkaJSNonRetriableError(\n            `Invalid partitions index: ${partition} for topic: ${topic}`\n          )\n        }\n\n        if (!replicas || !Array.isArray(replicas)) {\n          throw new KafkaJSNonRetriableError(\n            `Invalid replica assignment: ${replicas} for topic: ${topic} on partition: ${partition}`\n          )\n        }\n\n        if (replicas.filter(replica => typeof replica !== 'number' || replica < 0).length >= 1) {\n          throw new KafkaJSNonRetriableError(\n            `Invalid replica assignment: ${replicas} for topic: ${topic} on partition: ${partition}. Replicas must be a non negative number`\n          )\n        }\n      }\n    }\n\n    const retrier = createRetry(retry)\n\n    return retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await cluster.refreshMetadata()\n        const broker = await cluster.findControllerBroker()\n        await broker.alterPartitionReassignments({ topics, timeout })\n      } catch (e) {\n        if (e.type === 'NOT_CONTROLLER') {\n          logger.warn('Could not reassign partitions', { error: e.message, retryCount, retryTime })\n          throw e\n        }\n\n        bail(e)\n      }\n    })\n  }\n\n  /**\n   * List the partition reassignments in progress.\n   * If a partition is not going through a reassignment, its AddingReplicas and RemovingReplicas fields will simply be empty.\n   * If a partition doesn't exist, no response will be returned for it.\n   * @param {Object} request\n   * @param {import(\"../../types\").TopicPartitions[]} request.topics topics and the paritions to be returned, if this is null will return all the topics.\n   * @param {number} [request.timeout]\n   * @returns {Promise<import(\"../../types\").ListPartitionReassignmentsResponse>}\n   */\n  const listPartitionReassignments = async ({ topics = null, timeout }) => {\n    if (topics) {\n      if (!Array.isArray(topics)) {\n        throw new KafkaJSNonRetriableError(`Invalid topics array ${topics}`)\n      }\n\n      if (topics.filter(({ topic }) => typeof topic !== 'string').length > 0) {\n        throw new KafkaJSNonRetriableError(\n          'Invalid topics array, the topic names have to be a valid string'\n        )\n      }\n\n      const topicNames = new Set(topics.map(({ topic }) => topic))\n      if (topicNames.size < topics.length) {\n        throw new KafkaJSNonRetriableError(\n          'Invalid topics array, it cannot have multiple entries for the same topic'\n        )\n      }\n\n      for (const { topic, partitions } of topics) {\n        if (!partitions || !Array.isArray(partitions)) {\n          throw new KafkaJSNonRetriableError(\n            `Invalid partition array: ${partitions} for topic: ${topic}`\n          )\n        }\n\n        if (\n          partitions.filter(partition => typeof partition !== 'number' || partition < 0).length >= 1\n        ) {\n          throw new KafkaJSNonRetriableError(\n            `Invalid partition array: ${partitions} for topic: ${topic}. The partition indices have to be a valid number greater than 0.`\n          )\n        }\n      }\n    }\n\n    const retrier = createRetry(retry)\n\n    return retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await cluster.refreshMetadata()\n        const broker = await cluster.findControllerBroker()\n        const response = await broker.listPartitionReassignments({ topics, timeout })\n\n        return { topics: response.topics }\n      } catch (e) {\n        if (e.type === 'NOT_CONTROLLER') {\n          logger.warn('Could not reassign partitions', { error: e.message, retryCount, retryTime })\n          throw e\n        }\n\n        bail(e)\n      }\n    })\n  }\n\n  /** @type {import(\"../../types\").Admin[\"on\"]} */\n  const on = (eventName, listener) => {\n    if (!eventNames.includes(eventName)) {\n      throw new KafkaJSNonRetriableError(`Event name should be one of ${eventKeys}`)\n    }\n\n    return instrumentationEmitter.addListener(unwrapEvent(eventName), event => {\n      event.type = wrapEvent(event.type)\n      Promise.resolve(listener(event)).catch(e => {\n        logger.error(`Failed to execute listener: ${e.message}`, {\n          eventName,\n          stack: e.stack,\n        })\n      })\n    })\n  }\n\n  /**\n   * @return {Object} logger\n   */\n  const getLogger = () => logger\n\n  return {\n    connect,\n    disconnect,\n    listTopics,\n    createTopics,\n    deleteTopics,\n    createPartitions,\n    fetchTopicMetadata,\n    describeCluster,\n    events,\n    fetchOffsets,\n    fetchTopicOffsets,\n    fetchTopicOffsetsByTimestamp,\n    setOffsets,\n    resetOffsets,\n    describeConfigs,\n    alterConfigs,\n    on,\n    logger: getLogger,\n    listGroups,\n    describeGroups,\n    deleteGroups,\n    describeAcls,\n    deleteAcls,\n    createAcls,\n    deleteTopicRecords,\n    alterPartitionReassignments,\n    listPartitionReassignments,\n  }\n}\n", "const KEEP_ALIVE_DELAY = 60000 // in ms\n\n/**\n * @returns {import(\"../../types\").ISocketFactory}\n */\nmodule.exports = () => {\n  const net = require('net')\n  const tls = require('tls')\n\n  return ({ host, port, ssl, onConnect }) => {\n    const socket = ssl\n      ? tls.connect(\n          Object.assign({ host, port }, !net.isIP(host) ? { servername: host } : {}, ssl),\n          onConnect\n        )\n      : net.connect({ host, port }, onConnect)\n\n    socket.setKeepAlive(true, KEEP_ALIVE_DELAY)\n\n    return socket\n  }\n}\n", "module.exports = fn => {\n  let called = false\n\n  return (...args) => {\n    if (!called) {\n      called = true\n      return fn(...args)\n    }\n  }\n}\n", "const {\n  createLogger,\n  LEVELS: { INFO },\n} = require('./loggers')\n\nconst InstrumentationEventEmitter = require('./instrumentation/emitter')\nconst LoggerConsole = require('./loggers/console')\nconst Cluster = require('./cluster')\nconst createProducer = require('./producer')\nconst createConsumer = require('./consumer')\nconst createAdmin = require('./admin')\nconst ISOLATION_LEVEL = require('./protocol/isolationLevel')\nconst defaultSocketFactory = require('./network/socketFactory')\nconst once = require('./utils/once')\nconst websiteUrl = require('./utils/websiteUrl')\n\nconst PRIVATE = {\n  CREATE_CLUSTER: Symbol('private:Kafka:createCluster'),\n  CLUSTER_RETRY: Symbol('private:Kafka:clusterRetry'),\n  LOGGER: Symbol('private:Kafka:logger'),\n  OFFSETS: Symbol('private:Kafka:offsets'),\n}\n\nconst DEFAULT_METADATA_MAX_AGE = 300000\nconst warnOfDefaultPartitioner = once(logger => {\n  if (process.env.KAFKAJS_NO_PARTITIONER_WARNING == null) {\n    logger.warn(\n      `KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at ${websiteUrl(\n        'docs/migration-guide-v2.0.0',\n        'producer-new-default-partitioner'\n      )} for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\"`\n    )\n  }\n})\n\nmodule.exports = class Client {\n  /**\n   * @param {Object} options\n   * @param {Array<string>} options.brokers example: ['127.0.0.1:9092', '127.0.0.1:9094']\n   * @param {Object} options.ssl\n   * @param {Object} options.sasl\n   * @param {string} options.clientId\n   * @param {number} [options.connectionTimeout=1000] - in milliseconds\n   * @param {number} options.authenticationTimeout - in milliseconds\n   * @param {number} options.reauthenticationThreshold - in milliseconds\n   * @param {number} [options.requestTimeout=30000] - in milliseconds\n   * @param {boolean} [options.enforceRequestTimeout]\n   * @param {import(\"../types\").RetryOptions} [options.retry]\n   * @param {import(\"../types\").ISocketFactory} [options.socketFactory]\n   */\n  constructor({\n    brokers,\n    ssl,\n    sasl,\n    clientId,\n    connectionTimeout = 1000,\n    authenticationTimeout,\n    reauthenticationThreshold,\n    requestTimeout,\n    enforceRequestTimeout = true,\n    retry,\n    socketFactory = defaultSocketFactory(),\n    logLevel = INFO,\n    logCreator = LoggerConsole,\n  }) {\n    this[PRIVATE.OFFSETS] = new Map()\n    this[PRIVATE.LOGGER] = createLogger({ level: logLevel, logCreator })\n    this[PRIVATE.CLUSTER_RETRY] = retry\n    this[PRIVATE.CREATE_CLUSTER] = ({\n      metadataMaxAge,\n      allowAutoTopicCreation = true,\n      maxInFlightRequests = null,\n      instrumentationEmitter = null,\n      isolationLevel,\n    }) =>\n      new Cluster({\n        logger: this[PRIVATE.LOGGER],\n        retry: this[PRIVATE.CLUSTER_RETRY],\n        offsets: this[PRIVATE.OFFSETS],\n        socketFactory,\n        brokers,\n        ssl,\n        sasl,\n        clientId,\n        connectionTimeout,\n        authenticationTimeout,\n        reauthenticationThreshold,\n        requestTimeout,\n        enforceRequestTimeout,\n        metadataMaxAge,\n        instrumentationEmitter,\n        allowAutoTopicCreation,\n        maxInFlightRequests,\n        isolationLevel,\n      })\n  }\n\n  /**\n   * @public\n   */\n  producer({\n    createPartitioner,\n    retry,\n    metadataMaxAge = DEFAULT_METADATA_MAX_AGE,\n    allowAutoTopicCreation,\n    idempotent,\n    transactionalId,\n    transactionTimeout,\n    maxInFlightRequests,\n  } = {}) {\n    const instrumentationEmitter = new InstrumentationEventEmitter()\n    const cluster = this[PRIVATE.CREATE_CLUSTER]({\n      metadataMaxAge,\n      allowAutoTopicCreation,\n      maxInFlightRequests,\n      instrumentationEmitter,\n    })\n\n    if (createPartitioner == null) {\n      warnOfDefaultPartitioner(this[PRIVATE.LOGGER])\n    }\n\n    return createProducer({\n      retry: { ...this[PRIVATE.CLUSTER_RETRY], ...retry },\n      logger: this[PRIVATE.LOGGER],\n      cluster,\n      createPartitioner,\n      idempotent,\n      transactionalId,\n      transactionTimeout,\n      instrumentationEmitter,\n    })\n  }\n\n  /**\n   * @public\n   */\n  consumer({\n    groupId,\n    partitionAssigners,\n    metadataMaxAge = DEFAULT_METADATA_MAX_AGE,\n    sessionTimeout,\n    rebalanceTimeout,\n    heartbeatInterval,\n    maxBytesPerPartition,\n    minBytes,\n    maxBytes,\n    maxWaitTimeInMs,\n    retry = { retries: 5 },\n    allowAutoTopicCreation,\n    maxInFlightRequests,\n    readUncommitted = false,\n    rackId = '',\n  } = {}) {\n    const isolationLevel = readUncommitted\n      ? ISOLATION_LEVEL.READ_UNCOMMITTED\n      : ISOLATION_LEVEL.READ_COMMITTED\n\n    const instrumentationEmitter = new InstrumentationEventEmitter()\n    const cluster = this[PRIVATE.CREATE_CLUSTER]({\n      metadataMaxAge,\n      allowAutoTopicCreation,\n      maxInFlightRequests,\n      isolationLevel,\n      instrumentationEmitter,\n    })\n\n    return createConsumer({\n      retry: { ...this[PRIVATE.CLUSTER_RETRY], ...retry },\n      logger: this[PRIVATE.LOGGER],\n      cluster,\n      groupId,\n      partitionAssigners,\n      sessionTimeout,\n      rebalanceTimeout,\n      heartbeatInterval,\n      maxBytesPerPartition,\n      minBytes,\n      maxBytes,\n      maxWaitTimeInMs,\n      isolationLevel,\n      instrumentationEmitter,\n      rackId,\n      metadataMaxAge,\n    })\n  }\n\n  /**\n   * @public\n   */\n  admin({ retry } = {}) {\n    const instrumentationEmitter = new InstrumentationEventEmitter()\n    const cluster = this[PRIVATE.CREATE_CLUSTER]({\n      allowAutoTopicCreation: false,\n      instrumentationEmitter,\n    })\n\n    return createAdmin({\n      retry: { ...this[PRIVATE.CLUSTER_RETRY], ...retry },\n      logger: this[PRIVATE.LOGGER],\n      instrumentationEmitter,\n      cluster,\n    })\n  }\n\n  /**\n   * @public\n   */\n  logger() {\n    return this[PRIVATE.LOGGER]\n  }\n}\n", "const Kafka = require('./src')\nconst PartitionAssigners = require('./src/consumer/assigners')\nconst AssignerProtocol = require('./src/consumer/assignerProtocol')\nconst Partitioners = require('./src/producer/partitioners')\nconst Compression = require('./src/protocol/message/compression')\nconst ConfigResourceTypes = require('./src/protocol/configResourceTypes')\nconst ConfigSource = require('./src/protocol/configSource')\nconst AclResourceTypes = require('./src/protocol/aclResourceTypes')\nconst AclOperationTypes = require('./src/protocol/aclOperationTypes')\nconst AclPermissionTypes = require('./src/protocol/aclPermissionTypes')\nconst ResourcePatternTypes = require('./src/protocol/resourcePatternTypes')\nconst { isRebalancing, isKafkaJSError, ...errors } = require('./src/errors')\nconst { LEVELS } = require('./src/loggers')\n\nmodule.exports = {\n  Kafka,\n  PartitionAssigners,\n  AssignerProtocol,\n  Partitioners,\n  logLevel: LEVELS,\n  CompressionTypes: Compression.Types,\n  CompressionCodecs: Compression.Codecs,\n  ConfigResourceTypes,\n  AclResourceTypes,\n  AclOperationTypes,\n  AclPermissionTypes,\n  ResourcePatternTypes,\n  ConfigSource,\n  ...errors,\n}\n", "import { ReportType } from \"../types/reports\";\n\nexport const error = {\n  // generic errors\n  UNAUTHORIZED: \"User is not authorized to access this resource.\",\n  NO_KEY: \"Must provide key for table.\",\n  MISSING_DATA: \"Missing required data.\",\n  INVALID_DATA: \"Provided data is not valid.\",\n  NO_MATCHING_RECORD: \"No matching record found.\",\n  SERVER_ERROR: \"An unspecified server error occured.\",\n  // bucket errors\n  S3_OBJECT_CREATION_ERROR: \"Report could not be created due to an S3 error.\",\n  S3_OBJECT_UPDATE_ERROR: \"Report could not be updated due to an S3 error.\",\n  S3_OBJECT_GET_ERROR: \"Error while fetching report.\",\n  // dynamo errors\n  DYNAMO_CREATION_ERROR: \"Report could not be created due to a database error.\",\n  DYNAMO_UPDATE_ERROR: \"Report could not be updated due to a database error.\",\n  // template errors\n  NO_TEMPLATE_NAME: \"Must request template for download.\",\n  INVALID_TEMPLATE_NAME: \"Requested template does not exist or does not match.\",\n  NOT_IN_DATABASE: \"Record not found in database.\",\n  UNABLE_TO_COPY:\n    \"Unable to copy over report if todays date is in the same period and year as a previous report.\",\n  MISSING_FORM_TEMPLATE: \"Form Template not found in S3.\",\n  MISSING_FIELD_DATA: \"Field Data not found in S3.\",\n  NO_WORKPLANS_FOUND: \"No record of Work Plans found in database\",\n  // admin action errors\n  ALREADY_ARCHIVED: \"Cannot update archived report.\",\n  ALREADY_LOCKED: \"Cannot update locked report.\",\n  REPORT_INCOMPLETE: \"Cannot submit incomplete form.\",\n} as const;\n\nexport const buckets = {\n  FORM_TEMPLATE: \"formTemplates\",\n  FIELD_DATA: \"fieldData\",\n};\n\n// STATES\nexport enum States {\n  AL = \"Alabama\",\n  AK = \"Alaska\",\n  AS = \"American Samoa\",\n  AZ = \"Arizona\",\n  AR = \"Arkansas\",\n  CA = \"California\",\n  CO = \"Colorado\",\n  CT = \"Connecticut\",\n  DE = \"Delaware\",\n  DC = \"District of Columbia\",\n  FM = \"Federated States of Micronesia\",\n  FL = \"Florida\",\n  GA = \"Georgia\",\n  GU = \"Guam\",\n  HI = \"Hawaii\",\n  ID = \"Idaho\",\n  IL = \"Illinois\",\n  IN = \"Indiana\",\n  IA = \"Iowa\",\n  KS = \"Kansas\",\n  KY = \"Kentucky\",\n  LA = \"Louisiana\",\n  ME = \"Maine\",\n  MH = \"Marshall Islands\",\n  MD = \"Maryland\",\n  MA = \"Massachusetts\",\n  MI = \"Michigan\",\n  MN = \"Minnesota\",\n  MS = \"Mississippi\",\n  MO = \"Missouri\",\n  MT = \"Montana\",\n  NE = \"Nebraska\",\n  NV = \"Nevada\",\n  NH = \"New Hampshire\",\n  NJ = \"New Jersey\",\n  NM = \"New Mexico\",\n  NY = \"New York\",\n  NC = \"North Carolina\",\n  ND = \"North Dakota\",\n  MP = \"Northern Mariana Islands\",\n  OH = \"Ohio\",\n  OK = \"Oklahoma\",\n  OR = \"Oregon\",\n  PW = \"Palau\",\n  PA = \"Pennsylvania\",\n  PR = \"Puerto Rico\",\n  RI = \"Rhode Island\",\n  SC = \"South Carolina\",\n  SD = \"South Dakota\",\n  TN = \"Tennessee\",\n  TX = \"Texas\",\n  UT = \"Utah\",\n  VT = \"Vermont\",\n  VI = \"Virgin Islands\",\n  VA = \"Virginia\",\n  WA = \"Washington\",\n  WV = \"West Virginia\",\n  WI = \"Wisconsin\",\n  WY = \"Wyoming\",\n}\n\n// REPORTS\n\nexport const reportTables: { [key in ReportType]: string } = {\n  SAR: process.env.SarReportsTable!,\n  WP: process.env.WpReportsTable!,\n};\n\nexport const reportBuckets: { [key in ReportType]: string } = {\n  SAR: process.env.SAR_FORM_BUCKET!,\n  WP: process.env.WP_FORM_BUCKET!,\n};\n\nexport const reportNames: { [key in ReportType]: string } = {\n  SAR: \"SAR\",\n  WP: \"Work Plan\",\n};\n\nexport const tableTopics: { [key in ReportType]: string } = {\n  SAR: \"sar-reports\",\n  WP: \"wp-reports\",\n};\n\nexport const bucketTopics: { [key in ReportType]: string } = {\n  SAR: \"sar-form\",\n  WP: \"wp-form\",\n};\n\nexport const DEFAULT_TARGET_POPULATION_NAMES = [\n  \"Older adults\",\n  \"Individuals with physical disabilities (PD)\",\n  \"Individuals with intellectual and developmental disabilities (I/DD)\",\n  \"Individuals with mental health and substance use disorders (MH/SUD)\",\n  \"HCBS infrastructure/system-level development\",\n];\n", "/* eslint-disable no-console */\nimport { GetObjectCommand } from \"@aws-sdk/client-s3\";\nimport { unmarshall } from \"@aws-sdk/util-dynamodb\";\nimport { Kafka, Producer } from \"kafkajs\";\nimport { createClient } from \"../../storage/s3-lib\";\nimport { S3EventRecord } from \"../types\";\n\ntype KafkaPayload = {\n  key: string;\n  value: string;\n  partition: number;\n  headers: {\n    eventName: string;\n    eventTime?: string;\n    eventID?: string;\n  };\n};\ntype SourceTopicMapping = {\n  sourceName: string;\n  topicName: string;\n};\n\nlet kafka: Kafka;\nlet producer: Producer;\n\nclass KafkaSourceLib {\n  /*\n   * Event types:\n   * cmd \u2013 command; restful publish\n   * cdc \u2013 change data capture; record upsert/delete in data store\n   * sys \u2013 system event; send email, archive logs\n   * fct \u2013 fact; user activity, notifications, logs\n   *\n   * topicPrefix = \"[data_center].[system_of_record].[business_domain].[event_type]\";\n   * version = \"some version\";\n   * tables = [list of table mappings];\n   * buckets = [list of bucket mappings];\n   */\n\n  topicPrefix: string;\n  version: string | null;\n  tables: SourceTopicMapping[];\n  buckets: SourceTopicMapping[];\n  connected: boolean;\n  topicNamespace: string;\n  stage: string;\n  constructor(\n    topicPrefix: string,\n    version: string | null,\n    tables: SourceTopicMapping[],\n    buckets: SourceTopicMapping[]\n  ) {\n    if (!process.env.BOOTSTRAP_BROKER_STRING_TLS) {\n      throw new Error(\"Missing Broker Config. \");\n    }\n    // Setup vars\n    this.stage = process.env.stage ?? \"\";\n    this.topicNamespace = process.env.topicNamespace\n      ? process.env.topicNamespace\n      : \"\";\n    this.topicPrefix = topicPrefix;\n    this.version = version;\n    this.tables = tables;\n    this.buckets = buckets;\n\n    const brokerStrings = process.env.BOOTSTRAP_BROKER_STRING_TLS;\n    kafka = new Kafka({\n      clientId: `mfp-${this.stage}`,\n      brokers: brokerStrings!.split(\",\"),\n      retry: {\n        initialRetryTime: 300,\n        retries: 8,\n      },\n      ssl: {\n        rejectUnauthorized: false,\n      },\n    });\n\n    // Attach Events\n    producer = kafka.producer();\n    this.connected = false;\n    const signalTraps = [\"SIGTERM\", \"SIGINT\", \"SIGUSR2\", \"beforeExit\"];\n    signalTraps.map((type) => {\n      process.removeListener(type, producer.disconnect);\n    });\n    signalTraps.map((type) => {\n      process.once(type, producer.disconnect);\n    });\n  }\n\n  stringify(e: any, prettyPrint?: boolean) {\n    if (prettyPrint === true) return JSON.stringify(e, null, 2);\n    return JSON.stringify(e);\n  }\n\n  /**\n   * Checks if a streamArn is a valid topic. Returns undefined otherwise\n   * @param streamARN - DynamoDB streamARN\n   * @returns\n   */\n  determineDynamoTopicName(streamARN: string) {\n    for (const table of this.tables) {\n      if (streamARN.includes(`/${table.sourceName}/`))\n        return this.topic(table.topicName);\n    }\n    console.log(`Topic not found for table arn: ${streamARN}`);\n  }\n\n  /**\n   * Checks if a bucketArn is a valid topic. Returns undefined otherwise.\n   * @param bucketArn - ARN formatted like 'arn:aws:s3:::{stack}-{stage}-{bucket}' e.g. arn:aws:s3:::database-main-mcpar\n   * @returns A formatted topic name with \"-form\" specified\n   */\n  determineS3TopicName(bucketArn: string) {\n    for (const bucket of this.buckets) {\n      if (bucketArn.includes(bucket.sourceName)) {\n        return this.topic(bucket.topicName);\n      }\n    }\n    console.log(`Topic not found for bucket arn: ${bucketArn}`);\n  }\n\n  unmarshall(r: any) {\n    return unmarshall(r);\n  }\n\n  createDynamoPayload(record: any): KafkaPayload {\n    const dynamodb = record.dynamodb;\n    const { eventID, eventName } = record;\n    const dynamoRecord = {\n      NewImage: this.unmarshall(dynamodb.NewImage),\n      OldImage: this.unmarshall(dynamodb.OldImage ?? {}),\n      Keys: this.unmarshall(dynamodb.Keys),\n    };\n    return {\n      key: Object.values(dynamoRecord.Keys).join(\"#\"),\n      value: this.stringify(dynamoRecord),\n      partition: 0,\n      headers: { eventID: eventID, eventName: eventName },\n    };\n  }\n\n  async createS3Payload(record: S3EventRecord): Promise<KafkaPayload> {\n    const { eventName, eventTime } = record;\n    let entry = \"\";\n    if (!eventName.includes(\"ObjectRemoved\")) {\n      const client = createClient();\n      const response = await client.send(\n        new GetObjectCommand({\n          Bucket: record.s3.bucket.name,\n          Key: record.s3.object.key,\n        })\n      );\n      const responseString = await response.Body?.transformToString();\n      if (responseString) {\n        entry = responseString;\n      } else {\n        throw new Error(\n          `Failed to fetch S3 object with key: \"${record.s3.object.key}\"`\n        );\n      }\n    }\n\n    return {\n      key: record.s3.object.key,\n      value: entry,\n      partition: 0,\n      headers: { eventName, eventTime },\n    };\n  }\n\n  topic(t: string) {\n    if (this.version) {\n      return `${this.topicNamespace}${this.topicPrefix}.${t}.${this.version}`;\n    } else {\n      return `${this.topicNamespace}${this.topicPrefix}.${t}`;\n    }\n  }\n\n  async createOutboundEvents(records: any[]) {\n    let outboundEvents: { [key: string]: any } = {};\n    for (const record of records) {\n      let payload, topicName;\n      if (record[\"s3\"]) {\n        // Handle any S3 events\n        const s3Record = record as S3EventRecord;\n        const key: string = s3Record.s3.object.key;\n        topicName = this.determineS3TopicName(s3Record.s3.bucket.arn);\n\n        // Filter for only the response info\n        if (\n          !topicName ||\n          !key.startsWith(\"fieldData/\") ||\n          !key.includes(\".json\")\n        ) {\n          continue;\n        }\n        payload = await this.createS3Payload(record);\n      } else {\n        // DYNAMO\n        topicName = this.determineDynamoTopicName(\n          String(record.eventSourceARN.toString())\n        );\n        if (!topicName) continue;\n        payload = this.createDynamoPayload(record);\n      }\n\n      //initialize configuration object keyed to topic for quick lookup\n      if (!(outboundEvents[topicName] instanceof Object))\n        outboundEvents[topicName] = {\n          topic: topicName,\n          messages: [],\n        };\n\n      //add messages to messages array for corresponding topic\n      outboundEvents[topicName].messages.push(payload);\n    }\n    return outboundEvents;\n  }\n\n  async handler(event: any) {\n    if (process.env.BOOTSTRAP_BROKER_STRING_TLS === \"localstack\") {\n      return;\n    }\n\n    if (!this.connected) {\n      await producer.connect();\n      this.connected = true;\n    }\n\n    // Warmup events have no records.\n    if (!event.Records) {\n      console.log(\"No records to process. Exiting.\");\n      return;\n    }\n\n    // if dynamo\n    const outboundEvents = await this.createOutboundEvents(event.Records);\n\n    const topicMessages = Object.values(outboundEvents);\n    console.log(`Batch configuration: ${this.stringify(topicMessages, true)}`);\n\n    if (topicMessages.length > 0) await producer.sendBatch({ topicMessages });\n    console.log(`Successfully processed ${event.Records.length} records.`);\n  }\n}\n\nexport default KafkaSourceLib;\n", "import { S3Client, GetObjectCommandOutput } from \"@aws-sdk/client-s3\";\nimport { logger } from \"../utils/debugging/debug-lib\";\n\nconst localConfig = {\n  endpoint: process.env.S3_LOCAL_ENDPOINT,\n  region: \"localhost\",\n  forcePathStyle: true,\n  credentials: {\n    accessKeyId: \"S3RVER\", // pragma: allowlist secret\n    secretAccessKey: \"S3RVER\", // pragma: allowlist secret\n  },\n  logger,\n};\n\nconst awsConfig = {\n  region: \"us-east-1\",\n  logger,\n};\n\nconst getConfig = () => {\n  return process.env.S3_LOCAL_ENDPOINT ? localConfig : awsConfig;\n};\n\nexport const createClient = () => new S3Client(getConfig());\n\nexport const parseS3Response = async (response: GetObjectCommandOutput) => {\n  const stringBody = await response.Body?.transformToString();\n  if (!stringBody) {\n    logger.warn(`Empty response from S3`);\n    return undefined;\n  }\n  return JSON.parse(stringBody);\n};\n", "import util from \"util\";\nimport { Logger } from \"@smithy/types\";\n\ntype LogLevel = \"trace\" | \"debug\" | \"info\" | \"warn\" | \"error\";\ntype LogEntry = {\n  date: Date;\n  level: LogLevel;\n  string: string;\n};\n\nconst logs: LogEntry[] = [];\n\nconst buildLoggerForLevel = (level: LogLevel) => {\n  return function (...content: any[]) {\n    logs.push({\n      date: new Date(),\n      level: level,\n      string: util.format.apply(null, content),\n    });\n\n    /*\n     * If we have a function logging thousands of messages,\n     * better to take the console performance hit mid-operation\n     * than to let memory usage run away as well.\n     */\n    if (logs.length > 99) {\n      flush();\n    }\n  };\n};\n\n/*\n * Individual functions are exported to support handler-lib;\n * This integrates SDK client logging with lambda logging.\n */\nexport const trace = buildLoggerForLevel(\"trace\");\nexport const debug = buildLoggerForLevel(\"debug\");\nexport const info = buildLoggerForLevel(\"info\");\nexport const warn = buildLoggerForLevel(\"warn\");\nexport const error = buildLoggerForLevel(\"error\");\n\nexport function flush() {\n  while (logs.length > 0) {\n    const { date, level, string } = logs.shift()!;\n    // eslint-disable-next-line no-console\n    console[level](date, string);\n  }\n}\n\n/*\n * This is only called at the beginning of a lambda handler,\n * so the log buffer should be empty anyway. But it doesn't\n * hurt to make sure!\n */\nexport const init = flush;\n\n/**\n * A logger suitable for passing to any AWS client constructor.\n * Note that the `trace` log level is excluded.\n *\n * This logger accumulates log messages in an internal buffer,\n * eventually flushing them to the console.\n */\nexport const logger: Logger = { debug, info, warn, error };\n", "import {\n  bucketTopics,\n  reportBuckets,\n  reportTables,\n  tableTopics,\n} from \"../../../utils/constants/constants\";\nimport KafkaSourceLib from \"../../../utils/kafka/kafka-source-lib\";\n\nconst topicPrefix = \"aws.mdct.mfp\";\nconst version = \"v0\";\nconst tables = [\n  { sourceName: reportTables.WP, topicName: tableTopics.WP },\n  { sourceName: reportTables.SAR, topicName: tableTopics.SAR },\n];\nconst buckets = [\n  { sourceName: reportBuckets.WP, topicName: bucketTopics.WP },\n  { sourceName: reportBuckets.SAR, topicName: bucketTopics.SAR },\n];\n\nconst postKafkaData = new KafkaSourceLib(topicPrefix, version, tables, buckets);\n\nexports.handler = postKafkaData.handler.bind(postKafkaData);\n"],
./.cdk/cdk.out/asset.e88bd42cdce1d51c3406f992e1b14911ea0e76b281a22a5f0a399db15b45dd85/index.js.map:4: TODO: remove\n\n\n  get _type() {\n    return this.type;\n  }\n\n  _typeCheck(_value) {\n    return true;\n  }\n\n  clone(spec) {\n    if (this._mutate) {\n      if (spec) Object.assign(this.spec, spec);\n      return this;\n    } // if the nested value is a schema we can skip cloning, since\n    // they are already immutable\n\n\n    const next = Object.create(Object.getPrototypeOf(this)); // @ts-expect-error this is readonly\n\n    next.type = this.type;\n    next._typeError = this._typeError;\n    next._whitelistError = this._whitelistError;\n    next._blacklistError = this._blacklistError;\n    next._whitelist = this._whitelist.clone();\n    next._blacklist = this._blacklist.clone();\n    next.exclusiveTests = _extends({}, this.exclusiveTests); // @ts-expect-error this is readonly\n\n    next.deps = [...this.deps];\n    next.conditions = [...this.conditions];\n    next.tests = [...this.tests];\n    next.transforms = [...this.transforms];\n    next.spec = (0, _nanoclone.default)(_extends({}, this.spec, spec));\n    return next;\n  }\n\n  label(label) {\n    let next = this.clone();\n    next.spec.label = label;\n    return next;\n  }\n\n  meta(...args) {\n    if (args.length === 0) return this.spec.meta;\n    let next = this.clone();\n    next.spec.meta = Object.assign(next.spec.meta || {}, args[0]);\n    return next;\n  } // withContext<TContext extends AnyObject>(): BaseSchema<\n  //   TCast,\n  //   TContext,\n  //   TOutput\n  // > {\n  //   return this as any;\n  // }\n\n\n  withMutation(fn) {\n    let before = this._mutate;\n    this._mutate = true;\n    let result = fn(this);\n    this._mutate = before;\n    return result;\n  }\n\n  concat(schema) {\n    if (!schema || schema === this) return this;\n    if (schema.type !== this.type && this.type !== 'mixed') throw new TypeError(`You cannot \\`concat()\\` schema's of different types: ${this.type} and ${schema.type}`);\n    let base = this;\n    let combined = schema.clone();\n\n    const mergedSpec = _extends({}, base.spec, combined.spec); // if (combined.spec.nullable === UNSET)\n    //   mergedSpec.nullable = base.spec.nullable;\n    // if (combined.spec.presence === UNSET)\n    //   mergedSpec.presence = base.spec.presence;\n\n\n    combined.spec = mergedSpec;\n    combined._typeError || (combined._typeError = base._typeError);\n    combined._whitelistError || (combined._whitelistError = base._whitelistError);\n    combined._blacklistError || (combined._blacklistError = base._blacklistError); // manually merge the blacklist/whitelist (the other `schema` takes\n    // precedence in case of conflicts)\n\n    combined._whitelist = base._whitelist.merge(schema._whitelist, schema._blacklist);\n    combined._blacklist = base._blacklist.merge(schema._blacklist, schema._whitelist); // start with the current tests\n\n    combined.tests = base.tests;\n    combined.exclusiveTests = base.exclusiveTests; // manually add the new tests to ensure\n    // the deduping logic is consistent\n\n    combined.withMutation(next => {\n      schema.tests.forEach(fn => {\n        next.test(fn.OPTIONS);\n      });\n    });\n    combined.transforms = [...base.transforms, ...combined.transforms];\n    return combined;\n  }\n\n  isType(v) {\n    if (this.spec.nullable && v === null) return true;\n    return this._typeCheck(v);\n  }\n\n  resolve(options) {\n    let schema = this;\n\n    if (schema.conditions.length) {\n      let conditions = schema.conditions;\n      schema = schema.clone();\n      schema.conditions = [];\n      schema = conditions.reduce((schema, condition) => condition.resolve(schema, options), schema);\n      schema = schema.resolve(options);\n    }\n\n    return schema;\n  }\n  /**\n   *\n   * @param {*} value\n   * @param {Object} options\n   * @param {*=} options.parent\n   * @param {*=} options.context\n   */\n\n\n  cast(value, options = {}) {\n    let resolvedSchema = this.resolve(_extends({\n      value\n    }, options));\n\n    let result = resolvedSchema._cast(value, options);\n\n    if (value !== undefined && options.assert !== false && resolvedSchema.isType(result) !== true) {\n      let formattedValue = (0, _printValue.default)(value);\n      let formattedResult = (0, _printValue.default)(result);\n      throw new TypeError(`The value of ${options.path || 'field'} could not be cast to a value ` + `that satisfies the schema type: \"${resolvedSchema._type}\". \\n\\n` + `attempted value: ${formattedValue} \\n` + (formattedResult !== formattedValue ? `result of cast: ${formattedResult}` : ''));\n    }\n\n    return result;\n  }\n\n  _cast(rawValue, _options) {\n    let value = rawValue === undefined ? rawValue : this.transforms.reduce((value, fn) => fn.call(this, value, rawValue, this), rawValue);\n\n    if (value === undefined) {\n      value = this.getDefault();\n    }\n\n    return value;\n  }\n\n  _validate(_value, options = {}, cb) {\n    let {\n      sync,\n      path,\n      from = [],\n      originalValue = _value,\n      strict = this.spec.strict,\n      abortEarly = this.spec.abortEarly\n    } = options;\n    let value = _value;\n\n    if (!strict) {\n      // this._validating = true;\n      value = this._cast(value, _extends({\n        assert: false\n      }, options)); // this._validating = false;\n    } // value is cast, we can check if it meets type requirements\n\n\n    let args = {\n      value,\n      path,\n      options,\n      originalValue,\n      schema: this,\n      label: this.spec.label,\n      sync,\n      from\n    };\n    let initialTests = [];\n    if (this._typeError) initialTests.push(this._typeError);\n    let finalTests = [];\n    if (this._whitelistError) finalTests.push(this._whitelistError);\n    if (this._blacklistError) finalTests.push(this._blacklistError);\n    (0, _runTests.default)({\n      args,\n      value,\n      path,\n      sync,\n      tests: initialTests,\n      endEarly: abortEarly\n    }, err => {\n      if (err) return void cb(err, value);\n      (0, _runTests.default)({\n        tests: this.tests.concat(finalTests),\n        args,\n        path,\n        sync,\n        value,\n        endEarly: abortEarly\n      }, cb);\n    });\n  }\n\n  validate(value, options, maybeCb) {\n    let schema = this.resolve(_extends({}, options, {\n      value\n    })); // callback case is for nested validations\n\n    return typeof maybeCb === 'function' ? schema._validate(value, options, maybeCb) : new Promise((resolve, reject) => schema._validate(value, options, (err, value) => {\n      if (err) reject(err);else resolve(value);\n    }));\n  }\n\n  validateSync(value, options) {\n    let schema = this.resolve(_extends({}, options, {\n      value\n    }));\n    let result;\n\n    schema._validate(value, _extends({}, options, {\n      sync: true\n    }), (err, value) => {\n      if (err) throw err;\n      result = value;\n    });\n\n    return result;\n  }\n\n  isValid(value, options) {\n    return this.validate(value, options).then(() => true, err => {\n      if (_ValidationError.default.isError(err)) return false;\n      throw err;\n    });\n  }\n\n  isValidSync(value, options) {\n    try {\n      this.validateSync(value, options);\n      return true;\n    } catch (err) {\n      if (_ValidationError.default.isError(err)) return false;\n      throw err;\n    }\n  }\n\n  _getDefault() {\n    let defaultValue = this.spec.default;\n\n    if (defaultValue == null) {\n      return defaultValue;\n    }\n\n    return typeof defaultValue === 'function' ? defaultValue.call(this) : (0, _nanoclone.default)(defaultValue);\n  }\n\n  getDefault(options) {\n    let schema = this.resolve(options || {});\n    return schema._getDefault();\n  }\n\n  default(def) {\n    if (arguments.length === 0) {\n      return this._getDefault();\n    }\n\n    let next = this.clone({\n      default: def\n    });\n    return next;\n  }\n\n  strict(isStrict = true) {\n    let next = this.clone();\n    next.spec.strict = isStrict;\n    return next;\n  }\n\n  _isPresent(value) {\n    return value != null;\n  }\n\n  defined(message = _locale.mixed.defined) {\n    return this.test({\n      message,\n      name: 'defined',\n      exclusive: true,\n\n      test(value) {\n        return value !== undefined;\n      }\n\n    });\n  }\n\n  required(message = _locale.mixed.required) {\n    return this.clone({\n      presence: 'required'\n    }).withMutation(s => s.test({\n      message,\n      name: 'required',\n      exclusive: true,\n\n      test(value) {\n        return this.schema._isPresent(value);\n      }\n\n    }));\n  }\n\n  notRequired() {\n    let next = this.clone({\n      presence: 'optional'\n    });\n    next.tests = next.tests.filter(test => test.OPTIONS.name !== 'required');\n    return next;\n  }\n\n  nullable(isNullable = true) {\n    let next = this.clone({\n      nullable: isNullable !== false\n    });\n    return next;\n  }\n\n  transform(fn) {\n    let next = this.clone();\n    next.transforms.push(fn);\n    return next;\n  }\n  /**\n   * Adds a test function to the schema's queue of tests.\n   * tests can be exclusive or non-exclusive.\n   *\n   * - exclusive tests, will replace any existing tests of the same name.\n   * - non-exclusive: can be stacked\n   *\n   * If a non-exclusive test is added to a schema with an exclusive test of the same name\n   * the exclusive test is removed and further tests of the same name will be stacked.\n   *\n   * If an exclusive test is added to a schema with non-exclusive tests of the same name\n   * the previous tests are removed and further tests of the same name will replace each other.\n   */\n\n\n  test(...args) {\n    let opts;\n\n    if (args.length === 1) {\n      if (typeof args[0] === 'function') {\n        opts = {\n          test: args[0]\n        };\n      } else {\n        opts = args[0];\n      }\n    } else if (args.length === 2) {\n      opts = {\n        name: args[0],\n        test: args[1]\n      };\n    } else {\n      opts = {\n        name: args[0],\n        message: args[1],\n        test: args[2]\n      };\n    }\n\n    if (opts.message === undefined) opts.message = _locale.mixed.default;\n    if (typeof opts.test !== 'function') throw new TypeError('`test` is a required parameters');\n    let next = this.clone();\n    let validate = (0, _createValidation.default)(opts);\n    let isExclusive = opts.exclusive || opts.name && next.exclusiveTests[opts.name] === true;\n\n    if (opts.exclusive) {\n      if (!opts.name) throw new TypeError('Exclusive tests must provide a unique `name` identifying the test');\n    }\n\n    if (opts.name) next.exclusiveTests[opts.name] = !!opts.exclusive;\n    next.tests = next.tests.filter(fn => {\n      if (fn.OPTIONS.name === opts.name) {\n        if (isExclusive) return false;\n        if (fn.OPTIONS.test === validate.OPTIONS.test) return false;\n      }\n\n      return true;\n    });\n    next.tests.push(validate);\n    return next;\n  }\n\n  when(keys, options) {\n    if (!Array.isArray(keys) && typeof keys !== 'string') {\n      options = keys;\n      keys = '.';\n    }\n\n    let next = this.clone();\n    let deps = (0, _toArray.default)(keys).map(key => new _Reference.default(key));\n    deps.forEach(dep => {\n      // @ts-ignore\n      if (dep.isSibling) next.deps.push(dep.key);\n    });\n    next.conditions.push(new _Condition.default(deps, options));\n    return next;\n  }\n\n  typeError(message) {\n    let next = this.clone();\n    next._typeError = (0, _createValidation.default)({\n      message,\n      name: 'typeError',\n\n      test(value) {\n        if (value !== undefined && !this.schema.isType(value)) return this.createError({\n          params: {\n            type: this.schema._type\n          }\n        });\n        return true;\n      }\n\n    });\n    return next;\n  }\n\n  oneOf(enums, message = _locale.mixed.oneOf) {\n    let next = this.clone();\n    enums.forEach(val => {\n      next._whitelist.add(val);\n\n      next._blacklist.delete(val);\n    });\n    next._whitelistError = (0, _createValidation.default)({\n      message,\n      name: 'oneOf',\n\n      test(value) {\n        if (value === undefined) return true;\n        let valids = this.schema._whitelist;\n        let resolved = valids.resolveAll(this.resolve);\n        return resolved.includes(value) ? true : this.createError({\n          params: {\n            values: valids.toArray().join(', '),\n            resolved\n          }\n        });\n      }\n\n    });\n    return next;\n  }\n\n  notOneOf(enums, message = _locale.mixed.notOneOf) {\n    let next = this.clone();\n    enums.forEach(val => {\n      next._blacklist.add(val);\n\n      next._whitelist.delete(val);\n    });\n    next._blacklistError = (0, _createValidation.default)({\n      message,\n      name: 'notOneOf',\n\n      test(value) {\n        let invalids = this.schema._blacklist;\n        let resolved = invalids.resolveAll(this.resolve);\n        if (resolved.includes(value)) return this.createError({\n          params: {\n            values: invalids.toArray().join(', '),\n            resolved\n          }\n        });\n        return true;\n      }\n\n    });\n    return next;\n  }\n\n  strip(strip = true) {\n    let next = this.clone();\n    next.spec.strip = strip;\n    return next;\n  }\n\n  describe() {\n    const next = this.clone();\n    const {\n      label,\n      meta\n    } = next.spec;\n    const description = {\n      meta,\n      label,\n      type: next.type,\n      oneOf: next._whitelist.describe(),\n      notOneOf: next._blacklist.describe(),\n      tests: next.tests.map(fn => ({\n        name: fn.OPTIONS.name,\n        params: fn.OPTIONS.params\n      })).filter((n, idx, list) => list.findIndex(c => c.name === n.name) === idx)\n    };\n    return description;\n  }\n\n} // eslint-disable-next-line @typescript-eslint/no-unused-vars\n\n\nexports.default = BaseSchema;\n// @ts-expect-error\nBaseSchema.prototype.__isYupSchema__ = true;\n\nfor (const method of ['validate', 'validateSync']) BaseSchema.prototype[`${method}At`] = function (path, value, options = {}) {\n  const {\n    parent,\n    parentPath,\n    schema\n  } = (0, _reach.getIn)(this, path, value, options.context);\n  return schema[method](parent && parent[parentPath], _extends({}, options, {\n    parent,\n    path\n  }));\n};\n\nfor (const alias of ['equals', 'is']) BaseSchema.prototype[alias] = BaseSchema.prototype.oneOf;\n\nfor (const alias of ['not', 'nope']) BaseSchema.prototype[alias] = BaseSchema.prototype.notOneOf;\n\nBaseSchema.prototype.optional = BaseSchema.prototype.notRequired;", "\"use strict\";\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\nexports.create = create;\nexports.default = void 0;\n\nvar _schema = _interopRequireDefault(require(\"./schema\"));\n\nfunction _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { default: obj }; }\n\nconst Mixed = _schema.default;\nvar _default = Mixed;\nexports.default = _default;\n\nfunction create() {\n  return new Mixed();\n} // XXX: this is using the Base schema so that `addMethod(mixed)` works as a base class\n\n\ncreate.prototype = Mixed.prototype;", "\"use strict\";\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\nexports.default = void 0;\n\nconst isAbsent = value => value == null;\n\nvar _default = isAbsent;\nexports.default = _default;", "\"use strict\";\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\nexports.create = create;\nexports.default = void 0;\n\nvar _schema = _interopRequireDefault(require(\"./schema\"));\n\nvar _locale = require(\"./locale\");\n\nvar _isAbsent = _interopRequireDefault(require(\"./util/isAbsent\"));\n\nfunction _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { default: obj }; }\n\nfunction create() {\n  return new BooleanSchema();\n}\n\nclass BooleanSchema extends _schema.default {\n  constructor() {\n    super({\n      type: 'boolean'\n    });\n    this.withMutation(() => {\n      this.transform(function (value) {\n        if (!this.isType(value)) {\n          if (/^(true|1)$/i.test(String(value))) return true;\n          if (/^(false|0)$/i.test(String(value))) return false;\n        }\n\n        return value;\n      });\n    });\n  }\n\n  _typeCheck(v) {\n    if (v instanceof Boolean) v = v.valueOf();\n    return typeof v === 'boolean';\n  }\n\n  isTrue(message = _locale.boolean.isValue) {\n    return this.test({\n      message,\n      name: 'is-value',\n      exclusive: true,\n      params: {\n        value: 'true'\n      },\n\n      test(value) {\n        return (0, _isAbsent.default)(value) || value === true;\n      }\n\n    });\n  }\n\n  isFalse(message = _locale.boolean.isValue) {\n    return this.test({\n      message,\n      name: 'is-value',\n      exclusive: true,\n      params: {\n        value: 'false'\n      },\n\n      test(value) {\n        return (0, _isAbsent.default)(value) || value === false;\n      }\n\n    });\n  }\n\n}\n\nexports.default = BooleanSchema;\ncreate.prototype = BooleanSchema.prototype;", "\"use strict\";\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\nexports.create = create;\nexports.default = void 0;\n\nvar _locale = require(\"./locale\");\n\nvar _isAbsent = _interopRequireDefault(require(\"./util/isAbsent\"));\n\nvar _schema = _interopRequireDefault(require(\"./schema\"));\n\nfunction _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { default: obj }; }\n\n// eslint-disable-next-line\nlet rEmail = /^((([a-z]|\\d|[!#\\$%&'\\*\\+\\-\\/=\\?\\^_`{\\|}~]|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])+(\\.([a-z]|\\d|[!#\\$%&'\\*\\+\\-\\/=\\?\\^_`{\\|}~]|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])+)*)|((\\x22)((((\\x20|\\x09)*(\\x0d\\x0a))?(\\x20|\\x09)+)?(([\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x7f]|\\x21|[\\x23-\\x5b]|[\\x5d-\\x7e]|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])|(\\\\([\\x01-\\x09\\x0b\\x0c\\x0d-\\x7f]|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF]))))*(((\\x20|\\x09)*(\\x0d\\x0a))?(\\x20|\\x09)+)?(\\x22)))@((([a-z]|\\d|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])|(([a-z]|\\d|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])([a-z]|\\d|-|\\.|_|~|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])*([a-z]|\\d|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])))\\.)+(([a-z]|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])|(([a-z]|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])([a-z]|\\d|-|\\.|_|~|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])*([a-z]|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])))$/i; // eslint-disable-next-line\n\nlet rUrl = /^((https?|ftp):)?\\/\\/(((([a-z]|\\d|-|\\.|_|~|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])|(%[\\da-f]{2})|[!\\$&'\\(\\)\\*\\+,;=]|:)*@)?(((\\d|[1-9]\\d|1\\d\\d|2[0-4]\\d|25[0-5])\\.(\\d|[1-9]\\d|1\\d\\d|2[0-4]\\d|25[0-5])\\.(\\d|[1-9]\\d|1\\d\\d|2[0-4]\\d|25[0-5])\\.(\\d|[1-9]\\d|1\\d\\d|2[0-4]\\d|25[0-5]))|((([a-z]|\\d|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])|(([a-z]|\\d|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])([a-z]|\\d|-|\\.|_|~|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])*([a-z]|\\d|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])))\\.)+(([a-z]|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])|(([a-z]|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])([a-z]|\\d|-|\\.|_|~|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])*([a-z]|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])))\\.?)(:\\d*)?)(\\/((([a-z]|\\d|-|\\.|_|~|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])|(%[\\da-f]{2})|[!\\$&'\\(\\)\\*\\+,;=]|:|@)+(\\/(([a-z]|\\d|-|\\.|_|~|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])|(%[\\da-f]{2})|[!\\$&'\\(\\)\\*\\+,;=]|:|@)*)*)?)?(\\?((([a-z]|\\d|-|\\.|_|~|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])|(%[\\da-f]{2})|[!\\$&'\\(\\)\\*\\+,;=]|:|@)|[\\uE000-\\uF8FF]|\\/|\\?)*)?(\\#((([a-z]|\\d|-|\\.|_|~|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])|(%[\\da-f]{2})|[!\\$&'\\(\\)\\*\\+,;=]|:|@)|\\/|\\?)*)?$/i; // eslint-disable-next-line\n\nlet rUUID = /^(?:[0-9a-f]{8}-[0-9a-f]{4}-[1-5][0-9a-f]{3}-[89ab][0-9a-f]{3}-[0-9a-f]{12}|00000000-0000-0000-0000-000000000000)$/i;\n\nlet isTrimmed = value => (0, _isAbsent.default)(value) || value === value.trim();\n\nlet objStringTag = {}.toString();\n\nfunction create() {\n  return new StringSchema();\n}\n\nclass StringSchema extends _schema.default {\n  constructor() {\n    super({\n      type: 'string'\n    });\n    this.withMutation(() => {\n      this.transform(function (value) {\n        if (this.isType(value)) return value;\n        if (Array.isArray(value)) return value;\n        const strValue = value != null && value.toString ? value.toString() : value;\n        if (strValue === objStringTag) return value;\n        return strValue;\n      });\n    });\n  }\n\n  _typeCheck(value) {\n    if (value instanceof String) value = value.valueOf();\n    return typeof value === 'string';\n  }\n\n  _isPresent(value) {\n    return super._isPresent(value) && !!value.length;\n  }\n\n  length(length, message = _locale.string.length) {\n    return this.test({\n      message,\n      name: 'length',\n      exclusive: true,\n      params: {\n        length\n      },\n\n      test(value) {\n        return (0, _isAbsent.default)(value) || value.length === this.resolve(length);\n      }\n\n    });\n  }\n\n  min(min, message = _locale.string.min) {\n    return this.test({\n      message,\n      name: 'min',\n      exclusive: true,\n      params: {\n        min\n      },\n\n      test(value) {\n        return (0, _isAbsent.default)(value) || value.length >= this.resolve(min);\n      }\n\n    });\n  }\n\n  max(max, message = _locale.string.max) {\n    return this.test({\n      name: 'max',\n      exclusive: true,\n      message,\n      params: {\n        max\n      },\n\n      test(value) {\n        return (0, _isAbsent.default)(value) || value.length <= this.resolve(max);\n      }\n\n    });\n  }\n\n  matches(regex, options) {\n    let excludeEmptyString = false;\n    let message;\n    let name;\n\n    if (options) {\n      if (typeof options === 'object') {\n        ({\n          excludeEmptyString = false,\n          message,\n          name\n        } = options);\n      } else {\n        message = options;\n      }\n    }\n\n    return this.test({\n      name: name || 'matches',\n      message: message || _locale.string.matches,\n      params: {\n        regex\n      },\n      test: value => (0, _isAbsent.default)(value) || value === '' && excludeEmptyString || value.search(regex) !== -1\n    });\n  }\n\n  email(message = _locale.string.email) {\n    return this.matches(rEmail, {\n      name: 'email',\n      message,\n      excludeEmptyString: true\n    });\n  }\n\n  url(message = _locale.string.url) {\n    return this.matches(rUrl, {\n      name: 'url',\n      message,\n      excludeEmptyString: true\n    });\n  }\n\n  uuid(message = _locale.string.uuid) {\n    return this.matches(rUUID, {\n      name: 'uuid',\n      message,\n      excludeEmptyString: false\n    });\n  } //-- transforms --\n\n\n  ensure() {\n    return this.default('').transform(val => val === null ? '' : val);\n  }\n\n  trim(message = _locale.string.trim) {\n    return this.transform(val => val != null ? val.trim() : val).test({\n      message,\n      name: 'trim',\n      test: isTrimmed\n    });\n  }\n\n  lowercase(message = _locale.string.lowercase) {\n    return this.transform(value => !(0, _isAbsent.default)(value) ? value.toLowerCase() : value).test({\n      message,\n      name: 'string_case',\n      exclusive: true,\n      test: value => (0, _isAbsent.default)(value) || value === value.toLowerCase()\n    });\n  }\n\n  uppercase(message = _locale.string.uppercase) {\n    return this.transform(value => !(0, _isAbsent.default)(value) ? value.toUpperCase() : value).test({\n      message,\n      name: 'string_case',\n      exclusive: true,\n      test: value => (0, _isAbsent.default)(value) || value === value.toUpperCase()\n    });\n  }\n\n}\n\nexports.default = StringSchema;\ncreate.prototype = StringSchema.prototype; //\n// String Interfaces\n//", "\"use strict\";\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\nexports.create = create;\nexports.default = void 0;\n\nvar _locale = require(\"./locale\");\n\nvar _isAbsent = _interopRequireDefault(require(\"./util/isAbsent\"));\n\nvar _schema = _interopRequireDefault(require(\"./schema\"));\n\nfunction _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { default: obj }; }\n\nlet isNaN = value => value != +value;\n\nfunction create() {\n  return new NumberSchema();\n}\n\nclass NumberSchema extends _schema.default {\n  constructor() {\n    super({\n      type: 'number'\n    });\n    this.withMutation(() => {\n      this.transform(function (value) {\n        let parsed = value;\n\n        if (typeof parsed === 'string') {\n          parsed = parsed.replace(/\\s/g, '');\n          if (parsed === '') return NaN; // don't use parseFloat to avoid positives on alpha-numeric strings\n\n          parsed = +parsed;\n        }\n\n        if (this.isType(parsed)) return parsed;\n        return parseFloat(parsed);\n      });\n    });\n  }\n\n  _typeCheck(value) {\n    if (value instanceof Number) value = value.valueOf();\n    return typeof value === 'number' && !isNaN(value);\n  }\n\n  min(min, message = _locale.number.min) {\n    return this.test({\n      message,\n      name: 'min',\n      exclusive: true,\n      params: {\n        min\n      },\n\n      test(value) {\n        return (0, _isAbsent.default)(value) || value >= this.resolve(min);\n      }\n\n    });\n  }\n\n  max(max, message = _locale.number.max) {\n    return this.test({\n      message,\n      name: 'max',\n      exclusive: true,\n      params: {\n        max\n      },\n\n      test(value) {\n        return (0, _isAbsent.default)(value) || value <= this.resolve(max);\n      }\n\n    });\n  }\n\n  lessThan(less, message = _locale.number.lessThan) {\n    return this.test({\n      message,\n      name: 'max',\n      exclusive: true,\n      params: {\n        less\n      },\n\n      test(value) {\n        return (0, _isAbsent.default)(value) || value < this.resolve(less);\n      }\n\n    });\n  }\n\n  moreThan(more, message = _locale.number.moreThan) {\n    return this.test({\n      message,\n      name: 'min',\n      exclusive: true,\n      params: {\n        more\n      },\n\n      test(value) {\n        return (0, _isAbsent.default)(value) || value > this.resolve(more);\n      }\n\n    });\n  }\n\n  positive(msg = _locale.number.positive) {\n    return this.moreThan(0, msg);\n  }\n\n  negative(msg = _locale.number.negative) {\n    return this.lessThan(0, msg);\n  }\n\n  integer(message = _locale.number.integer) {\n    return this.test({\n      name: 'integer',\n      message,\n      test: val => (0, _isAbsent.default)(val) || Number.isInteger(val)\n    });\n  }\n\n  truncate() {\n    return this.transform(value => !(0, _isAbsent.default)(value) ? value | 0 : value);\n  }\n\n  round(method) {\n    var _method;\n\n    let avail = ['ceil', 'floor', 'round', 'trunc'];\n    method = ((_method = method) == null ? void 0 : _method.toLowerCase()) || 'round'; // this exists for symemtry with the new Math.trunc\n\n    if (method === 'trunc') return this.truncate();\n    if (avail.indexOf(method.toLowerCase()) === -1) throw new TypeError('Only valid options for round() are: ' + avail.join(', '));\n    return this.transform(value => !(0, _isAbsent.default)(value) ? Math[method](value) : value);\n  }\n\n}\n\nexports.default = NumberSchema;\ncreate.prototype = NumberSchema.prototype; //\n// Number Interfaces\n//", "\"use strict\";\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\nexports.default = parseIsoDate;\n\n/* eslint-disable */\n\n/**\n *\n * Date.parse with progressive enhancement for ISO 8601 <https://github.com/csnover/js-iso8601>\n * NON-CONFORMANT EDITION.\n * \u00A9 2011 Colin Snover <http://zetafleet.com>\n * Released under MIT license.\n */\n//              1 YYYY                 2 MM        3 DD              4 HH     5 mm        6 ss            7 msec         8 Z 9 \u00B1    10 tzHH    11 tzmm\nvar isoReg = /^(\\d{4}|[+\\-]\\d{6})(?:-?(\\d{2})(?:-?(\\d{2}))?)?(?:[ T]?(\\d{2}):?(\\d{2})(?::?(\\d{2})(?:[,\\.](\\d{1,}))?)?(?:(Z)|([+\\-])(\\d{2})(?::?(\\d{2}))?)?)?$/;\n\nfunction parseIsoDate(date) {\n  var numericKeys = [1, 4, 5, 6, 7, 10, 11],\n      minutesOffset = 0,\n      timestamp,\n      struct;\n\n  if (struct = isoReg.exec(date)) {\n    // avoid NaN timestamps caused by \u201Cundefined\u201D values being passed to Date.UTC\n    for (var i = 0, k; k = numericKeys[i]; ++i) struct[k] = +struct[k] || 0; // allow undefined days and months\n\n\n    struct[2] = (+struct[2] || 1) - 1;\n    struct[3] = +struct[3] || 1; // allow arbitrary sub-second precision beyond milliseconds\n\n    struct[7] = struct[7] ? String(struct[7]).substr(0, 3) : 0; // timestamps without timezone identifiers should be considered local time\n\n    if ((struct[8] === undefined || struct[8] === '') && (struct[9] === undefined || struct[9] === '')) timestamp = +new Date(struct[1], struct[2], struct[3], struct[4], struct[5], struct[6], struct[7]);else {\n      if (struct[8] !== 'Z' && struct[9] !== undefined) {\n        minutesOffset = struct[10] * 60 + struct[11];\n        if (struct[9] === '+') minutesOffset = 0 - minutesOffset;\n      }\n\n      timestamp = Date.UTC(struct[1], struct[2], struct[3], struct[4], struct[5] + minutesOffset, struct[6], struct[7]);\n    }\n  } else timestamp = Date.parse ? Date.parse(date) : NaN;\n\n  return timestamp;\n}", "\"use strict\";\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\nexports.create = create;\nexports.default = void 0;\n\nvar _isodate = _interopRequireDefault(require(\"./util/isodate\"));\n\nvar _locale = require(\"./locale\");\n\nvar _isAbsent = _interopRequireDefault(require(\"./util/isAbsent\"));\n\nvar _Reference = _interopRequireDefault(require(\"./Reference\"));\n\nvar _schema = _interopRequireDefault(require(\"./schema\"));\n\nfunction _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { default: obj }; }\n\n// @ts-ignore\nlet invalidDate = new Date('');\n\nlet isDate = obj => Object.prototype.toString.call(obj) === '[object Date]';\n\nfunction create() {\n  return new DateSchema();\n}\n\nclass DateSchema extends _schema.default {\n  constructor() {\n    super({\n      type: 'date'\n    });\n    this.withMutation(() => {\n      this.transform(function (value) {\n        if (this.isType(value)) return value;\n        value = (0, _isodate.default)(value); // 0 is a valid timestamp equivalent to 1970-01-01T00:00:00Z(unix epoch) or before.\n\n        return !isNaN(value) ? new Date(value) : invalidDate;\n      });\n    });\n  }\n\n  _typeCheck(v) {\n    return isDate(v) && !isNaN(v.getTime());\n  }\n\n  prepareParam(ref, name) {\n    let param;\n\n    if (!_Reference.default.isRef(ref)) {\n      let cast = this.cast(ref);\n      if (!this._typeCheck(cast)) throw new TypeError(`\\`${name}\\` must be a Date or a value that can be \\`cast()\\` to a Date`);\n      param = cast;\n    } else {\n      param = ref;\n    }\n\n    return param;\n  }\n\n  min(min, message = _locale.date.min) {\n    let limit = this.prepareParam(min, 'min');\n    return this.test({\n      message,\n      name: 'min',\n      exclusive: true,\n      params: {\n        min\n      },\n\n      test(value) {\n        return (0, _isAbsent.default)(value) || value >= this.resolve(limit);\n      }\n\n    });\n  }\n\n  max(max, message = _locale.date.max) {\n    let limit = this.prepareParam(max, 'max');\n    return this.test({\n      message,\n      name: 'max',\n      exclusive: true,\n      params: {\n        max\n      },\n\n      test(value) {\n        return (0, _isAbsent.default)(value) || value <= this.resolve(limit);\n      }\n\n    });\n  }\n\n}\n\nexports.default = DateSchema;\nDateSchema.INVALID_DATE = invalidDate;\ncreate.prototype = DateSchema.prototype;\ncreate.INVALID_DATE = invalidDate;", "/**\n * A specialized version of `_.reduce` for arrays without support for\n * iteratee shorthands.\n *\n * @private\n * @param {Array} [array] The array to iterate over.\n * @param {Function} iteratee The function invoked per iteration.\n * @param {*} [accumulator] The initial value.\n * @param {boolean} [initAccum] Specify using the first element of `array` as\n *  the initial value.\n * @returns {*} Returns the accumulated value.\n */\nfunction arrayReduce(array, iteratee, accumulator, initAccum) {\n  var index = -1,\n      length = array == null ? 0 : array.length;\n\n  if (initAccum && length) {\n    accumulator = array[++index];\n  }\n  while (++index < length) {\n    accumulator = iteratee(accumulator, array[index], index, array);\n  }\n  return accumulator;\n}\n\nmodule.exports = arrayReduce;\n", "/**\n * The base implementation of `_.propertyOf` without support for deep paths.\n *\n * @private\n * @param {Object} object The object to query.\n * @returns {Function} Returns the new accessor function.\n */\nfunction basePropertyOf(object) {\n  return function(key) {\n    return object == null ? undefined : object[key];\n  };\n}\n\nmodule.exports = basePropertyOf;\n", "var basePropertyOf = require('./_basePropertyOf');\n\n/** Used to map Latin Unicode letters to basic Latin letters. */\nvar deburredLetters = {\n  // Latin-1 Supplement block.\n  '\\xc0': 'A',  '\\xc1': 'A', '\\xc2': 'A', '\\xc3': 'A', '\\xc4': 'A', '\\xc5': 'A',\n  '\\xe0': 'a',  '\\xe1': 'a', '\\xe2': 'a', '\\xe3': 'a', '\\xe4': 'a', '\\xe5': 'a',\n  '\\xc7': 'C',  '\\xe7': 'c',\n  '\\xd0': 'D',  '\\xf0': 'd',\n  '\\xc8': 'E',  '\\xc9': 'E', '\\xca': 'E', '\\xcb': 'E',\n  '\\xe8': 'e',  '\\xe9': 'e', '\\xea': 'e', '\\xeb': 'e',\n  '\\xcc': 'I',  '\\xcd': 'I', '\\xce': 'I', '\\xcf': 'I',\n  '\\xec': 'i',  '\\xed': 'i', '\\xee': 'i', '\\xef': 'i',\n  '\\xd1': 'N',  '\\xf1': 'n',\n  '\\xd2': 'O',  '\\xd3': 'O', '\\xd4': 'O', '\\xd5': 'O', '\\xd6': 'O', '\\xd8': 'O',\n  '\\xf2': 'o',  '\\xf3': 'o', '\\xf4': 'o', '\\xf5': 'o', '\\xf6': 'o', '\\xf8': 'o',\n  '\\xd9': 'U',  '\\xda': 'U', '\\xdb': 'U', '\\xdc': 'U',\n  '\\xf9': 'u',  '\\xfa': 'u', '\\xfb': 'u', '\\xfc': 'u',\n  '\\xdd': 'Y',  '\\xfd': 'y', '\\xff': 'y',\n  '\\xc6': 'Ae', '\\xe6': 'ae',\n  '\\xde': 'Th', '\\xfe': 'th',\n  '\\xdf': 'ss',\n  // Latin Extended-A block.\n  '\\u0100': 'A',  '\\u0102': 'A', '\\u0104': 'A',\n  '\\u0101': 'a',  '\\u0103': 'a', '\\u0105': 'a',\n  '\\u0106': 'C',  '\\u0108': 'C', '\\u010a': 'C', '\\u010c': 'C',\n  '\\u0107': 'c',  '\\u0109': 'c', '\\u010b': 'c', '\\u010d': 'c',\n  '\\u010e': 'D',  '\\u0110': 'D', '\\u010f': 'd', '\\u0111': 'd',\n  '\\u0112': 'E',  '\\u0114': 'E', '\\u0116': 'E', '\\u0118': 'E', '\\u011a': 'E',\n  '\\u0113': 'e',  '\\u0115': 'e', '\\u0117': 'e', '\\u0119': 'e', '\\u011b': 'e',\n  '\\u011c': 'G',  '\\u011e': 'G', '\\u0120': 'G', '\\u0122': 'G',\n  '\\u011d': 'g',  '\\u011f': 'g', '\\u0121': 'g', '\\u0123': 'g',\n  '\\u0124': 'H',  '\\u0126': 'H', '\\u0125': 'h', '\\u0127': 'h',\n  '\\u0128': 'I',  '\\u012a': 'I', '\\u012c': 'I', '\\u012e': 'I', '\\u0130': 'I',\n  '\\u0129': 'i',  '\\u012b': 'i', '\\u012d': 'i', '\\u012f': 'i', '\\u0131': 'i',\n  '\\u0134': 'J',  '\\u0135': 'j',\n  '\\u0136': 'K',  '\\u0137': 'k', '\\u0138': 'k',\n  '\\u0139': 'L',  '\\u013b': 'L', '\\u013d': 'L', '\\u013f': 'L', '\\u0141': 'L',\n  '\\u013a': 'l',  '\\u013c': 'l', '\\u013e': 'l', '\\u0140': 'l', '\\u0142': 'l',\n  '\\u0143': 'N',  '\\u0145': 'N', '\\u0147': 'N', '\\u014a': 'N',\n  '\\u0144': 'n',  '\\u0146': 'n', '\\u0148': 'n', '\\u014b': 'n',\n  '\\u014c': 'O',  '\\u014e': 'O', '\\u0150': 'O',\n  '\\u014d': 'o',  '\\u014f': 'o', '\\u0151': 'o',\n  '\\u0154': 'R',  '\\u0156': 'R', '\\u0158': 'R',\n  '\\u0155': 'r',  '\\u0157': 'r', '\\u0159': 'r',\n  '\\u015a': 'S',  '\\u015c': 'S', '\\u015e': 'S', '\\u0160': 'S',\n  '\\u015b': 's',  '\\u015d': 's', '\\u015f': 's', '\\u0161': 's',\n  '\\u0162': 'T',  '\\u0164': 'T', '\\u0166': 'T',\n  '\\u0163': 't',  '\\u0165': 't', '\\u0167': 't',\n  '\\u0168': 'U',  '\\u016a': 'U', '\\u016c': 'U', '\\u016e': 'U', '\\u0170': 'U', '\\u0172': 'U',\n  '\\u0169': 'u',  '\\u016b': 'u', '\\u016d': 'u', '\\u016f': 'u', '\\u0171': 'u', '\\u0173': 'u',\n  '\\u0174': 'W',  '\\u0175': 'w',\n  '\\u0176': 'Y',  '\\u0177': 'y', '\\u0178': 'Y',\n  '\\u0179': 'Z',  '\\u017b': 'Z', '\\u017d': 'Z',\n  '\\u017a': 'z',  '\\u017c': 'z', '\\u017e': 'z',\n  '\\u0132': 'IJ', '\\u0133': 'ij',\n  '\\u0152': 'Oe', '\\u0153': 'oe',\n  '\\u0149': \"'n\", '\\u017f': 's'\n};\n\n/**\n * Used by `_.deburr` to convert Latin-1 Supplement and Latin Extended-A\n * letters to basic Latin letters.\n *\n * @private\n * @param {string} letter The matched letter to deburr.\n * @returns {string} Returns the deburred letter.\n */\nvar deburrLetter = basePropertyOf(deburredLetters);\n\nmodule.exports = deburrLetter;\n", "var deburrLetter = require('./_deburrLetter'),\n    toString = require('./toString');\n\n/** Used to match Latin Unicode letters (excluding mathematical operators). */\nvar reLatin = /[\\xc0-\\xd6\\xd8-\\xf6\\xf8-\\xff\\u0100-\\u017f]/g;\n\n/** Used to compose unicode character classes. */\nvar rsComboMarksRange = '\\\\u0300-\\\\u036f',\n    reComboHalfMarksRange = '\\\\ufe20-\\\\ufe2f',\n    rsComboSymbolsRange = '\\\\u20d0-\\\\u20ff',\n    rsComboRange = rsComboMarksRange + reComboHalfMarksRange + rsComboSymbolsRange;\n\n/** Used to compose unicode capture groups. */\nvar rsCombo = '[' + rsComboRange + ']';\n\n/**\n * Used to match [combining diacritical marks](https://en.wikipedia.org/wiki/Combining_Diacritical_Marks) and\n * [combining diacritical marks for symbols](https://en.wikipedia.org/wiki/Combining_Diacritical_Marks_for_Symbols).\n */\nvar reComboMark = RegExp(rsCombo, 'g');\n\n/**\n * Deburrs `string` by converting\n * [Latin-1 Supplement](https://en.wikipedia.org/wiki/Latin-1_Supplement_(Unicode_block)#Character_table)\n * and [Latin Extended-A](https://en.wikipedia.org/wiki/Latin_Extended-A)\n * letters to basic Latin letters and removing\n * [combining diacritical marks](https://en.wikipedia.org/wiki/Combining_Diacritical_Marks).\n *\n * @static\n * @memberOf _\n * @since 3.0.0\n * @category String\n * @param {string} [string=''] The string to deburr.\n * @returns {string} Returns the deburred string.\n * @example\n *\n * _.deburr('d\u00E9j\u00E0 vu');\n * // => 'deja vu'\n */\nfunction deburr(string) {\n  string = toString(string);\n  return string && string.replace(reLatin, deburrLetter).replace(reComboMark, '');\n}\n\nmodule.exports = deburr;\n", "/** Used to match words composed of alphanumeric characters. */\nvar reAsciiWord = /[^\\x00-\\x2f\\x3a-\\x40\\x5b-\\x60\\x7b-\\x7f]+/g;\n\n/**\n * Splits an ASCII `string` into an array of its words.\n *\n * @private\n * @param {string} The string to inspect.\n * @returns {Array} Returns the words of `string`.\n */\nfunction asciiWords(string) {\n  return string.match(reAsciiWord) || [];\n}\n\nmodule.exports = asciiWords;\n", "/** Used to detect strings that need a more robust regexp to match words. */\nvar reHasUnicodeWord = /[a-z][A-Z]|[A-Z]{2}[a-z]|[0-9][a-zA-Z]|[a-zA-Z][0-9]|[^a-zA-Z0-9 ]/;\n\n/**\n * Checks if `string` contains a word composed of Unicode symbols.\n *\n * @private\n * @param {string} string The string to inspect.\n * @returns {boolean} Returns `true` if a word is found, else `false`.\n */\nfunction hasUnicodeWord(string) {\n  return reHasUnicodeWord.test(string);\n}\n\nmodule.exports = hasUnicodeWord;\n", "/** Used to compose unicode character classes. */\nvar rsAstralRange = '\\\\ud800-\\\\udfff',\n    rsComboMarksRange = '\\\\u0300-\\\\u036f',\n    reComboHalfMarksRange = '\\\\ufe20-\\\\ufe2f',\n    rsComboSymbolsRange = '\\\\u20d0-\\\\u20ff',\n    rsComboRange = rsComboMarksRange + reComboHalfMarksRange + rsComboSymbolsRange,\n    rsDingbatRange = '\\\\u2700-\\\\u27bf',\n    rsLowerRange = 'a-z\\\\xdf-\\\\xf6\\\\xf8-\\\\xff',\n    rsMathOpRange = '\\\\xac\\\\xb1\\\\xd7\\\\xf7',\n    rsNonCharRange = '\\\\x00-\\\\x2f\\\\x3a-\\\\x40\\\\x5b-\\\\x60\\\\x7b-\\\\xbf',\n    rsPunctuationRange = '\\\\u2000-\\\\u206f',\n    rsSpaceRange = ' \\\\t\\\\x0b\\\\f\\\\xa0\\\\ufeff\\\\n\\\\r\\\\u2028\\\\u2029\\\\u1680\\\\u180e\\\\u2000\\\\u2001\\\\u2002\\\\u2003\\\\u2004\\\\u2005\\\\u2006\\\\u2007\\\\u2008\\\\u2009\\\\u200a\\\\u202f\\\\u205f\\\\u3000',\n    rsUpperRange = 'A-Z\\\\xc0-\\\\xd6\\\\xd8-\\\\xde',\n    rsVarRange = '\\\\ufe0e\\\\ufe0f',\n    rsBreakRange = rsMathOpRange + rsNonCharRange + rsPunctuationRange + rsSpaceRange;\n\n/** Used to compose unicode capture groups. */\nvar rsApos = \"['\\u2019]\",\n    rsBreak = '[' + rsBreakRange + ']',\n    rsCombo = '[' + rsComboRange + ']',\n    rsDigits = '\\\\d+',\n    rsDingbat = '[' + rsDingbatRange + ']',\n    rsLower = '[' + rsLowerRange + ']',\n    rsMisc = '[^' + rsAstralRange + rsBreakRange + rsDigits + rsDingbatRange + rsLowerRange + rsUpperRange + ']',\n    rsFitz = '\\\\ud83c[\\\\udffb-\\\\udfff]',\n    rsModifier = '(?:' + rsCombo + '|' + rsFitz + ')',\n    rsNonAstral = '[^' + rsAstralRange + ']',\n    rsRegional = '(?:\\\\ud83c[\\\\udde6-\\\\uddff]){2}',\n    rsSurrPair = '[\\\\ud800-\\\\udbff][\\\\udc00-\\\\udfff]',\n    rsUpper = '[' + rsUpperRange + ']',\n    rsZWJ = '\\\\u200d';\n\n/** Used to compose unicode regexes. */\nvar rsMiscLower = '(?:' + rsLower + '|' + rsMisc + ')',\n    rsMiscUpper = '(?:' + rsUpper + '|' + rsMisc + ')',\n    rsOptContrLower = '(?:' + rsApos + '(?:d|ll|m|re|s|t|ve))?',\n    rsOptContrUpper = '(?:' + rsApos + '(?:D|LL|M|RE|S|T|VE))?',\n    reOptMod = rsModifier + '?',\n    rsOptVar = '[' + rsVarRange + ']?',\n    rsOptJoin = '(?:' + rsZWJ + '(?:' + [rsNonAstral, rsRegional, rsSurrPair].join('|') + ')' + rsOptVar + reOptMod + ')*',\n    rsOrdLower = '\\\\d*(?:1st|2nd|3rd|(?![123])\\\\dth)(?=\\\\b|[A-Z_])',\n    rsOrdUpper = '\\\\d*(?:1ST|2ND|3RD|(?![123])\\\\dTH)(?=\\\\b|[a-z_])',\n    rsSeq = rsOptVar + reOptMod + rsOptJoin,\n    rsEmoji = '(?:' + [rsDingbat, rsRegional, rsSurrPair].join('|') + ')' + rsSeq;\n\n/** Used to match complex or compound words. */\nvar reUnicodeWord = RegExp([\n  rsUpper + '?' + rsLower + '+' + rsOptContrLower + '(?=' + [rsBreak, rsUpper, '$'].join('|') + ')',\n  rsMiscUpper + '+' + rsOptContrUpper + '(?=' + [rsBreak, rsUpper + rsMiscLower, '$'].join('|') + ')',\n  rsUpper + '?' + rsMiscLower + '+' + rsOptContrLower,\n  rsUpper + '+' + rsOptContrUpper,\n  rsOrdUpper,\n  rsOrdLower,\n  rsDigits,\n  rsEmoji\n].join('|'), 'g');\n\n/**\n * Splits a Unicode `string` into an array of its words.\n *\n * @private\n * @param {string} The string to inspect.\n * @returns {Array} Returns the words of `string`.\n */\nfunction unicodeWords(string) {\n  return string.match(reUnicodeWord) || [];\n}\n\nmodule.exports = unicodeWords;\n", "var asciiWords = require('./_asciiWords'),\n    hasUnicodeWord = require('./_hasUnicodeWord'),\n    toString = require('./toString'),\n    unicodeWords = require('./_unicodeWords');\n\n/**\n * Splits `string` into an array of its words.\n *\n * @static\n * @memberOf _\n * @since 3.0.0\n * @category String\n * @param {string} [string=''] The string to inspect.\n * @param {RegExp|string} [pattern] The pattern to match words.\n * @param- {Object} [guard] Enables use as an iteratee for methods like `_.map`.\n * @returns {Array} Returns the words of `string`.\n * @example\n *\n * _.words('fred, barney, & pebbles');\n * // => ['fred', 'barney', 'pebbles']\n *\n * _.words('fred, barney, & pebbles', /[^, ]+/g);\n * // => ['fred', 'barney', '&', 'pebbles']\n */\nfunction words(string, pattern, guard) {\n  string = toString(string);\n  pattern = guard ? undefined : pattern;\n\n  if (pattern === undefined) {\n    return hasUnicodeWord(string) ? unicodeWords(string) : asciiWords(string);\n  }\n  return string.match(pattern) || [];\n}\n\nmodule.exports = words;\n", "var arrayReduce = require('./_arrayReduce'),\n    deburr = require('./deburr'),\n    words = require('./words');\n\n/** Used to compose unicode capture groups. */\nvar rsApos = \"['\\u2019]\";\n\n/** Used to match apostrophes. */\nvar reApos = RegExp(rsApos, 'g');\n\n/**\n * Creates a function like `_.camelCase`.\n *\n * @private\n * @param {Function} callback The function to combine each word.\n * @returns {Function} Returns the new compounder function.\n */\nfunction createCompounder(callback) {\n  return function(string) {\n    return arrayReduce(words(deburr(string).replace(reApos, '')), callback, '');\n  };\n}\n\nmodule.exports = createCompounder;\n", "var createCompounder = require('./_createCompounder');\n\n/**\n * Converts `string` to\n * [snake case](https://en.wikipedia.org/wiki/Snake_case).\n *\n * @static\n * @memberOf _\n * @since 3.0.0\n * @category String\n * @param {string} [string=''] The string to convert.\n * @returns {string} Returns the snake cased string.\n * @example\n *\n * _.snakeCase('Foo Bar');\n * // => 'foo_bar'\n *\n * _.snakeCase('fooBar');\n * // => 'foo_bar'\n *\n * _.snakeCase('--FOO-BAR--');\n * // => 'foo_bar'\n */\nvar snakeCase = createCompounder(function(result, word, index) {\n  return result + (index ? '_' : '') + word.toLowerCase();\n});\n\nmodule.exports = snakeCase;\n", "/**\n * The base implementation of `_.slice` without an iteratee call guard.\n *\n * @private\n * @param {Array} array The array to slice.\n * @param {number} [start=0] The start position.\n * @param {number} [end=array.length] The end position.\n * @returns {Array} Returns the slice of `array`.\n */\nfunction baseSlice(array, start, end) {\n  var index = -1,\n      length = array.length;\n\n  if (start < 0) {\n    start = -start > length ? 0 : (length + start);\n  }\n  end = end > length ? length : end;\n  if (end < 0) {\n    end += length;\n  }\n  length = start > end ? 0 : ((end - start) >>> 0);\n  start >>>= 0;\n\n  var result = Array(length);\n  while (++index < length) {\n    result[index] = array[index + start];\n  }\n  return result;\n}\n\nmodule.exports = baseSlice;\n", "var baseSlice = require('./_baseSlice');\n\n/**\n * Casts `array` to a slice if it's needed.\n *\n * @private\n * @param {Array} array The array to inspect.\n * @param {number} start The start position.\n * @param {number} [end=array.length] The end position.\n * @returns {Array} Returns the cast slice.\n */\nfunction castSlice(array, start, end) {\n  var length = array.length;\n  end = end === undefined ? length : end;\n  return (!start && end >= length) ? array : baseSlice(array, start, end);\n}\n\nmodule.exports = castSlice;\n", "/** Used to compose unicode character classes. */\nvar rsAstralRange = '\\\\ud800-\\\\udfff',\n    rsComboMarksRange = '\\\\u0300-\\\\u036f',\n    reComboHalfMarksRange = '\\\\ufe20-\\\\ufe2f',\n    rsComboSymbolsRange = '\\\\u20d0-\\\\u20ff',\n    rsComboRange = rsComboMarksRange + reComboHalfMarksRange + rsComboSymbolsRange,\n    rsVarRange = '\\\\ufe0e\\\\ufe0f';\n\n/** Used to compose unicode capture groups. */\nvar rsZWJ = '\\\\u200d';\n\n/** Used to detect strings with [zero-width joiners or code points from the astral planes](http://eev.ee/blog/2015/09/12/dark-corners-of-unicode/). */\nvar reHasUnicode = RegExp('[' + rsZWJ + rsAstralRange  + rsComboRange + rsVarRange + ']');\n\n/**\n * Checks if `string` contains Unicode symbols.\n *\n * @private\n * @param {string} string The string to inspect.\n * @returns {boolean} Returns `true` if a symbol is found, else `false`.\n */\nfunction hasUnicode(string) {\n  return reHasUnicode.test(string);\n}\n\nmodule.exports = hasUnicode;\n", "/**\n * Converts an ASCII `string` to an array.\n *\n * @private\n * @param {string} string The string to convert.\n * @returns {Array} Returns the converted array.\n */\nfunction asciiToArray(string) {\n  return string.split('');\n}\n\nmodule.exports = asciiToArray;\n", "/** Used to compose unicode character classes. */\nvar rsAstralRange = '\\\\ud800-\\\\udfff',\n    rsComboMarksRange = '\\\\u0300-\\\\u036f',\n    reComboHalfMarksRange = '\\\\ufe20-\\\\ufe2f',\n    rsComboSymbolsRange = '\\\\u20d0-\\\\u20ff',\n    rsComboRange = rsComboMarksRange + reComboHalfMarksRange + rsComboSymbolsRange,\n    rsVarRange = '\\\\ufe0e\\\\ufe0f';\n\n/** Used to compose unicode capture groups. */\nvar rsAstral = '[' + rsAstralRange + ']',\n    rsCombo = '[' + rsComboRange + ']',\n    rsFitz = '\\\\ud83c[\\\\udffb-\\\\udfff]',\n    rsModifier = '(?:' + rsCombo + '|' + rsFitz + ')',\n    rsNonAstral = '[^' + rsAstralRange + ']',\n    rsRegional = '(?:\\\\ud83c[\\\\udde6-\\\\uddff]){2}',\n    rsSurrPair = '[\\\\ud800-\\\\udbff][\\\\udc00-\\\\udfff]',\n    rsZWJ = '\\\\u200d';\n\n/** Used to compose unicode regexes. */\nvar reOptMod = rsModifier + '?',\n    rsOptVar = '[' + rsVarRange + ']?',\n    rsOptJoin = '(?:' + rsZWJ + '(?:' + [rsNonAstral, rsRegional, rsSurrPair].join('|') + ')' + rsOptVar + reOptMod + ')*',\n    rsSeq = rsOptVar + reOptMod + rsOptJoin,\n    rsSymbol = '(?:' + [rsNonAstral + rsCombo + '?', rsCombo, rsRegional, rsSurrPair, rsAstral].join('|') + ')';\n\n/** Used to match [string symbols](https://mathiasbynens.be/notes/javascript-unicode). */\nvar reUnicode = RegExp(rsFitz + '(?=' + rsFitz + ')|' + rsSymbol + rsSeq, 'g');\n\n/**\n * Converts a Unicode `string` to an array.\n *\n * @private\n * @param {string} string The string to convert.\n * @returns {Array} Returns the converted array.\n */\nfunction unicodeToArray(string) {\n  return string.match(reUnicode) || [];\n}\n\nmodule.exports = unicodeToArray;\n", "var asciiToArray = require('./_asciiToArray'),\n    hasUnicode = require('./_hasUnicode'),\n    unicodeToArray = require('./_unicodeToArray');\n\n/**\n * Converts `string` to an array.\n *\n * @private\n * @param {string} string The string to convert.\n * @returns {Array} Returns the converted array.\n */\nfunction stringToArray(string) {\n  return hasUnicode(string)\n    ? unicodeToArray(string)\n    : asciiToArray(string);\n}\n\nmodule.exports = stringToArray;\n", "var castSlice = require('./_castSlice'),\n    hasUnicode = require('./_hasUnicode'),\n    stringToArray = require('./_stringToArray'),\n    toString = require('./toString');\n\n/**\n * Creates a function like `_.lowerFirst`.\n *\n * @private\n * @param {string} methodName The name of the `String` case method to use.\n * @returns {Function} Returns the new case function.\n */\nfunction createCaseFirst(methodName) {\n  return function(string) {\n    string = toString(string);\n\n    var strSymbols = hasUnicode(string)\n      ? stringToArray(string)\n      : undefined;\n\n    var chr = strSymbols\n      ? strSymbols[0]\n      : string.charAt(0);\n\n    var trailing = strSymbols\n      ? castSlice(strSymbols, 1).join('')\n      : string.slice(1);\n\n    return chr[methodName]() + trailing;\n  };\n}\n\nmodule.exports = createCaseFirst;\n", "var createCaseFirst = require('./_createCaseFirst');\n\n/**\n * Converts the first character of `string` to upper case.\n *\n * @static\n * @memberOf _\n * @since 4.0.0\n * @category String\n * @param {string} [string=''] The string to convert.\n * @returns {string} Returns the converted string.\n * @example\n *\n * _.upperFirst('fred');\n * // => 'Fred'\n *\n * _.upperFirst('FRED');\n * // => 'FRED'\n */\nvar upperFirst = createCaseFirst('toUpperCase');\n\nmodule.exports = upperFirst;\n", "var toString = require('./toString'),\n    upperFirst = require('./upperFirst');\n\n/**\n * Converts the first character of `string` to upper case and the remaining\n * to lower case.\n *\n * @static\n * @memberOf _\n * @since 3.0.0\n * @category String\n * @param {string} [string=''] The string to capitalize.\n * @returns {string} Returns the capitalized string.\n * @example\n *\n * _.capitalize('FRED');\n * // => 'Fred'\n */\nfunction capitalize(string) {\n  return upperFirst(toString(string).toLowerCase());\n}\n\nmodule.exports = capitalize;\n", "var capitalize = require('./capitalize'),\n    createCompounder = require('./_createCompounder');\n\n/**\n * Converts `string` to [camel case](https://en.wikipedia.org/wiki/CamelCase).\n *\n * @static\n * @memberOf _\n * @since 3.0.0\n * @category String\n * @param {string} [string=''] The string to convert.\n * @returns {string} Returns the camel cased string.\n * @example\n *\n * _.camelCase('Foo Bar');\n * // => 'fooBar'\n *\n * _.camelCase('--foo-bar--');\n * // => 'fooBar'\n *\n * _.camelCase('__FOO_BAR__');\n * // => 'fooBar'\n */\nvar camelCase = createCompounder(function(result, word, index) {\n  word = word.toLowerCase();\n  return result + (index ? capitalize(word) : word);\n});\n\nmodule.exports = camelCase;\n", "var baseAssignValue = require('./_baseAssignValue'),\n    baseForOwn = require('./_baseForOwn'),\n    baseIteratee = require('./_baseIteratee');\n\n/**\n * The opposite of `_.mapValues`; this method creates an object with the\n * same values as `object` and keys generated by running each own enumerable\n * string keyed property of `object` thru `iteratee`. The iteratee is invoked\n * with three arguments: (value, key, object).\n *\n * @static\n * @memberOf _\n * @since 3.8.0\n * @category Object\n * @param {Object} object The object to iterate over.\n * @param {Function} [iteratee=_.identity] The function invoked per iteration.\n * @returns {Object} Returns the new mapped object.\n * @see _.mapValues\n * @example\n *\n * _.mapKeys({ 'a': 1, 'b': 2 }, function(value, key) {\n *   return key + value;\n * });\n * // => { 'a1': 1, 'b2': 2 }\n */\nfunction mapKeys(object, iteratee) {\n  var result = {};\n  iteratee = baseIteratee(iteratee, 3);\n\n  baseForOwn(object, function(value, key, object) {\n    baseAssignValue(result, iteratee(value, key, object), value);\n  });\n  return result;\n}\n\nmodule.exports = mapKeys;\n", "\n/**\n * Topological sorting function\n *\n * @param {Array} edges\n * @returns {Array}\n */\n\nmodule.exports = function(edges) {\n  return toposort(uniqueNodes(edges), edges)\n}\n\nmodule.exports.array = toposort\n\nfunction toposort(nodes, edges) {\n  var cursor = nodes.length\n    , sorted = new Array(cursor)\n    , visited = {}\n    , i = cursor\n    // Better data structures make algorithm much faster.\n    , outgoingEdges = makeOutgoingEdges(edges)\n    , nodesHash = makeNodesHash(nodes)\n\n  // check for unknown nodes\n  edges.forEach(function(edge) {\n    if (!nodesHash.has(edge[0]) || !nodesHash.has(edge[1])) {\n      throw new Error('Unknown node. There is an unknown node in the supplied edges.')\n    }\n  })\n\n  while (i--) {\n    if (!visited[i]) visit(nodes[i], i, new Set())\n  }\n\n  return sorted\n\n  function visit(node, i, predecessors) {\n    if(predecessors.has(node)) {\n      var nodeRep\n      try {\n        nodeRep = \", node was:\" + JSON.stringify(node)\n      } catch(e) {\n        nodeRep = \"\"\n      }\n      throw new Error('Cyclic dependency' + nodeRep)\n    }\n\n    if (!nodesHash.has(node)) {\n      throw new Error('Found unknown node. Make sure to provided all involved nodes. Unknown node: '+JSON.stringify(node))\n    }\n\n    if (visited[i]) return;\n    visited[i] = true\n\n    var outgoing = outgoingEdges.get(node) || new Set()\n    outgoing = Array.from(outgoing)\n\n    if (i = outgoing.length) {\n      predecessors.add(node)\n      do {\n        var child = outgoing[--i]\n        visit(child, nodesHash.get(child), predecessors)\n      } while (i)\n      predecessors.delete(node)\n    }\n\n    sorted[--cursor] = node\n  }\n}\n\nfunction uniqueNodes(arr){\n  var res = new Set()\n  for (var i = 0, len = arr.length; i < len; i++) {\n    var edge = arr[i]\n    res.add(edge[0])\n    res.add(edge[1])\n  }\n  return Array.from(res)\n}\n\nfunction makeOutgoingEdges(arr){\n  var edges = new Map()\n  for (var i = 0, len = arr.length; i < len; i++) {\n    var edge = arr[i]\n    if (!edges.has(edge[0])) edges.set(edge[0], new Set())\n    if (!edges.has(edge[1])) edges.set(edge[1], new Set())\n    edges.get(edge[0]).add(edge[1])\n  }\n  return edges\n}\n\nfunction makeNodesHash(arr){\n  var res = new Map()\n  for (var i = 0, len = arr.length; i < len; i++) {\n    res.set(arr[i], i)\n  }\n  return res\n}\n", "\"use strict\";\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\nexports.default = sortFields;\n\nvar _has = _interopRequireDefault(require(\"lodash/has\"));\n\nvar _toposort = _interopRequireDefault(require(\"toposort\"));\n\nvar _propertyExpr = require(\"property-expr\");\n\nvar _Reference = _interopRequireDefault(require(\"../Reference\"));\n\nvar _isSchema = _interopRequireDefault(require(\"./isSchema\"));\n\nfunction _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { default: obj }; }\n\n// @ts-expect-error\nfunction sortFields(fields, excludedEdges = []) {\n  let edges = [];\n  let nodes = new Set();\n  let excludes = new Set(excludedEdges.map(([a, b]) => `${a}-${b}`));\n\n  function addNode(depPath, key) {\n    let node = (0, _propertyExpr.split)(depPath)[0];\n    nodes.add(node);\n    if (!excludes.has(`${key}-${node}`)) edges.push([key, node]);\n  }\n\n  for (const key in fields) if ((0, _has.default)(fields, key)) {\n    let value = fields[key];\n    nodes.add(key);\n    if (_Reference.default.isRef(value) && value.isSibling) addNode(value.path, key);else if ((0, _isSchema.default)(value) && 'deps' in value) value.deps.forEach(path => addNode(path, key));\n  }\n\n  return _toposort.default.array(Array.from(nodes), edges).reverse();\n}", "\"use strict\";\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\nexports.default = sortByKeyOrder;\n\nfunction findIndex(arr, err) {\n  let idx = Infinity;\n  arr.some((key, ii) => {\n    var _err$path;\n\n    if (((_err$path = err.path) == null ? void 0 : _err$path.indexOf(key)) !== -1) {\n      idx = ii;\n      return true;\n    }\n  });\n  return idx;\n}\n\nfunction sortByKeyOrder(keys) {\n  return (a, b) => {\n    return findIndex(keys, a) - findIndex(keys, b);\n  };\n}", "\"use strict\";\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\nexports.create = create;\nexports.default = void 0;\n\nvar _has = _interopRequireDefault(require(\"lodash/has\"));\n\nvar _snakeCase = _interopRequireDefault(require(\"lodash/snakeCase\"));\n\nvar _camelCase = _interopRequireDefault(require(\"lodash/camelCase\"));\n\nvar _mapKeys = _interopRequireDefault(require(\"lodash/mapKeys\"));\n\nvar _mapValues = _interopRequireDefault(require(\"lodash/mapValues\"));\n\nvar _propertyExpr = require(\"property-expr\");\n\nvar _locale = require(\"./locale\");\n\nvar _sortFields = _interopRequireDefault(require(\"./util/sortFields\"));\n\nvar _sortByKeyOrder = _interopRequireDefault(require(\"./util/sortByKeyOrder\"));\n\nvar _runTests = _interopRequireDefault(require(\"./util/runTests\"));\n\nvar _ValidationError = _interopRequireDefault(require(\"./ValidationError\"));\n\nvar _schema = _interopRequireDefault(require(\"./schema\"));\n\nfunction _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { default: obj }; }\n\nfunction _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nlet isObject = obj => Object.prototype.toString.call(obj) === '[object Object]';\n\nfunction unknown(ctx, value) {\n  let known = Object.keys(ctx.fields);\n  return Object.keys(value).filter(key => known.indexOf(key) === -1);\n}\n\nconst defaultSort = (0, _sortByKeyOrder.default)([]);\n\nclass ObjectSchema extends _schema.default {\n  constructor(spec) {\n    super({\n      type: 'object'\n    });\n    this.fields = Object.create(null);\n    this._sortErrors = defaultSort;\n    this._nodes = [];\n    this._excludedEdges = [];\n    this.withMutation(() => {\n      this.transform(function coerce(value) {\n        if (typeof value === 'string') {\n          try {\n            value = JSON.parse(value);\n          } catch (err) {\n            value = null;\n          }\n        }\n\n        if (this.isType(value)) return value;\n        return null;\n      });\n\n      if (spec) {\n        this.shape(spec);\n      }\n    });\n  }\n\n  _typeCheck(value) {\n    return isObject(value) || typeof value === 'function';\n  }\n\n  _cast(_value, options = {}) {\n    var _options$stripUnknown;\n\n    let value = super._cast(_value, options); //should ignore nulls here\n\n\n    if (value === undefined) return this.getDefault();\n    if (!this._typeCheck(value)) return value;\n    let fields = this.fields;\n    let strip = (_options$stripUnknown = options.stripUnknown) != null ? _options$stripUnknown : this.spec.noUnknown;\n\n    let props = this._nodes.concat(Object.keys(value).filter(v => this._nodes.indexOf(v) === -1));\n\n    let intermediateValue = {}; // is filled during the transform below\n\n    let innerOptions = _extends({}, options, {\n      parent: intermediateValue,\n      __validating: options.__validating || false\n    });\n\n    let isChanged = false;\n\n    for (const prop of props) {\n      let field = fields[prop];\n      let exists = (0, _has.default)(value, prop);\n\n      if (field) {\n        let fieldValue;\n        let inputValue = value[prop]; // safe to mutate since this is fired in sequence\n\n        innerOptions.path = (options.path ? `${options.path}.` : '') + prop; // innerOptions.value = value[prop];\n\n        field = field.resolve({\n          value: inputValue,\n          context: options.context,\n          parent: intermediateValue\n        });\n        let fieldSpec = 'spec' in field ? field.spec : undefined;\n        let strict = fieldSpec == null ? void 0 : fieldSpec.strict;\n\n        if (fieldSpec == null ? void 0 : fieldSpec.strip) {\n          isChanged = isChanged || prop in value;\n          continue;\n        }\n\n        fieldValue = !options.__validating || !strict ? // TODO: use _cast, this is double resolving\n        field.cast(value[prop], innerOptions) : value[prop];\n\n        if (fieldValue !== undefined) {\n          intermediateValue[prop] = fieldValue;\n        }\n      } else if (exists && !strip) {\n        intermediateValue[prop] = value[prop];\n      }\n\n      if (intermediateValue[prop] !== value[prop]) {\n        isChanged = true;\n      }\n    }\n\n    return isChanged ? intermediateValue : value;\n  }\n\n  _validate(_value, opts = {}, callback) {\n    let errors = [];\n    let {\n      sync,\n      from = [],\n      originalValue = _value,\n      abortEarly = this.spec.abortEarly,\n      recursive = this.spec.recursive\n    } = opts;\n    from = [{\n      schema: this,\n      value: originalValue\n    }, ...from]; // this flag is needed for handling `strict` correctly in the context of\n    // validation vs just casting. e.g strict() on a field is only used when validating\n\n    opts.__validating = true;\n    opts.originalValue = originalValue;\n    opts.from = from;\n\n    super._validate(_value, opts, (err, value) => {\n      if (err) {\n        if (!_ValidationError.default.isError(err) || abortEarly) {\n          return void callback(err, value);\n        }\n\n        errors.push(err);\n      }\n\n      if (!recursive || !isObject(value)) {\n        callback(errors[0] || null, value);\n        return;\n      }\n\n      originalValue = originalValue || value;\n\n      let tests = this._nodes.map(key => (_, cb) => {\n        let path = key.indexOf('.') === -1 ? (opts.path ? `${opts.path}.` : '') + key : `${opts.path || ''}[\"${key}\"]`;\n        let field = this.fields[key];\n\n        if (field && 'validate' in field) {\n          field.validate(value[key], _extends({}, opts, {\n            // @ts-ignore\n            path,\n            from,\n            // inner fields are always strict:\n            // 1. this isn't strict so the casting will also have cast inner values\n            // 2. this is strict in which case the nested values weren't cast either\n            strict: true,\n            parent: value,\n            originalValue: originalValue[key]\n          }), cb);\n          return;\n        }\n\n        cb(null);\n      });\n\n      (0, _runTests.default)({\n        sync,\n        tests,\n        value,\n        errors,\n        endEarly: abortEarly,\n        sort: this._sortErrors,\n        path: opts.path\n      }, callback);\n    });\n  }\n\n  clone(spec) {\n    const next = super.clone(spec);\n    next.fields = _extends({}, this.fields);\n    next._nodes = this._nodes;\n    next._excludedEdges = this._excludedEdges;\n    next._sortErrors = this._sortErrors;\n    return next;\n  }\n\n  concat(schema) {\n    let next = super.concat(schema);\n    let nextFields = next.fields;\n\n    for (let [field, schemaOrRef] of Object.entries(this.fields)) {\n      const target = nextFields[field];\n\n      if (target === undefined) {\n        nextFields[field] = schemaOrRef;\n      } else if (target instanceof _schema.default && schemaOrRef instanceof _schema.default) {\n        nextFields[field] = schemaOrRef.concat(target);\n      }\n    }\n\n    return next.withMutation(() => next.shape(nextFields, this._excludedEdges));\n  }\n\n  getDefaultFromShape() {\n    let dft = {};\n\n    this._nodes.forEach(key => {\n      const field = this.fields[key];\n      dft[key] = 'default' in field ? field.getDefault() : undefined;\n    });\n\n    return dft;\n  }\n\n  _getDefault() {\n    if ('default' in this.spec) {\n      return super._getDefault();\n    } // if there is no default set invent one\n\n\n    if (!this._nodes.length) {\n      return undefined;\n    }\n\n    return this.getDefaultFromShape();\n  }\n\n  shape(additions, excludes = []) {\n    let next = this.clone();\n    let fields = Object.assign(next.fields, additions);\n    next.fields = fields;\n    next._sortErrors = (0, _sortByKeyOrder.default)(Object.keys(fields));\n\n    if (excludes.length) {\n      // this is a convenience for when users only supply a single pair\n      if (!Array.isArray(excludes[0])) excludes = [excludes];\n      next._excludedEdges = [...next._excludedEdges, ...excludes];\n    }\n\n    next._nodes = (0, _sortFields.default)(fields, next._excludedEdges);\n    return next;\n  }\n\n  pick(keys) {\n    const picked = {};\n\n    for (const key of keys) {\n      if (this.fields[key]) picked[key] = this.fields[key];\n    }\n\n    return this.clone().withMutation(next => {\n      next.fields = {};\n      return next.shape(picked);\n    });\n  }\n\n  omit(keys) {\n    const next = this.clone();\n    const fields = next.fields;\n    next.fields = {};\n\n    for (const key of keys) {\n      delete fields[key];\n    }\n\n    return next.withMutation(() => next.shape(fields));\n  }\n\n  from(from, to, alias) {\n    let fromGetter = (0, _propertyExpr.getter)(from, true);\n    return this.transform(obj => {\n      if (obj == null) return obj;\n      let newObj = obj;\n\n      if ((0, _has.default)(obj, from)) {\n        newObj = _extends({}, obj);\n        if (!alias) delete newObj[from];\n        newObj[to] = fromGetter(obj);\n      }\n\n      return newObj;\n    });\n  }\n\n  noUnknown(noAllow = true, message = _locale.object.noUnknown) {\n    if (typeof noAllow === 'string') {\n      message = noAllow;\n      noAllow = true;\n    }\n\n    let next = this.test({\n      name: 'noUnknown',\n      exclusive: true,\n      message: message,\n\n      test(value) {\n        if (value == null) return true;\n        const unknownKeys = unknown(this.schema, value);\n        return !noAllow || unknownKeys.length === 0 || this.createError({\n          params: {\n            unknown: unknownKeys.join(', ')\n          }\n        });\n      }\n\n    });\n    next.spec.noUnknown = noAllow;\n    return next;\n  }\n\n  unknown(allow = true, message = _locale.object.noUnknown) {\n    return this.noUnknown(!allow, message);\n  }\n\n  transformKeys(fn) {\n    return this.transform(obj => obj && (0, _mapKeys.default)(obj, (_, key) => fn(key)));\n  }\n\n  camelCase() {\n    return this.transformKeys(_camelCase.default);\n  }\n\n  snakeCase() {\n    return this.transformKeys(_snakeCase.default);\n  }\n\n  constantCase() {\n    return this.transformKeys(key => (0, _snakeCase.default)(key).toUpperCase());\n  }\n\n  describe() {\n    let base = super.describe();\n    base.fields = (0, _mapValues.default)(this.fields, value => value.describe());\n    return base;\n  }\n\n}\n\nexports.default = ObjectSchema;\n\nfunction create(spec) {\n  return new ObjectSchema(spec);\n}\n\ncreate.prototype = ObjectSchema.prototype;", "\"use strict\";\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\nexports.create = create;\nexports.default = void 0;\n\nvar _isAbsent = _interopRequireDefault(require(\"./util/isAbsent\"));\n\nvar _isSchema = _interopRequireDefault(require(\"./util/isSchema\"));\n\nvar _printValue = _interopRequireDefault(require(\"./util/printValue\"));\n\nvar _locale = require(\"./locale\");\n\nvar _runTests = _interopRequireDefault(require(\"./util/runTests\"));\n\nvar _ValidationError = _interopRequireDefault(require(\"./ValidationError\"));\n\nvar _schema = _interopRequireDefault(require(\"./schema\"));\n\nfunction _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { default: obj }; }\n\nfunction _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction create(type) {\n  return new ArraySchema(type);\n}\n\nclass ArraySchema extends _schema.default {\n  constructor(type) {\n    super({\n      type: 'array'\n    }); // `undefined` specifically means uninitialized, as opposed to\n    // \"no subtype\"\n\n    this.innerType = void 0;\n    this.innerType = type;\n    this.withMutation(() => {\n      this.transform(function (values) {\n        if (typeof values === 'string') try {\n          values = JSON.parse(values);\n        } catch (err) {\n          values = null;\n        }\n        return this.isType(values) ? values : null;\n      });\n    });\n  }\n\n  _typeCheck(v) {\n    return Array.isArray(v);\n  }\n\n  get _subType() {\n    return this.innerType;\n  }\n\n  _cast(_value, _opts) {\n    const value = super._cast(_value, _opts); //should ignore nulls here\n\n\n    if (!this._typeCheck(value) || !this.innerType) return value;\n    let isChanged = false;\n    const castArray = value.map((v, idx) => {\n      const castElement = this.innerType.cast(v, _extends({}, _opts, {\n        path: `${_opts.path || ''}[${idx}]`\n      }));\n\n      if (castElement !== v) {\n        isChanged = true;\n      }\n\n      return castElement;\n    });\n    return isChanged ? castArray : value;\n  }\n\n  _validate(_value, options = {}, callback) {\n    var _options$abortEarly, _options$recursive;\n\n    let errors = [];\n    let sync = options.sync;\n    let path = options.path;\n    let innerType = this.innerType;\n    let endEarly = (_options$abortEarly = options.abortEarly) != null ? _options$abortEarly : this.spec.abortEarly;\n    let recursive = (_options$recursive = options.recursive) != null ? _options$recursive : this.spec.recursive;\n    let originalValue = options.originalValue != null ? options.originalValue : _value;\n\n    super._validate(_value, options, (err, value) => {\n      if (err) {\n        if (!_ValidationError.default.isError(err) || endEarly) {\n          return void callback(err, value);\n        }\n\n        errors.push(err);\n      }\n\n      if (!recursive || !innerType || !this._typeCheck(value)) {\n        callback(errors[0] || null, value);\n        return;\n      }\n\n      originalValue = originalValue || value; // #950 Ensure that sparse array empty slots are validated\n\n      let tests = new Array(value.length);\n\n      for (let idx = 0; idx < value.length; idx++) {\n        let item = value[idx];\n        let path = `${options.path || ''}[${idx}]`; // object._validate note for isStrict explanation\n\n        let innerOptions = _extends({}, options, {\n          path,\n          strict: true,\n          parent: value,\n          index: idx,\n          originalValue: originalValue[idx]\n        });\n\n        tests[idx] = (_, cb) => innerType.validate(item, innerOptions, cb);\n      }\n\n      (0, _runTests.default)({\n        sync,\n        path,\n        value,\n        errors,\n        endEarly,\n        tests\n      }, callback);\n    });\n  }\n\n  clone(spec) {\n    const next = super.clone(spec);\n    next.innerType = this.innerType;\n    return next;\n  }\n\n  concat(schema) {\n    let next = super.concat(schema);\n    next.innerType = this.innerType;\n    if (schema.innerType) next.innerType = next.innerType ? // @ts-expect-error Lazy doesn't have concat()\n    next.innerType.concat(schema.innerType) : schema.innerType;\n    return next;\n  }\n\n  of(schema) {\n    // FIXME: this should return a new instance of array without the default to be\n    let next = this.clone();\n    if (!(0, _isSchema.default)(schema)) throw new TypeError('`array.of()` sub-schema must be a valid yup schema not: ' + (0, _printValue.default)(schema)); // FIXME(ts):\n\n    next.innerType = schema;\n    return next;\n  }\n\n  length(length, message = _locale.array.length) {\n    return this.test({\n      message,\n      name: 'length',\n      exclusive: true,\n      params: {\n        length\n      },\n\n      test(value) {\n        return (0, _isAbsent.default)(value) || value.length === this.resolve(length);\n      }\n\n    });\n  }\n\n  min(min, message) {\n    message = message || _locale.array.min;\n    return this.test({\n      message,\n      name: 'min',\n      exclusive: true,\n      params: {\n        min\n      },\n\n      // FIXME(ts): Array<typeof T>\n      test(value) {\n        return (0, _isAbsent.default)(value) || value.length >= this.resolve(min);\n      }\n\n    });\n  }\n\n  max(max, message) {\n    message = message || _locale.array.max;\n    return this.test({\n      message,\n      name: 'max',\n      exclusive: true,\n      params: {\n        max\n      },\n\n      test(value) {\n        return (0, _isAbsent.default)(value) || value.length <= this.resolve(max);\n      }\n\n    });\n  }\n\n  ensure() {\n    return this.default(() => []).transform((val, original) => {\n      // We don't want to return `null` for nullable schema\n      if (this._typeCheck(val)) return val;\n      return original == null ? [] : [].concat(original);\n    });\n  }\n\n  compact(rejector) {\n    let reject = !rejector ? v => !!v : (v, i, a) => !rejector(v, i, a);\n    return this.transform(values => values != null ? values.filter(reject) : values);\n  }\n\n  describe() {\n    let base = super.describe();\n    if (this.innerType) base.innerType = this.innerType.describe();\n    return base;\n  }\n\n  nullable(isNullable = true) {\n    return super.nullable(isNullable);\n  }\n\n  defined() {\n    return super.defined();\n  }\n\n  required(msg) {\n    return super.required(msg);\n  }\n\n}\n\nexports.default = ArraySchema;\ncreate.prototype = ArraySchema.prototype; //\n// Interfaces\n//", "\"use strict\";\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\nexports.create = create;\nexports.default = void 0;\n\nvar _isSchema = _interopRequireDefault(require(\"./util/isSchema\"));\n\nfunction _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { default: obj }; }\n\nfunction create(builder) {\n  return new Lazy(builder);\n}\n\nclass Lazy {\n  constructor(builder) {\n    this.type = 'lazy';\n    this.__isYupSchema__ = true;\n    this.__inputType = void 0;\n    this.__outputType = void 0;\n\n    this._resolve = (value, options = {}) => {\n      let schema = this.builder(value, options);\n      if (!(0, _isSchema.default)(schema)) throw new TypeError('lazy() functions must return a valid schema');\n      return schema.resolve(options);\n    };\n\n    this.builder = builder;\n  }\n\n  resolve(options) {\n    return this._resolve(options.value, options);\n  }\n\n  cast(value, options) {\n    return this._resolve(value, options).cast(value, options);\n  }\n\n  validate(value, options, maybeCb) {\n    // @ts-expect-error missing public callback on type\n    return this._resolve(value, options).validate(value, options, maybeCb);\n  }\n\n  validateSync(value, options) {\n    return this._resolve(value, options).validateSync(value, options);\n  }\n\n  validateAt(path, value, options) {\n    return this._resolve(value, options).validateAt(path, value, options);\n  }\n\n  validateSyncAt(path, value, options) {\n    return this._resolve(value, options).validateSyncAt(path, value, options);\n  }\n\n  describe() {\n    return null;\n  }\n\n  isValid(value, options) {\n    return this._resolve(value, options).isValid(value, options);\n  }\n\n  isValidSync(value, options) {\n    return this._resolve(value, options).isValidSync(value, options);\n  }\n\n}\n\nvar _default = Lazy;\nexports.default = _default;", "\"use strict\";\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\nexports.default = setLocale;\n\nvar _locale = _interopRequireDefault(require(\"./locale\"));\n\nfunction _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { default: obj }; }\n\nfunction setLocale(custom) {\n  Object.keys(custom).forEach(type => {\n    // @ts-ignore\n    Object.keys(custom[type]).forEach(method => {\n      // @ts-ignore\n      _locale.default[type][method] = custom[type][method];\n    });\n  });\n}", "\"use strict\";\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\nObject.defineProperty(exports, \"ArraySchema\", {\n  enumerable: true,\n  get: function () {\n    return _array.default;\n  }\n});\nObject.defineProperty(exports, \"BaseSchema\", {\n  enumerable: true,\n  get: function () {\n    return _schema.default;\n  }\n});\nObject.defineProperty(exports, \"BooleanSchema\", {\n  enumerable: true,\n  get: function () {\n    return _boolean.default;\n  }\n});\nObject.defineProperty(exports, \"DateSchema\", {\n  enumerable: true,\n  get: function () {\n    return _date.default;\n  }\n});\nObject.defineProperty(exports, \"MixedSchema\", {\n  enumerable: true,\n  get: function () {\n    return _mixed.default;\n  }\n});\nObject.defineProperty(exports, \"NumberSchema\", {\n  enumerable: true,\n  get: function () {\n    return _number.default;\n  }\n});\nObject.defineProperty(exports, \"ObjectSchema\", {\n  enumerable: true,\n  get: function () {\n    return _object.default;\n  }\n});\nObject.defineProperty(exports, \"StringSchema\", {\n  enumerable: true,\n  get: function () {\n    return _string.default;\n  }\n});\nObject.defineProperty(exports, \"ValidationError\", {\n  enumerable: true,\n  get: function () {\n    return _ValidationError.default;\n  }\n});\nexports.addMethod = addMethod;\nObject.defineProperty(exports, \"array\", {\n  enumerable: true,\n  get: function () {\n    return _array.create;\n  }\n});\nObject.defineProperty(exports, \"bool\", {\n  enumerable: true,\n  get: function () {\n    return _boolean.create;\n  }\n});\nObject.defineProperty(exports, \"boolean\", {\n  enumerable: true,\n  get: function () {\n    return _boolean.create;\n  }\n});\nObject.defineProperty(exports, \"date\", {\n  enumerable: true,\n  get: function () {\n    return _date.create;\n  }\n});\nObject.defineProperty(exports, \"isSchema\", {\n  enumerable: true,\n  get: function () {\n    return _isSchema.default;\n  }\n});\nObject.defineProperty(exports, \"lazy\", {\n  enumerable: true,\n  get: function () {\n    return _Lazy.create;\n  }\n});\nObject.defineProperty(exports, \"mixed\", {\n  enumerable: true,\n  get: function () {\n    return _mixed.create;\n  }\n});\nObject.defineProperty(exports, \"number\", {\n  enumerable: true,\n  get: function () {\n    return _number.create;\n  }\n});\nObject.defineProperty(exports, \"object\", {\n  enumerable: true,\n  get: function () {\n    return _object.create;\n  }\n});\nObject.defineProperty(exports, \"reach\", {\n  enumerable: true,\n  get: function () {\n    return _reach.default;\n  }\n});\nObject.defineProperty(exports, \"ref\", {\n  enumerable: true,\n  get: function () {\n    return _Reference.create;\n  }\n});\nObject.defineProperty(exports, \"setLocale\", {\n  enumerable: true,\n  get: function () {\n    return _setLocale.default;\n  }\n});\nObject.defineProperty(exports, \"string\", {\n  enumerable: true,\n  get: function () {\n    return _string.create;\n  }\n});\n\nvar _mixed = _interopRequireWildcard(require(\"./mixed\"));\n\nvar _boolean = _interopRequireWildcard(require(\"./boolean\"));\n\nvar _string = _interopRequireWildcard(require(\"./string\"));\n\nvar _number = _interopRequireWildcard(require(\"./number\"));\n\nvar _date = _interopRequireWildcard(require(\"./date\"));\n\nvar _object = _interopRequireWildcard(require(\"./object\"));\n\nvar _array = _interopRequireWildcard(require(\"./array\"));\n\nvar _Reference = require(\"./Reference\");\n\nvar _Lazy = require(\"./Lazy\");\n\nvar _ValidationError = _interopRequireDefault(require(\"./ValidationError\"));\n\nvar _reach = _interopRequireDefault(require(\"./util/reach\"));\n\nvar _isSchema = _interopRequireDefault(require(\"./util/isSchema\"));\n\nvar _setLocale = _interopRequireDefault(require(\"./setLocale\"));\n\nvar _schema = _interopRequireDefault(require(\"./schema\"));\n\nfunction _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { default: obj }; }\n\nfunction _getRequireWildcardCache(nodeInterop) { if (typeof WeakMap !== \"function\") return null; var cacheBabelInterop = new WeakMap(); var cacheNodeInterop = new WeakMap(); return (_getRequireWildcardCache = function (nodeInterop) { return nodeInterop ? cacheNodeInterop : cacheBabelInterop; })(nodeInterop); }\n\nfunction _interopRequireWildcard(obj, nodeInterop) { if (!nodeInterop && obj && obj.__esModule) { return obj; } if (obj === null || typeof obj !== \"object\" && typeof obj !== \"function\") { return { default: obj }; } var cache = _getRequireWildcardCache(nodeInterop); if (cache && cache.has(obj)) { return cache.get(obj); } var newObj = {}; var hasPropertyDescriptor = Object.defineProperty && Object.getOwnPropertyDescriptor; for (var key in obj) { if (key !== \"default\" && Object.prototype.hasOwnProperty.call(obj, key)) { var desc = hasPropertyDescriptor ? Object.getOwnPropertyDescriptor(obj, key) : null; if (desc && (desc.get || desc.set)) { Object.defineProperty(newObj, key, desc); } else { newObj[key] = obj[key]; } } } newObj.default = obj; if (cache) { cache.set(obj, newObj); } return newObj; }\n\nfunction addMethod(schemaType, name, fn) {\n  if (!schemaType || !(0, _isSchema.default)(schemaType.prototype)) throw new TypeError('You must provide a yup schema constructor function');\n  if (typeof name !== 'string') throw new TypeError('A Method name must be provided');\n  if (typeof fn !== 'function') throw new TypeError('Method function must be provided');\n  schemaType.prototype[name] = fn;\n}", "import handler from \"../handler-lib\";\n// utils\nimport { error } from \"../../utils/constants/constants\";\nimport {\n  calculateCompletionStatus,\n  isComplete,\n} from \"../../utils/validation/completionStatus\";\nimport { isAuthorizedToFetchState } from \"../../utils/auth/authorization\";\nimport {\n  parseSpecificReportParameters,\n  parseStateReportParameters,\n} from \"../../utils/auth/parameters\";\n// types\nimport {\n  getReportFieldData,\n  getReportFormTemplate,\n  getReportMetadata,\n  queryReportMetadatasForState,\n} from \"../../storage/reports\";\nimport {\n  badRequest,\n  forbidden,\n  notFound,\n  ok,\n} from \"../../utils/responses/response-lib\";\n\nexport const fetchReport = handler(async (event, _context) => {\n  const { allParamsValid, reportType, state, id } =\n    parseSpecificReportParameters(event);\n  if (!allParamsValid) {\n    return badRequest(error.NO_KEY);\n  }\n\n  if (!isAuthorizedToFetchState(event, state)) {\n    return forbidden(error.UNAUTHORIZED);\n  }\n\n  const reportMetadata = await getReportMetadata(reportType, state, id);\n  if (!reportMetadata) {\n    return notFound(error.NO_MATCHING_RECORD);\n  }\n\n  const fieldData = await getReportFieldData(reportMetadata);\n  if (!fieldData) {\n    return notFound(error.NO_MATCHING_RECORD);\n  }\n\n  const formTemplate = await getReportFormTemplate(reportMetadata);\n  if (!formTemplate) {\n    return notFound(error.NO_MATCHING_RECORD);\n  }\n\n  if (!reportMetadata.completionStatus) {\n    reportMetadata.completionStatus = await calculateCompletionStatus(\n      fieldData,\n      formTemplate\n    );\n    reportMetadata.isComplete = isComplete(reportMetadata.completionStatus);\n  }\n\n  return ok({\n    ...reportMetadata,\n    formTemplate,\n    fieldData,\n  });\n});\n\nexport const fetchReportsByState = handler(async (event, _context) => {\n  const { allParamsValid, reportType, state } =\n    parseStateReportParameters(event);\n  if (!allParamsValid) {\n    return badRequest(error.NO_KEY);\n  }\n\n  if (!isAuthorizedToFetchState(event, state!)) {\n    return forbidden(error.UNAUTHORIZED);\n  }\n\n  const reportsByState = await queryReportMetadatasForState(reportType, state);\n\n  return ok(reportsByState);\n});\n", "import util from \"util\";\nimport { Logger } from \"@smithy/types\";\n\ntype LogLevel = \"trace\" | \"debug\" | \"info\" | \"warn\" | \"error\";\ntype LogEntry = {\n  date: Date;\n  level: LogLevel;\n  string: string;\n};\n\nconst logs: LogEntry[] = [];\n\nconst buildLoggerForLevel = (level: LogLevel) => {\n  return function (...content: any[]) {\n    logs.push({\n      date: new Date(),\n      level: level,\n      string: util.format.apply(null, content),\n    });\n\n    /*\n     * If we have a function logging thousands of messages,\n     * better to take the console performance hit mid-operation\n     * than to let memory usage run away as well.\n     */\n    if (logs.length > 99) {\n      flush();\n    }\n  };\n};\n\n/*\n * Individual functions are exported to support handler-lib;\n * This integrates SDK client logging with lambda logging.\n */\nexport const trace = buildLoggerForLevel(\"trace\");\nexport const debug = buildLoggerForLevel(\"debug\");\nexport const info = buildLoggerForLevel(\"info\");\nexport const warn = buildLoggerForLevel(\"warn\");\nexport const error = buildLoggerForLevel(\"error\");\n\nexport function flush() {\n  while (logs.length > 0) {\n    const { date, level, string } = logs.shift()!;\n    // eslint-disable-next-line no-console\n    console[level](date, string);\n  }\n}\n\n/*\n * This is only called at the beginning of a lambda handler,\n * so the log buffer should be empty anyway. But it doesn't\n * hurt to make sure!\n */\nexport const init = flush;\n\n/**\n * A logger suitable for passing to any AWS client constructor.\n * Note that the `trace` log level is excluded.\n *\n * This logger accumulates log messages in an internal buffer,\n * eventually flushing them to the console.\n */\nexport const logger: Logger = { debug, info, warn, error };\n", "import { SSMClient, GetParameterCommand } from \"@aws-sdk/client-ssm\";\nimport jwt_decode from \"jwt-decode\";\nimport { CognitoJwtVerifier } from \"aws-jwt-verify\";\n// types\nimport { APIGatewayProxyEvent, UserRoles } from \"../types\";\nimport { logger } from \"../debugging/debug-lib\";\n\ninterface DecodedToken {\n  \"custom:cms_roles\": UserRoles;\n  \"custom:cms_state\": string | undefined;\n}\n\nconst loadCognitoValues = async () => {\n  if (\n    process.env.COGNITO_USER_POOL_ID &&\n    process.env.COGNITO_USER_POOL_CLIENT_ID\n  ) {\n    return {\n      userPoolId: process.env.COGNITO_USER_POOL_ID,\n      userPoolClientId: process.env.COGNITO_USER_POOL_CLIENT_ID,\n    };\n  } else {\n    const ssmClient = new SSMClient({ logger });\n    const stage = process.env.stage!;\n    const getParam = async (identifier: string) => {\n      const command = new GetParameterCommand({\n        Name: `/${stage}/ui-auth/${identifier}`,\n      });\n      const result = await ssmClient.send(command);\n      return result.Parameter?.Value;\n    };\n    const userPoolId = await getParam(\"cognito_user_pool_id\");\n    const userPoolClientId = await getParam(\"cognito_user_pool_client_id\");\n    if (userPoolId && userPoolClientId) {\n      process.env[\"COGNITO_USER_POOL_ID\"] = userPoolId;\n      process.env[\"COGNITO_USER_POOL_CLIENT_ID\"] = userPoolClientId;\n      return { userPoolId, userPoolClientId };\n    } else {\n      throw new Error(\"cannot load cognito values\");\n    }\n  }\n};\n\nexport const isAuthenticated = async (event: APIGatewayProxyEvent) => {\n  const isLocalStack = event.requestContext.accountId === \"000000000000\";\n  if (isLocalStack) {\n    return true;\n  }\n\n  const cognitoValues = await loadCognitoValues();\n\n  // Verifier that expects valid access tokens:\n  const verifier = CognitoJwtVerifier.create({\n    userPoolId: cognitoValues.userPoolId,\n    tokenUse: \"id\",\n    clientId: cognitoValues.userPoolClientId,\n  });\n\n  try {\n    await verifier.verify(event?.headers?.[\"x-api-key\"]!);\n    return true;\n  } catch {\n    return false;\n  }\n};\n\nexport const hasPermissions = (\n  event: APIGatewayProxyEvent,\n  allowedRoles: UserRoles[],\n  state?: string\n) => {\n  let isAllowed = false;\n  // decode the idToken\n  if (event?.headers?.[\"x-api-key\"]) {\n    const decoded = jwt_decode(event.headers[\"x-api-key\"]) as DecodedToken;\n    const idmUserRoles = decoded[\"custom:cms_roles\"];\n    const idmUserState = decoded[\"custom:cms_state\"];\n    const mfpUserRole = idmUserRoles\n      ?.split(\",\")\n      .find((role) => role.includes(\"mdctmfp\")) as UserRoles;\n\n    isAllowed =\n      allowedRoles.includes(mfpUserRole) &&\n      (!state || idmUserState?.includes(state))!;\n  }\n\n  return isAllowed;\n};\n\nexport const isAuthorizedToFetchState = (\n  event: APIGatewayProxyEvent,\n  state: string\n) => {\n  // If this is a state user for the matching state, authorize them.\n  if (hasPermissions(event, [UserRoles.STATE_USER], state)) {\n    return true;\n  }\n\n  const nonStateUserRoles = Object.values(UserRoles).filter(\n    (role) => role !== UserRoles.STATE_USER\n  );\n\n  // If they are any other user type, they don't need to belong to this state.\n  return hasPermissions(event, nonStateUserRoles);\n};\n", "// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n// SPDX-License-Identifier: Apache-2.0\n/**\n * Base Error for all other errors in this file\n */\nexport class JwtBaseError extends Error {\n}\n/**\n * An error that is raised because an actual value does not match with the expected value\n */\nexport class FailedAssertionError extends JwtBaseError {\n    constructor(msg, actual, expected) {\n        super(msg);\n        this.failedAssertion = {\n            actual,\n            expected,\n        };\n    }\n}\n/**\n * JWT errors\n */\nexport class JwtParseError extends JwtBaseError {\n    constructor(msg, error) {\n        const message = error != null ? `${msg}: ${error}` : msg;\n        super(message);\n    }\n}\nexport class ParameterValidationError extends JwtBaseError {\n}\nexport class JwtInvalidSignatureError extends JwtBaseError {\n}\nexport class JwtInvalidSignatureAlgorithmError extends FailedAssertionError {\n}\nexport class JwtInvalidClaimError extends FailedAssertionError {\n    withRawJwt({ header, payload }) {\n        this.rawJwt = {\n            header,\n            payload,\n        };\n        return this;\n    }\n}\nexport class JwtInvalidIssuerError extends JwtInvalidClaimError {\n}\nexport class JwtInvalidAudienceError extends JwtInvalidClaimError {\n}\nexport class JwtInvalidScopeError extends JwtInvalidClaimError {\n}\nexport class JwtExpiredError extends JwtInvalidClaimError {\n}\nexport class JwtNotBeforeError extends JwtInvalidClaimError {\n}\n/**\n * Amazon Cognito specific erros\n */\nexport class CognitoJwtInvalidGroupError extends JwtInvalidClaimError {\n}\nexport class CognitoJwtInvalidTokenUseError extends JwtInvalidClaimError {\n}\nexport class CognitoJwtInvalidClientIdError extends JwtInvalidClaimError {\n}\n/**\n * ASN.1 errors\n */\nexport class Asn1DecodingError extends JwtBaseError {\n}\n/**\n * JWK errors\n */\nexport class JwksValidationError extends JwtBaseError {\n}\nexport class JwkValidationError extends JwtBaseError {\n}\nexport class JwtWithoutValidKidError extends JwtBaseError {\n}\nexport class KidNotFoundInJwksError extends JwtBaseError {\n}\nexport class WaitPeriodNotYetEndedJwkError extends JwtBaseError {\n}\nexport class JwksNotAvailableInCacheError extends JwtBaseError {\n}\nexport class JwkInvalidUseError extends FailedAssertionError {\n}\nexport class JwkInvalidKtyError extends FailedAssertionError {\n}\n/**\n * HTTPS fetch errors\n */\nexport class FetchError extends JwtBaseError {\n    constructor(uri, msg) {\n        super(`Failed to fetch ${uri}: ${msg}`);\n    }\n}\nexport class NonRetryableFetchError extends FetchError {\n}\n/**\n * Web compatibility errors\n */\nexport class NotSupportedError extends JwtBaseError {\n}\n", "// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n// SPDX-License-Identifier: Apache-2.0\n//\n// Node.js implementations for the node-web-compatibility layer\nimport { createPublicKey, createVerify } from \"crypto\";\nimport { constructPublicKeyInDerFormat } from \"./asn1.js\";\nimport { fetchJson } from \"./https-node.js\";\n/**\n * Enum to map supported JWT signature algorithms with OpenSSL message digest algorithm names\n */\nvar JwtSignatureAlgorithms;\n(function (JwtSignatureAlgorithms) {\n    JwtSignatureAlgorithms[\"RS256\"] = \"RSA-SHA256\";\n    JwtSignatureAlgorithms[\"RS384\"] = \"RSA-SHA384\";\n    JwtSignatureAlgorithms[\"RS512\"] = \"RSA-SHA512\";\n})(JwtSignatureAlgorithms || (JwtSignatureAlgorithms = {}));\nexport const nodeWebCompat = {\n    fetchJson,\n    transformJwkToKeyObjectSync: (jwk) => createPublicKey({\n        key: constructPublicKeyInDerFormat(Buffer.from(jwk.n, \"base64\"), Buffer.from(jwk.e, \"base64\")),\n        format: \"der\",\n        type: \"spki\",\n    }),\n    transformJwkToKeyObjectAsync: async (jwk) => createPublicKey({\n        key: constructPublicKeyInDerFormat(Buffer.from(jwk.n, \"base64\"), Buffer.from(jwk.e, \"base64\")),\n        format: \"der\",\n        type: \"spki\",\n    }),\n    parseB64UrlString: (b64) => Buffer.from(b64, \"base64\").toString(\"utf8\"),\n    verifySignatureSync: ({ alg, keyObject, jwsSigningInput, signature }) => \n    // eslint-disable-next-line security/detect-object-injection\n    createVerify(JwtSignatureAlgorithms[alg])\n        .update(jwsSigningInput)\n        .verify(keyObject, signature, \"base64\"),\n    verifySignatureAsync: async ({ alg, keyObject, jwsSigningInput, signature, }) => \n    // eslint-disable-next-line security/detect-object-injection\n    createVerify(JwtSignatureAlgorithms[alg])\n        .update(jwsSigningInput)\n        .verify(keyObject, signature, \"base64\"),\n    defaultFetchTimeouts: {\n        socketIdle: 500,\n        response: 1500,\n    },\n    setTimeoutUnref: (...args) => setTimeout(...args).unref(),\n};\n", "// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n// SPDX-License-Identifier: Apache-2.0\n//\n// Utility to encode RSA public keys (a pair of modulus (n) and exponent (e)) into DER-encoding, per ASN.1 specification.\nimport { Asn1DecodingError } from \"./error.js\";\n/** Enum with possible values for supported ASN.1 classes */\nvar Asn1Class;\n(function (Asn1Class) {\n    Asn1Class[Asn1Class[\"Universal\"] = 0] = \"Universal\";\n})(Asn1Class || (Asn1Class = {}));\n/** Enum with possible values for supported ASN.1 encodings */\nvar Asn1Encoding;\n(function (Asn1Encoding) {\n    Asn1Encoding[Asn1Encoding[\"Primitive\"] = 0] = \"Primitive\";\n    Asn1Encoding[Asn1Encoding[\"Constructed\"] = 1] = \"Constructed\";\n})(Asn1Encoding || (Asn1Encoding = {}));\n/** Enum with possible values for supported ASN.1 tags */\nvar Asn1Tag;\n(function (Asn1Tag) {\n    Asn1Tag[Asn1Tag[\"BitString\"] = 3] = \"BitString\";\n    Asn1Tag[Asn1Tag[\"ObjectIdentifier\"] = 6] = \"ObjectIdentifier\";\n    Asn1Tag[Asn1Tag[\"Sequence\"] = 16] = \"Sequence\";\n    Asn1Tag[Asn1Tag[\"Null\"] = 5] = \"Null\";\n    Asn1Tag[Asn1Tag[\"Integer\"] = 2] = \"Integer\";\n})(Asn1Tag || (Asn1Tag = {}));\n/**\n * Encode an ASN.1 identifier per ASN.1 spec (DER-encoding)\n * See https://www.itu.int/ITU-T/studygroups/com17/languages/X.690-0207.pdf chapter 8.1.2\n *\n * @param identifier - The ASN.1 identifier\n * @returns The buffer\n */\nfunction encodeIdentifier(identifier) {\n    const identifierAsNumber = (identifier.class << 7) |\n        (identifier.primitiveOrConstructed << 5) |\n        identifier.tag;\n    return Buffer.from([identifierAsNumber]);\n}\n/**\n * Encode the length of an ASN.1 type per ASN.1 spec (DER-encoding)\n * See https://www.itu.int/ITU-T/studygroups/com17/languages/X.690-0207.pdf chapter 8.1.3\n *\n * @param length - The length of the ASN.1 type\n * @returns The buffer\n */\nfunction encodeLength(length) {\n    if (length < 128) {\n        return Buffer.from([length]);\n    }\n    const integers = [];\n    while (length > 0) {\n        integers.push(length % 256);\n        length = length >> 8;\n    }\n    integers.reverse();\n    return Buffer.from([128 | integers.length, ...integers]);\n}\n/**\n * Encode a buffer (that represent an integer) as integer per ASN.1 spec (DER-encoding)\n * See https://www.itu.int/ITU-T/studygroups/com17/languages/X.690-0207.pdf chapter 8.3\n *\n * @param buffer - The buffer that represent an integer to encode\n * @returns The buffer\n */\nfunction encodeBufferAsInteger(buffer) {\n    return Buffer.concat([\n        encodeIdentifier({\n            class: Asn1Class.Universal,\n            primitiveOrConstructed: Asn1Encoding.Primitive,\n            tag: Asn1Tag.Integer,\n        }),\n        encodeLength(buffer.length),\n        buffer,\n    ]);\n}\n/**\n * Encode an object identifier (a string such as \"1.2.840.113549.1.1.1\") per ASN.1 spec (DER-encoding)\n * See https://www.itu.int/ITU-T/studygroups/com17/languages/X.690-0207.pdf chapter 8.19\n *\n * @param oid - The object identifier to encode\n * @returns The buffer\n */\nfunction encodeObjectIdentifier(oid) {\n    const oidComponents = oid.split(\".\").map((i) => parseInt(i));\n    const firstSubidentifier = oidComponents[0] * 40 + oidComponents[1];\n    const subsequentSubidentifiers = oidComponents\n        .slice(2)\n        .reduce((expanded, component) => {\n        const bytes = [];\n        do {\n            bytes.push(component % 128);\n            component = component >> 7;\n        } while (component);\n        return expanded.concat(bytes.map((b, index) => (index ? b + 128 : b)).reverse());\n    }, []);\n    const oidBuffer = Buffer.from([\n        firstSubidentifier,\n        ...subsequentSubidentifiers,\n    ]);\n    return Buffer.concat([\n        encodeIdentifier({\n            class: Asn1Class.Universal,\n            primitiveOrConstructed: Asn1Encoding.Primitive,\n            tag: Asn1Tag.ObjectIdentifier,\n        }),\n        encodeLength(oidBuffer.length),\n        oidBuffer,\n    ]);\n}\n/**\n * Encode a buffer as bit string per ASN.1 spec (DER-encoding)\n * See https://www.itu.int/ITU-T/studygroups/com17/languages/X.690-0207.pdf chapter 8.6\n *\n * @param buffer - The buffer to encode\n * @returns The buffer\n */\nfunction encodeBufferAsBitString(buffer) {\n    const bitString = Buffer.concat([Buffer.from([0]), buffer]);\n    return Buffer.concat([\n        encodeIdentifier({\n            class: Asn1Class.Universal,\n            primitiveOrConstructed: Asn1Encoding.Primitive,\n            tag: Asn1Tag.BitString,\n        }),\n        encodeLength(bitString.length),\n        bitString,\n    ]);\n}\n/**\n * Encode a sequence of DER-encoded items per ASN.1 spec (DER-encoding)\n * See https://www.itu.int/ITU-T/studygroups/com17/languages/X.690-0207.pdf chapter 8.9\n *\n * @param sequenceItems - The sequence of DER-encoded items\n * @returns The buffer\n */\nfunction encodeSequence(sequenceItems) {\n    const concatenated = Buffer.concat(sequenceItems);\n    return Buffer.concat([\n        encodeIdentifier({\n            class: Asn1Class.Universal,\n            primitiveOrConstructed: Asn1Encoding.Constructed,\n            tag: Asn1Tag.Sequence,\n        }),\n        encodeLength(concatenated.length),\n        concatenated,\n    ]);\n}\n/**\n * Encode null per ASN.1 spec (DER-encoding)\n * See https://www.itu.int/ITU-T/studygroups/com17/languages/X.690-0207.pdf chapter 8.8\n *\n * @returns The buffer\n */\nfunction encodeNull() {\n    return Buffer.concat([\n        encodeIdentifier({\n            class: Asn1Class.Universal,\n            primitiveOrConstructed: Asn1Encoding.Primitive,\n            tag: Asn1Tag.Null,\n        }),\n        encodeLength(0),\n    ]);\n}\n/**\n * RSA encryption object identifier constant\n *\n * From: https://tools.ietf.org/html/rfc8017\n *\n * pkcs-1    OBJECT IDENTIFIER ::= {\n *     iso(1) member-body(2) us(840) rsadsi(113549) pkcs(1) 1\n * }\n *\n * -- When rsaEncryption is used in an AlgorithmIdentifier,\n * -- the parameters MUST be present and MUST be NULL.\n * --\n * rsaEncryption    OBJECT IDENTIFIER ::= { pkcs-1 1 }\n *\n * See also: http://www.oid-info.com/get/1.2.840.113549.1.1.1\n */\nconst ALGORITHM_RSA_ENCRYPTION = encodeSequence([\n    encodeObjectIdentifier(\"1.2.840.113549.1.1.1\"),\n    encodeNull(), // parameters\n]);\n/**\n * Transform an RSA public key, which is a pair of modulus (n) and exponent (e),\n *  into a buffer per ASN.1 spec (DER-encoding)\n *\n * @param n - The modulus of the public key as buffer\n * @param e - The exponent of the public key as buffer\n * @returns The buffer, which is the public key encoded per ASN.1 spec (DER-encoding)\n */\nexport function constructPublicKeyInDerFormat(n, e) {\n    return encodeSequence([\n        ALGORITHM_RSA_ENCRYPTION,\n        encodeBufferAsBitString(encodeSequence([encodeBufferAsInteger(n), encodeBufferAsInteger(e)])),\n    ]);\n}\n/**\n * Decode an ASN.1 identifier (a number) into its parts: class, primitiveOrConstructed, tag\n * See https://www.itu.int/ITU-T/studygroups/com17/languages/X.690-0207.pdf chapter 8.1.2\n *\n * @param identifier - The identifier\n * @returns An object with properties class, primitiveOrConstructed, tag\n */\nfunction decodeIdentifier(identifier) {\n    if (identifier >> 3 === 0b11111) {\n        throw new Asn1DecodingError(\"Decoding of identifier with tag > 30 not implemented\");\n    }\n    return {\n        class: identifier >> 6,\n        primitiveOrConstructed: (identifier >> 5) & 0b001,\n        tag: identifier & 0b11111, // bit 1-5\n    };\n}\n/**\n * Decode an ASN.1 block of length value combinations,\n * and return the length and byte range of the first length value combination.\n * See https://www.itu.int/ITU-T/studygroups/com17/languages/X.690-0207.pdf chapter 8.1.3 - 8.1.5\n *\n * @param blockOfLengthValues - The ASN.1 length value\n * @returns The length and byte range of the first included length value\n */\nfunction decodeLengthValue(blockOfLengthValues) {\n    if (!(blockOfLengthValues[0] & 0b10000000)) {\n        return {\n            length: blockOfLengthValues[0],\n            firstByteOffset: 1,\n            lastByteOffset: 1 + blockOfLengthValues[0],\n        };\n    }\n    const nrLengthOctets = blockOfLengthValues[0] & 0b01111111;\n    const length = Buffer.from(blockOfLengthValues.subarray(1, 1 + 1 + nrLengthOctets)).readUIntBE(0, nrLengthOctets);\n    return {\n        length,\n        firstByteOffset: 1 + nrLengthOctets,\n        lastByteOffset: 1 + nrLengthOctets + length,\n    };\n}\n/**\n * Decode an ASN.1 sequence into its constituent parts, each part being an identifier-length-value triplet\n * See https://www.itu.int/ITU-T/studygroups/com17/languages/X.690-0207.pdf chapter 8.9\n *\n * @param sequenceValue - The ASN.1 sequence value\n * @returns Array of identifier-length-value triplets\n */\nfunction decodeSequence(sequence) {\n    const { tag } = decodeIdentifier(sequence[0]);\n    if (tag !== Asn1Tag.Sequence) {\n        throw new Asn1DecodingError(`Expected a sequence to decode, but got tag ${tag}`);\n    }\n    const { firstByteOffset, lastByteOffset } = decodeLengthValue(sequence.subarray(1));\n    const sequenceValue = sequence.subarray(1 + firstByteOffset, 1 + 1 + lastByteOffset);\n    const parts = [];\n    let offset = 0;\n    while (offset < sequenceValue.length) {\n        // Silence false postive: accessing an octet in a Buffer at a particular index\n        // is to be done with index operator: [index]\n        // eslint-disable-next-line security/detect-object-injection\n        const identifier = decodeIdentifier(sequenceValue[offset]);\n        const next = decodeLengthValue(sequenceValue.subarray(offset + 1));\n        const value = sequenceValue.subarray(offset + 1 + next.firstByteOffset, offset + 1 + next.lastByteOffset);\n        parts.push({ identifier, length: next.length, value });\n        offset += 1 + next.lastByteOffset;\n    }\n    return parts;\n}\n/**\n * Decode an ASN.1 sequence that is wrapped in a bit string\n * (Which is the way RSA public keys are encoded in ASN.1 DER-encoding)\n * See https://www.itu.int/ITU-T/studygroups/com17/languages/X.690-0207.pdf chapter 8.6 and 8.9\n *\n * @param bitStringValue - The ASN.1 bit string value\n * @returns Array of identifier-length-value triplets\n */\nfunction decodeBitStringWrappedSequenceValue(bitStringValue) {\n    const wrappedSequence = bitStringValue.subarray(1);\n    return decodeSequence(wrappedSequence);\n}\n/**\n * Decode an ASN.1 DER-encoded public key, into its modulus (n) and exponent (e)\n *\n * @param publicKey - The ASN.1 DER-encoded public key\n * @returns Object with modulus (n) and exponent (e)\n */\nexport function deconstructPublicKeyInDerFormat(publicKey) {\n    const [, pubkeyinfo] = decodeSequence(publicKey);\n    const [n, e] = decodeBitStringWrappedSequenceValue(pubkeyinfo.value);\n    return { n: n.value, e: e.value };\n}\n", "// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n// SPDX-License-Identifier: Apache-2.0\n//\n// NodeJS implementation for fetching JSON documents over HTTPS\nimport { request } from \"https\";\nimport { validateHttpsJsonResponse } from \"./https-common.js\";\nimport { pipeline } from \"stream\";\nimport { TextDecoder } from \"util\";\nimport { safeJsonParse } from \"./safe-json-parse.js\";\nimport { FetchError, NonRetryableFetchError } from \"./error.js\";\n/**\n * Execute a HTTPS request\n * @param uri - The URI\n * @param requestOptions - The RequestOptions to use\n * @param data - Data to send to the URI (e.g. POST data)\n * @returns - The response as parsed JSON\n */\nexport async function fetchJson(uri, requestOptions, data) {\n    let responseTimeout;\n    return new Promise((resolve, reject) => {\n        const req = request(uri, {\n            method: \"GET\",\n            ...requestOptions,\n        }, (response) => {\n            // Capture response data\n            // @types/node is incomplete so cast to any\n            // eslint-disable-next-line @typescript-eslint/no-explicit-any\n            pipeline([\n                response,\n                getJsonDestination(uri, response.statusCode, response.headers),\n            ], done);\n        });\n        if (requestOptions?.responseTimeout) {\n            responseTimeout = setTimeout(() => done(new FetchError(uri, `Response time-out (after ${requestOptions.responseTimeout} ms.)`)), requestOptions.responseTimeout);\n            responseTimeout.unref(); // Don't block Node from exiting\n        }\n        function done(...args) {\n            if (responseTimeout)\n                clearTimeout(responseTimeout);\n            if (args[0] == null) {\n                resolve(args[1]);\n                return;\n            }\n            // In case of errors, let the Agent (if any) know to abandon the socket\n            // This is probably best, because the socket may have become stale\n            /* istanbul ignore next */\n            req.socket?.emit(\"agentRemove\");\n            // Turn error into FetchError so the URI is nicely captured in the message\n            let error = args[0];\n            if (!(error instanceof FetchError)) {\n                error = new FetchError(uri, error.message);\n            }\n            req.destroy();\n            reject(error);\n        }\n        // Handle errors while sending request\n        req.on(\"error\", done);\n        // Signal end of request (include optional data)\n        req.end(data);\n    });\n}\n/**\n * Ensures the HTTPS response contains valid JSON\n *\n * @param uri - The URI you were requesting\n * @param statusCode - The response status code to your HTTPS request\n * @param headers - The response headers to your HTTPS request\n *\n * @returns - Async function that can be used as destination in a stream.pipeline, it will return the JSON, if valid, or throw an error otherwise\n */\nfunction getJsonDestination(uri, statusCode, headers) {\n    return async (responseIterable) => {\n        validateHttpsJsonResponse(uri, statusCode, headers[\"content-type\"]);\n        const collected = [];\n        for await (const chunk of responseIterable) {\n            collected.push(chunk);\n        }\n        try {\n            return safeJsonParse(new TextDecoder(\"utf8\", { fatal: true, ignoreBOM: true }).decode(Buffer.concat(collected)));\n        }\n        catch (err) {\n            throw new NonRetryableFetchError(uri, err);\n        }\n    };\n}\n", "// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n// SPDX-License-Identifier: Apache-2.0\n//\n// Lower-level HTTPS functionalities, common for Node.js and Web\nimport { FetchError, NonRetryableFetchError } from \"./error.js\";\n/**\n * Sanity check a HTTPS response where we expect to get JSON data back\n *\n * @param uri the uri that was being requested\n * @param statusCode the HTTP status code, should be 200\n * @param contentType the value of the \"Content-Type\" header in the response, should start with \"application/json\"\n * @returns void - throws an error if the status code or content type aren't as expected\n */\nexport function validateHttpsJsonResponse(uri, statusCode, contentType) {\n    if (statusCode === 429) {\n        throw new FetchError(uri, \"Too many requests\");\n    }\n    else if (statusCode !== 200) {\n        throw new NonRetryableFetchError(uri, `Status code is ${statusCode}, expected 200`);\n    }\n    if (!contentType ||\n        !contentType.toLowerCase().startsWith(\"application/json\")) {\n        throw new NonRetryableFetchError(uri, `Content-type is \"${contentType}\", expected \"application/json\"`);\n    }\n}\n", "// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n// SPDX-License-Identifier: Apache-2.0\n//\n// Utility to parse JSON safely\n/**\n * Check if a piece of JSON is a JSON object, and not e.g. a mere string or null\n *\n * @param j - the JSON\n */\nexport function isJsonObject(j) {\n    // It is not enough to check that `typeof j === \"object\"`\n    // because in JS `typeof null` is also \"object\", and so is `typeof []`.\n    // So we need to check that j is an object, and not null, and not an array\n    return typeof j === \"object\" && !Array.isArray(j) && j !== null;\n}\n/**\n * Parse a string as JSON, while removing __proto__ and constructor, so JS prototype pollution is prevented\n *\n * @param s - the string to JSON parse\n */\nexport function safeJsonParse(s) {\n    return JSON.parse(s, (_, value) => {\n        if (typeof value === \"object\" && !Array.isArray(value) && value !== null) {\n            delete value.__proto__;\n            delete value.constructor;\n        }\n        return value;\n    });\n}\n", "// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n// SPDX-License-Identifier: Apache-2.0\n//\n// Utilities for fetching the JWKS URI, to get the public keys with which to verify JWTs\nimport { NonRetryableFetchError } from \"./error.js\";\nimport { nodeWebCompat } from \"#node-web-compat\";\n/**\n * Execute a HTTPS request\n * @param uri - The URI\n * @param requestOptions - The RequestOptions to use (depending on the runtime context, either Node.js RequestOptions or Web Fetch init)\n * @param data - Data to send to the URI (e.g. POST data)\n * @returns - The response as parsed JSON\n */\nexport const fetchJson = nodeWebCompat.fetchJson;\n/**\n * HTTPS Fetcher for URIs with JSON body\n *\n * @param defaultRequestOptions - The default RequestOptions to use on individual HTTPS requests\n */\nexport class SimpleJsonFetcher {\n    constructor(props) {\n        this.defaultRequestOptions = {\n            timeout: nodeWebCompat.defaultFetchTimeouts.socketIdle,\n            responseTimeout: nodeWebCompat.defaultFetchTimeouts.response,\n            ...props?.defaultRequestOptions,\n        };\n    }\n    /**\n     * Execute a HTTPS request (with 1 immediate retry in case of errors)\n     * @param uri - The URI\n     * @param requestOptions - The RequestOptions to use\n     * @param data - Data to send to the URI (e.g. POST data)\n     * @returns - The response as parsed JSON\n     */\n    async fetch(uri, requestOptions, data) {\n        requestOptions = { ...this.defaultRequestOptions, ...requestOptions };\n        try {\n            return await fetchJson(uri, requestOptions, data);\n        }\n        catch (err) {\n            if (err instanceof NonRetryableFetchError) {\n                throw err;\n            }\n            // Retry once, immediately\n            return fetchJson(uri, requestOptions, data);\n        }\n    }\n}\n", "// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n// SPDX-License-Identifier: Apache-2.0\n//\n// Utilities to assert that supplied values match with expected values\nimport { FailedAssertionError } from \"./error.js\";\n/**\n * Assert value is a non-empty string and equal to the expected value,\n * or throw an error otherwise\n *\n * @param name - Name for the value being checked\n * @param actual - The value to check\n * @param expected - The expected value\n * @param errorConstructor - Constructor for the concrete error to be thrown\n */\nexport function assertStringEquals(name, actual, expected, errorConstructor = FailedAssertionError) {\n    if (!actual) {\n        throw new errorConstructor(`Missing ${name}. Expected: ${expected}`, actual, expected);\n    }\n    if (typeof actual !== \"string\") {\n        throw new errorConstructor(`${name} is not of type string`, actual, expected);\n    }\n    if (expected !== actual) {\n        throw new errorConstructor(`${name} not allowed: ${actual}. Expected: ${expected}`, actual, expected);\n    }\n}\n/**\n * Assert value is a non-empty string and is indeed one of the expected values,\n * or throw an error otherwise\n *\n * @param name - Name for the value being checked\n * @param actual - The value to check\n * @param expected - The array of expected values. For your convenience you can provide\n * @param errorConstructor - Constructor for the concrete error to be thrown\n * a string here as well, which will mean an array with just that string\n */\nexport function assertStringArrayContainsString(name, actual, expected, errorConstructor = FailedAssertionError) {\n    if (!actual) {\n        throw new errorConstructor(`Missing ${name}. ${expectationMessage(expected)}`, actual, expected);\n    }\n    if (typeof actual !== \"string\") {\n        throw new errorConstructor(`${name} is not of type string`, actual, expected);\n    }\n    return assertStringArraysOverlap(name, actual, expected, errorConstructor);\n}\n/**\n * Assert value is an array of strings, where at least one of the strings is indeed one of the expected values,\n * or throw an error otherwise\n *\n * @param name - Name for the value being checked\n * @param actual - The value to check, must be an array of strings, or a single string (which will be treated\n * as an array with just that string)\n * @param expected - The array of expected values. For your convenience you can provide\n * a string here as well, which will mean an array with just that string\n * @param errorConstructor - Constructor for the concrete error to be thrown\n */\nexport function assertStringArraysOverlap(name, actual, expected, errorConstructor = FailedAssertionError) {\n    if (!actual) {\n        throw new errorConstructor(`Missing ${name}. ${expectationMessage(expected)}`, actual, expected);\n    }\n    const expectedAsSet = new Set(Array.isArray(expected) ? expected : [expected]);\n    if (typeof actual === \"string\") {\n        actual = [actual];\n    }\n    if (!Array.isArray(actual)) {\n        throw new errorConstructor(`${name} is not an array`, actual, expected);\n    }\n    const overlaps = actual.some((actualItem) => {\n        if (typeof actualItem !== \"string\") {\n            throw new errorConstructor(`${name} includes elements that are not of type string`, actual, expected);\n        }\n        return expectedAsSet.has(actualItem);\n    });\n    if (!overlaps) {\n        throw new errorConstructor(`${name} not allowed: ${actual.join(\", \")}. ${expectationMessage(expected)}`, actual, expected);\n    }\n}\n/**\n * Get a nicely readable message regarding an expectation\n *\n * @param expected - The expected value.\n */\nfunction expectationMessage(expected) {\n    if (Array.isArray(expected)) {\n        if (expected.length > 1) {\n            return `Expected one of: ${expected.join(\", \")}`;\n        }\n        return `Expected: ${expected[0]}`;\n    }\n    return `Expected: ${expected}`;\n}\n/**\n * Assert value is not a promise, or throw an error otherwise\n *\n * @param actual - The value to check\n * @param errorFactory - Function that returns the error to be thrown\n */\nexport function assertIsNotPromise(actual, errorFactory) {\n    if (actual && typeof actual.then === \"function\") {\n        throw errorFactory();\n    }\n}\n", "// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n// SPDX-License-Identifier: Apache-2.0\nimport { SimpleJsonFetcher, fetchJson } from \"./https.js\";\nimport { isJsonObject } from \"./safe-json-parse.js\";\nimport { JwkValidationError, JwksNotAvailableInCacheError, JwksValidationError, KidNotFoundInJwksError, WaitPeriodNotYetEndedJwkError, JwtWithoutValidKidError, JwkInvalidUseError, JwkInvalidKtyError, } from \"./error.js\";\nimport { nodeWebCompat } from \"#node-web-compat\";\nimport { assertStringEquals } from \"./assert.js\";\nconst optionalJwkFieldNames = [\n    \"use\",\n    \"alg\",\n    \"kid\",\n    \"n\",\n    \"e\", // https://datatracker.ietf.org/doc/html/rfc7518#section-6.3.1.2\n];\nconst mandatoryJwkFieldNames = [\n    \"kty\", // https://datatracker.ietf.org/doc/html/rfc7517#section-4.1\n];\nexport function findJwkInJwks(jwks, kid) {\n    return jwks.keys.find((jwk) => jwk.kid != null && jwk.kid === kid);\n}\nexport async function fetchJwks(jwksUri) {\n    const jwks = await fetchJson(jwksUri);\n    assertIsJwks(jwks);\n    return jwks;\n}\nexport async function fetchJwk(jwksUri, decomposedJwt) {\n    if (!decomposedJwt.header.kid) {\n        throw new JwtWithoutValidKidError(\"JWT header does not have valid kid claim\");\n    }\n    const jwks = await fetchJwks(jwksUri);\n    const jwk = findJwkInJwks(jwks, decomposedJwt.header.kid);\n    if (!jwk) {\n        throw new KidNotFoundInJwksError(`JWK for kid \"${decomposedJwt.header.kid}\" not found in the JWKS`);\n    }\n    return jwk;\n}\nexport function assertIsJwks(jwks) {\n    if (!jwks) {\n        throw new JwksValidationError(\"JWKS empty\");\n    }\n    if (!isJsonObject(jwks)) {\n        throw new JwksValidationError(\"JWKS should be an object\");\n    }\n    if (!Object.keys(jwks).includes(\"keys\")) {\n        throw new JwksValidationError(\"JWKS does not include keys\");\n    }\n    if (!Array.isArray(jwks.keys)) {\n        throw new JwksValidationError(\"JWKS keys should be an array\");\n    }\n    for (const jwk of jwks.keys) {\n        assertIsJwk(jwk);\n    }\n}\nexport function assertIsRsaSignatureJwk(jwk) {\n    // Check JWK use\n    assertStringEquals(\"JWK use\", jwk.use, \"sig\", JwkInvalidUseError);\n    // Check JWK kty\n    assertStringEquals(\"JWK kty\", jwk.kty, \"RSA\", JwkInvalidKtyError);\n    // Check modulus (n) has a value\n    if (!jwk.n)\n        throw new JwkValidationError(\"Missing modulus (n)\");\n    // Check exponent (e) has a value\n    if (!jwk.e)\n        throw new JwkValidationError(\"Missing exponent (e)\");\n}\nexport function assertIsJwk(jwk) {\n    if (!jwk) {\n        throw new JwkValidationError(\"JWK empty\");\n    }\n    if (!isJsonObject(jwk)) {\n        throw new JwkValidationError(\"JWK should be an object\");\n    }\n    for (const field of mandatoryJwkFieldNames) {\n        // disable eslint rule because `field` is trusted\n        // eslint-disable-next-line security/detect-object-injection\n        if (typeof jwk[field] !== \"string\") {\n            throw new JwkValidationError(`JWK ${field} should be a string`);\n        }\n    }\n    for (const field of optionalJwkFieldNames) {\n        // disable eslint rule because `field` is trusted\n        // eslint-disable-next-line security/detect-object-injection\n        if (field in jwk && typeof jwk[field] !== \"string\") {\n            throw new JwkValidationError(`JWK ${field} should be a string`);\n        }\n    }\n}\nexport function isJwks(jwks) {\n    try {\n        assertIsJwks(jwks);\n        return true;\n    }\n    catch {\n        return false;\n    }\n}\nexport function isJwk(jwk) {\n    try {\n        assertIsJwk(jwk);\n        return true;\n    }\n    catch {\n        return false;\n    }\n}\nexport class SimplePenaltyBox {\n    constructor(props) {\n        this.waitingUris = new Map();\n        this.waitSeconds = props?.waitSeconds ?? 10;\n    }\n    async wait(jwksUri) {\n        // SimplePenaltyBox does not actually wait but bluntly throws an error\n        // Any waiting and retries are expected to be done upstream (e.g. in the browser / app)\n        if (this.waitingUris.has(jwksUri)) {\n            throw new WaitPeriodNotYetEndedJwkError(\"Not allowed to fetch JWKS yet, still waiting for back off period to end\");\n        }\n    }\n    release(jwksUri) {\n        const i = this.waitingUris.get(jwksUri);\n        if (i) {\n            clearTimeout(i);\n            this.waitingUris.delete(jwksUri);\n        }\n    }\n    registerFailedAttempt(jwksUri) {\n        const i = nodeWebCompat.setTimeoutUnref(() => {\n            this.waitingUris.delete(jwksUri);\n        }, this.waitSeconds * 1000);\n        this.waitingUris.set(jwksUri, i);\n    }\n    registerSuccessfulAttempt(jwksUri) {\n        this.release(jwksUri);\n    }\n}\nexport class SimpleJwksCache {\n    constructor(props) {\n        this.jwksCache = new Map();\n        this.fetchingJwks = new Map();\n        this.penaltyBox = props?.penaltyBox ?? new SimplePenaltyBox();\n        this.fetcher = props?.fetcher ?? new SimpleJsonFetcher();\n    }\n    addJwks(jwksUri, jwks) {\n        this.jwksCache.set(jwksUri, jwks);\n    }\n    async getJwks(jwksUri) {\n        const existingFetch = this.fetchingJwks.get(jwksUri);\n        if (existingFetch) {\n            return existingFetch;\n        }\n        const jwksPromise = this.fetcher.fetch(jwksUri).then((res) => {\n            assertIsJwks(res);\n            return res;\n        });\n        this.fetchingJwks.set(jwksUri, jwksPromise);\n        let jwks;\n        try {\n            jwks = await jwksPromise;\n        }\n        finally {\n            this.fetchingJwks.delete(jwksUri);\n        }\n        this.jwksCache.set(jwksUri, jwks);\n        return jwks;\n    }\n    getCachedJwk(jwksUri, decomposedJwt) {\n        if (typeof decomposedJwt.header.kid !== \"string\") {\n            throw new JwtWithoutValidKidError(\"JWT header does not have valid kid claim\");\n        }\n        if (!this.jwksCache.has(jwksUri)) {\n            throw new JwksNotAvailableInCacheError(`JWKS for uri ${jwksUri} not yet available in cache`);\n        }\n        const jwk = findJwkInJwks(this.jwksCache.get(jwksUri), decomposedJwt.header.kid);\n        if (!jwk) {\n            throw new KidNotFoundInJwksError(`JWK for kid ${decomposedJwt.header.kid} not found in the JWKS`);\n        }\n        return jwk;\n    }\n    async getJwk(jwksUri, decomposedJwt) {\n        if (typeof decomposedJwt.header.kid !== \"string\") {\n            throw new JwtWithoutValidKidError(\"JWT header does not have valid kid claim\");\n        }\n        // Try to get JWK from cache:\n        const cachedJwks = this.jwksCache.get(jwksUri);\n        if (cachedJwks) {\n            const cachedJwk = findJwkInJwks(cachedJwks, decomposedJwt.header.kid);\n            if (cachedJwk) {\n                return cachedJwk;\n            }\n        }\n        // Await any wait period that is currently in effect\n        // This prevents us from flooding the JWKS URI with requests\n        await this.penaltyBox.wait(jwksUri, decomposedJwt.header.kid);\n        // Fetch the JWKS and (try to) locate the JWK\n        const jwks = await this.getJwks(jwksUri);\n        const jwk = findJwkInJwks(jwks, decomposedJwt.header.kid);\n        // If the JWK could not be located, someone might be messing around with us\n        // Register the failed attempt with the penaltyBox, so it can enforce a wait period\n        // before trying again next time (instead of flooding the JWKS URI with requests)\n        if (!jwk) {\n            this.penaltyBox.registerFailedAttempt(jwksUri, decomposedJwt.header.kid);\n            throw new KidNotFoundInJwksError(`JWK for kid \"${decomposedJwt.header.kid}\" not found in the JWKS`);\n        }\n        else {\n            this.penaltyBox.registerSuccessfulAttempt(jwksUri, decomposedJwt.header.kid);\n        }\n        return jwk;\n    }\n}\n", "// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n// SPDX-License-Identifier: Apache-2.0\nexport const supportedSignatureAlgorithms = [\n    \"RS256\",\n    \"RS384\",\n    \"RS512\",\n];\n", "// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n// SPDX-License-Identifier: Apache-2.0\nimport { assertStringArrayContainsString, assertStringArraysOverlap, } from \"./assert.js\";\nimport { safeJsonParse, isJsonObject } from \"./safe-json-parse.js\";\nimport { JwtExpiredError, JwtNotBeforeError, JwtInvalidIssuerError, JwtInvalidAudienceError, JwtInvalidScopeError, JwtParseError, ParameterValidationError, } from \"./error.js\";\nimport { nodeWebCompat } from \"#node-web-compat\";\n/**\n * Assert that the argument is a valid JWT header object.\n * Throws an error in case it is not.\n *\n * @param header\n * @returns void\n */\nfunction assertJwtHeader(header) {\n    if (!isJsonObject(header)) {\n        throw new JwtParseError(\"JWT header is not an object\");\n    }\n    if (header.alg !== undefined && typeof header.alg !== \"string\") {\n        throw new JwtParseError(\"JWT header alg claim is not a string\");\n    }\n    if (header.kid !== undefined && typeof header.kid !== \"string\") {\n        throw new JwtParseError(\"JWT header kid claim is not a string\");\n    }\n}\n/**\n * Assert that the argument is a valid JWT payload object.\n * Throws an error in case it is not.\n *\n * @param payload\n * @returns void\n */\nfunction assertJwtPayload(payload) {\n    if (!isJsonObject(payload)) {\n        throw new JwtParseError(\"JWT payload is not an object\");\n    }\n    if (payload.exp !== undefined && !Number.isFinite(payload.exp)) {\n        throw new JwtParseError(\"JWT payload exp claim is not a number\");\n    }\n    if (payload.iss !== undefined && typeof payload.iss !== \"string\") {\n        throw new JwtParseError(\"JWT payload iss claim is not a string\");\n    }\n    if (payload.aud !== undefined &&\n        typeof payload.aud !== \"string\" &&\n        (!Array.isArray(payload.aud) ||\n            payload.aud.some((aud) => typeof aud !== \"string\"))) {\n        throw new JwtParseError(\"JWT payload aud claim is not a string or array of strings\");\n    }\n    if (payload.nbf !== undefined && !Number.isFinite(payload.nbf)) {\n        throw new JwtParseError(\"JWT payload nbf claim is not a number\");\n    }\n    if (payload.iat !== undefined && !Number.isFinite(payload.iat)) {\n        throw new JwtParseError(\"JWT payload iat claim is not a number\");\n    }\n    if (payload.scope !== undefined && typeof payload.scope !== \"string\") {\n        throw new JwtParseError(\"JWT payload scope claim is not a string\");\n    }\n    if (payload.jti !== undefined && typeof payload.jti !== \"string\") {\n        throw new JwtParseError(\"JWT payload jti claim is not a string\");\n    }\n}\n/**\n * Sanity check, decompose and JSON parse a JWT string into its constituent parts:\n * - header object\n * - payload object\n * - signature string\n *\n * @param jwt The JWT (as string)\n * @returns the decomposed JWT\n */\nexport function decomposeJwt(jwt) {\n    // Sanity checks on JWT\n    if (!jwt) {\n        throw new JwtParseError(\"Empty JWT\");\n    }\n    if (typeof jwt !== \"string\") {\n        throw new JwtParseError(\"JWT is not a string\");\n    }\n    if (!jwt.match(/^[A-Za-z0-9_-]+\\.[A-Za-z0-9_-]+\\.[A-Za-z0-9_-]+$/)) {\n        throw new JwtParseError(\"JWT string does not consist of exactly 3 parts (header, payload, signature)\");\n    }\n    const [headerB64, payloadB64, signatureB64] = jwt.split(\".\");\n    // B64 decode header and payload\n    const [headerString, payloadString] = [headerB64, payloadB64].map(nodeWebCompat.parseB64UrlString);\n    // Parse header\n    let header;\n    try {\n        header = safeJsonParse(headerString);\n    }\n    catch (err) {\n        throw new JwtParseError(\"Invalid JWT. Header is not a valid JSON object\", err);\n    }\n    assertJwtHeader(header);\n    // parse payload\n    let payload;\n    try {\n        payload = safeJsonParse(payloadString);\n    }\n    catch (err) {\n        throw new JwtParseError(\"Invalid JWT. Payload is not a valid JSON object\", err);\n    }\n    assertJwtPayload(payload);\n    return {\n        header,\n        headerB64,\n        payload,\n        payloadB64,\n        signatureB64,\n    };\n}\n/**\n * Validate JWT payload fields. Throws an error in case there's any validation issue.\n *\n * @param payload The (JSON parsed) JWT payload\n * @param options The options to use during validation\n * @returns void\n */\nexport function validateJwtFields(payload, options) {\n    // Check expiry\n    if (payload.exp !== undefined) {\n        if (payload.exp + (options.graceSeconds ?? 0) < Date.now() / 1000) {\n            throw new JwtExpiredError(`Token expired at ${new Date(payload.exp * 1000).toISOString()}`, payload.exp);\n        }\n    }\n    // Check not before\n    if (payload.nbf !== undefined) {\n        if (payload.nbf - (options.graceSeconds ?? 0) > Date.now() / 1000) {\n            throw new JwtNotBeforeError(`Token can't be used before ${new Date(payload.nbf * 1000).toISOString()}`, payload.nbf);\n        }\n    }\n    // Check JWT issuer\n    if (options.issuer !== null) {\n        if (options.issuer === undefined) {\n            throw new ParameterValidationError(\"issuer must be provided or set to null explicitly\");\n        }\n        assertStringArrayContainsString(\"Issuer\", payload.iss, options.issuer, JwtInvalidIssuerError);\n    }\n    // Check audience\n    if (options.audience !== null) {\n        if (options.audience === undefined) {\n            throw new ParameterValidationError(\"audience must be provided or set to null explicitly\");\n        }\n        assertStringArraysOverlap(\"Audience\", payload.aud, options.audience, JwtInvalidAudienceError);\n    }\n    // Check scope\n    if (options.scope != null) {\n        assertStringArraysOverlap(\"Scope\", payload.scope?.split(\" \"), options.scope, JwtInvalidScopeError);\n    }\n}\n", "// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n// SPDX-License-Identifier: Apache-2.0\nimport { SimpleJwksCache, isJwk, isJwks, fetchJwk, assertIsRsaSignatureJwk, findJwkInJwks, } from \"./jwk.js\";\nimport { assertIsNotPromise, assertStringArrayContainsString, assertStringEquals, } from \"./assert.js\";\nimport { supportedSignatureAlgorithms, } from \"./jwt-model.js\";\nimport { decomposeJwt, validateJwtFields } from \"./jwt.js\";\nimport { JwtInvalidClaimError, JwtInvalidIssuerError, JwtInvalidSignatureAlgorithmError, JwtInvalidSignatureError, KidNotFoundInJwksError, ParameterValidationError, } from \"./error.js\";\nimport { nodeWebCompat } from \"#node-web-compat\";\n/**\n * Sanity check the JWT header and the selected JWK\n *\n * @param header: the JWT header (decoded and JSON parsed)\n * @param jwk: the JWK\n */\nfunction validateJwtHeaderAndJwk(header, jwk) {\n    // Check that the JWK is in fact a JWK for RSA signatures\n    assertIsRsaSignatureJwk(jwk);\n    // Check that JWT signature algorithm matches JWK\n    if (jwk.alg) {\n        assertStringEquals(\"JWT signature algorithm\", header.alg, jwk.alg, JwtInvalidSignatureAlgorithmError);\n    }\n    // Check JWT signature algorithm is one of the supported signature algorithms\n    assertStringArrayContainsString(\"JWT signature algorithm\", header.alg, supportedSignatureAlgorithms, JwtInvalidSignatureAlgorithmError);\n}\n/**\n * Verify a JWT asynchronously (thus allowing for the JWKS to be fetched from the JWKS URI)\n *\n * @param jwt The JWT\n * @param jwksUri The JWKS URI, where the JWKS can be fetched from\n * @param options Verification options\n * @returns Promise that resolves to the payload of the JWT\u2013\u2013if the JWT is valid, otherwise the promise rejects\n */\nexport async function verifyJwt(jwt, jwksUri, options) {\n    return verifyDecomposedJwt(decomposeJwt(jwt), jwksUri, options);\n}\n/**\n * Verify (asynchronously) a JWT that is already decomposed (by function `decomposeJwt`)\n *\n * @param decomposedJwt The decomposed JWT\n * @param jwksUri The JWKS URI, where the JWKS can be fetched from\n * @param options Verification options\n * @param jwkFetcher A function that can execute the fetch of the JWKS from the JWKS URI\n * @param transformJwkToKeyObjectFn A function that can transform a JWK into a crypto native key object\n * @returns Promise that resolves to the payload of the JWT\u2013\u2013if the JWT is valid, otherwise the promise rejects\n */\nasync function verifyDecomposedJwt(decomposedJwt, jwksUri, options, jwkFetcher = fetchJwk, transformJwkToKeyObjectFn = nodeWebCompat.transformJwkToKeyObjectAsync) {\n    const { header, headerB64, payload, payloadB64, signatureB64 } = decomposedJwt;\n    const jwk = await jwkFetcher(jwksUri, decomposedJwt);\n    validateJwtHeaderAndJwk(decomposedJwt.header, jwk);\n    // Transform the JWK to native key format, that can be used with verifySignature\n    const keyObject = await transformJwkToKeyObjectFn(jwk, header.alg, payload.iss);\n    // Verify the JWT signature\n    const valid = await nodeWebCompat.verifySignatureAsync({\n        jwsSigningInput: `${headerB64}.${payloadB64}`,\n        signature: signatureB64,\n        alg: header.alg,\n        keyObject,\n    });\n    if (!valid) {\n        throw new JwtInvalidSignatureError(\"Invalid signature\");\n    }\n    try {\n        validateJwtFields(payload, options);\n        if (options.customJwtCheck) {\n            await options.customJwtCheck({ header, payload, jwk });\n        }\n    }\n    catch (err) {\n        if (options.includeRawJwtInErrors && err instanceof JwtInvalidClaimError) {\n            throw err.withRawJwt(decomposedJwt);\n        }\n        throw err;\n    }\n    return payload;\n}\n/**\n * Verify a JWT synchronously, using a JWKS or JWK that has already been fetched\n *\n * @param jwt The JWT\n * @param jwkOrJwks The JWKS that includes the right JWK (indexed by kid). Alternatively, provide the right JWK directly\n * @param options Verification options\n * @param transformJwkToKeyObjectFn A function that can transform a JWK into a crypto native key object\n * @returns The (JSON parsed) payload of the JWT\u2013\u2013if the JWT is valid, otherwise an error is thrown\n */\nexport function verifyJwtSync(jwt, jwkOrJwks, options, transformJwkToKeyObjectFn = nodeWebCompat.transformJwkToKeyObjectSync) {\n    return verifyDecomposedJwtSync(decomposeJwt(jwt), jwkOrJwks, options, transformJwkToKeyObjectFn);\n}\n/**\n * Verify (synchronously) a JWT that is already decomposed (by function `decomposeJwt`)\n *\n * @param decomposedJwt The decomposed JWT\n * @param jwkOrJwks The JWKS that includes the right JWK (indexed by kid). Alternatively, provide the right JWK directly\n * @param options Verification options\n * @param transformJwkToKeyObjectFn A function that can transform a JWK into a crypto native key object\n * @returns The (JSON parsed) payload of the JWT\u2013\u2013if the JWT is valid, otherwise an error is thrown\n */\nfunction verifyDecomposedJwtSync(decomposedJwt, jwkOrJwks, options, transformJwkToKeyObjectFn) {\n    const { header, headerB64, payload, payloadB64, signatureB64 } = decomposedJwt;\n    let jwk;\n    if (isJwk(jwkOrJwks)) {\n        jwk = jwkOrJwks;\n    }\n    else if (isJwks(jwkOrJwks)) {\n        const locatedJwk = header.kid\n            ? findJwkInJwks(jwkOrJwks, header.kid)\n            : undefined;\n        if (!locatedJwk) {\n            throw new KidNotFoundInJwksError(`JWK for kid ${header.kid} not found in the JWKS`);\n        }\n        jwk = locatedJwk;\n    }\n    else {\n        throw new ParameterValidationError([\n            `Expected a valid JWK or JWKS (parsed as JavaScript object), but received: ${jwkOrJwks}.`,\n            \"If you're passing a JWKS URI, use the async verify() method instead, it will download and parse the JWKS for you\",\n        ].join());\n    }\n    validateJwtHeaderAndJwk(decomposedJwt.header, jwk);\n    // Transform the JWK to native key format, that can be used with verifySignature\n    const keyObject = transformJwkToKeyObjectFn(jwk, header.alg, payload.iss);\n    // Verify the JWT signature (JWS)\n    const valid = nodeWebCompat.verifySignatureSync({\n        jwsSigningInput: `${headerB64}.${payloadB64}`,\n        signature: signatureB64,\n        alg: header.alg,\n        keyObject,\n    });\n    if (!valid) {\n        throw new JwtInvalidSignatureError(\"Invalid signature\");\n    }\n    try {\n        validateJwtFields(payload, options);\n        if (options.customJwtCheck) {\n            const res = options.customJwtCheck({ header, payload, jwk });\n            assertIsNotPromise(res, () => new ParameterValidationError(\"Custom JWT checks must be synchronous but a promise was returned\"));\n        }\n    }\n    catch (err) {\n        if (options.includeRawJwtInErrors && err instanceof JwtInvalidClaimError) {\n            throw err.withRawJwt(decomposedJwt);\n        }\n        throw err;\n    }\n    return payload;\n}\n/**\n * Abstract class representing a verifier for JWTs signed with RSA (e.g. RS256, RS384, RS512)\n *\n * A class is used, because there is state:\n * - The JWKS is fetched (downloaded) from the JWKS URI and cached in memory\n * - Verification properties at verifier level, are used as default options for individual verify calls\n *\n * When instantiating this class, relevant type parameters should be provided, for your concrete case:\n * @param StillToProvide The verification options that you want callers of verify to provide on individual verify calls\n * @param SpecificVerifyProperties The verification options that you'll use\n * @param IssuerConfig The issuer config that you'll use (config options are used as default verification options)\n * @param MultiIssuer Verify multiple issuers (true) or just a single one (false)\n */\nexport class JwtRsaVerifierBase {\n    constructor(verifyProperties, jwksCache = new SimpleJwksCache()) {\n        this.jwksCache = jwksCache;\n        this.issuersConfig = new Map();\n        this.publicKeyCache = new KeyObjectCache();\n        if (Array.isArray(verifyProperties)) {\n            if (!verifyProperties.length) {\n                throw new ParameterValidationError(\"Provide at least one issuer configuration\");\n            }\n            for (const prop of verifyProperties) {\n                if (this.issuersConfig.has(prop.issuer)) {\n                    throw new ParameterValidationError(`issuer ${prop.issuer} supplied multiple times`);\n                }\n                this.issuersConfig.set(prop.issuer, this.withJwksUri(prop));\n            }\n        }\n        else {\n            this.issuersConfig.set(verifyProperties.issuer, this.withJwksUri(verifyProperties));\n        }\n    }\n    get expectedIssuers() {\n        return Array.from(this.issuersConfig.keys());\n    }\n    getIssuerConfig(issuer) {\n        if (!issuer) {\n            if (this.issuersConfig.size !== 1) {\n                throw new ParameterValidationError(\"issuer must be provided\");\n            }\n            issuer = this.issuersConfig.keys().next().value;\n        }\n        const config = this.issuersConfig.get(issuer);\n        if (!config) {\n            throw new ParameterValidationError(`issuer not configured: ${issuer}`);\n        }\n        return config;\n    }\n    /**\n     * This method loads a JWKS that you provide, into the JWKS cache, so that it is\n     * available for JWT verification. Use this method to speed up the first JWT verification\n     * (when the JWKS would otherwise have to be downloaded from the JWKS uri), or to provide the JWKS\n     * in case the JwtVerifier does not have internet access to download the JWKS\n     *\n     * @param jwksThe JWKS\n     * @param issuer The issuer for which you want to cache the JWKS\n     *  Supply this field, if you instantiated the JwtVerifier with multiple issuers\n     * @returns void\n     */\n    cacheJwks(...[jwks, issuer]) {\n        const issuerConfig = this.getIssuerConfig(issuer);\n        this.jwksCache.addJwks(issuerConfig.jwksUri, jwks);\n        this.publicKeyCache.clearCache(issuerConfig.issuer);\n    }\n    /**\n     * Hydrate the JWKS cache for (all of) the configured issuer(s).\n     * This will fetch and cache the latest and greatest JWKS for concerned issuer(s).\n     *\n     * @param issuer The issuer to fetch the JWKS for\n     * @returns void\n     */\n    async hydrate() {\n        const jwksFetches = this.expectedIssuers\n            .map((issuer) => this.getIssuerConfig(issuer).jwksUri)\n            .map((jwksUri) => this.jwksCache.getJwks(jwksUri));\n        await Promise.all(jwksFetches);\n    }\n    /**\n     * Verify (synchronously) a JWT that is signed using RS256 / RS384 / RS512.\n     *\n     * @param jwt The JWT, as string\n     * @param props Verification properties\n     * @returns The payload of the JWT\u2013\u2013if the JWT is valid, otherwise an error is thrown\n     */\n    verifySync(...[jwt, properties]) {\n        const { decomposedJwt, jwksUri, verifyProperties } = this.getVerifyParameters(jwt, properties);\n        return this.verifyDecomposedJwtSync(decomposedJwt, jwksUri, verifyProperties);\n    }\n    /**\n     * Verify (synchronously) an already decomposed JWT, that is signed using RS256 / RS384 / RS512.\n     *\n     * @param decomposedJwt The decomposed Jwt\n     * @param jwk The JWK to verify the JWTs signature with\n     * @param verifyProperties The properties to use for verification\n     * @returns The payload of the JWT\u2013\u2013if the JWT is valid, otherwise an error is thrown\n     */\n    verifyDecomposedJwtSync(decomposedJwt, jwksUri, verifyProperties) {\n        const jwk = this.jwksCache.getCachedJwk(jwksUri, decomposedJwt);\n        return verifyDecomposedJwtSync(decomposedJwt, jwk, verifyProperties, this.publicKeyCache.transformJwkToKeyObjectSync.bind(this.publicKeyCache));\n    }\n    /**\n     * Verify (asynchronously) a JWT that is signed using RS256 / RS384 / RS512.\n     * This call is asynchronous, and the JWKS will be fetched from the JWKS uri,\n     * in case it is not yet available in the cache.\n     *\n     * @param jwt The JWT, as string\n     * @param props Verification properties\n     * @returns Promise that resolves to the payload of the JWT\u2013\u2013if the JWT is valid, otherwise the promise rejects\n     */\n    async verify(...[jwt, properties]) {\n        const { decomposedJwt, jwksUri, verifyProperties } = this.getVerifyParameters(jwt, properties);\n        return this.verifyDecomposedJwt(decomposedJwt, jwksUri, verifyProperties);\n    }\n    /**\n     * Verify (asynchronously) an already decomposed JWT, that is signed using RS256 / RS384 / RS512.\n     *\n     * @param decomposedJwt The decomposed Jwt\n     * @param jwk The JWK to verify the JWTs signature with\n     * @param verifyProperties The properties to use for verification\n     * @returns The payload of the JWT\u2013\u2013if the JWT is valid, otherwise an error is thrown\n     */\n    verifyDecomposedJwt(decomposedJwt, jwksUri, verifyProperties) {\n        return verifyDecomposedJwt(decomposedJwt, jwksUri, verifyProperties, this.jwksCache.getJwk.bind(this.jwksCache), this.publicKeyCache.transformJwkToKeyObjectAsync.bind(this.publicKeyCache));\n    }\n    /**\n     * Get the verification parameters to use, by merging the issuer configuration,\n     * with the overriding properties that are now provided\n     *\n     * @param jwt: the JWT that is going to be verified\n     * @param verifyProperties: the overriding properties, that override the issuer configuration\n     * @returns The merged verification parameters\n     */\n    getVerifyParameters(jwt, verifyProperties) {\n        const decomposedJwt = decomposeJwt(jwt);\n        assertStringArrayContainsString(\"Issuer\", decomposedJwt.payload.iss, this.expectedIssuers, JwtInvalidIssuerError);\n        const issuerConfig = this.getIssuerConfig(decomposedJwt.payload.iss);\n        return {\n            decomposedJwt,\n            jwksUri: issuerConfig.jwksUri,\n            verifyProperties: {\n                ...issuerConfig,\n                ...verifyProperties,\n            },\n        };\n    }\n    /**\n     * Get issuer config with JWKS URI, by adding a default JWKS URI if needed\n     *\n     * @param config: the issuer config.\n     * @returns The config with JWKS URI\n     */\n    withJwksUri(config) {\n        if (config.jwksUri) {\n            return config;\n        }\n        const issuerUri = new URL(config.issuer).pathname.replace(/\\/$/, \"\");\n        return {\n            jwksUri: new URL(`${issuerUri}/.well-known/jwks.json`, config.issuer)\n                .href,\n            ...config,\n        };\n    }\n}\n/**\n * Class representing a verifier for JWTs signed with RSA (e.g. RS256 / RS384 / RS512)\n */\nexport class JwtRsaVerifier extends JwtRsaVerifierBase {\n    // eslint-disable-next-line @typescript-eslint/explicit-module-boundary-types\n    static create(verifyProperties, additionalProperties) {\n        return new this(verifyProperties, additionalProperties?.jwksCache);\n    }\n}\n/**\n * Class representing a cache of RSA public keys in native key object format\n *\n * Because it takes a bit of compute time to turn a JWK into native key object format,\n * we want to cache this computation.\n */\nexport class KeyObjectCache {\n    constructor(transformJwkToKeyObjectSyncFn = nodeWebCompat.transformJwkToKeyObjectSync, transformJwkToKeyObjectAsyncFn = nodeWebCompat.transformJwkToKeyObjectAsync) {\n        this.transformJwkToKeyObjectSyncFn = transformJwkToKeyObjectSyncFn;\n        this.transformJwkToKeyObjectAsyncFn = transformJwkToKeyObjectAsyncFn;\n        this.publicKeys = new Map();\n    }\n    /**\n     * Transform the JWK into an RSA public key in native key object format.\n     * If the transformed JWK is already in the cache, it is returned from the cache instead.\n     *\n     * @param jwk: the JWK\n     * @param jwtHeaderAlg: the alg from the JWT header (used if absent on JWK)\n     * @param issuer: the issuer that uses the JWK for signing JWTs (used for caching the transformation)\n     * @returns the RSA public key in native key object format\n     */\n    transformJwkToKeyObjectSync(jwk, jwtHeaderAlg, issuer) {\n        const alg = jwk.alg ?? jwtHeaderAlg;\n        if (!issuer || !jwk.kid || !alg) {\n            return this.transformJwkToKeyObjectSyncFn(jwk, alg, issuer);\n        }\n        const fromCache = this.publicKeys.get(issuer)?.get(jwk.kid)?.get(alg);\n        if (fromCache)\n            return fromCache;\n        const publicKey = this.transformJwkToKeyObjectSyncFn(jwk, alg, issuer);\n        this.putKeyObjectInCache(issuer, jwk.kid, alg, publicKey);\n        return publicKey;\n    }\n    /**\n     * Transform the JWK into an RSA public key in native key object format (async).\n     * If the transformed JWK is already in the cache, it is returned from the cache instead.\n     *\n     * @param jwk: the JWK\n     * @param jwtHeaderAlg: the alg from the JWT header (used if absent on JWK)\n     * @param issuer: the issuer that uses the JWK for signing JWTs (used for caching the transformation)\n     * @returns the RSA public key in native key object format\n     */\n    async transformJwkToKeyObjectAsync(jwk, jwtHeaderAlg, issuer) {\n        const alg = jwk.alg ?? jwtHeaderAlg;\n        if (!issuer || !jwk.kid || !alg) {\n            return this.transformJwkToKeyObjectAsyncFn(jwk, alg, issuer);\n        }\n        const fromCache = this.publicKeys.get(issuer)?.get(jwk.kid)?.get(alg);\n        if (fromCache)\n            return fromCache;\n        const publicKey = await this.transformJwkToKeyObjectAsyncFn(jwk, alg, issuer);\n        this.putKeyObjectInCache(issuer, jwk.kid, alg, publicKey);\n        return publicKey;\n    }\n    putKeyObjectInCache(issuer, kid, alg, publicKey) {\n        const cachedIssuer = this.publicKeys.get(issuer);\n        const cachedIssuerKid = cachedIssuer?.get(kid);\n        if (cachedIssuerKid) {\n            cachedIssuerKid.set(alg, publicKey);\n        }\n        else if (cachedIssuer) {\n            cachedIssuer.set(kid, new Map([[alg, publicKey]]));\n        }\n        else {\n            this.publicKeys.set(issuer, new Map([[kid, new Map([[alg, publicKey]])]]));\n        }\n    }\n    clearCache(issuer) {\n        this.publicKeys.delete(issuer);\n    }\n}\n", "// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n// SPDX-License-Identifier: Apache-2.0\nimport { CognitoJwtInvalidClientIdError, CognitoJwtInvalidGroupError, CognitoJwtInvalidTokenUseError, JwtInvalidClaimError, ParameterValidationError, } from \"./error.js\";\nimport { JwtRsaVerifierBase } from \"./jwt-rsa.js\";\nimport { assertStringArrayContainsString, assertStringEquals, assertStringArraysOverlap, } from \"./assert.js\";\n/**\n * Validate claims of a decoded Cognito JWT.\n * This function throws an error in case there's any validation issue.\n *\n * @param payload - The JSON parsed payload of the Cognito JWT\n * @param options - Validation options\n * @param options.groups - The cognito groups, of which at least one must be present in the JWT's cognito:groups claim\n * @param options.tokenUse - The required token use of the JWT: \"id\" or \"access\"\n * @param options.clientId - The required clientId of the JWT. May be an array of string, of which at least one must match\n * @returns void\n */\nexport function validateCognitoJwtFields(payload, options) {\n    // Check groups\n    if (options.groups != null) {\n        assertStringArraysOverlap(\"Cognito group\", payload[\"cognito:groups\"], options.groups, CognitoJwtInvalidGroupError);\n    }\n    // Check token use\n    assertStringArrayContainsString(\"Token use\", payload.token_use, [\"id\", \"access\"], CognitoJwtInvalidTokenUseError);\n    if (options.tokenUse !== null) {\n        if (options.tokenUse === undefined) {\n            throw new ParameterValidationError(\"tokenUse must be provided or set to null explicitly\");\n        }\n        assertStringEquals(\"Token use\", payload.token_use, options.tokenUse, CognitoJwtInvalidTokenUseError);\n    }\n    // Check clientId aka audience\n    if (options.clientId !== null) {\n        if (options.clientId === undefined) {\n            throw new ParameterValidationError(\"clientId must be provided or set to null explicitly\");\n        }\n        if (payload.token_use === \"id\") {\n            assertStringArrayContainsString('Client ID (\"audience\")', payload.aud, options.clientId, CognitoJwtInvalidClientIdError);\n        }\n        else {\n            assertStringArrayContainsString(\"Client ID\", payload.client_id, options.clientId, CognitoJwtInvalidClientIdError);\n        }\n    }\n}\n/**\n * Class representing a verifier for JWTs signed by Amazon Cognito\n */\nexport class CognitoJwtVerifier extends JwtRsaVerifierBase {\n    constructor(props, jwksCache) {\n        const issuerConfig = Array.isArray(props)\n            ? props.map((p) => ({\n                ...p,\n                ...CognitoJwtVerifier.parseUserPoolId(p.userPoolId),\n                audience: null, // checked instead by validateCognitoJwtFields\n            }))\n            : {\n                ...props,\n                ...CognitoJwtVerifier.parseUserPoolId(props.userPoolId),\n                audience: null, // checked instead by validateCognitoJwtFields\n            };\n        super(issuerConfig, jwksCache);\n    }\n    /**\n     * Parse a User Pool ID, to extract the issuer and JWKS URI\n     *\n     * @param userPoolId The User Pool ID\n     * @returns The issuer and JWKS URI for the User Pool\n     */\n    static parseUserPoolId(userPoolId) {\n        // Disable safe regexp check as userPoolId is provided by developer, i.e. is not user input\n        // eslint-disable-next-line security/detect-unsafe-regex\n        const match = userPoolId.match(/^(?<region>(\\w+-)?\\w+-\\w+-\\d)+_\\w+$/);\n        if (!match) {\n            throw new ParameterValidationError(`Invalid Cognito User Pool ID: ${userPoolId}`);\n        }\n        const region = match.groups.region;\n        const issuer = `https://cognito-idp.${region}.amazonaws.com/${userPoolId}`;\n        return {\n            issuer,\n            jwksUri: `${issuer}/.well-known/jwks.json`,\n        };\n    }\n    // eslint-disable-next-line @typescript-eslint/explicit-module-boundary-types\n    static create(verifyProperties, additionalProperties) {\n        return new this(verifyProperties, additionalProperties?.jwksCache);\n    }\n    /**\n     * Verify (synchronously) a JWT that is signed by Amazon Cognito.\n     *\n     * @param jwt The JWT, as string\n     * @param props Verification properties\n     * @returns The payload of the JWT\u2013\u2013if the JWT is valid, otherwise an error is thrown\n     */\n    verifySync(...[jwt, properties]) {\n        const { decomposedJwt, jwksUri, verifyProperties } = this.getVerifyParameters(jwt, properties);\n        this.verifyDecomposedJwtSync(decomposedJwt, jwksUri, verifyProperties);\n        try {\n            validateCognitoJwtFields(decomposedJwt.payload, verifyProperties);\n        }\n        catch (err) {\n            if (verifyProperties.includeRawJwtInErrors &&\n                err instanceof JwtInvalidClaimError) {\n                throw err.withRawJwt(decomposedJwt);\n            }\n            throw err;\n        }\n        return decomposedJwt.payload;\n    }\n    /**\n     * Verify (asynchronously) a JWT that is signed by Amazon Cognito.\n     * This call is asynchronous, and the JWKS will be fetched from the JWKS uri,\n     * in case it is not yet available in the cache.\n     *\n     * @param jwt The JWT, as string\n     * @param props Verification properties\n     * @returns Promise that resolves to the payload of the JWT\u2013\u2013if the JWT is valid, otherwise the promise rejects\n     */\n    async verify(...[jwt, properties]) {\n        const { decomposedJwt, jwksUri, verifyProperties } = this.getVerifyParameters(jwt, properties);\n        await this.verifyDecomposedJwt(decomposedJwt, jwksUri, verifyProperties);\n        try {\n            validateCognitoJwtFields(decomposedJwt.payload, verifyProperties);\n        }\n        catch (err) {\n            if (verifyProperties.includeRawJwtInErrors &&\n                err instanceof JwtInvalidClaimError) {\n                throw err.withRawJwt(decomposedJwt);\n            }\n            throw err;\n        }\n        return decomposedJwt.payload;\n    }\n    /**\n     * This method loads a JWKS that you provide, into the JWKS cache, so that it is\n     * available for JWT verification. Use this method to speed up the first JWT verification\n     * (when the JWKS would otherwise have to be downloaded from the JWKS uri), or to provide the JWKS\n     * in case the JwtVerifier does not have internet access to download the JWKS\n     *\n     * @param jwks The JWKS\n     * @param userPoolId The userPoolId for which you want to cache the JWKS\n     *  Supply this field, if you instantiated the CognitoJwtVerifier with multiple userPoolIds\n     * @returns void\n     */\n    cacheJwks(...[jwks, userPoolId]) {\n        let issuer;\n        if (userPoolId !== undefined) {\n            issuer = CognitoJwtVerifier.parseUserPoolId(userPoolId).issuer;\n        }\n        else if (this.expectedIssuers.length > 1) {\n            throw new ParameterValidationError(\"userPoolId must be provided\");\n        }\n        const issuerConfig = this.getIssuerConfig(issuer);\n        super.cacheJwks(jwks, issuerConfig.issuer);\n    }\n}\n", "// GLOBAL\n\nexport interface AnyObject {\n  [key: string]: any;\n}\n\nexport interface CompletionData {\n  [key: string]: boolean | CompletionData;\n}\n\n/**\n * Abridged copy of the type used by `aws-lambda@1.0.7` (from `@types/aws-lambda@8.10.88`)\n * We only this package for these types, and we use only a subset of the\n * properties. Since `aws-lambda` depends on `aws-sdk` (that is, SDK v2),\n * we can save ourselves a big dependency with this small redundancy.\n */\n\nexport interface APIGatewayProxyEventPathParameters {\n  [name: string]: string | undefined;\n}\n\nexport interface APIGatewayProxyEvent {\n  body: string | null;\n  headers: Record<string, string | undefined>;\n  multiValueHeaders: Record<string, string | undefined>;\n  httpMethod: string;\n  isBase64Encoded: boolean;\n  path: string;\n  pathParameters: Record<string, string | undefined> | null;\n  queryStringParameters: Record<string, string | undefined> | null;\n  multiValueQueryStringParameters: Record<string, string | undefined> | null;\n  stageVariables: Record<string, string | undefined> | null;\n  /** The context is complicated, and we don't (as of 2023) use it at all. */\n  requestContext: any;\n  resource: string;\n}\n\n// ALERTS\n\nexport enum AlertTypes {\n  ERROR = \"error\",\n  INFO = \"info\",\n  SUCCESS = \"success\",\n  WARNING = \"warning\",\n}\n\n// TIME\n\nexport interface DateShape {\n  year: number;\n  month: number;\n  day: number;\n}\n\nexport interface TimeShape {\n  hour: number;\n  minute: number;\n  second: number;\n}\n\n// OTHER\n\nexport interface CustomHtmlElement {\n  type: string;\n  content: string | any;\n  as?: string;\n  props?: AnyObject;\n}\n\nexport interface ErrorVerbiage {\n  title: string;\n  description: string | CustomHtmlElement[];\n}\n\nconst states = [\n  \"AL\",\n  \"AK\",\n  \"AS\", // American Samoa\n  \"AZ\",\n  \"AR\",\n  \"CA\",\n  \"CO\",\n  \"CT\",\n  \"DE\",\n  \"DC\",\n  \"FL\",\n  \"FM\", // Federated States of Micronesia\n  \"GA\",\n  \"GU\", // Guam\n  \"HI\",\n  \"ID\",\n  \"IL\",\n  \"IN\",\n  \"IA\",\n  \"KS\",\n  \"KY\",\n  \"LA\",\n  \"ME\",\n  \"MH\", // Marshall Islands\n  \"MD\",\n  \"MA\",\n  \"MI\",\n  \"MN\",\n  \"MS\",\n  \"MO\",\n  \"MP\", // Northern Mariana Islands\n  \"MT\",\n  \"NE\",\n  \"NV\",\n  \"NH\",\n  \"NJ\",\n  \"NM\",\n  \"NY\",\n  \"NC\",\n  \"ND\",\n  \"OH\",\n  \"OK\",\n  \"OR\",\n  \"PA\",\n  \"PR\",\n  \"PW\", // Palau\n  \"RI\",\n  \"SC\",\n  \"SD\",\n  \"TN\",\n  \"TX\",\n  \"UT\",\n  \"VT\",\n  \"VA\",\n  \"VI\", // Virgin Islands\n  \"WA\",\n  \"WV\",\n  \"WI\",\n  \"WY\",\n] as const;\nexport type State = typeof states[number];\n\nexport const isState = (state: unknown): state is State => {\n  return states.includes(state as State);\n};\n\nexport interface FormTemplateVersion {\n  md5Hash: string;\n  versionNumber: number;\n  id: string;\n  lastAltered: string;\n  reportType: string;\n}\n\nexport const enum TemplateKeys {\n  WP = \"templates/MFP-Work-Plan-Help-File.pdf\",\n  SAR = \"templates/MFP-Semi-Annual-Rprt-Help-File.pdf\",\n}\n\n/**\n * S3Create event\n * https://docs.aws.amazon.com/AmazonS3/latest/dev/notification-content-structure.html\n */\n\nexport interface S3EventRecordGlacierRestoreEventData {\n  lifecycleRestorationExpiryTime: string;\n  lifecycleRestoreStorageClass: string;\n}\nexport interface S3EventRecordGlacierEventData {\n  restoreEventData: S3EventRecordGlacierRestoreEventData;\n}\n\nexport interface S3EventRecord {\n  eventVersion: string;\n  eventSource: string;\n  awsRegion: string;\n  eventTime: string;\n  eventName: string;\n  userIdentity: {\n    principalId: string;\n  };\n  requestParameters: {\n    sourceIPAddress: string;\n  };\n  responseElements: {\n    \"x-amz-request-id\": string;\n    \"x-amz-id-2\": string;\n  };\n  s3: {\n    s3SchemaVersion: string;\n    configurationId: string;\n    bucket: {\n      name: string;\n      ownerIdentity: {\n        principalId: string;\n      };\n      arn: string;\n    };\n    object: {\n      key: string;\n      size: number;\n      eTag: string;\n      versionId?: string | undefined;\n      sequencer: string;\n    };\n  };\n  glacierEventData?: S3EventRecordGlacierEventData | undefined;\n}\n\n/**\n * Use this type to create a type guard for filtering arrays of objects\n * by the presence of certain attributes.\n *\n * @example\n * interface Foo {\n *    bar: string;\n *    baz?: string;\n *    buzz?: string;\n *    bizz?: string;\n * }\n * type RequireBaz = SomeRequired<Foo, 'baz'>\n * const array: Foo[] = [\n *  { bar: 'always here' },\n *  { bar: 'always here', baz: 'sometimes here' }\n * ]\n * array.filter((f): f is RequireBaz => typeof f.baz !== 'undefined' )\n * // `array`'s type now shows bar and baz as required.\n * array.map((f) => return f.baz)\n */\nexport type SomeRequired<T, K extends keyof T> = Required<Pick<T, K>> &\n  Omit<T, K>;\n\n/**\n * Instructs Typescript to complain if it detects that this function may be reachable.\n * Useful for the default branch of a switch statement that verifiably covers every case.\n */\nexport const assertExhaustive = (_: never): void => {};\n", "import { FormJson } from \"./formFields\";\nimport { AnyObject, CustomHtmlElement } from \"./other\";\n\n// REPORT STRUCTURE\n\nexport interface ReportJson {\n  id?: string;\n  type: ReportType;\n  name: string;\n  basePath: string;\n  routes: ReportRoute[];\n  validationSchema?: AnyObject;\n  /**\n   * The validationJson property is populated at the moment any form template\n   * is stored in S3 for the first time. It will be populated from that moment on.\n   */\n  validationJson?: AnyObject;\n}\n\nexport type ReportRoute = ReportRouteWithForm | ReportRouteWithoutForm;\n\nexport interface ReportRouteBase {\n  name: string;\n  path: string;\n  pageType?: string;\n  conditionallyRender?: string;\n}\n\nexport type ReportRouteWithForm =\n  | StandardReportPageShape\n  | DrawerReportPageShape\n  | ModalDrawerReportPageShape\n  | ModalOverlayReportPageShape\n  | OverlayModalPageShape\n  | EntityDetailsOverlayShape\n  | DynamicModalOverlayReportPageShape;\n\nexport interface ReportPageShapeBase extends ReportRouteBase {\n  children?: never;\n  verbiage: ReportPageVerbiage;\n}\n\nexport interface StandardReportPageShape extends ReportPageShapeBase {\n  form: FormJson;\n  dashboard?: never;\n  modalForm?: never;\n  drawerForm?: never;\n  entityType?: never;\n  entitySteps?: never;\n  template?: never;\n  initiatives?: never;\n  objectiveCards?: never;\n}\n\nexport interface DrawerReportPageShape extends ReportPageShapeBase {\n  entityType: string;\n  verbiage: DrawerReportPageVerbiage;\n  drawerForm: FormJson;\n  modalForm?: never;\n  form?: never;\n  entitySteps?: never;\n  dashboard?: never;\n  template?: never;\n  initiatives?: never;\n  objectiveCards?: never;\n}\n\nexport interface ModalDrawerReportPageShape extends ReportPageShapeBase {\n  entityType: string;\n  verbiage: ModalDrawerReportPageVerbiage;\n  modalForm: FormJson;\n  drawerForm: FormJson;\n  form?: never;\n  entitySteps?: never;\n  dashboard?: never;\n  template?: never;\n  initiatives?: never;\n  objectiveCards?: never;\n}\n\nexport interface ModalOverlayReportPageShape extends ReportPageShapeBase {\n  initiativeId: string | undefined;\n  entityType: string;\n  entityInfo?: string[];\n  verbiage: ModalOverlayReportPageVerbiage;\n  modalForm: FormJson;\n  drawerForm?: never;\n  form?: never;\n  dashboard: EntityDetailsDashboardOverlayShape;\n  entitySteps?: (EntityDetailsOverlayShape | OverlayModalPageShape)[];\n  template?: never;\n  initiatives?: never;\n  objectiveCards?: never;\n}\n\nexport interface DynamicModalOverlayReportPageShape\n  extends ReportPageShapeBase {\n  entityType: string;\n  entityInfo: string[];\n  verbiage: ModalOverlayReportPageVerbiage;\n  drawerForm?: never;\n  modalForm?: never;\n  form?: never;\n  entitySteps?: never;\n  dashboard?: never;\n  initiatives: {\n    initiativeId: string;\n    name: string;\n    topic: string;\n    dashboard: FormJson;\n    entitySteps: (EntityDetailsOverlayShape | OverlayModalPageShape)[];\n  }[];\n  objectiveCards?: never;\n  /** Only used during form template transformation; will be absent after transformation */\n  template?: AnyObject;\n}\n\nexport interface OverlayModalPageShape extends ReportPageShapeBase {\n  entityType: string;\n  stepName: string;\n  hint: string;\n  verbiage: ModalOverlayReportPageVerbiage;\n  modalForm: FormJson;\n  drawerForm?: FormJson;\n  form?: never;\n  entitySteps?: never;\n  dashboard?: never;\n  template?: never;\n  initiatives?: never;\n  objectiveCards?: {\n    modalForm?: FormJson;\n  }[];\n}\n\nexport interface EntityDetailsOverlayShape extends ReportPageShapeBase {\n  stepName: string;\n  hint: string;\n  form: FormJson;\n  verbiage: EntityOverlayPageVerbiage;\n  entityType?: never;\n  dashboard?: never;\n  modalForm?: never;\n  drawerForm?: never;\n  entitySteps?: never;\n  template?: never;\n  initiatives?: never;\n  objectiveCards?: {\n    modalForm?: FormJson;\n  }[];\n}\n\nexport interface EntityDetailsDashboardOverlayShape\n  extends ReportPageShapeBase {\n  dashboard?: never;\n  template?: never;\n  initiatives?: never;\n  objectiveCards?: never;\n}\n\nexport interface ReportRouteWithoutForm extends ReportRouteBase {\n  children?: ReportRoute[];\n  pageType?: string;\n  entityType?: never;\n  verbiage?: never;\n  modalForm?: never;\n  drawerForm?: never;\n  form?: never;\n  entitySteps?: never;\n  dashboard?: never;\n  template?: never;\n  initiatives?: never;\n  objectiveCards?: never;\n}\n\nexport interface ReportPageVerbiage {\n  intro: {\n    section: string;\n    subsection?: string;\n    hint?: string;\n    info?: string | CustomHtmlElement[];\n  };\n  closeOutWarning?: AnyObject;\n  closeOutModal?: AnyObject;\n}\n\nexport interface DrawerReportPageVerbiage extends ReportPageVerbiage {\n  dashboardTitle: string;\n  countEntitiesInTitle?: boolean;\n  drawerTitle: string;\n  drawerInfo?: CustomHtmlElement[];\n  missingEntityMessage?: CustomHtmlElement[];\n}\n\nexport interface ModalDrawerReportPageVerbiage\n  extends DrawerReportPageVerbiage {\n  addEntityButtonText: string;\n  editEntityButtonText: string;\n  readOnlyEntityButtonText: string;\n  addEditModalAddTitle: string;\n  addEditModalEditTitle: string;\n  addEditModalMessage: string;\n  deleteEntityButtonAltText: string;\n  deleteModalTitle: string;\n  deleteModalConfirmButtonText: string;\n  deleteModalWarning: string;\n  entityUnfinishedMessage: string;\n  enterEntityDetailsButtonText: string;\n  readOnlyEntityDetailsButtonText: string;\n  editEntityDetailsButtonText: string;\n}\n\nexport interface ModalOverlayReportPageVerbiage\n  extends EntityOverlayPageVerbiage {\n  addEntityButtonText: string;\n  dashboardTitle: string;\n  countEntitiesInTitle: boolean;\n  tableHeader: string;\n  addEditModalHint: string;\n  emptyDashboardText: string;\n}\n\nexport interface EntityOverlayPageVerbiage extends ReportPageVerbiage {\n  closeOutWarning?: {\n    title?: string;\n    description?: string;\n  };\n  closeOutModal?: {\n    closeOutModalButtonText?: string;\n    closeOutModalTitle?: string;\n    closeOutModalBodyText?: string;\n    closeOutModalConfirmButtonText?: string;\n  };\n}\n\nexport enum ReportType {\n  WP = \"WP\",\n  SAR = \"SAR\",\n}\n/**\n * Check if unknown value is a report type\n *\n * @param reportType possible report type value\n * @returns type assertion for value\n */\nexport function isReportType(reportType: unknown): reportType is ReportType {\n  return Object.values(ReportType).includes(reportType as ReportType);\n}\n", "// USERS\n\nexport enum UserRoles {\n  ADMIN = \"mdctmfp-bor\", // \"MDCT MFP Business Owner Representative\"\n  HELP_DESK = \"mdctmfp-help-desk\", // \"MDCT MFP Help Desk\"\n  INTERNAL = \"mdctmfp-cms-internal-user\", // \"MDCT MFP Internal User\"\n  APPROVER = \"mdctmfp-approver\", // \"MDCT MFP Approver\"\n  STATE_USER = \"mdctmfp-state-user\", // \"MDCT MFP State User\"\n}\n\nexport interface MFPUser {\n  email: string;\n  given_name: string;\n  family_name: string;\n  full_name: string;\n  state?: string;\n  userRole?: string;\n  userIsAdmin?: boolean;\n  userIsReadOnly?: boolean;\n  userIsEndUser?: boolean;\n}\n\nexport interface UserContextShape {\n  user?: MFPUser;\n  showLocalLogins?: boolean;\n  logout: () => Promise<void>;\n  loginWithIDM: () => void;\n  updateTimeout: () => void;\n  getExpiration: Function;\n}\n", "/**\n * The response for a successful request.\n * Should include a body for GET, PUT, or POST.\n * Need not include a body for DELETE\n */\nexport const ok = (body?: Object) => new HttpResponse(StatusCodes.Ok, body);\n\n/**\n * The response for a successful POST or PUT request,\n * which resulted in the creation of a new resource.\n */\nexport const created = (body: Object) =>\n  new HttpResponse(StatusCodes.Created, body);\n\n/**\n * The response for a successful request which returns no content.\n */\nexport const noContent = (body?: Object) =>\n  new HttpResponse(StatusCodes.NoContent, body);\n\n/**\n * The response for a failed request, due to client-side issues.\n * Typically indicates a missing parameter or malformed body.\n */\nexport const badRequest = (body?: Object) =>\n  new HttpResponse(StatusCodes.BadRequest, body);\n\n/**\n * The response for a client without any authorization.\n * Typically indicates an issue with the request's headers or token.\n *\n * Note: The usual name for HTTP 401 is \"Unauthorized\", but that's misleading.\n * Authentication is for identity; authorization is for permissions.\n */\nexport const unauthenticated = (body?: Object) =>\n  new HttpResponse(StatusCodes.Unauthenticated, body);\n\n/**\n * The response for a client without sufficient permissions.\n * This is specific to the requested operation.\n * For example, a regular user requesting an admin-only endpoint.\n */\nexport const forbidden = (body?: Object) =>\n  new HttpResponse(StatusCodes.Forbidden, body);\n\n/**\n * The response for a request that assumes the existence of a missing resource.\n * For example, attempting to submit a report that isn't in the database.\n */\nexport const notFound = (body?: Object) =>\n  new HttpResponse(StatusCodes.NotFound, body);\n\n/**\n * The response for a request that assumes the server is in a different state.\n * For example, attempting to submit a report that's already submitted.\n */\nexport const conflict = (body?: Object) =>\n  new HttpResponse(StatusCodes.Conflict, body);\n\n/**\n * The response for a request that errored out on the server side.\n * Typically indicates there is nothing the client can do to resolve the issue.\n */\nexport const internalServerError = (body?: Object) =>\n  new HttpResponse(StatusCodes.InternalServerError, body);\n\n/**\n * Note: Production code shouldn't need to reference this directly.\n * Use a helper method instead.\n *\n * This enum is listed mainly for the purpose of unit testing.\n */\nexport enum StatusCodes {\n  Ok = 200,\n  Created = 201,\n  NoContent = 204,\n  BadRequest = 400,\n  Unauthenticated = 401,\n  Forbidden = 403,\n  NotFound = 404,\n  Conflict = 409,\n  InternalServerError = 500,\n}\n\n/**\n * Note: Production code shouldn't need to reference this directly.\n * Use a helper method instead.\n */\nexport class HttpResponse {\n  readonly statusCode: number;\n  readonly body: string | undefined;\n  readonly headers = {\n    \"Access-Control-Allow-Origin\": \"*\",\n    \"Access-Control-Allow-Credentials\": true,\n  };\n  constructor(statusCode: number, body?: Object | undefined) {\n    this.statusCode = statusCode;\n    if (body !== undefined) {\n      this.body = JSON.stringify(body);\n    }\n  }\n}\n", "import { ReportType } from \"../types/reports\";\n\nexport const error = {\n  // generic errors\n  UNAUTHORIZED: \"User is not authorized to access this resource.\",\n  NO_KEY: \"Must provide key for table.\",\n  MISSING_DATA: \"Missing required data.\",\n  INVALID_DATA: \"Provided data is not valid.\",\n  NO_MATCHING_RECORD: \"No matching record found.\",\n  SERVER_ERROR: \"An unspecified server error occured.\",\n  // bucket errors\n  S3_OBJECT_CREATION_ERROR: \"Report could not be created due to an S3 error.\",\n  S3_OBJECT_UPDATE_ERROR: \"Report could not be updated due to an S3 error.\",\n  S3_OBJECT_GET_ERROR: \"Error while fetching report.\",\n  // dynamo errors\n  DYNAMO_CREATION_ERROR: \"Report could not be created due to a database error.\",\n  DYNAMO_UPDATE_ERROR: \"Report could not be updated due to a database error.\",\n  // template errors\n  NO_TEMPLATE_NAME: \"Must request template for download.\",\n  INVALID_TEMPLATE_NAME: \"Requested template does not exist or does not match.\",\n  NOT_IN_DATABASE: \"Record not found in database.\",\n  UNABLE_TO_COPY:\n    \"Unable to copy over report if todays date is in the same period and year as a previous report.\",\n  MISSING_FORM_TEMPLATE: \"Form Template not found in S3.\",\n  MISSING_FIELD_DATA: \"Field Data not found in S3.\",\n  NO_WORKPLANS_FOUND: \"No record of Work Plans found in database\",\n  // admin action errors\n  ALREADY_ARCHIVED: \"Cannot update archived report.\",\n  ALREADY_LOCKED: \"Cannot update locked report.\",\n  REPORT_INCOMPLETE: \"Cannot submit incomplete form.\",\n} as const;\n\nexport const buckets = {\n  FORM_TEMPLATE: \"formTemplates\",\n  FIELD_DATA: \"fieldData\",\n};\n\n// STATES\nexport enum States {\n  AL = \"Alabama\",\n  AK = \"Alaska\",\n  AS = \"American Samoa\",\n  AZ = \"Arizona\",\n  AR = \"Arkansas\",\n  CA = \"California\",\n  CO = \"Colorado\",\n  CT = \"Connecticut\",\n  DE = \"Delaware\",\n  DC = \"District of Columbia\",\n  FM = \"Federated States of Micronesia\",\n  FL = \"Florida\",\n  GA = \"Georgia\",\n  GU = \"Guam\",\n  HI = \"Hawaii\",\n  ID = \"Idaho\",\n  IL = \"Illinois\",\n  IN = \"Indiana\",\n  IA = \"Iowa\",\n  KS = \"Kansas\",\n  KY = \"Kentucky\",\n  LA = \"Louisiana\",\n  ME = \"Maine\",\n  MH = \"Marshall Islands\",\n  MD = \"Maryland\",\n  MA = \"Massachusetts\",\n  MI = \"Michigan\",\n  MN = \"Minnesota\",\n  MS = \"Mississippi\",\n  MO = \"Missouri\",\n  MT = \"Montana\",\n  NE = \"Nebraska\",\n  NV = \"Nevada\",\n  NH = \"New Hampshire\",\n  NJ = \"New Jersey\",\n  NM = \"New Mexico\",\n  NY = \"New York\",\n  NC = \"North Carolina\",\n  ND = \"North Dakota\",\n  MP = \"Northern Mariana Islands\",\n  OH = \"Ohio\",\n  OK = \"Oklahoma\",\n  OR = \"Oregon\",\n  PW = \"Palau\",\n  PA = \"Pennsylvania\",\n  PR = \"Puerto Rico\",\n  RI = \"Rhode Island\",\n  SC = \"South Carolina\",\n  SD = \"South Dakota\",\n  TN = \"Tennessee\",\n  TX = \"Texas\",\n  UT = \"Utah\",\n  VT = \"Vermont\",\n  VI = \"Virgin Islands\",\n  VA = \"Virginia\",\n  WA = \"Washington\",\n  WV = \"West Virginia\",\n  WI = \"Wisconsin\",\n  WY = \"Wyoming\",\n}\n\n// REPORTS\n\nexport const reportTables: { [key in ReportType]: string } = {\n  SAR: process.env.SarReportsTable!,\n  WP: process.env.WpReportsTable!,\n};\n\nexport const reportBuckets: { [key in ReportType]: string } = {\n  SAR: process.env.SAR_FORM_BUCKET!,\n  WP: process.env.WP_FORM_BUCKET!,\n};\n\nexport const reportNames: { [key in ReportType]: string } = {\n  SAR: \"SAR\",\n  WP: \"Work Plan\",\n};\n\nexport const tableTopics: { [key in ReportType]: string } = {\n  SAR: \"sar-reports\",\n  WP: \"wp-reports\",\n};\n\nexport const bucketTopics: { [key in ReportType]: string } = {\n  SAR: \"sar-form\",\n  WP: \"wp-form\",\n};\n\nexport const DEFAULT_TARGET_POPULATION_NAMES = [\n  \"Older adults\",\n  \"Individuals with physical disabilities (PD)\",\n  \"Individuals with intellectual and developmental disabilities (I/DD)\",\n  \"Individuals with mental health and substance use disorders (MH/SUD)\",\n  \"HCBS infrastructure/system-level development\",\n];\n", "/*! @license DOMPurify 3.2.4 | (c) Cure53 and other contributors | Released under the Apache license 2.0 and Mozilla Public License 2.0 | github.com/cure53/DOMPurify/blob/3.2.4/LICENSE */\n\nconst {\n  entries,\n  setPrototypeOf,\n  isFrozen,\n  getPrototypeOf,\n  getOwnPropertyDescriptor\n} = Object;\nlet {\n  freeze,\n  seal,\n  create\n} = Object; // eslint-disable-line import/no-mutable-exports\nlet {\n  apply,\n  construct\n} = typeof Reflect !== 'undefined' && Reflect;\nif (!freeze) {\n  freeze = function freeze(x) {\n    return x;\n  };\n}\nif (!seal) {\n  seal = function seal(x) {\n    return x;\n  };\n}\nif (!apply) {\n  apply = function apply(fun, thisValue, args) {\n    return fun.apply(thisValue, args);\n  };\n}\nif (!construct) {\n  construct = function construct(Func, args) {\n    return new Func(...args);\n  };\n}\nconst arrayForEach = unapply(Array.prototype.forEach);\nconst arrayLastIndexOf = unapply(Array.prototype.lastIndexOf);\nconst arrayPop = unapply(Array.prototype.pop);\nconst arrayPush = unapply(Array.prototype.push);\nconst arraySplice = unapply(Array.prototype.splice);\nconst stringToLowerCase = unapply(String.prototype.toLowerCase);\nconst stringToString = unapply(String.prototype.toString);\nconst stringMatch = unapply(String.prototype.match);\nconst stringReplace = unapply(String.prototype.replace);\nconst stringIndexOf = unapply(String.prototype.indexOf);\nconst stringTrim = unapply(String.prototype.trim);\nconst objectHasOwnProperty = unapply(Object.prototype.hasOwnProperty);\nconst regExpTest = unapply(RegExp.prototype.test);\nconst typeErrorCreate = unconstruct(TypeError);\n/**\n * Creates a new function that calls the given function with a specified thisArg and arguments.\n *\n * @param func - The function to be wrapped and called.\n * @returns A new function that calls the given function with a specified thisArg and arguments.\n */\nfunction unapply(func) {\n  return function (thisArg) {\n    for (var _len = arguments.length, args = new Array(_len > 1 ? _len - 1 : 0), _key = 1; _key < _len; _key++) {\n      args[_key - 1] = arguments[_key];\n    }\n    return apply(func, thisArg, args);\n  };\n}\n/**\n * Creates a new function that constructs an instance of the given constructor function with the provided arguments.\n *\n * @param func - The constructor function to be wrapped and called.\n * @returns A new function that constructs an instance of the given constructor function with the provided arguments.\n */\nfunction unconstruct(func) {\n  return function () {\n    for (var _len2 = arguments.length, args = new Array(_len2), _key2 = 0; _key2 < _len2; _key2++) {\n      args[_key2] = arguments[_key2];\n    }\n    return construct(func, args);\n  };\n}\n/**\n * Add properties to a lookup table\n *\n * @param set - The set to which elements will be added.\n * @param array - The array containing elements to be added to the set.\n * @param transformCaseFunc - An optional function to transform the case of each element before adding to the set.\n * @returns The modified set with added elements.\n */\nfunction addToSet(set, array) {\n  let transformCaseFunc = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : stringToLowerCase;\n  if (setPrototypeOf) {\n    // Make 'in' and truthy checks like Boolean(set.constructor)\n    // independent of any properties defined on Object.prototype.\n    // Prevent prototype setters from intercepting set as a this value.\n    setPrototypeOf(set, null);\n  }\n  let l = array.length;\n  while (l--) {\n    let element = array[l];\n    if (typeof element === 'string') {\n      const lcElement = transformCaseFunc(element);\n      if (lcElement !== element) {\n        // Config presets (e.g. tags.js, attrs.js) are immutable.\n        if (!isFrozen(array)) {\n          array[l] = lcElement;\n        }\n        element = lcElement;\n      }\n    }\n    set[element] = true;\n  }\n  return set;\n}\n/**\n * Clean up an array to harden against CSPP\n *\n * @param array - The array to be cleaned.\n * @returns The cleaned version of the array\n */\nfunction cleanArray(array) {\n  for (let index = 0; index < array.length; index++) {\n    const isPropertyExist = objectHasOwnProperty(array, index);\n    if (!isPropertyExist) {\n      array[index] = null;\n    }\n  }\n  return array;\n}\n/**\n * Shallow clone an object\n *\n * @param object - The object to be cloned.\n * @returns A new object that copies the original.\n */\nfunction clone(object) {\n  const newObject = create(null);\n  for (const [property, value] of entries(object)) {\n    const isPropertyExist = objectHasOwnProperty(object, property);\n    if (isPropertyExist) {\n      if (Array.isArray(value)) {\n        newObject[property] = cleanArray(value);\n      } else if (value && typeof value === 'object' && value.constructor === Object) {\n        newObject[property] = clone(value);\n      } else {\n        newObject[property] = value;\n      }\n    }\n  }\n  return newObject;\n}\n/**\n * This method automatically checks if the prop is function or getter and behaves accordingly.\n *\n * @param object - The object to look up the getter function in its prototype chain.\n * @param prop - The property name for which to find the getter function.\n * @returns The getter function found in the prototype chain or a fallback function.\n */\nfunction lookupGetter(object, prop) {\n  while (object !== null) {\n    const desc = getOwnPropertyDescriptor(object, prop);\n    if (desc) {\n      if (desc.get) {\n        return unapply(desc.get);\n      }\n      if (typeof desc.value === 'function') {\n        return unapply(desc.value);\n      }\n    }\n    object = getPrototypeOf(object);\n  }\n  function fallbackValue() {\n    return null;\n  }\n  return fallbackValue;\n}\n\nconst html$1 = freeze(['a', 'abbr', 'acronym', 'address', 'area', 'article', 'aside', 'audio', 'b', 'bdi', 'bdo', 'big', 'blink', 'blockquote', 'body', 'br', 'button', 'canvas', 'caption', 'center', 'cite', 'code', 'col', 'colgroup', 'content', 'data', 'datalist', 'dd', 'decorator', 'del', 'details', 'dfn', 'dialog', 'dir', 'div', 'dl', 'dt', 'element', 'em', 'fieldset', 'figcaption', 'figure', 'font', 'footer', 'form', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'head', 'header', 'hgroup', 'hr', 'html', 'i', 'img', 'input', 'ins', 'kbd', 'label', 'legend', 'li', 'main', 'map', 'mark', 'marquee', 'menu', 'menuitem', 'meter', 'nav', 'nobr', 'ol', 'optgroup', 'option', 'output', 'p', 'picture', 'pre', 'progress', 'q', 'rp', 'rt', 'ruby', 's', 'samp', 'section', 'select', 'shadow', 'small', 'source', 'spacer', 'span', 'strike', 'strong', 'style', 'sub', 'summary', 'sup', 'table', 'tbody', 'td', 'template', 'textarea', 'tfoot', 'th', 'thead', 'time', 'tr', 'track', 'tt', 'u', 'ul', 'var', 'video', 'wbr']);\nconst svg$1 = freeze(['svg', 'a', 'altglyph', 'altglyphdef', 'altglyphitem', 'animatecolor', 'animatemotion', 'animatetransform', 'circle', 'clippath', 'defs', 'desc', 'ellipse', 'filter', 'font', 'g', 'glyph', 'glyphref', 'hkern', 'image', 'line', 'lineargradient', 'marker', 'mask', 'metadata', 'mpath', 'path', 'pattern', 'polygon', 'polyline', 'radialgradient', 'rect', 'stop', 'style', 'switch', 'symbol', 'text', 'textpath', 'title', 'tref', 'tspan', 'view', 'vkern']);\nconst svgFilters = freeze(['feBlend', 'feColorMatrix', 'feComponentTransfer', 'feComposite', 'feConvolveMatrix', 'feDiffuseLighting', 'feDisplacementMap', 'feDistantLight', 'feDropShadow', 'feFlood', 'feFuncA', 'feFuncB', 'feFuncG', 'feFuncR', 'feGaussianBlur', 'feImage', 'feMerge', 'feMergeNode', 'feMorphology', 'feOffset', 'fePointLight', 'feSpecularLighting', 'feSpotLight', 'feTile', 'feTurbulence']);\n// List of SVG elements that are disallowed by default.\n// We still need to know them so that we can do namespace\n// checks properly in case one wants to add them to\n// allow-list.\nconst svgDisallowed = freeze(['animate', 'color-profile', 'cursor', 'discard', 'font-face', 'font-face-format', 'font-face-name', 'font-face-src', 'font-face-uri', 'foreignobject', 'hatch', 'hatchpath', 'mesh', 'meshgradient', 'meshpatch', 'meshrow', 'missing-glyph', 'script', 'set', 'solidcolor', 'unknown', 'use']);\nconst mathMl$1 = freeze(['math', 'menclose', 'merror', 'mfenced', 'mfrac', 'mglyph', 'mi', 'mlabeledtr', 'mmultiscripts', 'mn', 'mo', 'mover', 'mpadded', 'mphantom', 'mroot', 'mrow', 'ms', 'mspace', 'msqrt', 'mstyle', 'msub', 'msup', 'msubsup', 'mtable', 'mtd', 'mtext', 'mtr', 'munder', 'munderover', 'mprescripts']);\n// Similarly to SVG, we want to know all MathML elements,\n// even those that we disallow by default.\nconst mathMlDisallowed = freeze(['maction', 'maligngroup', 'malignmark', 'mlongdiv', 'mscarries', 'mscarry', 'msgroup', 'mstack', 'msline', 'msrow', 'semantics', 'annotation', 'annotation-xml', 'mprescripts', 'none']);\nconst text = freeze(['#text']);\n\nconst html = freeze(['accept', 'action', 'align', 'alt', 'autocapitalize', 'autocomplete', 'autopictureinpicture', 'autoplay', 'background', 'bgcolor', 'border', 'capture', 'cellpadding', 'cellspacing', 'checked', 'cite', 'class', 'clear', 'color', 'cols', 'colspan', 'controls', 'controlslist', 'coords', 'crossorigin', 'datetime', 'decoding', 'default', 'dir', 'disabled', 'disablepictureinpicture', 'disableremoteplayback', 'download', 'draggable', 'enctype', 'enterkeyhint', 'face', 'for', 'headers', 'height', 'hidden', 'high', 'href', 'hreflang', 'id', 'inputmode', 'integrity', 'ismap', 'kind', 'label', 'lang', 'list', 'loading', 'loop', 'low', 'max', 'maxlength', 'media', 'method', 'min', 'minlength', 'multiple', 'muted', 'name', 'nonce', 'noshade', 'novalidate', 'nowrap', 'open', 'optimum', 'pattern', 'placeholder', 'playsinline', 'popover', 'popovertarget', 'popovertargetaction', 'poster', 'preload', 'pubdate', 'radiogroup', 'readonly', 'rel', 'required', 'rev', 'reversed', 'role', 'rows', 'rowspan', 'spellcheck', 'scope', 'selected', 'shape', 'size', 'sizes', 'span', 'srclang', 'start', 'src', 'srcset', 'step', 'style', 'summary', 'tabindex', 'title', 'translate', 'type', 'usemap', 'valign', 'value', 'width', 'wrap', 'xmlns', 'slot']);\nconst svg = freeze(['accent-height', 'accumulate', 'additive', 'alignment-baseline', 'amplitude', 'ascent', 'attributename', 'attributetype', 'azimuth', 'basefrequency', 'baseline-shift', 'begin', 'bias', 'by', 'class', 'clip', 'clippathunits', 'clip-path', 'clip-rule', 'color', 'color-interpolation', 'color-interpolation-filters', 'color-profile', 'color-rendering', 'cx', 'cy', 'd', 'dx', 'dy', 'diffuseconstant', 'direction', 'display', 'divisor', 'dur', 'edgemode', 'elevation', 'end', 'exponent', 'fill', 'fill-opacity', 'fill-rule', 'filter', 'filterunits', 'flood-color', 'flood-opacity', 'font-family', 'font-size', 'font-size-adjust', 'font-stretch', 'font-style', 'font-variant', 'font-weight', 'fx', 'fy', 'g1', 'g2', 'glyph-name', 'glyphref', 'gradientunits', 'gradienttransform', 'height', 'href', 'id', 'image-rendering', 'in', 'in2', 'intercept', 'k', 'k1', 'k2', 'k3', 'k4', 'kerning', 'keypoints', 'keysplines', 'keytimes', 'lang', 'lengthadjust', 'letter-spacing', 'kernelmatrix', 'kernelunitlength', 'lighting-color', 'local', 'marker-end', 'marker-mid', 'marker-start', 'markerheight', 'markerunits', 'markerwidth', 'maskcontentunits', 'maskunits', 'max', 'mask', 'media', 'method', 'mode', 'min', 'name', 'numoctaves', 'offset', 'operator', 'opacity', 'order', 'orient', 'orientation', 'origin', 'overflow', 'paint-order', 'path', 'pathlength', 'patterncontentunits', 'patterntransform', 'patternunits', 'points', 'preservealpha', 'preserveaspectratio', 'primitiveunits', 'r', 'rx', 'ry', 'radius', 'refx', 'refy', 'repeatcount', 'repeatdur', 'restart', 'result', 'rotate', 'scale', 'seed', 'shape-rendering', 'slope', 'specularconstant', 'specularexponent', 'spreadmethod', 'startoffset', 'stddeviation', 'stitchtiles', 'stop-color', 'stop-opacity', 'stroke-dasharray', 'stroke-dashoffset', 'stroke-linecap', 'stroke-linejoin', 'stroke-miterlimit', 'stroke-opacity', 'stroke', 'stroke-width', 'style', 'surfacescale', 'systemlanguage', 'tabindex', 'tablevalues', 'targetx', 'targety', 'transform', 'transform-origin', 'text-anchor', 'text-decoration', 'text-rendering', 'textlength', 'type', 'u1', 'u2', 'unicode', 'values', 'viewbox', 'visibility', 'version', 'vert-adv-y', 'vert-origin-x', 'vert-origin-y', 'width', 'word-spacing', 'wrap', 'writing-mode', 'xchannelselector', 'ychannelselector', 'x', 'x1', 'x2', 'xmlns', 'y', 'y1', 'y2', 'z', 'zoomandpan']);\nconst mathMl = freeze(['accent', 'accentunder', 'align', 'bevelled', 'close', 'columnsalign', 'columnlines', 'columnspan', 'denomalign', 'depth', 'dir', 'display', 'displaystyle', 'encoding', 'fence', 'frame', 'height', 'href', 'id', 'largeop', 'length', 'linethickness', 'lspace', 'lquote', 'mathbackground', 'mathcolor', 'mathsize', 'mathvariant', 'maxsize', 'minsize', 'movablelimits', 'notation', 'numalign', 'open', 'rowalign', 'rowlines', 'rowspacing', 'rowspan', 'rspace', 'rquote', 'scriptlevel', 'scriptminsize', 'scriptsizemultiplier', 'selection', 'separator', 'separators', 'stretchy', 'subscriptshift', 'supscriptshift', 'symmetric', 'voffset', 'width', 'xmlns']);\nconst xml = freeze(['xlink:href', 'xml:id', 'xlink:title', 'xml:space', 'xmlns:xlink']);\n\n// eslint-disable-next-line unicorn/better-regex\nconst MUSTACHE_EXPR = seal(/\\{\\{[\\w\\W]*|[\\w\\W]*\\}\\}/gm); // Specify template detection regex for SAFE_FOR_TEMPLATES mode\nconst ERB_EXPR = seal(/<%[\\w\\W]*|[\\w\\W]*%>/gm);\nconst TMPLIT_EXPR = seal(/\\$\\{[\\w\\W]*/gm); // eslint-disable-line unicorn/better-regex\nconst DATA_ATTR = seal(/^data-[\\-\\w.\\u00B7-\\uFFFF]+$/); // eslint-disable-line no-useless-escape\nconst ARIA_ATTR = seal(/^aria-[\\-\\w]+$/); // eslint-disable-line no-useless-escape\nconst IS_ALLOWED_URI = seal(/^(?:(?:(?:f|ht)tps?|mailto|tel|callto|sms|cid|xmpp):|[^a-z]|[a-z+.\\-]+(?:[^a-z+.\\-:]|$))/i // eslint-disable-line no-useless-escape\n);\nconst IS_SCRIPT_OR_DATA = seal(/^(?:\\w+script|data):/i);\nconst ATTR_WHITESPACE = seal(/[\\u0000-\\u0020\\u00A0\\u1680\\u180E\\u2000-\\u2029\\u205F\\u3000]/g // eslint-disable-line no-control-regex\n);\nconst DOCTYPE_NAME = seal(/^html$/i);\nconst CUSTOM_ELEMENT = seal(/^[a-z][.\\w]*(-[.\\w]+)+$/i);\n\nvar EXPRESSIONS = /*#__PURE__*/Object.freeze({\n  __proto__: null,\n  ARIA_ATTR: ARIA_ATTR,\n  ATTR_WHITESPACE: ATTR_WHITESPACE,\n  CUSTOM_ELEMENT: CUSTOM_ELEMENT,\n  DATA_ATTR: DATA_ATTR,\n  DOCTYPE_NAME: DOCTYPE_NAME,\n  ERB_EXPR: ERB_EXPR,\n  IS_ALLOWED_URI: IS_ALLOWED_URI,\n  IS_SCRIPT_OR_DATA: IS_SCRIPT_OR_DATA,\n  MUSTACHE_EXPR: MUSTACHE_EXPR,\n  TMPLIT_EXPR: TMPLIT_EXPR\n});\n\n/* eslint-disable @typescript-eslint/indent */\n// https://developer.mozilla.org/en-US/docs/Web/API/Node/nodeType\nconst NODE_TYPE = {\n  element: 1,\n  attribute: 2,\n  text: 3,\n  cdataSection: 4,\n  entityReference: 5,\n  // Deprecated\n  entityNode: 6,\n  // Deprecated\n  progressingInstruction: 7,\n  comment: 8,\n  document: 9,\n  documentType: 10,\n  documentFragment: 11,\n  notation: 12 // Deprecated\n};\nconst getGlobal = function getGlobal() {\n  return typeof window === 'undefined' ? null : window;\n};\n/**\n * Creates a no-op policy for internal use only.\n * Don't export this function outside this module!\n * @param trustedTypes The policy factory.\n * @param purifyHostElement The Script element used to load DOMPurify (to determine policy name suffix).\n * @return The policy created (or null, if Trusted Types\n * are not supported or creating the policy failed).\n */\nconst _createTrustedTypesPolicy = function _createTrustedTypesPolicy(trustedTypes, purifyHostElement) {\n  if (typeof trustedTypes !== 'object' || typeof trustedTypes.createPolicy !== 'function') {\n    return null;\n  }\n  // Allow the callers to control the unique policy name\n  // by adding a data-tt-policy-suffix to the script element with the DOMPurify.\n  // Policy creation with duplicate names throws in Trusted Types.\n  let suffix = null;\n  const ATTR_NAME = 'data-tt-policy-suffix';\n  if (purifyHostElement && purifyHostElement.hasAttribute(ATTR_NAME)) {\n    suffix = purifyHostElement.getAttribute(ATTR_NAME);\n  }\n  const policyName = 'dompurify' + (suffix ? '#' + suffix : '');\n  try {\n    return trustedTypes.createPolicy(policyName, {\n      createHTML(html) {\n        return html;\n      },\n      createScriptURL(scriptUrl) {\n        return scriptUrl;\n      }\n    });\n  } catch (_) {\n    // Policy creation failed (most likely another DOMPurify script has\n    // already run). Skip creating the policy, as this will only cause errors\n    // if TT are enforced.\n    console.warn('TrustedTypes policy ' + policyName + ' could not be created.');\n    return null;\n  }\n};\nconst _createHooksMap = function _createHooksMap() {\n  return {\n    afterSanitizeAttributes: [],\n    afterSanitizeElements: [],\n    afterSanitizeShadowDOM: [],\n    beforeSanitizeAttributes: [],\n    beforeSanitizeElements: [],\n    beforeSanitizeShadowDOM: [],\n    uponSanitizeAttribute: [],\n    uponSanitizeElement: [],\n    uponSanitizeShadowNode: []\n  };\n};\nfunction createDOMPurify() {\n  let window = arguments.length > 0 && arguments[0] !== undefined ? arguments[0] : getGlobal();\n  const DOMPurify = root => createDOMPurify(root);\n  DOMPurify.version = '3.2.4';\n  DOMPurify.removed = [];\n  if (!window || !window.document || window.document.nodeType !== NODE_TYPE.document || !window.Element) {\n    // Not running in a browser, provide a factory function\n    // so that you can pass your own Window\n    DOMPurify.isSupported = false;\n    return DOMPurify;\n  }\n  let {\n    document\n  } = window;\n  const originalDocument = document;\n  const currentScript = originalDocument.currentScript;\n  const {\n    DocumentFragment,\n    HTMLTemplateElement,\n    Node,\n    Element,\n    NodeFilter,\n    NamedNodeMap = window.NamedNodeMap || window.MozNamedAttrMap,\n    HTMLFormElement,\n    DOMParser,\n    trustedTypes\n  } = window;\n  const ElementPrototype = Element.prototype;\n  const cloneNode = lookupGetter(ElementPrototype, 'cloneNode');\n  const remove = lookupGetter(ElementPrototype, 'remove');\n  const getNextSibling = lookupGetter(ElementPrototype, 'nextSibling');\n  const getChildNodes = lookupGetter(ElementPrototype, 'childNodes');\n  const getParentNode = lookupGetter(ElementPrototype, 'parentNode');\n  // As per issue #47, the web-components registry is inherited by a\n  // new document created via createHTMLDocument. As per the spec\n  // (http://w3c.github.io/webcomponents/spec/custom/#creating-and-passing-registries)\n  // a new empty registry is used when creating a template contents owner\n  // document, so we use that as our parent document to ensure nothing\n  // is inherited.\n  if (typeof HTMLTemplateElement === 'function') {\n    const template = document.createElement('template');\n    if (template.content && template.content.ownerDocument) {\n      document = template.content.ownerDocument;\n    }\n  }\n  let trustedTypesPolicy;\n  let emptyHTML = '';\n  const {\n    implementation,\n    createNodeIterator,\n    createDocumentFragment,\n    getElementsByTagName\n  } = document;\n  const {\n    importNode\n  } = originalDocument;\n  let hooks = _createHooksMap();\n  /**\n   * Expose whether this browser supports running the full DOMPurify.\n   */\n  DOMPurify.isSupported = typeof entries === 'function' && typeof getParentNode === 'function' && implementation && implementation.createHTMLDocument !== undefined;\n  const {\n    MUSTACHE_EXPR,\n    ERB_EXPR,\n    TMPLIT_EXPR,\n    DATA_ATTR,\n    ARIA_ATTR,\n    IS_SCRIPT_OR_DATA,\n    ATTR_WHITESPACE,\n    CUSTOM_ELEMENT\n  } = EXPRESSIONS;\n  let {\n    IS_ALLOWED_URI: IS_ALLOWED_URI$1\n  } = EXPRESSIONS;\n  /**\n   * We consider the elements and attributes below to be safe. Ideally\n   * don't add any new ones but feel free to remove unwanted ones.\n   */\n  /* allowed element names */\n  let ALLOWED_TAGS = null;\n  const DEFAULT_ALLOWED_TAGS = addToSet({}, [...html$1, ...svg$1, ...svgFilters, ...mathMl$1, ...text]);\n  /* Allowed attribute names */\n  let ALLOWED_ATTR = null;\n  const DEFAULT_ALLOWED_ATTR = addToSet({}, [...html, ...svg, ...mathMl, ...xml]);\n  /*\n   * Configure how DOMPurify should handle custom elements and their attributes as well as customized built-in elements.\n   * @property {RegExp|Function|null} tagNameCheck one of [null, regexPattern, predicate]. Default: `null` (disallow any custom elements)\n   * @property {RegExp|Function|null} attributeNameCheck one of [null, regexPattern, predicate]. Default: `null` (disallow any attributes not on the allow list)\n   * @property {boolean} allowCustomizedBuiltInElements allow custom elements derived from built-ins if they pass CUSTOM_ELEMENT_HANDLING.tagNameCheck. Default: `false`.\n   */\n  let CUSTOM_ELEMENT_HANDLING = Object.seal(create(null, {\n    tagNameCheck: {\n      writable: true,\n      configurable: false,\n      enumerable: true,\n      value: null\n    },\n    attributeNameCheck: {\n      writable: true,\n      configurable: false,\n      enumerable: true,\n      value: null\n    },\n    allowCustomizedBuiltInElements: {\n      writable: true,\n      configurable: false,\n      enumerable: true,\n      value: false\n    }\n  }));\n  /* Explicitly forbidden tags (overrides ALLOWED_TAGS/ADD_TAGS) */\n  let FORBID_TAGS = null;\n  /* Explicitly forbidden attributes (overrides ALLOWED_ATTR/ADD_ATTR) */\n  let FORBID_ATTR = null;\n  /* Decide if ARIA attributes are okay */\n  let ALLOW_ARIA_ATTR = true;\n  /* Decide if custom data attributes are okay */\n  let ALLOW_DATA_ATTR = true;\n  /* Decide if unknown protocols are okay */\n  let ALLOW_UNKNOWN_PROTOCOLS = false;\n  /* Decide if self-closing tags in attributes are allowed.\n   * Usually removed due to a mXSS issue in jQuery 3.0 */\n  let ALLOW_SELF_CLOSE_IN_ATTR = true;\n  /* Output should be safe for common template engines.\n   * This means, DOMPurify removes data attributes, mustaches and ERB\n   */\n  let SAFE_FOR_TEMPLATES = false;\n  /* Output should be safe even for XML used within HTML and alike.\n   * This means, DOMPurify removes comments when containing risky content.\n   */\n  let SAFE_FOR_XML = true;\n  /* Decide if document with <html>... should be returned */\n  let WHOLE_DOCUMENT = false;\n  /* Track whether config is already set on this instance of DOMPurify. */\n  let SET_CONFIG = false;\n  /* Decide if all elements (e.g. style, script) must be children of\n   * document.body. By default, browsers might move them to document.head */\n  let FORCE_BODY = false;\n  /* Decide if a DOM `HTMLBodyElement` should be returned, instead of a html\n   * string (or a TrustedHTML object if Trusted Types are supported).\n   * If `WHOLE_DOCUMENT` is enabled a `HTMLHtmlElement` will be returned instead\n   */\n  let RETURN_DOM = false;\n  /* Decide if a DOM `DocumentFragment` should be returned, instead of a html\n   * string  (or a TrustedHTML object if Trusted Types are supported) */\n  let RETURN_DOM_FRAGMENT = false;\n  /* Try to return a Trusted Type object instead of a string, return a string in\n   * case Trusted Types are not supported  */\n  let RETURN_TRUSTED_TYPE = false;\n  /* Output should be free from DOM clobbering attacks?\n   * This sanitizes markups named with colliding, clobberable built-in DOM APIs.\n   */\n  let SANITIZE_DOM = true;\n  /* Achieve full DOM Clobbering protection by isolating the namespace of named\n   * properties and JS variables, mitigating attacks that abuse the HTML/DOM spec rules.\n   *\n   * HTML/DOM spec rules that enable DOM Clobbering:\n   *   - Named Access on Window (\u00A77.3.3)\n   *   - DOM Tree Accessors (\u00A73.1.5)\n   *   - Form Element Parent-Child Relations (\u00A74.10.3)\n   *   - Iframe srcdoc / Nested WindowProxies (\u00A74.8.5)\n   *   - HTMLCollection (\u00A74.2.10.2)\n   *\n   * Namespace isolation is implemented by prefixing `id` and `name` attributes\n   * with a constant string, i.e., `user-content-`\n   */\n  let SANITIZE_NAMED_PROPS = false;\n  const SANITIZE_NAMED_PROPS_PREFIX = 'user-content-';\n  /* Keep element content when removing element? */\n  let KEEP_CONTENT = true;\n  /* If a `Node` is passed to sanitize(), then performs sanitization in-place instead\n   * of importing it into a new Document and returning a sanitized copy */\n  let IN_PLACE = false;\n  /* Allow usage of profiles like html, svg and mathMl */\n  let USE_PROFILES = {};\n  /* Tags to ignore content of when KEEP_CONTENT is true */\n  let FORBID_CONTENTS = null;\n  const DEFAULT_FORBID_CONTENTS = addToSet({}, ['annotation-xml', 'audio', 'colgroup', 'desc', 'foreignobject', 'head', 'iframe', 'math', 'mi', 'mn', 'mo', 'ms', 'mtext', 'noembed', 'noframes', 'noscript', 'plaintext', 'script', 'style', 'svg', 'template', 'thead', 'title', 'video', 'xmp']);\n  /* Tags that are safe for data: URIs */\n  let DATA_URI_TAGS = null;\n  const DEFAULT_DATA_URI_TAGS = addToSet({}, ['audio', 'video', 'img', 'source', 'image', 'track']);\n  /* Attributes safe for values like \"javascript:\" */\n  let URI_SAFE_ATTRIBUTES = null;\n  const DEFAULT_URI_SAFE_ATTRIBUTES = addToSet({}, ['alt', 'class', 'for', 'id', 'label', 'name', 'pattern', 'placeholder', 'role', 'summary', 'title', 'value', 'style', 'xmlns']);\n  const MATHML_NAMESPACE = 'http://www.w3.org/1998/Math/MathML';\n  const SVG_NAMESPACE = 'http://www.w3.org/2000/svg';\n  const HTML_NAMESPACE = 'http://www.w3.org/1999/xhtml';\n  /* Document namespace */\n  let NAMESPACE = HTML_NAMESPACE;\n  let IS_EMPTY_INPUT = false;\n  /* Allowed XHTML+XML namespaces */\n  let ALLOWED_NAMESPACES = null;\n  const DEFAULT_ALLOWED_NAMESPACES = addToSet({}, [MATHML_NAMESPACE, SVG_NAMESPACE, HTML_NAMESPACE], stringToString);\n  let MATHML_TEXT_INTEGRATION_POINTS = addToSet({}, ['mi', 'mo', 'mn', 'ms', 'mtext']);\n  let HTML_INTEGRATION_POINTS = addToSet({}, ['annotation-xml']);\n  // Certain elements are allowed in both SVG and HTML\n  // namespace. We need to specify them explicitly\n  // so that they don't get erroneously deleted from\n  // HTML namespace.\n  const COMMON_SVG_AND_HTML_ELEMENTS = addToSet({}, ['title', 'style', 'font', 'a', 'script']);\n  /* Parsing of strict XHTML documents */\n  let PARSER_MEDIA_TYPE = null;\n  const SUPPORTED_PARSER_MEDIA_TYPES = ['application/xhtml+xml', 'text/html'];\n  const DEFAULT_PARSER_MEDIA_TYPE = 'text/html';\n  let transformCaseFunc = null;\n  /* Keep a reference to config to pass to hooks */\n  let CONFIG = null;\n  /* Ideally, do not touch anything below this line */\n  /* ______________________________________________ */\n  const formElement = document.createElement('form');\n  const isRegexOrFunction = function isRegexOrFunction(testValue) {\n    return testValue instanceof RegExp || testValue instanceof Function;\n  };\n  /**\n   * _parseConfig\n   *\n   * @param cfg optional config literal\n   */\n  // eslint-disable-next-line complexity\n  const _parseConfig = function _parseConfig() {\n    let cfg = arguments.length > 0 && arguments[0] !== undefined ? arguments[0] : {};\n    if (CONFIG && CONFIG === cfg) {\n      return;\n    }\n    /* Shield configuration object from tampering */\n    if (!cfg || typeof cfg !== 'object') {\n      cfg = {};\n    }\n    /* Shield configuration object from prototype pollution */\n    cfg = clone(cfg);\n    PARSER_MEDIA_TYPE =\n    // eslint-disable-next-line unicorn/prefer-includes\n    SUPPORTED_PARSER_MEDIA_TYPES.indexOf(cfg.PARSER_MEDIA_TYPE) === -1 ? DEFAULT_PARSER_MEDIA_TYPE : cfg.PARSER_MEDIA_TYPE;\n    // HTML tags and attributes are not case-sensitive, converting to lowercase. Keeping XHTML as is.\n    transformCaseFunc = PARSER_MEDIA_TYPE === 'application/xhtml+xml' ? stringToString : stringToLowerCase;\n    /* Set configuration parameters */\n    ALLOWED_TAGS = objectHasOwnProperty(cfg, 'ALLOWED_TAGS') ? addToSet({}, cfg.ALLOWED_TAGS, transformCaseFunc) : DEFAULT_ALLOWED_TAGS;\n    ALLOWED_ATTR = objectHasOwnProperty(cfg, 'ALLOWED_ATTR') ? addToSet({}, cfg.ALLOWED_ATTR, transformCaseFunc) : DEFAULT_ALLOWED_ATTR;\n    ALLOWED_NAMESPACES = objectHasOwnProperty(cfg, 'ALLOWED_NAMESPACES') ? addToSet({}, cfg.ALLOWED_NAMESPACES, stringToString) : DEFAULT_ALLOWED_NAMESPACES;\n    URI_SAFE_ATTRIBUTES = objectHasOwnProperty(cfg, 'ADD_URI_SAFE_ATTR') ? addToSet(clone(DEFAULT_URI_SAFE_ATTRIBUTES), cfg.ADD_URI_SAFE_ATTR, transformCaseFunc) : DEFAULT_URI_SAFE_ATTRIBUTES;\n    DATA_URI_TAGS = objectHasOwnProperty(cfg, 'ADD_DATA_URI_TAGS') ? addToSet(clone(DEFAULT_DATA_URI_TAGS), cfg.ADD_DATA_URI_TAGS, transformCaseFunc) : DEFAULT_DATA_URI_TAGS;\n    FORBID_CONTENTS = objectHasOwnProperty(cfg, 'FORBID_CONTENTS') ? addToSet({}, cfg.FORBID_CONTENTS, transformCaseFunc) : DEFAULT_FORBID_CONTENTS;\n    FORBID_TAGS = objectHasOwnProperty(cfg, 'FORBID_TAGS') ? addToSet({}, cfg.FORBID_TAGS, transformCaseFunc) : {};\n    FORBID_ATTR = objectHasOwnProperty(cfg, 'FORBID_ATTR') ? addToSet({}, cfg.FORBID_ATTR, transformCaseFunc) : {};\n    USE_PROFILES = objectHasOwnProperty(cfg, 'USE_PROFILES') ? cfg.USE_PROFILES : false;\n    ALLOW_ARIA_ATTR = cfg.ALLOW_ARIA_ATTR !== false; // Default true\n    ALLOW_DATA_ATTR = cfg.ALLOW_DATA_ATTR !== false; // Default true\n    ALLOW_UNKNOWN_PROTOCOLS = cfg.ALLOW_UNKNOWN_PROTOCOLS || false; // Default false\n    ALLOW_SELF_CLOSE_IN_ATTR = cfg.ALLOW_SELF_CLOSE_IN_ATTR !== false; // Default true\n    SAFE_FOR_TEMPLATES = cfg.SAFE_FOR_TEMPLATES || false; // Default false\n    SAFE_FOR_XML = cfg.SAFE_FOR_XML !== false; // Default true\n    WHOLE_DOCUMENT = cfg.WHOLE_DOCUMENT || false; // Default false\n    RETURN_DOM = cfg.RETURN_DOM || false; // Default false\n    RETURN_DOM_FRAGMENT = cfg.RETURN_DOM_FRAGMENT || false; // Default false\n    RETURN_TRUSTED_TYPE = cfg.RETURN_TRUSTED_TYPE || false; // Default false\n    FORCE_BODY = cfg.FORCE_BODY || false; // Default false\n    SANITIZE_DOM = cfg.SANITIZE_DOM !== false; // Default true\n    SANITIZE_NAMED_PROPS = cfg.SANITIZE_NAMED_PROPS || false; // Default false\n    KEEP_CONTENT = cfg.KEEP_CONTENT !== false; // Default true\n    IN_PLACE = cfg.IN_PLACE || false; // Default false\n    IS_ALLOWED_URI$1 = cfg.ALLOWED_URI_REGEXP || IS_ALLOWED_URI;\n    NAMESPACE = cfg.NAMESPACE || HTML_NAMESPACE;\n    MATHML_TEXT_INTEGRATION_POINTS = cfg.MATHML_TEXT_INTEGRATION_POINTS || MATHML_TEXT_INTEGRATION_POINTS;\n    HTML_INTEGRATION_POINTS = cfg.HTML_INTEGRATION_POINTS || HTML_INTEGRATION_POINTS;\n    CUSTOM_ELEMENT_HANDLING = cfg.CUSTOM_ELEMENT_HANDLING || {};\n    if (cfg.CUSTOM_ELEMENT_HANDLING && isRegexOrFunction(cfg.CUSTOM_ELEMENT_HANDLING.tagNameCheck)) {\n      CUSTOM_ELEMENT_HANDLING.tagNameCheck = cfg.CUSTOM_ELEMENT_HANDLING.tagNameCheck;\n    }\n    if (cfg.CUSTOM_ELEMENT_HANDLING && isRegexOrFunction(cfg.CUSTOM_ELEMENT_HANDLING.attributeNameCheck)) {\n      CUSTOM_ELEMENT_HANDLING.attributeNameCheck = cfg.CUSTOM_ELEMENT_HANDLING.attributeNameCheck;\n    }\n    if (cfg.CUSTOM_ELEMENT_HANDLING && typeof cfg.CUSTOM_ELEMENT_HANDLING.allowCustomizedBuiltInElements === 'boolean') {\n      CUSTOM_ELEMENT_HANDLING.allowCustomizedBuiltInElements = cfg.CUSTOM_ELEMENT_HANDLING.allowCustomizedBuiltInElements;\n    }\n    if (SAFE_FOR_TEMPLATES) {\n      ALLOW_DATA_ATTR = false;\n    }\n    if (RETURN_DOM_FRAGMENT) {\n      RETURN_DOM = true;\n    }\n    /* Parse profile info */\n    if (USE_PROFILES) {\n      ALLOWED_TAGS = addToSet({}, text);\n      ALLOWED_ATTR = [];\n      if (USE_PROFILES.html === true) {\n        addToSet(ALLOWED_TAGS, html$1);\n        addToSet(ALLOWED_ATTR, html);\n      }\n      if (USE_PROFILES.svg === true) {\n        addToSet(ALLOWED_TAGS, svg$1);\n        addToSet(ALLOWED_ATTR, svg);\n        addToSet(ALLOWED_ATTR, xml);\n      }\n      if (USE_PROFILES.svgFilters === true) {\n        addToSet(ALLOWED_TAGS, svgFilters);\n        addToSet(ALLOWED_ATTR, svg);\n        addToSet(ALLOWED_ATTR, xml);\n      }\n      if (USE_PROFILES.mathMl === true) {\n        addToSet(ALLOWED_TAGS, mathMl$1);\n        addToSet(ALLOWED_ATTR, mathMl);\n        addToSet(ALLOWED_ATTR, xml);\n      }\n    }\n    /* Merge configuration parameters */\n    if (cfg.ADD_TAGS) {\n      if (ALLOWED_TAGS === DEFAULT_ALLOWED_TAGS) {\n        ALLOWED_TAGS = clone(ALLOWED_TAGS);\n      }\n      addToSet(ALLOWED_TAGS, cfg.ADD_TAGS, transformCaseFunc);\n    }\n    if (cfg.ADD_ATTR) {\n      if (ALLOWED_ATTR === DEFAULT_ALLOWED_ATTR) {\n        ALLOWED_ATTR = clone(ALLOWED_ATTR);\n      }\n      addToSet(ALLOWED_ATTR, cfg.ADD_ATTR, transformCaseFunc);\n    }\n    if (cfg.ADD_URI_SAFE_ATTR) {\n      addToSet(URI_SAFE_ATTRIBUTES, cfg.ADD_URI_SAFE_ATTR, transformCaseFunc);\n    }\n    if (cfg.FORBID_CONTENTS) {\n      if (FORBID_CONTENTS === DEFAULT_FORBID_CONTENTS) {\n        FORBID_CONTENTS = clone(FORBID_CONTENTS);\n      }\n      addToSet(FORBID_CONTENTS, cfg.FORBID_CONTENTS, transformCaseFunc);\n    }\n    /* Add #text in case KEEP_CONTENT is set to true */\n    if (KEEP_CONTENT) {\n      ALLOWED_TAGS['#text'] = true;\n    }\n    /* Add html, head and body to ALLOWED_TAGS in case WHOLE_DOCUMENT is true */\n    if (WHOLE_DOCUMENT) {\n      addToSet(ALLOWED_TAGS, ['html', 'head', 'body']);\n    }\n    /* Add tbody to ALLOWED_TAGS in case tables are permitted, see #286, #365 */\n    if (ALLOWED_TAGS.table) {\n      addToSet(ALLOWED_TAGS, ['tbody']);\n      delete FORBID_TAGS.tbody;\n    }\n    if (cfg.TRUSTED_TYPES_POLICY) {\n      if (typeof cfg.TRUSTED_TYPES_POLICY.createHTML !== 'function') {\n        throw typeErrorCreate('TRUSTED_TYPES_POLICY configuration option must provide a \"createHTML\" hook.');\n      }\n      if (typeof cfg.TRUSTED_TYPES_POLICY.createScriptURL !== 'function') {\n        throw typeErrorCreate('TRUSTED_TYPES_POLICY configuration option must provide a \"createScriptURL\" hook.');\n      }\n      // Overwrite existing TrustedTypes policy.\n      trustedTypesPolicy = cfg.TRUSTED_TYPES_POLICY;\n      // Sign local variables required by `sanitize`.\n      emptyHTML = trustedTypesPolicy.createHTML('');\n    } else {\n      // Uninitialized policy, attempt to initialize the internal dompurify policy.\n      if (trustedTypesPolicy === undefined) {\n        trustedTypesPolicy = _createTrustedTypesPolicy(trustedTypes, currentScript);\n      }\n      // If creating the internal policy succeeded sign internal variables.\n      if (trustedTypesPolicy !== null && typeof emptyHTML === 'string') {\n        emptyHTML = trustedTypesPolicy.createHTML('');\n      }\n    }\n    // Prevent further manipulation of configuration.\n    // Not available in IE8, Safari 5, etc.\n    if (freeze) {\n      freeze(cfg);\n    }\n    CONFIG = cfg;\n  };\n  /* Keep track of all possible SVG and MathML tags\n   * so that we can perform the namespace checks\n   * correctly. */\n  const ALL_SVG_TAGS = addToSet({}, [...svg$1, ...svgFilters, ...svgDisallowed]);\n  const ALL_MATHML_TAGS = addToSet({}, [...mathMl$1, ...mathMlDisallowed]);\n  /**\n   * @param element a DOM element whose namespace is being checked\n   * @returns Return false if the element has a\n   *  namespace that a spec-compliant parser would never\n   *  return. Return true otherwise.\n   */\n  const _checkValidNamespace = function _checkValidNamespace(element) {\n    let parent = getParentNode(element);\n    // In JSDOM, if we're inside shadow DOM, then parentNode\n    // can be null. We just simulate parent in this case.\n    if (!parent || !parent.tagName) {\n      parent = {\n        namespaceURI: NAMESPACE,\n        tagName: 'template'\n      };\n    }\n    const tagName = stringToLowerCase(element.tagName);\n    const parentTagName = stringToLowerCase(parent.tagName);\n    if (!ALLOWED_NAMESPACES[element.namespaceURI]) {\n      return false;\n    }\n    if (element.namespaceURI === SVG_NAMESPACE) {\n      // The only way to switch from HTML namespace to SVG\n      // is via <svg>. If it happens via any other tag, then\n      // it should be killed.\n      if (parent.namespaceURI === HTML_NAMESPACE) {\n        return tagName === 'svg';\n      }\n      // The only way to switch from MathML to SVG is via`\n      // svg if parent is either <annotation-xml> or MathML\n      // text integration points.\n      if (parent.namespaceURI === MATHML_NAMESPACE) {\n        return tagName === 'svg' && (parentTagName === 'annotation-xml' || MATHML_TEXT_INTEGRATION_POINTS[parentTagName]);\n      }\n      // We only allow elements that are defined in SVG\n      // spec. All others are disallowed in SVG namespace.\n      return Boolean(ALL_SVG_TAGS[tagName]);\n    }\n    if (element.namespaceURI === MATHML_NAMESPACE) {\n      // The only way to switch from HTML namespace to MathML\n      // is via <math>. If it happens via any other tag, then\n      // it should be killed.\n      if (parent.namespaceURI === HTML_NAMESPACE) {\n        return tagName === 'math';\n      }\n      // The only way to switch from SVG to MathML is via\n      // <math> and HTML integration points\n      if (parent.namespaceURI === SVG_NAMESPACE) {\n        return tagName === 'math' && HTML_INTEGRATION_POINTS[parentTagName];\n      }\n      // We only allow elements that are defined in MathML\n      // spec. All others are disallowed in MathML namespace.\n      return Boolean(ALL_MATHML_TAGS[tagName]);\n    }\n    if (element.namespaceURI === HTML_NAMESPACE) {\n      // The only way to switch from SVG to HTML is via\n      // HTML integration points, and from MathML to HTML\n      // is via MathML text integration points\n      if (parent.namespaceURI === SVG_NAMESPACE && !HTML_INTEGRATION_POINTS[parentTagName]) {\n        return false;\n      }\n      if (parent.namespaceURI === MATHML_NAMESPACE && !MATHML_TEXT_INTEGRATION_POINTS[parentTagName]) {\n        return false;\n      }\n      // We disallow tags that are specific for MathML\n      // or SVG and should never appear in HTML namespace\n      return !ALL_MATHML_TAGS[tagName] && (COMMON_SVG_AND_HTML_ELEMENTS[tagName] || !ALL_SVG_TAGS[tagName]);\n    }\n    // For XHTML and XML documents that support custom namespaces\n    if (PARSER_MEDIA_TYPE === 'application/xhtml+xml' && ALLOWED_NAMESPACES[element.namespaceURI]) {\n      return true;\n    }\n    // The code should never reach this place (this means\n    // that the element somehow got namespace that is not\n    // HTML, SVG, MathML or allowed via ALLOWED_NAMESPACES).\n    // Return false just in case.\n    return false;\n  };\n  /**\n   * _forceRemove\n   *\n   * @param node a DOM node\n   */\n  const _forceRemove = function _forceRemove(node) {\n    arrayPush(DOMPurify.removed, {\n      element: node\n    });\n    try {\n      // eslint-disable-next-line unicorn/prefer-dom-node-remove\n      getParentNode(node).removeChild(node);\n    } catch (_) {\n      remove(node);\n    }\n  };\n  /**\n   * _removeAttribute\n   *\n   * @param name an Attribute name\n   * @param element a DOM node\n   */\n  const _removeAttribute = function _removeAttribute(name, element) {\n    try {\n      arrayPush(DOMPurify.removed, {\n        attribute: element.getAttributeNode(name),\n        from: element\n      });\n    } catch (_) {\n      arrayPush(DOMPurify.removed, {\n        attribute: null,\n        from: element\n      });\n    }\n    element.removeAttribute(name);\n    // We void attribute values for unremovable \"is\" attributes\n    if (name === 'is') {\n      if (RETURN_DOM || RETURN_DOM_FRAGMENT) {\n        try {\n          _forceRemove(element);\n        } catch (_) {}\n      } else {\n        try {\n          element.setAttribute(name, '');\n        } catch (_) {}\n      }\n    }\n  };\n  /**\n   * _initDocument\n   *\n   * @param dirty - a string of dirty markup\n   * @return a DOM, filled with the dirty markup\n   */\n  const _initDocument = function _initDocument(dirty) {\n    /* Create a HTML document */\n    let doc = null;\n    let leadingWhitespace = null;\n    if (FORCE_BODY) {\n      dirty = '<remove></remove>' + dirty;\n    } else {\n      /* If FORCE_BODY isn't used, leading whitespace needs to be preserved manually */\n      const matches = stringMatch(dirty, /^[\\r\\n\\t ]+/);\n      leadingWhitespace = matches && matches[0];\n    }\n    if (PARSER_MEDIA_TYPE === 'application/xhtml+xml' && NAMESPACE === HTML_NAMESPACE) {\n      // Root of XHTML doc must contain xmlns declaration (see https://www.w3.org/TR/xhtml1/normative.html#strict)\n      dirty = '<html xmlns=\"http://www.w3.org/1999/xhtml\"><head></head><body>' + dirty + '</body></html>';\n    }\n    const dirtyPayload = trustedTypesPolicy ? trustedTypesPolicy.createHTML(dirty) : dirty;\n    /*\n     * Use the DOMParser API by default, fallback later if needs be\n     * DOMParser not work for svg when has multiple root element.\n     */\n    if (NAMESPACE === HTML_NAMESPACE) {\n      try {\n        doc = new DOMParser().parseFromString(dirtyPayload, PARSER_MEDIA_TYPE);\n      } catch (_) {}\n    }\n    /* Use createHTMLDocument in case DOMParser is not available */\n    if (!doc || !doc.documentElement) {\n      doc = implementation.createDocument(NAMESPACE, 'template', null);\n      try {\n        doc.documentElement.innerHTML = IS_EMPTY_INPUT ? emptyHTML : dirtyPayload;\n      } catch (_) {\n        // Syntax error if dirtyPayload is invalid xml\n      }\n    }\n    const body = doc.body || doc.documentElement;\n    if (dirty && leadingWhitespace) {\n      body.insertBefore(document.createTextNode(leadingWhitespace), body.childNodes[0] || null);\n    }\n    /* Work on whole document or just its body */\n    if (NAMESPACE === HTML_NAMESPACE) {\n      return getElementsByTagName.call(doc, WHOLE_DOCUMENT ? 'html' : 'body')[0];\n    }\n    return WHOLE_DOCUMENT ? doc.documentElement : body;\n  };\n  /**\n   * Creates a NodeIterator object that you can use to traverse filtered lists of nodes or elements in a document.\n   *\n   * @param root The root element or node to start traversing on.\n   * @return The created NodeIterator\n   */\n  const _createNodeIterator = function _createNodeIterator(root) {\n    return createNodeIterator.call(root.ownerDocument || root, root,\n    // eslint-disable-next-line no-bitwise\n    NodeFilter.SHOW_ELEMENT | NodeFilter.SHOW_COMMENT | NodeFilter.SHOW_TEXT | NodeFilter.SHOW_PROCESSING_INSTRUCTION | NodeFilter.SHOW_CDATA_SECTION, null);\n  };\n  /**\n   * _isClobbered\n   *\n   * @param element element to check for clobbering attacks\n   * @return true if clobbered, false if safe\n   */\n  const _isClobbered = function _isClobbered(element) {\n    return element instanceof HTMLFormElement && (typeof element.nodeName !== 'string' || typeof element.textContent !== 'string' || typeof element.removeChild !== 'function' || !(element.attributes instanceof NamedNodeMap) || typeof element.removeAttribute !== 'function' || typeof element.setAttribute !== 'function' || typeof element.namespaceURI !== 'string' || typeof element.insertBefore !== 'function' || typeof element.hasChildNodes !== 'function');\n  };\n  /**\n   * Checks whether the given object is a DOM node.\n   *\n   * @param value object to check whether it's a DOM node\n   * @return true is object is a DOM node\n   */\n  const _isNode = function _isNode(value) {\n    return typeof Node === 'function' && value instanceof Node;\n  };\n  function _executeHooks(hooks, currentNode, data) {\n    arrayForEach(hooks, hook => {\n      hook.call(DOMPurify, currentNode, data, CONFIG);\n    });\n  }\n  /**\n   * _sanitizeElements\n   *\n   * @protect nodeName\n   * @protect textContent\n   * @protect removeChild\n   * @param currentNode to check for permission to exist\n   * @return true if node was killed, false if left alive\n   */\n  const _sanitizeElements = function _sanitizeElements(currentNode) {\n    let content = null;\n    /* Execute a hook if present */\n    _executeHooks(hooks.beforeSanitizeElements, currentNode, null);\n    /* Check if element is clobbered or can clobber */\n    if (_isClobbered(currentNode)) {\n      _forceRemove(currentNode);\n      return true;\n    }\n    /* Now let's check the element's type and name */\n    const tagName = transformCaseFunc(currentNode.nodeName);\n    /* Execute a hook if present */\n    _executeHooks(hooks.uponSanitizeElement, currentNode, {\n      tagName,\n      allowedTags: ALLOWED_TAGS\n    });\n    /* Detect mXSS attempts abusing namespace confusion */\n    if (currentNode.hasChildNodes() && !_isNode(currentNode.firstElementChild) && regExpTest(/<[/\\w]/g, currentNode.innerHTML) && regExpTest(/<[/\\w]/g, currentNode.textContent)) {\n      _forceRemove(currentNode);\n      return true;\n    }\n    /* Remove any occurrence of processing instructions */\n    if (currentNode.nodeType === NODE_TYPE.progressingInstruction) {\n      _forceRemove(currentNode);\n      return true;\n    }\n    /* Remove any kind of possibly harmful comments */\n    if (SAFE_FOR_XML && currentNode.nodeType === NODE_TYPE.comment && regExpTest(/<[/\\w]/g, currentNode.data)) {\n      _forceRemove(currentNode);\n      return true;\n    }\n    /* Remove element if anything forbids its presence */\n    if (!ALLOWED_TAGS[tagName] || FORBID_TAGS[tagName]) {\n      /* Check if we have a custom element to handle */\n      if (!FORBID_TAGS[tagName] && _isBasicCustomElement(tagName)) {\n        if (CUSTOM_ELEMENT_HANDLING.tagNameCheck instanceof RegExp && regExpTest(CUSTOM_ELEMENT_HANDLING.tagNameCheck, tagName)) {\n          return false;\n        }\n        if (CUSTOM_ELEMENT_HANDLING.tagNameCheck instanceof Function && CUSTOM_ELEMENT_HANDLING.tagNameCheck(tagName)) {\n          return false;\n        }\n      }\n      /* Keep content except for bad-listed elements */\n      if (KEEP_CONTENT && !FORBID_CONTENTS[tagName]) {\n        const parentNode = getParentNode(currentNode) || currentNode.parentNode;\n        const childNodes = getChildNodes(currentNode) || currentNode.childNodes;\n        if (childNodes && parentNode) {\n          const childCount = childNodes.length;\n          for (let i = childCount - 1; i >= 0; --i) {\n            const childClone = cloneNode(childNodes[i], true);\n            childClone.__removalCount = (currentNode.__removalCount || 0) + 1;\n            parentNode.insertBefore(childClone, getNextSibling(currentNode));\n          }\n        }\n      }\n      _forceRemove(currentNode);\n      return true;\n    }\n    /* Check whether element has a valid namespace */\n    if (currentNode instanceof Element && !_checkValidNamespace(currentNode)) {\n      _forceRemove(currentNode);\n      return true;\n    }\n    /* Make sure that older browsers don't get fallback-tag mXSS */\n    if ((tagName === 'noscript' || tagName === 'noembed' || tagName === 'noframes') && regExpTest(/<\\/no(script|embed|frames)/i, currentNode.innerHTML)) {\n      _forceRemove(currentNode);\n      return true;\n    }\n    /* Sanitize element content to be template-safe */\n    if (SAFE_FOR_TEMPLATES && currentNode.nodeType === NODE_TYPE.text) {\n      /* Get the element's text content */\n      content = currentNode.textContent;\n      arrayForEach([MUSTACHE_EXPR, ERB_EXPR, TMPLIT_EXPR], expr => {\n        content = stringReplace(content, expr, ' ');\n      });\n      if (currentNode.textContent !== content) {\n        arrayPush(DOMPurify.removed, {\n          element: currentNode.cloneNode()\n        });\n        currentNode.textContent = content;\n      }\n    }\n    /* Execute a hook if present */\n    _executeHooks(hooks.afterSanitizeElements, currentNode, null);\n    return false;\n  };\n  /**\n   * _isValidAttribute\n   *\n   * @param lcTag Lowercase tag name of containing element.\n   * @param lcName Lowercase attribute name.\n   * @param value Attribute value.\n   * @return Returns true if `value` is valid, otherwise false.\n   */\n  // eslint-disable-next-line complexity\n  const _isValidAttribute = function _isValidAttribute(lcTag, lcName, value) {\n    /* Make sure attribute cannot clobber */\n    if (SANITIZE_DOM && (lcName === 'id' || lcName === 'name') && (value in document || value in formElement)) {\n      return false;\n    }\n    /* Allow valid data-* attributes: At least one character after \"-\"\n        (https://html.spec.whatwg.org/multipage/dom.html#embedding-custom-non-visible-data-with-the-data-*-attributes)\n        XML-compatible (https://html.spec.whatwg.org/multipage/infrastructure.html#xml-compatible and http://www.w3.org/TR/xml/#d0e804)\n        We don't need to check the value; it's always URI safe. */\n    if (ALLOW_DATA_ATTR && !FORBID_ATTR[lcName] && regExpTest(DATA_ATTR, lcName)) ; else if (ALLOW_ARIA_ATTR && regExpTest(ARIA_ATTR, lcName)) ; else if (!ALLOWED_ATTR[lcName] || FORBID_ATTR[lcName]) {\n      if (\n      // First condition does a very basic check if a) it's basically a valid custom element tagname AND\n      // b) if the tagName passes whatever the user has configured for CUSTOM_ELEMENT_HANDLING.tagNameCheck\n      // and c) if the attribute name passes whatever the user has configured for CUSTOM_ELEMENT_HANDLING.attributeNameCheck\n      _isBasicCustomElement(lcTag) && (CUSTOM_ELEMENT_HANDLING.tagNameCheck instanceof RegExp && regExpTest(CUSTOM_ELEMENT_HANDLING.tagNameCheck, lcTag) || CUSTOM_ELEMENT_HANDLING.tagNameCheck instanceof Function && CUSTOM_ELEMENT_HANDLING.tagNameCheck(lcTag)) && (CUSTOM_ELEMENT_HANDLING.attributeNameCheck instanceof RegExp && regExpTest(CUSTOM_ELEMENT_HANDLING.attributeNameCheck, lcName) || CUSTOM_ELEMENT_HANDLING.attributeNameCheck instanceof Function && CUSTOM_ELEMENT_HANDLING.attributeNameCheck(lcName)) ||\n      // Alternative, second condition checks if it's an `is`-attribute, AND\n      // the value passes whatever the user has configured for CUSTOM_ELEMENT_HANDLING.tagNameCheck\n      lcName === 'is' && CUSTOM_ELEMENT_HANDLING.allowCustomizedBuiltInElements && (CUSTOM_ELEMENT_HANDLING.tagNameCheck instanceof RegExp && regExpTest(CUSTOM_ELEMENT_HANDLING.tagNameCheck, value) || CUSTOM_ELEMENT_HANDLING.tagNameCheck instanceof Function && CUSTOM_ELEMENT_HANDLING.tagNameCheck(value))) ; else {\n        return false;\n      }\n      /* Check value is safe. First, is attr inert? If so, is safe */\n    } else if (URI_SAFE_ATTRIBUTES[lcName]) ; else if (regExpTest(IS_ALLOWED_URI$1, stringReplace(value, ATTR_WHITESPACE, ''))) ; else if ((lcName === 'src' || lcName === 'xlink:href' || lcName === 'href') && lcTag !== 'script' && stringIndexOf(value, 'data:') === 0 && DATA_URI_TAGS[lcTag]) ; else if (ALLOW_UNKNOWN_PROTOCOLS && !regExpTest(IS_SCRIPT_OR_DATA, stringReplace(value, ATTR_WHITESPACE, ''))) ; else if (value) {\n      return false;\n    } else ;\n    return true;\n  };\n  /**\n   * _isBasicCustomElement\n   * checks if at least one dash is included in tagName, and it's not the first char\n   * for more sophisticated checking see https://github.com/sindresorhus/validate-element-name\n   *\n   * @param tagName name of the tag of the node to sanitize\n   * @returns Returns true if the tag name meets the basic criteria for a custom element, otherwise false.\n   */\n  const _isBasicCustomElement = function _isBasicCustomElement(tagName) {\n    return tagName !== 'annotation-xml' && stringMatch(tagName, CUSTOM_ELEMENT);\n  };\n  /**\n   * _sanitizeAttributes\n   *\n   * @protect attributes\n   * @protect nodeName\n   * @protect removeAttribute\n   * @protect setAttribute\n   *\n   * @param currentNode to sanitize\n   */\n  const _sanitizeAttributes = function _sanitizeAttributes(currentNode) {\n    /* Execute a hook if present */\n    _executeHooks(hooks.beforeSanitizeAttributes, currentNode, null);\n    const {\n      attributes\n    } = currentNode;\n    /* Check if we have attributes; if not we might have a text node */\n    if (!attributes || _isClobbered(currentNode)) {\n      return;\n    }\n    const hookEvent = {\n      attrName: '',\n      attrValue: '',\n      keepAttr: true,\n      allowedAttributes: ALLOWED_ATTR,\n      forceKeepAttr: undefined\n    };\n    let l = attributes.length;\n    /* Go backwards over all attributes; safely remove bad ones */\n    while (l--) {\n      const attr = attributes[l];\n      const {\n        name,\n        namespaceURI,\n        value: attrValue\n      } = attr;\n      const lcName = transformCaseFunc(name);\n      let value = name === 'value' ? attrValue : stringTrim(attrValue);\n      /* Execute a hook if present */\n      hookEvent.attrName = lcName;\n      hookEvent.attrValue = value;\n      hookEvent.keepAttr = true;\n      hookEvent.forceKeepAttr = undefined; // Allows developers to see this is a property they can set\n      _executeHooks(hooks.uponSanitizeAttribute, currentNode, hookEvent);\n      value = hookEvent.attrValue;\n      /* Full DOM Clobbering protection via namespace isolation,\n       * Prefix id and name attributes with `user-content-`\n       */\n      if (SANITIZE_NAMED_PROPS && (lcName === 'id' || lcName === 'name')) {\n        // Remove the attribute with this value\n        _removeAttribute(name, currentNode);\n        // Prefix the value and later re-create the attribute with the sanitized value\n        value = SANITIZE_NAMED_PROPS_PREFIX + value;\n      }\n      /* Work around a security issue with comments inside attributes */\n      if (SAFE_FOR_XML && regExpTest(/((--!?|])>)|<\\/(style|title)/i, value)) {\n        _removeAttribute(name, currentNode);\n        continue;\n      }\n      /* Did the hooks approve of the attribute? */\n      if (hookEvent.forceKeepAttr) {\n        continue;\n      }\n      /* Remove attribute */\n      _removeAttribute(name, currentNode);\n      /* Did the hooks approve of the attribute? */\n      if (!hookEvent.keepAttr) {\n        continue;\n      }\n      /* Work around a security issue in jQuery 3.0 */\n      if (!ALLOW_SELF_CLOSE_IN_ATTR && regExpTest(/\\/>/i, value)) {\n        _removeAttribute(name, currentNode);\n        continue;\n      }\n      /* Sanitize attribute content to be template-safe */\n      if (SAFE_FOR_TEMPLATES) {\n        arrayForEach([MUSTACHE_EXPR, ERB_EXPR, TMPLIT_EXPR], expr => {\n          value = stringReplace(value, expr, ' ');\n        });\n      }\n      /* Is `value` valid for this attribute? */\n      const lcTag = transformCaseFunc(currentNode.nodeName);\n      if (!_isValidAttribute(lcTag, lcName, value)) {\n        continue;\n      }\n      /* Handle attributes that require Trusted Types */\n      if (trustedTypesPolicy && typeof trustedTypes === 'object' && typeof trustedTypes.getAttributeType === 'function') {\n        if (namespaceURI) ; else {\n          switch (trustedTypes.getAttributeType(lcTag, lcName)) {\n            case 'TrustedHTML':\n              {\n                value = trustedTypesPolicy.createHTML(value);\n                break;\n              }\n            case 'TrustedScriptURL':\n              {\n                value = trustedTypesPolicy.createScriptURL(value);\n                break;\n              }\n          }\n        }\n      }\n      /* Handle invalid data-* attribute set by try-catching it */\n      try {\n        if (namespaceURI) {\n          currentNode.setAttributeNS(namespaceURI, name, value);\n        } else {\n          /* Fallback to setAttribute() for browser-unrecognized namespaces e.g. \"x-schema\". */\n          currentNode.setAttribute(name, value);\n        }\n        if (_isClobbered(currentNode)) {\n          _forceRemove(currentNode);\n        } else {\n          arrayPop(DOMPurify.removed);\n        }\n      } catch (_) {}\n    }\n    /* Execute a hook if present */\n    _executeHooks(hooks.afterSanitizeAttributes, currentNode, null);\n  };\n  /**\n   * _sanitizeShadowDOM\n   *\n   * @param fragment to iterate over recursively\n   */\n  const _sanitizeShadowDOM = function _sanitizeShadowDOM(fragment) {\n    let shadowNode = null;\n    const shadowIterator = _createNodeIterator(fragment);\n    /* Execute a hook if present */\n    _executeHooks(hooks.beforeSanitizeShadowDOM, fragment, null);\n    while (shadowNode = shadowIterator.nextNode()) {\n      /* Execute a hook if present */\n      _executeHooks(hooks.uponSanitizeShadowNode, shadowNode, null);\n      /* Sanitize tags and elements */\n      _sanitizeElements(shadowNode);\n      /* Check attributes next */\n      _sanitizeAttributes(shadowNode);\n      /* Deep shadow DOM detected */\n      if (shadowNode.content instanceof DocumentFragment) {\n        _sanitizeShadowDOM(shadowNode.content);\n      }\n    }\n    /* Execute a hook if present */\n    _executeHooks(hooks.afterSanitizeShadowDOM, fragment, null);\n  };\n  // eslint-disable-next-line complexity\n  DOMPurify.sanitize = function (dirty) {\n    let cfg = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : {};\n    let body = null;\n    let importedNode = null;\n    let currentNode = null;\n    let returnNode = null;\n    /* Make sure we have a string to sanitize.\n      DO NOT return early, as this will return the wrong type if\n      the user has requested a DOM object rather than a string */\n    IS_EMPTY_INPUT = !dirty;\n    if (IS_EMPTY_INPUT) {\n      dirty = '<!-->';\n    }\n    /* Stringify, in case dirty is an object */\n    if (typeof dirty !== 'string' && !_isNode(dirty)) {\n      if (typeof dirty.toString === 'function') {\n        dirty = dirty.toString();\n        if (typeof dirty !== 'string') {\n          throw typeErrorCreate('dirty is not a string, aborting');\n        }\n      } else {\n        throw typeErrorCreate('toString is not a function');\n      }\n    }\n    /* Return dirty HTML if DOMPurify cannot run */\n    if (!DOMPurify.isSupported) {\n      return dirty;\n    }\n    /* Assign config vars */\n    if (!SET_CONFIG) {\n      _parseConfig(cfg);\n    }\n    /* Clean up removed elements */\n    DOMPurify.removed = [];\n    /* Check if dirty is correctly typed for IN_PLACE */\n    if (typeof dirty === 'string') {\n      IN_PLACE = false;\n    }\n    if (IN_PLACE) {\n      /* Do some early pre-sanitization to avoid unsafe root nodes */\n      if (dirty.nodeName) {\n        const tagName = transformCaseFunc(dirty.nodeName);\n        if (!ALLOWED_TAGS[tagName] || FORBID_TAGS[tagName]) {\n          throw typeErrorCreate('root node is forbidden and cannot be sanitized in-place');\n        }\n      }\n    } else if (dirty instanceof Node) {\n      /* If dirty is a DOM element, append to an empty document to avoid\n         elements being stripped by the parser */\n      body = _initDocument('<!---->');\n      importedNode = body.ownerDocument.importNode(dirty, true);\n      if (importedNode.nodeType === NODE_TYPE.element && importedNode.nodeName === 'BODY') {\n        /* Node is already a body, use as is */\n        body = importedNode;\n      } else if (importedNode.nodeName === 'HTML') {\n        body = importedNode;\n      } else {\n        // eslint-disable-next-line unicorn/prefer-dom-node-append\n        body.appendChild(importedNode);\n      }\n    } else {\n      /* Exit directly if we have nothing to do */\n      if (!RETURN_DOM && !SAFE_FOR_TEMPLATES && !WHOLE_DOCUMENT &&\n      // eslint-disable-next-line unicorn/prefer-includes\n      dirty.indexOf('<') === -1) {\n        return trustedTypesPolicy && RETURN_TRUSTED_TYPE ? trustedTypesPolicy.createHTML(dirty) : dirty;\n      }\n      /* Initialize the document to work on */\n      body = _initDocument(dirty);\n      /* Check we have a DOM node from the data */\n      if (!body) {\n        return RETURN_DOM ? null : RETURN_TRUSTED_TYPE ? emptyHTML : '';\n      }\n    }\n    /* Remove first element node (ours) if FORCE_BODY is set */\n    if (body && FORCE_BODY) {\n      _forceRemove(body.firstChild);\n    }\n    /* Get node iterator */\n    const nodeIterator = _createNodeIterator(IN_PLACE ? dirty : body);\n    /* Now start iterating over the created document */\n    while (currentNode = nodeIterator.nextNode()) {\n      /* Sanitize tags and elements */\n      _sanitizeElements(currentNode);\n      /* Check attributes next */\n      _sanitizeAttributes(currentNode);\n      /* Shadow DOM detected, sanitize it */\n      if (currentNode.content instanceof DocumentFragment) {\n        _sanitizeShadowDOM(currentNode.content);\n      }\n    }\n    /* If we sanitized `dirty` in-place, return it. */\n    if (IN_PLACE) {\n      return dirty;\n    }\n    /* Return sanitized string or DOM */\n    if (RETURN_DOM) {\n      if (RETURN_DOM_FRAGMENT) {\n        returnNode = createDocumentFragment.call(body.ownerDocument);\n        while (body.firstChild) {\n          // eslint-disable-next-line unicorn/prefer-dom-node-append\n          returnNode.appendChild(body.firstChild);\n        }\n      } else {\n        returnNode = body;\n      }\n      if (ALLOWED_ATTR.shadowroot || ALLOWED_ATTR.shadowrootmode) {\n        /*\n          AdoptNode() is not used because internal state is not reset\n          (e.g. the past names map of a HTMLFormElement), this is safe\n          in theory but we would rather not risk another attack vector.\n          The state that is cloned by importNode() is explicitly defined\n          by the specs.\n        */\n        returnNode = importNode.call(originalDocument, returnNode, true);\n      }\n      return returnNode;\n    }\n    let serializedHTML = WHOLE_DOCUMENT ? body.outerHTML : body.innerHTML;\n    /* Serialize doctype if allowed */\n    if (WHOLE_DOCUMENT && ALLOWED_TAGS['!doctype'] && body.ownerDocument && body.ownerDocument.doctype && body.ownerDocument.doctype.name && regExpTest(DOCTYPE_NAME, body.ownerDocument.doctype.name)) {\n      serializedHTML = '<!DOCTYPE ' + body.ownerDocument.doctype.name + '>\\n' + serializedHTML;\n    }\n    /* Sanitize final string template-safe */\n    if (SAFE_FOR_TEMPLATES) {\n      arrayForEach([MUSTACHE_EXPR, ERB_EXPR, TMPLIT_EXPR], expr => {\n        serializedHTML = stringReplace(serializedHTML, expr, ' ');\n      });\n    }\n    return trustedTypesPolicy && RETURN_TRUSTED_TYPE ? trustedTypesPolicy.createHTML(serializedHTML) : serializedHTML;\n  };\n  DOMPurify.setConfig = function () {\n    let cfg = arguments.length > 0 && arguments[0] !== undefined ? arguments[0] : {};\n    _parseConfig(cfg);\n    SET_CONFIG = true;\n  };\n  DOMPurify.clearConfig = function () {\n    CONFIG = null;\n    SET_CONFIG = false;\n  };\n  DOMPurify.isValidAttribute = function (tag, attr, value) {\n    /* Initialize shared config vars if necessary. */\n    if (!CONFIG) {\n      _parseConfig({});\n    }\n    const lcTag = transformCaseFunc(tag);\n    const lcName = transformCaseFunc(attr);\n    return _isValidAttribute(lcTag, lcName, value);\n  };\n  DOMPurify.addHook = function (entryPoint, hookFunction) {\n    if (typeof hookFunction !== 'function') {\n      return;\n    }\n    arrayPush(hooks[entryPoint], hookFunction);\n  };\n  DOMPurify.removeHook = function (entryPoint, hookFunction) {\n    if (hookFunction !== undefined) {\n      const index = arrayLastIndexOf(hooks[entryPoint], hookFunction);\n      return index === -1 ? undefined : arraySplice(hooks[entryPoint], index, 1)[0];\n    }\n    return arrayPop(hooks[entryPoint]);\n  };\n  DOMPurify.removeHooks = function (entryPoint) {\n    hooks[entryPoint] = [];\n  };\n  DOMPurify.removeAllHooks = function () {\n    hooks = _createHooksMap();\n  };\n  return DOMPurify;\n}\nvar purify = createDOMPurify();\n\nexport { purify as default };\n//# sourceMappingURL=purify.es.mjs.map\n", "import createDOMPurify from \"dompurify\";\nimport { JSDOM } from \"jsdom\";\n\nconst windowEmulator: any = new JSDOM(\"\").window;\nconst DOMPurify = createDOMPurify(windowEmulator);\n\n/*\n * DOMPurify prevents all XSS attacks by default. With these settings, it also\n * prevents \"deception\" attacks. If an attacker could put <div style=\"...\">\n * into the site's admin banner, they could make give the banner any appearance,\n * overlaid anywhere on the page. For example, a fake \"session expired\" modal\n * with a malicious link. Thus, this very strict DOMPurify config.\n */\nDOMPurify.setConfig({\n  // Only these tags will be allowed through\n  ALLOWED_TAGS: [\"ul\", \"ol\", \"li\", \"a\", \"#text\"],\n  // On those tags, only these attributes are allowed\n  ALLOWED_ATTR: [\"href\", \"alt\"],\n  // If a tag is removed, so will all its child elements & text\n  KEEP_CONTENT: false,\n});\n\n// sanitize string\nexport const sanitizeString = (string: string) => {\n  if (DOMPurify.isSupported) {\n    return DOMPurify.sanitize(string);\n  }\n};\n\n// iterates over array items, sanitizing items recursively\nexport const sanitizeArray = (array: unknown[]): unknown[] =>\n  array.map((entry: unknown) => sanitizeEntry(entry));\n\n// iterates over object key-value pairs, sanitizing values recursively\nexport const sanitizeObject = (object: { [key: string]: unknown }) => {\n  if (object) {\n    const entries = Object.entries(object);\n    const sanitizedEntries = entries.map((entry: [string, unknown]) => {\n      const [key, value] = entry;\n      return [key, sanitizeEntry(value)];\n    });\n    return Object.fromEntries(sanitizedEntries);\n  }\n};\n\nconst sanitizerMap: any = {\n  string: sanitizeString,\n  array: sanitizeArray,\n  object: sanitizeObject,\n};\n\n// return sanitized entry, or if safe type, return entry\nconst sanitizeEntry = (entry: unknown) => {\n  const entryType = Array.isArray(entry) ? \"array\" : typeof entry;\n  const sanitizer = sanitizerMap[entryType];\n  return sanitizer?.(entry) || entry;\n};\n", "// utils\nimport * as logger from \"../utils/debugging/debug-lib\";\nimport { isAuthenticated } from \"../utils/auth/authorization\";\nimport {\n  HttpResponse,\n  internalServerError,\n  unauthenticated,\n} from \"../utils/responses/response-lib\";\nimport { error } from \"../utils/constants/constants\";\nimport { sanitizeObject } from \"../utils/sanitize/sanitize\";\n// types\nimport { APIGatewayProxyEvent } from \"../utils/types\";\n\ntype LambdaFunction = (\n  event: APIGatewayProxyEvent, // eslint-disable-line no-unused-vars\n  context: any // eslint-disable-line no-unused-vars\n) => Promise<HttpResponse>;\n\nexport default function handler(lambda: LambdaFunction) {\n  return async function (event: APIGatewayProxyEvent, context: any) {\n    // Start debugger\n    logger.init();\n    logger.debug(\"API event: %O\", {\n      body: event.body,\n      pathParameters: event.pathParameters,\n      queryStringParameters: event.queryStringParameters,\n    });\n    if (await isAuthenticated(event)) {\n      try {\n        if (event.body) {\n          const newEventBody = sanitizeObject(JSON.parse(event.body));\n          event.body = JSON.stringify(newEventBody);\n        }\n        return await lambda(event, context);\n      } catch (error: any) {\n        logger.error(\"Error: %O\", error);\n\n        const body = { error: error.message };\n        return internalServerError(body);\n      } finally {\n        logger.flush();\n      }\n    } else {\n      return unauthenticated(error.UNAUTHORIZED);\n    }\n  };\n}\n", "import { object } from \"yup\";\nimport { error } from \"../constants/constants\";\n// types\nimport { AnyObject } from \"../types\";\n// utils\nimport { nested, endDate } from \"./completionSchemas\";\nimport { completionSchemaMap as schemaMap } from \"./completionSchemaMap\";\n\n// map field validation types to validation schema\nexport const mapValidationTypesToSchema = (fieldValidationTypes: AnyObject) => {\n  let validationSchema: AnyObject = {};\n  // for each field to be validated,\n  Object.entries(fieldValidationTypes).forEach(\n    (fieldValidationType: [string, string | AnyObject]) => {\n      const [key, fieldValidation] = fieldValidationType;\n\n      // if standard validation type, set corresponding schema from map\n      if (typeof fieldValidation === \"string\") {\n        const correspondingSchema = schemaMap[fieldValidation];\n        if (correspondingSchema) {\n          validationSchema[key] = correspondingSchema;\n        }\n      }\n      // else if nested validation type, make and set nested schema\n      else if (fieldValidation.nested) {\n        validationSchema[key] = makeNestedFieldSchema(fieldValidation);\n        // else if not nested, make and set other dependent field types\n      } else if (fieldValidation.type === \"endDate\") {\n        validationSchema[key] = makeEndDateFieldSchema(fieldValidation);\n      }\n    }\n  );\n  return validationSchema;\n};\n\n// return created endDate schema\nexport const makeEndDateFieldSchema = (fieldValidationObject: AnyObject) => {\n  const { dependentFieldName } = fieldValidationObject;\n  return endDate(dependentFieldName);\n};\n\n// return created nested field schema\nexport const makeNestedFieldSchema = (fieldValidationObject: AnyObject) => {\n  const { type, parentFieldName, parentOptionId } = fieldValidationObject;\n  if (fieldValidationObject.type === \"endDate\") {\n    return nested(\n      () => makeEndDateFieldSchema(fieldValidationObject),\n      parentFieldName,\n      parentOptionId\n    );\n  } else {\n    const fieldValidationSchema = schemaMap[type];\n    return nested(() => fieldValidationSchema, parentFieldName, parentOptionId);\n  }\n};\n\nexport const validateFieldData = async (\n  validationJson: AnyObject,\n  unvalidatedFieldData: AnyObject\n) => {\n  let validatedFieldData: AnyObject | undefined = undefined;\n  // filter field validation to just what's needed for the passed fields\n  const filteredFieldDataValidationJson = filterValidationSchema(\n    validationJson,\n    unvalidatedFieldData\n  );\n  // transform field validation instructions to yup validation schema\n  const fieldDataValidationSchema = object().shape(\n    mapValidationTypesToSchema(filteredFieldDataValidationJson)\n  );\n  if (fieldDataValidationSchema) {\n    validatedFieldData = await validateData(\n      fieldDataValidationSchema,\n      unvalidatedFieldData\n    );\n  }\n  return validatedFieldData;\n};\n\n// filter field validation to just what's needed for the passed fields\nexport const filterValidationSchema = (\n  validationObject: AnyObject,\n  data: AnyObject\n): AnyObject => {\n  const validationEntries = Object.entries(validationObject);\n  const dataKeys = Object.keys(data);\n  const filteredEntries = validationEntries.filter(\n    (entry: [string, string | AnyObject]) => {\n      const [entryKey] = entry;\n      return dataKeys.includes(entryKey);\n    }\n  );\n  return Object.fromEntries(filteredEntries);\n};\n\nexport const validateData = async (\n  validationSchema: AnyObject,\n  data: AnyObject,\n  options?: AnyObject\n) => {\n  try {\n    // returns valid data to be passed through API\n    return await validationSchema.validate(data, {\n      stripUnknown: true,\n      ...options,\n    });\n  } catch {\n    throw new Error(error.INVALID_DATA);\n  }\n};\n", "import {\n  array,\n  boolean,\n  mixed,\n  object,\n  string,\n  number as yupNumber,\n} from \"yup\";\nimport { Choice } from \"../types\";\n\nexport const error = {\n  REQUIRED_GENERIC: \"A response is required\",\n  REQUIRED_CHECKBOX: \"Select at least one response\",\n  INVALID_GENERIC: \"Response must be valid\",\n  INVALID_EMAIL: \"Response must be a valid email address\",\n  INVALID_URL: \"Response must be a valid hyperlink/URL\",\n  INVALID_DATE: \"Response must be a valid date\",\n  INVALID_END_DATE: \"End date can't be before start date\",\n  NUMBER_LESS_THAN_ZERO: \"Response must be greater than or equal to zero\",\n  INVALID_NUMBER: \"Response must be a valid number\",\n  INVALID_NUMBER_OR_NA: 'Response must be a valid number or \"N/A\"',\n  INVALID_RATIO: \"Response must be a valid ratio\",\n};\n\n// TEXT - Helpers\nconst isWhitespaceString = (value?: string) => value?.trim().length === 0;\n\n// TEXT\nconst textSchema = () =>\n  string()\n    .typeError(error.INVALID_GENERIC)\n    .test({\n      test: (value) => !isWhitespaceString(value),\n      message: error.REQUIRED_GENERIC,\n    });\n\nexport const text = () => textSchema().required();\nexport const textOptional = () => textSchema().notRequired().nullable();\n\n// NUMBER - Helpers\nconst validNAValues = [\"N/A\", \"Data not available\"];\n\n/** This regex must be at least as permissive as the one in ui-src */\nconst validNumberRegex = /^\\.$|[0-9]/;\n\nconst validIntegerRegex = /^[0-9\\s,]+$/;\n\n// NUMBER - Number or Valid Strings\nconst numberSchema = () =>\n  string()\n    .test({\n      message: error.INVALID_NUMBER_OR_NA,\n      test: (value) => {\n        if (value) {\n          const isValidStringValue = validNAValues.includes(value);\n          const isValidNumberValue = validNumberRegex.test(value);\n          return isValidStringValue || isValidNumberValue;\n        } else return true;\n      },\n    })\n    .test({\n      test: (value) => !isWhitespaceString(value),\n      message: error.REQUIRED_GENERIC,\n    })\n    .test({\n      test: (value) => {\n        if (validNumberRegex.test(value!)) {\n          return parseFloat(value!) >= 0;\n        } else return true;\n      },\n      message: error.NUMBER_LESS_THAN_ZERO,\n    });\n\nconst valueCleaningNumberSchema = (value: string, charsToReplace: RegExp) => {\n  return yupNumber().transform((_value) => {\n    return Number(value.replace(charsToReplace, \"\"));\n  });\n};\n\nexport const number = () => numberSchema().required();\nexport const numberOptional = () => numberSchema().notRequired().nullable();\n\n// Integer or Valid Strings\nexport const validIntegerSchema = () =>\n  string().test({\n    message: error.INVALID_NUMBER_OR_NA,\n    test: (value) => {\n      if (value) {\n        const isValidStringValue = validNAValues.includes(value);\n        const isValidIntegerValue = validIntegerRegex.test(value);\n        return isValidStringValue || isValidIntegerValue;\n      } else return true;\n    },\n  });\n\nexport const validInteger = () =>\n  validIntegerSchema()\n    .required(error.REQUIRED_GENERIC)\n    .test({\n      test: (value) => !isWhitespaceString(value),\n      message: error.REQUIRED_GENERIC,\n    });\n\nexport const validIntegerOptional = () =>\n  validIntegerSchema().notRequired().nullable();\n\n// Number - Ratio\nexport const ratio = () =>\n  mixed()\n    .test({\n      message: error.REQUIRED_GENERIC,\n      test: (val) => val != \"\",\n    })\n    .required(error.REQUIRED_GENERIC)\n    .test({\n      message: error.INVALID_RATIO,\n      test: (val) => {\n        const replaceCharsRegex = /[,.:]/g;\n        const ratio = val?.split(\":\");\n\n        // Double check and make sure that a ratio contains numbers on both sides\n        if (\n          !ratio ||\n          ratio.length != 2 ||\n          ratio[0].trim().length == 0 ||\n          ratio[1].trim().length == 0\n        ) {\n          return false;\n        }\n\n        // Check if the left side of the ratio is a valid number\n        const firstTest = valueCleaningNumberSchema(\n          ratio[0],\n          replaceCharsRegex\n        ).isValidSync(val);\n\n        // Check if the right side of the ratio is a valid number\n        const secondTest = valueCleaningNumberSchema(\n          ratio[1],\n          replaceCharsRegex\n        ).isValidSync(val);\n\n        // If both sides are valid numbers, return true!\n        return firstTest && secondTest;\n      },\n    });\n\n// EMAIL\n\nexport const email = () => textSchema().email(error.INVALID_EMAIL).required();\nexport const emailOptional = () =>\n  textSchema().email(error.INVALID_EMAIL).notRequired().nullable();\n\n// URL\nexport const url = () => textSchema().url(error.INVALID_URL).required();\nexport const urlOptional = () =>\n  textSchema().url(error.INVALID_URL).notRequired().nullable();\n\n// DATE\nconst dateSchema = () =>\n  string()\n    .matches(dateFormatRegex, error.INVALID_DATE)\n    .test({\n      message: error.REQUIRED_GENERIC,\n      test: (value) => !isWhitespaceString(value),\n    });\n\nexport const date = () => dateSchema().required(error.REQUIRED_GENERIC);\nexport const dateOptional = () => dateSchema().notRequired().nullable();\n\nexport const endDate = (startDateField: string) =>\n  date()\n    .typeError(error.INVALID_DATE)\n    .test({\n      message: error.INVALID_END_DATE,\n      test: (endDateString, context) => {\n        return isEndDateAfterStartDate(\n          context.parent[startDateField],\n          endDateString as string\n        );\n      },\n    });\n\nexport const isEndDateAfterStartDate = (\n  startDateString: string,\n  endDateString: string\n) => {\n  const startDate = new Date(startDateString);\n  const endDate = new Date(endDateString!);\n  return endDate >= startDate;\n};\n\n// DROPDOWN\nexport const dropdown = () =>\n  object({ label: textSchema(), value: textSchema() }).required(\n    error.REQUIRED_GENERIC\n  );\n\n// CHECKBOX\nexport const checkboxSchema = () =>\n  array()\n    .of(object({ key: text(), value: text() }))\n    .required(error.REQUIRED_GENERIC);\nexport const checkbox = () =>\n  checkboxSchema()\n    .min(1, error.REQUIRED_GENERIC)\n    .required(error.REQUIRED_GENERIC);\nexport const checkboxOptional = () =>\n  checkboxSchema().min(0, error.REQUIRED_GENERIC).notRequired().nullable();\nexport const checkboxSingle = () => boolean();\n\n// RADIO\nexport const radioSchema = () =>\n  array()\n    .of(object({ key: textSchema(), value: textSchema() }))\n    .min(0);\n\nexport const radio = () =>\n  radioSchema().min(1, error.REQUIRED_GENERIC).required();\nexport const radioOptional = () => radioSchema().notRequired().nullable();\n\n// DYNAMIC\nexport const dynamic = () =>\n  array()\n    .min(1)\n    .of(\n      object().shape({\n        id: textSchema(),\n        name: textSchema(),\n      })\n    )\n    .required(error.REQUIRED_GENERIC);\nexport const dynamicOptional = () => dynamic().notRequired();\n\n// NESTED\nexport const nested = (\n  fieldSchema: Function,\n  parentFieldName: string,\n  parentOptionId: string\n) => {\n  const fieldTypeMap = {\n    array: array(),\n    string: string(),\n    date: dateSchema(),\n    object: object(),\n  };\n  const fieldType: keyof typeof fieldTypeMap = fieldSchema().type;\n  const baseSchema: any = fieldTypeMap[fieldType];\n  return baseSchema.when(parentFieldName, {\n    is: (value: Choice[]) =>\n      // look for parentOptionId in checked choices\n      value?.find((option: Choice) => option.key.endsWith(parentOptionId)),\n    then: () => fieldSchema(), // returns standard field schema (required)\n    otherwise: () => baseSchema, // returns not-required Yup base schema\n  });\n};\n\n// REGEX\nexport const dateFormatRegex =\n  /^((0[1-9]|1[0-2])\\/(0[1-9]|1\\d|2\\d|3[01])\\/(19|20)\\d{2})|((0[1-9]|1[0-2])(0[1-9]|1\\d|2\\d|3[01])(19|20)\\d{2})$/;\n", "import * as schema from \"./completionSchemas\";\n\nexport const completionSchemaMap: any = {\n  text: schema.text(),\n  textOptional: schema.textOptional(),\n  number: schema.number(),\n  numberOptional: schema.numberOptional(),\n  ratio: schema.ratio(),\n  email: schema.email(),\n  emailOptional: schema.emailOptional(),\n  url: schema.url(),\n  urlOptional: schema.urlOptional(),\n  date: schema.date(),\n  dateOptional: schema.dateOptional(),\n  dropdown: schema.dropdown(),\n  checkbox: schema.checkbox(),\n  checkboxOptional: schema.checkboxOptional(),\n  checkboxSingle: schema.checkboxSingle(),\n  radio: schema.radio(),\n  radioOptional: schema.radioOptional(),\n  dynamic: schema.dynamic(),\n  dynamicOptional: schema.dynamicOptional(),\n  validInteger: schema.validInteger(),\n  validIntegerOptional: schema.validIntegerOptional(),\n};\n", "// types\nimport { DEFAULT_TARGET_POPULATION_NAMES } from \"../constants/constants\";\nimport {\n  AnyObject,\n  CompletionData,\n  FormJson,\n  FieldChoice,\n  Choice,\n  FormField,\n  ReportRoute,\n} from \"../types\";\n// utils\nimport { validateFieldData } from \"./completionValidation\";\n\nexport const isComplete = (completionStatus: CompletionData): boolean => {\n  const flatten = (obj: AnyObject, out: AnyObject) => {\n    Object.keys(obj).forEach((key) => {\n      if (typeof obj[key] == \"object\") {\n        out = flatten(obj[key], out);\n      } else {\n        out[key] = obj[key];\n      }\n    });\n    return out;\n  };\n\n  const flattenedStatus = flatten(completionStatus, {});\n\n  for (const status in flattenedStatus) {\n    if (flattenedStatus[status] === false) {\n      return false;\n    }\n  }\n  return true;\n};\n\n// Entry point for calculating completion status\nexport const calculateCompletionStatus = async (\n  fieldData: AnyObject,\n  formTemplate: AnyObject\n) => {\n  // Parent Dictionary for holding all route completion status\n\n  const validationJson = formTemplate.validationJson;\n\n  const areFieldsValid = async (\n    fieldsToBeValidated: Record<string, string>\n  ) => {\n    let areAllFieldsValid = false;\n    try {\n      // all fields successfully validated if validatedFields is not undefined\n      areAllFieldsValid =\n        (await validateFieldData(validationJson, fieldsToBeValidated)) !==\n        undefined;\n    } catch {\n      // Silently ignore error, will result in false\n    }\n    return areAllFieldsValid;\n  };\n\n  const calculateFormCompletion = async (\n    nestedFormTemplate: FormJson,\n    dataForObject: AnyObject = fieldData\n  ) => {\n    // Build an object of k:v for fields to validate\n    let fieldsToBeValidated: Record<string, string> = {};\n    // Repeat fields can't be validated at same time, so holding their completion status here\n    let repeatersValid = true; //default to true in case of no repeat fields\n\n    const getNestedFields = (\n      fieldChoices: FieldChoice[],\n      selectedChoices: Choice[]\n    ) => {\n      let selectedChoicesIds = selectedChoices\n        .map((choice: Choice) => choice.key)\n        .map((choiceId: string) => choiceId?.split(\"-\").pop());\n      let selectedChoicesWithChildren = fieldChoices?.filter(\n        (fieldChoice: FieldChoice) =>\n          selectedChoicesIds.includes(fieldChoice.id) && fieldChoice.children\n      );\n      let fieldIds: string[] = [];\n      selectedChoicesWithChildren?.forEach((selectedChoice: FieldChoice) => {\n        selectedChoice.children?.forEach((childChoice: FormField) => {\n          fieldIds.push(childChoice.id);\n          if (childChoice.props?.choices && dataForObject?.[childChoice.id]) {\n            let childFields = getNestedFields(\n              childChoice.props?.choices,\n              dataForObject[childChoice.id]\n            );\n            fieldIds.push(...childFields);\n          }\n        });\n      });\n      return fieldIds;\n    };\n    // Iterate over all fields in form\n    for (var formField of nestedFormTemplate?.fields || []) {\n      // Key: Form Field ID, Value: Report Data for field\n      if (Array.isArray(dataForObject[formField.id])) {\n        let nestedFields: string[] = getNestedFields(\n          formField.props?.choices,\n          dataForObject[formField.id]\n        );\n        nestedFields?.forEach((nestedField: string) => {\n          fieldsToBeValidated[nestedField] = dataForObject[nestedField]\n            ? dataForObject[nestedField]\n            : null;\n        });\n      }\n\n      fieldsToBeValidated[formField.id] = dataForObject[formField.id]\n        ? dataForObject[formField.id]\n        : null;\n    }\n    // Validate all fields en masse, passing flag that uses required validation schema\n    return repeatersValid && (await areFieldsValid(fieldsToBeValidated));\n  };\n\n  const isDefaultPopulationApplicable = (targetPopulations: AnyObject[]) => {\n    const filteredPopulations = targetPopulations?.filter((population) => {\n      const isDefault = DEFAULT_TARGET_POPULATION_NAMES.includes(\n        population.transitionBenchmarks_targetPopulationName\n      );\n\n      const isApplicable =\n        population?.transitionBenchmarks_applicableToMfpDemonstration?.[0]\n          ?.value === \"Yes\";\n      return isDefault && isApplicable;\n    });\n    return filteredPopulations.length >= 1;\n  };\n\n  const calculateEntityCompletion = async (\n    nestedFormTemplates: FormJson[],\n    entityType: string\n  ) => {\n    let atLeastOneTargetPopApplicable = false;\n    //value for holding combined result\n    var areAllFormsComplete = true;\n    for (var nestedFormTemplate of nestedFormTemplates) {\n      if (fieldData[entityType] && fieldData[entityType].length > 0) {\n        // if target population, at least one must be applicable to be complete\n        if (\n          entityType === \"targetPopulations\" &&\n          nestedFormTemplate?.id === \"tb-drawer\"\n        ) {\n          atLeastOneTargetPopApplicable = isDefaultPopulationApplicable(\n            fieldData[entityType]\n          );\n        }\n        // iterate over each entity (eg transition benchmark)\n        for (var dataForEntity of fieldData[entityType]) {\n          // get completion status for entity, using the correct form template\n          const isEntityComplete = await calculateFormCompletion(\n            nestedFormTemplate,\n            dataForEntity\n          );\n          // update combined result\n          areAllFormsComplete &&= isEntityComplete;\n        }\n      } else {\n        //Entity not present in report data, so check to see if it is required and update combined result\n        areAllFormsComplete &&=\n          formTemplate.entities && !formTemplate.entities[entityType]?.required;\n      }\n    }\n    if (entityType === \"targetPopulations\" && !atLeastOneTargetPopApplicable) {\n      return false;\n    }\n    return areAllFormsComplete;\n  };\n\n  const calculateEntityWithStepsCompletion = async (\n    stepFormTemplates: any[],\n    entityType: string\n  ) => {\n    if (!fieldData[entityType] || fieldData[entityType].length <= 0)\n      return false;\n\n    var areAllFormsComplete = true;\n    for (let i = 0; i < stepFormTemplates.length; i++) {\n      let stepForm = stepFormTemplates[i];\n      for (var entityFields of fieldData[entityType]) {\n        //modal overlay pages should have an array of key stepType in fieldData, automatic false if it doesn't exist or array is empty\n        if (\n          stepForm.pageType === \"overlayModal\" &&\n          (!entityFields[stepForm.stepType] ||\n            entityFields[stepForm.stepType].length <= 0)\n        ) {\n          areAllFormsComplete &&= false;\n        } else if (stepForm.stepType === \"closeOutInformation\") {\n          //skip over closeOut at the moment until we can make WP copies\n        } else {\n          //detemine which fieldData to match to the stepForm\n          const entityFieldsList = entityFields[stepForm.stepType]\n            ? entityFields[stepForm.stepType]\n            : [entityFields];\n          //loop through all children that belong to that entity and validate the values\n          for (var stepFields of entityFieldsList) {\n            if (stepForm?.objectiveCards) {\n              for (let card of stepForm.objectiveCards) {\n                if (card?.modalForm) {\n                  const nestedFormTemplate = card.modalForm;\n\n                  if (nestedFormTemplate?.objectiveId !== stepFields?.id) {\n                    continue;\n                  }\n                  const isEntityComplete = await calculateFormCompletion(\n                    nestedFormTemplate,\n                    stepFields\n                  );\n                  areAllFormsComplete &&= isEntityComplete;\n                }\n              }\n            } else {\n              const nestedFormTemplate = stepForm.form\n                ? stepForm.form\n                : stepForm.modalForm;\n\n              //WP uses modaloverlay so it doesn't have an initiativeId, only SAR does\n              if (\n                nestedFormTemplate?.initiativeId !== stepFields?.id &&\n                formTemplate.type === \"SAR\"\n              ) {\n                continue;\n              }\n              const isEntityComplete = await calculateFormCompletion(\n                nestedFormTemplate,\n                stepFields\n              );\n              areAllFormsComplete &&= isEntityComplete;\n            }\n          }\n        }\n      }\n    }\n\n    return areAllFormsComplete;\n  };\n\n  const calculateDynamicModalOverlayCompletion = async (\n    initiatives: any[],\n    entityType: string\n  ) => {\n    let areAllFormsComplete = true;\n\n    for (let initiative of initiatives) {\n      const isComplete = await calculateEntityWithStepsCompletion(\n        initiative.entitySteps,\n        entityType\n      );\n      if (!isComplete) {\n        areAllFormsComplete = false;\n        break;\n      }\n    }\n    return areAllFormsComplete;\n  };\n\n  const calculateRouteCompletion = async (route: ReportRoute) => {\n    let routeCompletion;\n    // Determine which type of page we are calculating status for\n    switch (route.pageType) {\n      case \"standard\":\n        if (!route.form) break;\n        // Standard forms use simple validation\n        routeCompletion = {\n          [route.path]: await calculateFormCompletion(route.form),\n        };\n        break;\n      case \"drawer\":\n        if (!route.drawerForm) break;\n        routeCompletion = {\n          [route.path]: await calculateEntityCompletion(\n            [route.drawerForm],\n            route.entityType\n          ),\n        };\n        break;\n      case \"modalDrawer\":\n        if (!route.drawerForm || !route.modalForm) break;\n        routeCompletion = {\n          [route.path]: await calculateEntityCompletion(\n            [route.drawerForm, route.modalForm],\n            route.entityType\n          ),\n        };\n        break;\n      case \"modalOverlay\":\n        if (!route.modalForm) break;\n        if (route.entitySteps) {\n          routeCompletion = {\n            [route.path]: await calculateEntityWithStepsCompletion(\n              route.entitySteps as [],\n              route.entityType\n            ),\n          };\n        } else {\n          routeCompletion = {\n            [route.path]: await calculateEntityCompletion(\n              [route.modalForm],\n              route.entityType\n            ),\n          };\n        }\n        break;\n      case \"dynamicModalOverlay\":\n        if (!route.initiatives) break;\n        routeCompletion = {\n          [route.path]: await calculateDynamicModalOverlayCompletion(\n            route.initiatives as [],\n            route.entityType\n          ),\n        };\n        break;\n      case \"reviewSubmit\":\n        // Don't evaluate the review and submit page\n        break;\n      default:\n        if (!route.children) break;\n        // Default behavior indicates that we are not on a form to be evaluated, which implies we have child routes to evaluate\n        routeCompletion = {\n          [route.path]: await calculateRoutesCompletion(route.children),\n        };\n        break;\n    }\n    return routeCompletion;\n  };\n\n  const calculateRoutesCompletion = async (routes: ReportRoute[]) => {\n    var completionDict: CompletionData = {};\n    // Iterate over each route\n    for (var route of routes || []) {\n      // Determine the status of each child in the route\n      const routeCompletionDict = await calculateRouteCompletion(route);\n      // Add completion status to parent dictionary\n      completionDict = { ...completionDict, ...routeCompletionDict };\n    }\n    return completionDict;\n  };\n\n  return await calculateRoutesCompletion(formTemplate.routes);\n};\n", "import { APIGatewayProxyEvent, isReportType, isState } from \"../types\";\nimport { logger } from \"../debugging/debug-lib\";\n\nexport const parseSpecificReportParameters = (event: APIGatewayProxyEvent) => {\n  const { reportType, state, id } = event.pathParameters ?? {};\n  if (!isReportType(reportType)) {\n    logger.warn(\"Invalid report type in path\");\n    return { allParamsValid: false as const };\n  }\n  if (!isState(state)) {\n    logger.warn(\"Invalid state in path\");\n    return { allParamsValid: false as const };\n  }\n  if (!id) {\n    logger.warn(\"Invalid report ID in path\");\n    return { allParamsValid: false as const };\n  }\n  return { allParamsValid: true as const, reportType, state, id };\n};\n\nexport const parseStateReportParameters = (event: APIGatewayProxyEvent) => {\n  const { reportType, state } = event.pathParameters ?? {};\n  if (!isReportType(reportType)) {\n    logger.warn(\"Invalid report type in path\");\n    return { allParamsValid: false as const };\n  }\n  if (!isState(state)) {\n    logger.warn(\"Invalid state in path\");\n    return { allParamsValid: false as const };\n  }\n  return { allParamsValid: true as const, reportType, state };\n};\n", "import { GetObjectCommand, PutObjectCommand } from \"@aws-sdk/client-s3\";\nimport {\n  GetCommand,\n  paginateQuery,\n  PutCommand,\n  QueryCommand,\n} from \"@aws-sdk/lib-dynamodb\";\nimport {\n  FormTemplateVersion,\n  ReportFieldData,\n  ReportJson,\n  ReportMetadataShape,\n  ReportType,\n  State,\n} from \"../utils/types\";\nimport {\n  createClient as createDynamoClient,\n  collectPageItems,\n} from \"./dynamodb-lib\";\nimport { createClient as createS3Client, parseS3Response } from \"./s3-lib\";\nimport { reportBuckets, reportTables } from \"../utils/constants/constants\";\n\nconst dynamoClient = createDynamoClient();\nconst s3Client = createS3Client();\n\nconst formTemplateVersionTable = process.env.FormTemplateVersionsTable!;\n\n/* METADATA (dynamo) */\n\nexport const putReportMetadata = async (metadata: ReportMetadataShape) => {\n  await dynamoClient.send(\n    new PutCommand({\n      TableName: reportTables[metadata.reportType],\n      Item: metadata,\n    })\n  );\n};\n\nexport const queryReportMetadatasForState = async (\n  reportType: ReportType,\n  state: State\n) => {\n  const table = reportTables[reportType];\n  const responsePages = paginateQuery(\n    { client: dynamoClient },\n    {\n      TableName: table,\n      KeyConditionExpression: \"#state = :state\",\n      ExpressionAttributeNames: { \"#state\": \"state\" },\n      ExpressionAttributeValues: { \":state\": state },\n    }\n  );\n  const metadatas = await collectPageItems(responsePages);\n  return metadatas as ReportMetadataShape[];\n};\n\nexport const getReportMetadata = async (\n  reportType: ReportType,\n  state: State,\n  id: string\n) => {\n  const table = reportTables[reportType];\n  const response = await dynamoClient.send(\n    new GetCommand({\n      TableName: table,\n      Key: { state, id },\n    })\n  );\n  return response.Item as ReportMetadataShape | undefined;\n};\n\n/* FIELD DATA (s3) */\n\nexport const putReportFieldData = async (\n  {\n    reportType,\n    state,\n    fieldDataId,\n  }: Pick<ReportMetadataShape, \"reportType\" | \"state\" | \"fieldDataId\">,\n  fieldData: ReportFieldData\n) => {\n  const bucket = reportBuckets[reportType];\n  await s3Client.send(\n    new PutObjectCommand({\n      Bucket: bucket,\n      ContentType: \"application/json\",\n      Key: `fieldData/${state}/${fieldDataId}.json`,\n      Body: JSON.stringify(fieldData),\n    })\n  );\n};\n\nexport const getReportFieldData = async ({\n  reportType,\n  state,\n  fieldDataId,\n}: Pick<ReportMetadataShape, \"reportType\" | \"state\" | \"fieldDataId\">) => {\n  const bucket = reportBuckets[reportType];\n  const response = await s3Client.send(\n    new GetObjectCommand({\n      Bucket: bucket,\n      Key: `fieldData/${state}/${fieldDataId}.json`,\n    })\n  );\n  const fieldData = await parseS3Response(response);\n  return fieldData as ReportFieldData | undefined;\n};\n\n/* FORM TEMPLATES (s3) */\n\nexport const putReportFormTemplate = async (\n  {\n    reportType,\n    formTemplateId,\n  }: Pick<ReportMetadataShape, \"reportType\" | \"formTemplateId\">,\n  formTemplate: ReportJson\n) => {\n  const bucket = reportBuckets[reportType];\n  await s3Client.send(\n    new PutObjectCommand({\n      Bucket: bucket,\n      ContentType: \"application/json\",\n      Key: `formTemplates/${formTemplateId}.json`,\n      Body: JSON.stringify(formTemplate),\n    })\n  );\n};\n\nexport const getReportFormTemplate = async ({\n  reportType,\n  formTemplateId,\n}: Pick<ReportMetadataShape, \"reportType\" | \"formTemplateId\">) => {\n  const bucket = reportBuckets[reportType];\n  const response = await s3Client.send(\n    new GetObjectCommand({\n      Bucket: bucket,\n      Key: `formTemplates/${formTemplateId}.json`,\n    })\n  );\n  const fieldData = await parseS3Response(response);\n  return fieldData as ReportJson | undefined;\n};\n\n/* FORM TEMPLATE VERSIONS (dynamo) */\n\nexport const putFormTemplateVersion = async (\n  formTemplateVersion: FormTemplateVersion\n) => {\n  await dynamoClient.send(\n    new PutCommand({\n      TableName: formTemplateVersionTable,\n      Item: formTemplateVersion,\n    })\n  );\n};\n\nexport const queryFormTemplateVersionByHash = async (\n  reportType: ReportType,\n  md5Hash: string\n) => {\n  const response = await dynamoClient.send(\n    new QueryCommand({\n      TableName: formTemplateVersionTable,\n      IndexName: \"HashIndex\",\n      KeyConditionExpression: \"reportType = :reportType AND md5Hash = :md5Hash\",\n      ExpressionAttributeValues: {\n        \":reportType\": reportType,\n        \":md5Hash\": md5Hash,\n      },\n      Limit: 1,\n    })\n  );\n  return response.Items?.[0] as FormTemplateVersion | undefined;\n};\n\nexport const queryLatestFormTemplateVersionNumber = async (\n  reportType: ReportType\n) => {\n  const response = await dynamoClient.send(\n    new QueryCommand({\n      TableName: formTemplateVersionTable,\n      KeyConditionExpression: \"reportType = :reportType\",\n      ExpressionAttributeValues: { \":reportType\": reportType },\n      Limit: 1,\n      ScanIndexForward: false, // false -> backwards -> highest version first\n    })\n  );\n  const latestFormTemplate = response.Items?.[0] as\n    | FormTemplateVersion\n    | undefined;\n  return latestFormTemplate?.versionNumber ?? 0;\n};\n", "import {\n  DynamoDBClient,\n  QueryCommandOutput,\n  ScanCommandOutput,\n} from \"@aws-sdk/client-dynamodb\";\nimport { DynamoDBDocumentClient, Paginator } from \"@aws-sdk/lib-dynamodb\";\n// utils\nimport { logger } from \"../utils/debugging/debug-lib\";\n\nconst localConfig = {\n  endpoint: process.env.DYNAMODB_URL,\n  region: \"localhost\",\n  credentials: {\n    accessKeyId: \"LOCALFAKEKEY\", // pragma: allowlist secret\n    secretAccessKey: \"LOCALFAKESECRET\", // pragma: allowlist secret\n  },\n  logger,\n};\n\nconst awsConfig = {\n  region: \"us-east-1\",\n  logger,\n};\n\nconst getConfig = () => {\n  return process.env.DYNAMODB_URL ? localConfig : awsConfig;\n};\n\nexport const createClient = () => {\n  return DynamoDBDocumentClient.from(new DynamoDBClient(getConfig()));\n};\n\nexport const collectPageItems = async <\n  T extends QueryCommandOutput | ScanCommandOutput\n>(\n  paginator: Paginator<T>\n) => {\n  let items: Record<string, any>[] = [];\n  for await (let page of paginator) {\n    items = items.concat(page.Items ?? []);\n  }\n  return items;\n};\n", "import { S3Client, GetObjectCommandOutput } from \"@aws-sdk/client-s3\";\nimport { logger } from \"../utils/debugging/debug-lib\";\n\nconst localConfig = {\n  endpoint: process.env.S3_LOCAL_ENDPOINT,\n  region: \"localhost\",\n  forcePathStyle: true,\n  credentials: {\n    accessKeyId: \"S3RVER\", // pragma: allowlist secret\n    secretAccessKey: \"S3RVER\", // pragma: allowlist secret\n  },\n  logger,\n};\n\nconst awsConfig = {\n  region: \"us-east-1\",\n  logger,\n};\n\nconst getConfig = () => {\n  return process.env.S3_LOCAL_ENDPOINT ? localConfig : awsConfig;\n};\n\nexport const createClient = () => new S3Client(getConfig());\n\nexport const parseS3Response = async (response: GetObjectCommandOutput) => {\n  const stringBody = await response.Body?.transformToString();\n  if (!stringBody) {\n    logger.warn(`Empty response from S3`);\n    return undefined;\n  }\n  return JSON.parse(stringBody);\n};\n"],
./.cdk/cdk.out/asset.f9350623bf35220f9dd6197e7dd6e23dbcba067e3d0c25c68bc67a1ef031ce3f/index.js.map:4: TODO: validate topics not null/empty\n    const fetch = this.lookupRequest(apiKeys.Fetch, requests.Fetch)\n\n    // Shuffle topic-partitions to ensure fair response allocation across partitions (KIP-74)\n    const flattenedTopicPartitions = topics.reduce((topicPartitions, { topic, partitions }) => {\n      partitions.forEach(partition => {\n        topicPartitions.push({ topic, partition })\n      })\n      return topicPartitions\n    }, [])\n\n    const shuffledTopicPartitions = shuffle(flattenedTopicPartitions)\n\n    // Consecutive partitions for the same topic can be combined into a single `topic` entry\n    const consolidatedTopicPartitions = shuffledTopicPartitions.reduce(\n      (topicPartitions, { topic, partition }) => {\n        const last = topicPartitions[topicPartitions.length - 1]\n\n        if (last != null && last.topic === topic) {\n          topicPartitions[topicPartitions.length - 1].partitions.push(partition)\n        } else {\n          topicPartitions.push({ topic, partitions: [partition] })\n        }\n\n        return topicPartitions\n      },\n      []\n    )\n\n    return await this[PRIVATE.SEND_REQUEST](\n      fetch({\n        replicaId,\n        isolationLevel,\n        maxWaitTime,\n        minBytes,\n        maxBytes,\n        topics: consolidatedTopicPartitions,\n        rackId,\n      })\n    )\n  }\n\n  /**\n   * @public\n   * @param {object} request\n   * @param {string} request.groupId The group id\n   * @param {number} request.groupGenerationId The generation of the group\n   * @param {string} request.memberId The member id assigned by the group coordinator\n   * @returns {Promise}\n   */\n  async heartbeat({ groupId, groupGenerationId, memberId }) {\n    const heartbeat = this.lookupRequest(apiKeys.Heartbeat, requests.Heartbeat)\n    return await this[PRIVATE.SEND_REQUEST](heartbeat({ groupId, groupGenerationId, memberId }))\n  }\n\n  /**\n   * @public\n   * @param {object} request\n   * @param {string} request.groupId The unique group id\n   * @param {import(\"../protocol/coordinatorTypes\").CoordinatorType} request.coordinatorType The type of coordinator to find\n   * @returns {Promise}\n   */\n  async findGroupCoordinator({ groupId, coordinatorType }) {\n    // TODO: validate groupId, mandatory\n    const findCoordinator = this.lookupRequest(apiKeys.GroupCoordinator, requests.GroupCoordinator)\n    return await this[PRIVATE.SEND_REQUEST](findCoordinator({ groupId, coordinatorType }))\n  }\n\n  /**\n   * @public\n   * @param {object} request\n   * @param {string} request.groupId The unique group id\n   * @param {number} request.sessionTimeout The coordinator considers the consumer dead if it receives\n   *                                no heartbeat after this timeout in ms\n   * @param {number} request.rebalanceTimeout The maximum time that the coordinator will wait for each member\n   *                                  to rejoin when rebalancing the group\n   * @param {string} [request.memberId=\"\"] The assigned consumer id or an empty string for a new consumer\n   * @param {string} [request.protocolType=\"consumer\"] Unique name for class of protocols implemented by group\n   * @param {Array} request.groupProtocols List of protocols that the member supports (assignment strategy)\n   *                                [{ name: 'AssignerName', metadata: '{\"version\": 1, \"topics\": []}' }]\n   * @returns {Promise}\n   */\n  async joinGroup({\n    groupId,\n    sessionTimeout,\n    rebalanceTimeout,\n    memberId = '',\n    protocolType = 'consumer',\n    groupProtocols,\n  }) {\n    const joinGroup = this.lookupRequest(apiKeys.JoinGroup, requests.JoinGroup)\n    const makeRequest = (assignedMemberId = memberId) =>\n      this[PRIVATE.SEND_REQUEST](\n        joinGroup({\n          groupId,\n          sessionTimeout,\n          rebalanceTimeout,\n          memberId: assignedMemberId,\n          protocolType,\n          groupProtocols,\n        })\n      )\n\n    try {\n      return await makeRequest()\n    } catch (error) {\n      if (error.name === 'KafkaJSMemberIdRequired') {\n        return makeRequest(error.memberId)\n      }\n\n      throw error\n    }\n  }\n\n  /**\n   * @public\n   * @param {object} request\n   * @param {string} request.groupId\n   * @param {string} request.memberId\n   * @returns {Promise}\n   */\n  async leaveGroup({ groupId, memberId }) {\n    const leaveGroup = this.lookupRequest(apiKeys.LeaveGroup, requests.LeaveGroup)\n    return await this[PRIVATE.SEND_REQUEST](leaveGroup({ groupId, memberId }))\n  }\n\n  /**\n   * @public\n   * @param {object} request\n   * @param {string} request.groupId\n   * @param {number} request.generationId\n   * @param {string} request.memberId\n   * @param {object} request.groupAssignment\n   * @returns {Promise}\n   */\n  async syncGroup({ groupId, generationId, memberId, groupAssignment }) {\n    const syncGroup = this.lookupRequest(apiKeys.SyncGroup, requests.SyncGroup)\n    return await this[PRIVATE.SEND_REQUEST](\n      syncGroup({\n        groupId,\n        generationId,\n        memberId,\n        groupAssignment,\n      })\n    )\n  }\n\n  /**\n   * @public\n   * @param {object} request\n   * @param {number} request.replicaId=-1 Broker id of the follower. For normal consumers, use -1\n   * @param {number} request.isolationLevel=1 This setting controls the visibility of transactional records (default READ_COMMITTED, Kafka >0.11 only)\n   * @param {TopicPartitionOffset[]} request.topics e.g:\n   *\n   * @typedef {Object} TopicPartitionOffset\n   * @property {string} topic\n   * @property {PartitionOffset[]} partitions\n   *\n   * @typedef {Object} PartitionOffset\n   * @property {number} partition\n   * @property {number} [timestamp=-1]\n   *\n   *\n   * @returns {Promise}\n   */\n  async listOffsets({ replicaId, isolationLevel, topics }) {\n    const listOffsets = this.lookupRequest(apiKeys.ListOffsets, requests.ListOffsets)\n    const result = await this[PRIVATE.SEND_REQUEST](\n      listOffsets({ replicaId, isolationLevel, topics })\n    )\n\n    // ListOffsets >= v1 will return a single `offset` rather than an array of `offsets` (ListOffsets V0).\n    // Normalize to just return `offset`.\n    for (const response of result.responses) {\n      response.partitions = response.partitions.map(({ offsets, ...partitionData }) => {\n        return offsets ? { ...partitionData, offset: offsets.pop() } : partitionData\n      })\n    }\n\n    return result\n  }\n\n  /**\n   * @public\n   * @param {object} request\n   * @param {string} request.groupId\n   * @param {number} request.groupGenerationId\n   * @param {string} request.memberId\n   * @param {number} [request.retentionTime=-1] -1 signals to the broker that its default configuration\n   *                                    should be used.\n   * @param {object} request.topics Topics to commit offsets, e.g:\n   *                  [\n   *                    {\n   *                      topic: 'topic-name',\n   *                      partitions: [\n   *                        { partition: 0, offset: '11' }\n   *                      ]\n   *                    }\n   *                  ]\n   * @returns {Promise}\n   */\n  async offsetCommit({ groupId, groupGenerationId, memberId, retentionTime, topics }) {\n    const offsetCommit = this.lookupRequest(apiKeys.OffsetCommit, requests.OffsetCommit)\n    return await this[PRIVATE.SEND_REQUEST](\n      offsetCommit({\n        groupId,\n        groupGenerationId,\n        memberId,\n        retentionTime,\n        topics,\n      })\n    )\n  }\n\n  /**\n   * @public\n   * @param {object} request\n   * @param {string} request.groupId\n   * @param {object} request.topics - If the topic array is null fetch offsets for all topics. e.g:\n   *                  [\n   *                    {\n   *                      topic: 'topic-name',\n   *                      partitions: [\n   *                        { partition: 0 }\n   *                      ]\n   *                    }\n   *                  ]\n   * @returns {Promise}\n   */\n  async offsetFetch({ groupId, topics }) {\n    const offsetFetch = this.lookupRequest(apiKeys.OffsetFetch, requests.OffsetFetch)\n    return await this[PRIVATE.SEND_REQUEST](offsetFetch({ groupId, topics }))\n  }\n\n  /**\n   * @public\n   * @param {object} request\n   * @param {Array} request.groupIds\n   * @returns {Promise}\n   */\n  async describeGroups({ groupIds }) {\n    const describeGroups = this.lookupRequest(apiKeys.DescribeGroups, requests.DescribeGroups)\n    return await this[PRIVATE.SEND_REQUEST](describeGroups({ groupIds }))\n  }\n\n  /**\n   * @public\n   * @param {object} request\n   * @param {Array} request.topics e.g:\n   *                 [\n   *                   {\n   *                     topic: 'topic-name',\n   *                     numPartitions: 1,\n   *                     replicationFactor: 1\n   *                   }\n   *                 ]\n   * @param {boolean} [request.validateOnly=false] If this is true, the request will be validated, but the topic\n   *                                       won't be created\n   * @param {number} [request.timeout=5000] The time in ms to wait for a topic to be completely created\n   *                                on the controller node\n   * @returns {Promise}\n   */\n  async createTopics({ topics, validateOnly = false, timeout = 5000 }) {\n    const createTopics = this.lookupRequest(apiKeys.CreateTopics, requests.CreateTopics)\n    return await this[PRIVATE.SEND_REQUEST](createTopics({ topics, validateOnly, timeout }))\n  }\n\n  /**\n   * @public\n   * @param {object} request\n   * @param {Array} request.topicPartitions e.g:\n   *                 [\n   *                   {\n   *                     topic: 'topic-name',\n   *                     count: 3,\n   *                     assignments: []\n   *                   }\n   *                 ]\n   * @param {boolean} [request.validateOnly=false] If this is true, the request will be validated, but the topic\n   *                                       won't be created\n   * @param {number} [request.timeout=5000] The time in ms to wait for a topic to be completely created\n   *                                on the controller node\n   * @returns {Promise<void>}\n   */\n  async createPartitions({ topicPartitions, validateOnly = false, timeout = 5000 }) {\n    const createPartitions = this.lookupRequest(apiKeys.CreatePartitions, requests.CreatePartitions)\n    return await this[PRIVATE.SEND_REQUEST](\n      createPartitions({ topicPartitions, validateOnly, timeout })\n    )\n  }\n\n  /**\n   * @public\n   * @param {object} request\n   * @param {string[]} request.topics An array of topics to be deleted\n   * @param {number} [request.timeout=5000] The time in ms to wait for a topic to be completely deleted on the\n   *                                controller node.\n   * @returns {Promise}\n   */\n  async deleteTopics({ topics, timeout = 5000 }) {\n    const deleteTopics = this.lookupRequest(apiKeys.DeleteTopics, requests.DeleteTopics)\n    return await this[PRIVATE.SEND_REQUEST](deleteTopics({ topics, timeout }))\n  }\n\n  /**\n   * @public\n   * @param {object} request\n   * @param {import(\"../../types\").ResourceConfigQuery[]} request.resources\n   *                                 [{\n   *                                   type: RESOURCE_TYPES.TOPIC,\n   *                                   name: 'topic-name',\n   *                                   configNames: ['compression.type', 'retention.ms']\n   *                                 }]\n   * @param {boolean} [request.includeSynonyms=false]\n   * @returns {Promise}\n   */\n  async describeConfigs({ resources, includeSynonyms = false }) {\n    const describeConfigs = this.lookupRequest(apiKeys.DescribeConfigs, requests.DescribeConfigs)\n    return await this[PRIVATE.SEND_REQUEST](describeConfigs({ resources, includeSynonyms }))\n  }\n\n  /**\n   * @public\n   * @param {object} request\n   * @param {import(\"../../types\").IResourceConfig[]} request.resources\n   *                                 [{\n   *                                  type: RESOURCE_TYPES.TOPIC,\n   *                                  name: 'topic-name',\n   *                                  configEntries: [\n   *                                    {\n   *                                      name: 'cleanup.policy',\n   *                                      value: 'compact'\n   *                                    }\n   *                                  ]\n   *                                 }]\n   * @param {boolean} [request.validateOnly=false]\n   * @returns {Promise}\n   */\n  async alterConfigs({ resources, validateOnly = false }) {\n    const alterConfigs = this.lookupRequest(apiKeys.AlterConfigs, requests.AlterConfigs)\n    return await this[PRIVATE.SEND_REQUEST](alterConfigs({ resources, validateOnly }))\n  }\n\n  /**\n   * Send an `InitProducerId` request to fetch a PID and bump the producer epoch.\n   *\n   * Request should be made to the transaction coordinator.\n   * @public\n   * @param {object} request\n   * @param {number} request.transactionTimeout The time in ms to wait for before aborting idle transactions\n   * @param {number} [request.transactionalId] The transactional id or null if the producer is not transactional\n   * @returns {Promise}\n   */\n  async initProducerId({ transactionalId, transactionTimeout }) {\n    const initProducerId = this.lookupRequest(apiKeys.InitProducerId, requests.InitProducerId)\n    return await this[PRIVATE.SEND_REQUEST](initProducerId({ transactionalId, transactionTimeout }))\n  }\n\n  /**\n   * Send an `AddPartitionsToTxn` request to mark a TopicPartition as participating in the transaction.\n   *\n   * Request should be made to the transaction coordinator.\n   * @public\n   * @param {object} request\n   * @param {string} request.transactionalId The transactional id corresponding to the transaction.\n   * @param {number} request.producerId Current producer id in use by the transactional id.\n   * @param {number} request.producerEpoch Current epoch associated with the producer id.\n   * @param {object[]} request.topics e.g:\n   *                  [\n   *                    {\n   *                      topic: 'topic-name',\n   *                      partitions: [ 0, 1]\n   *                    }\n   *                  ]\n   * @returns {Promise}\n   */\n  async addPartitionsToTxn({ transactionalId, producerId, producerEpoch, topics }) {\n    const addPartitionsToTxn = this.lookupRequest(\n      apiKeys.AddPartitionsToTxn,\n      requests.AddPartitionsToTxn\n    )\n    return await this[PRIVATE.SEND_REQUEST](\n      addPartitionsToTxn({ transactionalId, producerId, producerEpoch, topics })\n    )\n  }\n\n  /**\n   * Send an `AddOffsetsToTxn` request.\n   *\n   * Request should be made to the transaction coordinator.\n   * @public\n   * @param {object} request\n   * @param {string} request.transactionalId The transactional id corresponding to the transaction.\n   * @param {number} request.producerId Current producer id in use by the transactional id.\n   * @param {number} request.producerEpoch Current epoch associated with the producer id.\n   * @param {string} request.groupId The unique group identifier (for the consumer group)\n   * @returns {Promise}\n   */\n  async addOffsetsToTxn({ transactionalId, producerId, producerEpoch, groupId }) {\n    const addOffsetsToTxn = this.lookupRequest(apiKeys.AddOffsetsToTxn, requests.AddOffsetsToTxn)\n    return await this[PRIVATE.SEND_REQUEST](\n      addOffsetsToTxn({ transactionalId, producerId, producerEpoch, groupId })\n    )\n  }\n\n  /**\n   * Send a `TxnOffsetCommit` request to persist the offsets in the `__consumer_offsets` topics.\n   *\n   * Request should be made to the consumer coordinator.\n   * @public\n   * @param {object} request\n   * @param {OffsetCommitTopic[]} request.topics\n   * @param {string} request.transactionalId The transactional id corresponding to the transaction.\n   * @param {string} request.groupId The unique group identifier (for the consumer group)\n   * @param {number} request.producerId Current producer id in use by the transactional id.\n   * @param {number} request.producerEpoch Current epoch associated with the producer id.\n   * @param {OffsetCommitTopic[]} request.topics\n   *\n   * @typedef {Object} OffsetCommitTopic\n   * @property {string} topic\n   * @property {OffsetCommitTopicPartition[]} partitions\n   *\n   * @typedef {Object} OffsetCommitTopicPartition\n   * @property {number} partition\n   * @property {number} offset\n   * @property {string} [metadata]\n   *\n   * @returns {Promise}\n   */\n  async txnOffsetCommit({ transactionalId, groupId, producerId, producerEpoch, topics }) {\n    const txnOffsetCommit = this.lookupRequest(apiKeys.TxnOffsetCommit, requests.TxnOffsetCommit)\n    return await this[PRIVATE.SEND_REQUEST](\n      txnOffsetCommit({ transactionalId, groupId, producerId, producerEpoch, topics })\n    )\n  }\n\n  /**\n   * Send an `EndTxn` request to indicate transaction should be committed or aborted.\n   *\n   * Request should be made to the transaction coordinator.\n   * @public\n   * @param {object} request\n   * @param {string} request.transactionalId The transactional id corresponding to the transaction.\n   * @param {number} request.producerId Current producer id in use by the transactional id.\n   * @param {number} request.producerEpoch Current epoch associated with the producer id.\n   * @param {boolean} request.transactionResult The result of the transaction (false = ABORT, true = COMMIT)\n   * @returns {Promise}\n   */\n  async endTxn({ transactionalId, producerId, producerEpoch, transactionResult }) {\n    const endTxn = this.lookupRequest(apiKeys.EndTxn, requests.EndTxn)\n    return await this[PRIVATE.SEND_REQUEST](\n      endTxn({ transactionalId, producerId, producerEpoch, transactionResult })\n    )\n  }\n\n  /**\n   * Send request for list of groups\n   * @public\n   * @returns {Promise}\n   */\n  async listGroups() {\n    const listGroups = this.lookupRequest(apiKeys.ListGroups, requests.ListGroups)\n    return await this[PRIVATE.SEND_REQUEST](listGroups())\n  }\n\n  /**\n   * Send request to delete groups\n   * @param {string[]} groupIds\n   * @public\n   * @returns {Promise}\n   */\n  async deleteGroups(groupIds) {\n    const deleteGroups = this.lookupRequest(apiKeys.DeleteGroups, requests.DeleteGroups)\n    return await this[PRIVATE.SEND_REQUEST](deleteGroups(groupIds))\n  }\n\n  /**\n   * Send request to delete records\n   * @public\n   * @param {object} request\n   * @param {TopicPartitionRecords[]} request.topics\n   *                          [\n   *                            {\n   *                              topic: 'my-topic-name',\n   *                              partitions: [\n   *                                { partition: 0, offset 2 },\n   *                                { partition: 1, offset 4 },\n   *                              ],\n   *                            }\n   *                          ]\n   * @returns {Promise<Array>} example:\n   *                          {\n   *                            throttleTime: 0\n   *                           [\n   *                              {\n   *                                topic: 'my-topic-name',\n   *                                partitions: [\n   *                                 { partition: 0, lowWatermark: '2n', errorCode: 0 },\n   *                                 { partition: 1, lowWatermark: '4n', errorCode: 0 },\n   *                               ],\n   *                             },\n   *                           ]\n   *                          }\n   *\n   * @typedef {object} TopicPartitionRecords\n   * @property {string} topic\n   * @property {PartitionRecord[]} partitions\n   *\n   * @typedef {object} PartitionRecord\n   * @property {number} partition\n   * @property {number} offset\n   */\n  async deleteRecords({ topics }) {\n    const deleteRecords = this.lookupRequest(apiKeys.DeleteRecords, requests.DeleteRecords)\n    return await this[PRIVATE.SEND_REQUEST](deleteRecords({ topics }))\n  }\n\n  /**\n   * @public\n   * @param {object} request\n   * @param {import(\"../../types\").AclEntry[]} request.acl e.g:\n   *                 [\n   *                   {\n   *                     resourceType: AclResourceTypes.TOPIC,\n   *                     resourceName: 'topic-name',\n   *                     resourcePatternType: ResourcePatternTypes.LITERAL,\n   *                     principal: 'User:bob',\n   *                     host: '*',\n   *                     operation: AclOperationTypes.ALL,\n   *                     permissionType: AclPermissionTypes.DENY,\n   *                   }\n   *                 ]\n   * @returns {Promise<void>}\n   */\n  async createAcls({ acl }) {\n    const createAcls = this.lookupRequest(apiKeys.CreateAcls, requests.CreateAcls)\n    return await this[PRIVATE.SEND_REQUEST](createAcls({ creations: acl }))\n  }\n\n  /**\n   * @public\n   * @param {import(\"../../types\").AclEntry} aclEntry\n   * @returns {Promise<void>}\n   */\n  async describeAcls({\n    resourceType,\n    resourceName,\n    resourcePatternType,\n    principal,\n    host,\n    operation,\n    permissionType,\n  }) {\n    const describeAcls = this.lookupRequest(apiKeys.DescribeAcls, requests.DescribeAcls)\n    return await this[PRIVATE.SEND_REQUEST](\n      describeAcls({\n        resourceType,\n        resourceName,\n        resourcePatternType,\n        principal,\n        host,\n        operation,\n        permissionType,\n      })\n    )\n  }\n\n  /**\n   * @public\n   * @param {Object} request\n   * @param {import(\"../../types\").AclEntry[]} request.filters\n   * @returns {Promise<void>}\n   */\n  async deleteAcls({ filters }) {\n    const deleteAcls = this.lookupRequest(apiKeys.DeleteAcls, requests.DeleteAcls)\n    return await this[PRIVATE.SEND_REQUEST](deleteAcls({ filters }))\n  }\n\n  /**\n   * @public\n   * @param {Object} request\n   * @param {import(\"../../types\").PartitionReassignment[]} request.topics\n   * @param {number} [request.timeout]\n   * @returns {Promise}\n   */\n  async alterPartitionReassignments({ topics, timeout }) {\n    const alterPartitionReassignments = this.lookupRequest(\n      apiKeys.AlterPartitionReassignments,\n      requests.AlterPartitionReassignments\n    )\n    return await this[PRIVATE.SEND_REQUEST](alterPartitionReassignments({ topics, timeout }))\n  }\n\n  /**\n   * @public\n   * @param {Object} request\n   * @param {import(\"../../types\").TopicPartitions[]} request.topics can be null\n   * @param {number} [request.timeout]\n   * @returns {Promise}\n   */\n  async listPartitionReassignments({ topics = null, timeout }) {\n    const listPartitionReassignments = this.lookupRequest(\n      apiKeys.ListPartitionReassignments,\n      requests.ListPartitionReassignments\n    )\n    return await this[PRIVATE.SEND_REQUEST](listPartitionReassignments({ topics, timeout }))\n  }\n\n  /**\n   * @private\n   */\n  async [PRIVATE.SEND_REQUEST](protocolRequest) {\n    try {\n      return await this.connectionPool.send(protocolRequest)\n    } catch (e) {\n      if (e.name === 'KafkaJSConnectionClosedError') {\n        await this.disconnect()\n      }\n\n      throw e\n    }\n  }\n}\n", "module.exports = {\n  maxRetryTime: 1000,\n  initialRetryTime: 50,\n  factor: 0.02, // randomization factor\n  multiplier: 1.5, // exponential factor\n  retries: 15, // max retries\n}\n", "module.exports = {\n  maxRetryTime: 30 * 1000,\n  initialRetryTime: 300,\n  factor: 0.2, // randomization factor\n  multiplier: 2, // exponential factor\n  retries: 5, // max retries\n}\n", "const { KafkaJSNumberOfRetriesExceeded, KafkaJSNonRetriableError } = require('../errors')\n\nconst isTestMode = process.env.NODE_ENV === 'test'\nconst RETRY_DEFAULT = isTestMode ? require('./defaults.test') : require('./defaults')\n\nconst random = (min, max) => {\n  return Math.random() * (max - min) + min\n}\n\nconst randomFromRetryTime = (factor, retryTime) => {\n  const delta = factor * retryTime\n  return Math.ceil(random(retryTime - delta, retryTime + delta))\n}\n\nconst UNRECOVERABLE_ERRORS = ['RangeError', 'ReferenceError', 'SyntaxError', 'TypeError']\nconst isErrorUnrecoverable = e => UNRECOVERABLE_ERRORS.includes(e.name)\nconst isErrorRetriable = error =>\n  (error.retriable || error.retriable !== false) && !isErrorUnrecoverable(error)\n\nconst createRetriable = (configs, resolve, reject, fn) => {\n  let aborted = false\n  const { factor, multiplier, maxRetryTime, retries } = configs\n\n  const bail = error => {\n    aborted = true\n    reject(error || new Error('Aborted'))\n  }\n\n  const calculateExponentialRetryTime = retryTime => {\n    return Math.min(randomFromRetryTime(factor, retryTime) * multiplier, maxRetryTime)\n  }\n\n  const retry = (retryTime, retryCount = 0) => {\n    if (aborted) return\n\n    const nextRetryTime = calculateExponentialRetryTime(retryTime)\n    const shouldRetry = retryCount < retries\n\n    const scheduleRetry = () => {\n      setTimeout(() => retry(nextRetryTime, retryCount + 1), retryTime)\n    }\n\n    fn(bail, retryCount, retryTime)\n      .then(resolve)\n      .catch(e => {\n        if (isErrorRetriable(e)) {\n          if (shouldRetry) {\n            scheduleRetry()\n          } else {\n            reject(\n              new KafkaJSNumberOfRetriesExceeded(e, { retryCount, retryTime, cause: e.cause || e })\n            )\n          }\n        } else {\n          reject(new KafkaJSNonRetriableError(e, { cause: e.cause || e }))\n        }\n      })\n  }\n\n  return retry\n}\n\n/**\n * @typedef {(fn: (bail: (err: Error) => void, retryCount: number, retryTime: number) => any) => Promise<ReturnType<fn>>} Retrier\n */\n\n/**\n * @param {import(\"../../types\").RetryOptions} [opts]\n * @returns {Retrier}\n */\nmodule.exports = (opts = {}) => fn => {\n  return new Promise((resolve, reject) => {\n    const configs = Object.assign({}, RETRY_DEFAULT, opts)\n    const start = createRetriable(configs, resolve, reject, fn)\n    start(randomFromRetryTime(configs.factor, configs.initialRetryTime))\n  })\n}\n", "module.exports = (a, b) => {\n  const result = []\n  const length = a.length\n  let i = 0\n\n  while (i < length) {\n    if (b.indexOf(a[i]) === -1) {\n      result.push(a[i])\n    }\n    i += 1\n  }\n\n  return result\n}\n", "const Broker = require('../broker')\nconst createRetry = require('../retry')\nconst shuffle = require('../utils/shuffle')\nconst arrayDiff = require('../utils/arrayDiff')\nconst { KafkaJSBrokerNotFound, KafkaJSProtocolError } = require('../errors')\n\nconst { keys, assign, values } = Object\nconst hasBrokerBeenReplaced = (broker, { host, port, rack }) =>\n  broker.connectionPool.host !== host ||\n  broker.connectionPool.port !== port ||\n  broker.connectionPool.rack !== rack\n\nmodule.exports = class BrokerPool {\n  /**\n   * @param {object} options\n   * @param {import(\"./connectionPoolBuilder\").ConnectionPoolBuilder} options.connectionPoolBuilder\n   * @param {import(\"../../types\").Logger} options.logger\n   * @param {import(\"../../types\").RetryOptions} [options.retry]\n   * @param {boolean} [options.allowAutoTopicCreation]\n   * @param {number} [options.authenticationTimeout]\n   * @param {number} [options.metadataMaxAge]\n   */\n  constructor({\n    connectionPoolBuilder,\n    logger,\n    retry,\n    allowAutoTopicCreation,\n    authenticationTimeout,\n    metadataMaxAge,\n  }) {\n    this.rootLogger = logger\n    this.connectionPoolBuilder = connectionPoolBuilder\n    this.metadataMaxAge = metadataMaxAge || 0\n    this.logger = logger.namespace('BrokerPool')\n    this.retrier = createRetry(assign({}, retry))\n\n    this.createBroker = options =>\n      new Broker({\n        allowAutoTopicCreation,\n        authenticationTimeout,\n        ...options,\n      })\n\n    this.brokers = {}\n    /** @type {Broker | undefined} */\n    this.seedBroker = undefined\n    /** @type {import(\"../../types\").BrokerMetadata | null} */\n    this.metadata = null\n    this.metadataExpireAt = null\n    this.versions = null\n  }\n\n  /**\n   * @public\n   * @returns {Boolean}\n   */\n  hasConnectedBrokers() {\n    const brokers = values(this.brokers)\n    return (\n      !!brokers.find(broker => broker.isConnected()) ||\n      (this.seedBroker ? this.seedBroker.isConnected() : false)\n    )\n  }\n\n  async createSeedBroker() {\n    if (this.seedBroker) {\n      await this.seedBroker.disconnect()\n    }\n\n    const connectionPool = await this.connectionPoolBuilder.build()\n\n    this.seedBroker = this.createBroker({\n      connectionPool,\n      logger: this.rootLogger,\n    })\n  }\n\n  /**\n   * @public\n   * @returns {Promise<void>}\n   */\n  async connect() {\n    if (this.hasConnectedBrokers()) {\n      return\n    }\n\n    if (!this.seedBroker) {\n      await this.createSeedBroker()\n    }\n\n    return this.retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await this.seedBroker.connect()\n        this.versions = this.seedBroker.versions\n      } catch (e) {\n        if (e.name === 'KafkaJSConnectionError' || e.type === 'ILLEGAL_SASL_STATE') {\n          // Connection builder will always rotate the seed broker\n          await this.createSeedBroker()\n          this.logger.error(\n            `Failed to connect to seed broker, trying another broker from the list: ${e.message}`,\n            { retryCount, retryTime }\n          )\n        } else {\n          this.logger.error(e.message, { retryCount, retryTime })\n        }\n\n        if (e.retriable) throw e\n        bail(e)\n      }\n    })\n  }\n\n  /**\n   * @public\n   * @returns {Promise}\n   */\n  async disconnect() {\n    this.seedBroker && (await this.seedBroker.disconnect())\n    await Promise.all(values(this.brokers).map(broker => broker.disconnect()))\n\n    this.brokers = {}\n    this.metadata = null\n    this.versions = null\n  }\n\n  /**\n   * @public\n   * @param {Object} destination\n   * @param {string} destination.host\n   * @param {number} destination.port\n   */\n  removeBroker({ host, port }) {\n    const removedBroker = values(this.brokers).find(\n      broker => broker.connectionPool.host === host && broker.connectionPool.port === port\n    )\n\n    if (removedBroker) {\n      delete this.brokers[removedBroker.nodeId]\n      this.metadataExpireAt = null\n\n      if (this.seedBroker.nodeId === removedBroker.nodeId) {\n        this.seedBroker = shuffle(values(this.brokers))[0]\n      }\n    }\n  }\n\n  /**\n   * @public\n   * @param {Array<String>} topics\n   * @returns {Promise<null>}\n   */\n  async refreshMetadata(topics) {\n    const broker = await this.findConnectedBroker()\n    const { host: seedHost, port: seedPort } = this.seedBroker.connectionPool\n\n    return this.retrier(async (bail, retryCount, retryTime) => {\n      try {\n        this.metadata = await broker.metadata(topics)\n        this.metadataExpireAt = Date.now() + this.metadataMaxAge\n\n        const replacedBrokers = []\n\n        this.brokers = await this.metadata.brokers.reduce(\n          async (resultPromise, { nodeId, host, port, rack }) => {\n            const result = await resultPromise\n\n            if (result[nodeId]) {\n              if (!hasBrokerBeenReplaced(result[nodeId], { host, port, rack })) {\n                return result\n              }\n\n              replacedBrokers.push(result[nodeId])\n            }\n\n            if (host === seedHost && port === seedPort) {\n              this.seedBroker.nodeId = nodeId\n              this.seedBroker.connectionPool.rack = rack\n              return assign(result, {\n                [nodeId]: this.seedBroker,\n              })\n            }\n\n            return assign(result, {\n              [nodeId]: this.createBroker({\n                logger: this.rootLogger,\n                versions: this.versions,\n                connectionPool: await this.connectionPoolBuilder.build({ host, port, rack }),\n                nodeId,\n              }),\n            })\n          },\n          this.brokers\n        )\n\n        const freshBrokerIds = this.metadata.brokers.map(({ nodeId }) => `${nodeId}`).sort()\n        const currentBrokerIds = keys(this.brokers).sort()\n        const unusedBrokerIds = arrayDiff(currentBrokerIds, freshBrokerIds)\n\n        const brokerDisconnects = unusedBrokerIds.map(nodeId => {\n          const broker = this.brokers[nodeId]\n          return broker.disconnect().then(() => {\n            delete this.brokers[nodeId]\n          })\n        })\n\n        const replacedBrokersDisconnects = replacedBrokers.map(broker => broker.disconnect())\n        await Promise.all([...brokerDisconnects, ...replacedBrokersDisconnects])\n      } catch (e) {\n        if (e.type === 'LEADER_NOT_AVAILABLE') {\n          throw e\n        }\n\n        bail(e)\n      }\n    })\n  }\n\n  /**\n   * Only refreshes metadata if the data is stale according to the `metadataMaxAge` param or does not contain information about the provided topics\n   *\n   * @public\n   * @param {Array<String>} topics\n   * @returns {Promise<null>}\n   */\n  async refreshMetadataIfNecessary(topics) {\n    const shouldRefresh =\n      this.metadata == null ||\n      this.metadataExpireAt == null ||\n      Date.now() > this.metadataExpireAt ||\n      !topics.every(topic =>\n        this.metadata.topicMetadata.some(topicMetadata => topicMetadata.topic === topic)\n      )\n\n    if (shouldRefresh) {\n      return this.refreshMetadata(topics)\n    }\n  }\n\n  /** @type {() => string[]} */\n  getNodeIds() {\n    return keys(this.brokers)\n  }\n\n  /**\n   * @public\n   * @param {object} options\n   * @param {string} options.nodeId\n   * @returns {Promise<Broker>}\n   */\n  async findBroker({ nodeId }) {\n    const broker = this.brokers[nodeId]\n\n    if (!broker) {\n      throw new KafkaJSBrokerNotFound(`Broker ${nodeId} not found in the cached metadata`)\n    }\n\n    await this.connectBroker(broker)\n    return broker\n  }\n\n  /**\n   * @public\n   * @param {(params: { nodeId: string, broker: Broker }) => Promise<T>} callback\n   * @returns {Promise<T>}\n   * @template T\n   */\n  async withBroker(callback) {\n    const brokers = shuffle(keys(this.brokers))\n    if (brokers.length === 0) {\n      throw new KafkaJSBrokerNotFound('No brokers in the broker pool')\n    }\n\n    for (const nodeId of brokers) {\n      const broker = await this.findBroker({ nodeId })\n      try {\n        return await callback({ nodeId, broker })\n      } catch (e) {}\n    }\n\n    return null\n  }\n\n  /**\n   * @public\n   * @returns {Promise<Broker>}\n   */\n  async findConnectedBroker() {\n    const nodeIds = shuffle(keys(this.brokers))\n    const connectedBrokerId = nodeIds.find(nodeId => this.brokers[nodeId].isConnected())\n\n    if (connectedBrokerId) {\n      return await this.findBroker({ nodeId: connectedBrokerId })\n    }\n\n    // Cycle through the nodes until one connects\n    for (const nodeId of nodeIds) {\n      try {\n        return await this.findBroker({ nodeId })\n      } catch (e) {}\n    }\n\n    // Failed to connect to all known brokers, metadata might be old\n    await this.connect()\n    return this.seedBroker\n  }\n\n  /**\n   * @private\n   * @param {Broker} broker\n   * @returns {Promise<null>}\n   */\n  async connectBroker(broker) {\n    if (broker.isConnected()) {\n      return\n    }\n\n    return this.retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await broker.connect()\n      } catch (e) {\n        if (e.name === 'KafkaJSConnectionError' || e.type === 'ILLEGAL_SASL_STATE') {\n          await broker.disconnect()\n        }\n\n        // To avoid reconnecting to an unavailable host, we bail on connection errors\n        // and refresh metadata on a higher level before reconnecting\n        if (e.name === 'KafkaJSConnectionError') {\n          return bail(e)\n        }\n\n        if (e.type === 'ILLEGAL_SASL_STATE') {\n          // Rebuild the connection pool since it can't recover from illegal SASL state\n          broker.connectionPool = await this.connectionPoolBuilder.build({\n            host: broker.connectionPool.host,\n            port: broker.connectionPool.port,\n            rack: broker.connectionPool.rack,\n          })\n\n          this.logger.error(`Failed to connect to broker, reconnecting`, { retryCount, retryTime })\n          throw new KafkaJSProtocolError(e, { retriable: true })\n        }\n\n        if (e.retriable) throw e\n        this.logger.error(e, { retryCount, retryTime, stack: e.stack })\n        bail(e)\n      }\n    })\n  }\n}\n", "/**\n * @template T\n * @param { (...args: any) => Promise<T> } [asyncFunction]\n * Promise returning function that will only ever be invoked sequentially.\n * @returns { (...args: any) => Promise<T> }\n * Function that may invoke asyncFunction if there is not a currently executing invocation.\n * Returns promise from the currently executing invocation.\n */\nmodule.exports = asyncFunction => {\n  let promise = null\n\n  return (...args) => {\n    if (promise == null) {\n      promise = asyncFunction(...args).finally(() => (promise = null))\n    }\n    return promise\n  }\n}\n", "/**\n * @param {Object} options\n * @param {import(\"../../types\").ISocketFactory} options.socketFactory\n * @param {string} options.host\n * @param {number} options.port\n * @param {Object} options.ssl\n * @param {() => void} options.onConnect\n * @param {(data: Buffer) => void} options.onData\n * @param {() => void} options.onEnd\n * @param {(err: Error) => void} options.onError\n * @param {() => void} options.onTimeout\n */\nmodule.exports = ({\n  socketFactory,\n  host,\n  port,\n  ssl,\n  onConnect,\n  onData,\n  onEnd,\n  onError,\n  onTimeout,\n}) => {\n  const socket = socketFactory({ host, port, ssl, onConnect })\n\n  socket.on('data', onData)\n  socket.on('end', onEnd)\n  socket.on('error', onError)\n  socket.on('timeout', onTimeout)\n\n  return socket\n}\n", "const Encoder = require('./encoder')\n\nmodule.exports = async ({ correlationId, clientId, request: { apiKey, apiVersion, encode } }) => {\n  const payload = await encode()\n  const requestPayload = new Encoder()\n    .writeInt16(apiKey)\n    .writeInt16(apiVersion)\n    .writeInt32(correlationId)\n    .writeString(clientId)\n    .writeEncoder(payload)\n\n  return new Encoder().writeInt32(requestPayload.size()).writeEncoder(requestPayload)\n}\n", "const EARLIEST_OFFSET = -2\nconst LATEST_OFFSET = -1\nconst INT_32_MAX_VALUE = Math.pow(2, 31) - 1\n\nmodule.exports = {\n  EARLIEST_OFFSET,\n  LATEST_OFFSET,\n  INT_32_MAX_VALUE,\n}\n", "module.exports = () => ({\n  KAFKAJS_DEBUG_PROTOCOL_BUFFERS: process.env.KAFKAJS_DEBUG_PROTOCOL_BUFFERS,\n  KAFKAJS_DEBUG_EXTENDED_PROTOCOL_BUFFERS: process.env.KAFKAJS_DEBUG_EXTENDED_PROTOCOL_BUFFERS,\n})\n", "/** @type {<T1 extends string>(namespace: T1) => <T2 extends string>(type: T2) => `${T1}.${T2}`} */\nmodule.exports = namespace => type => `${namespace}.${type}`\n", "const InstrumentationEventType = require('../instrumentation/eventType')\nconst eventType = InstrumentationEventType('network')\n\nmodule.exports = {\n  NETWORK_REQUEST: eventType('request'),\n  NETWORK_REQUEST_TIMEOUT: eventType('request_timeout'),\n  NETWORK_REQUEST_QUEUE_SIZE: eventType('request_queue_size'),\n}\n", "const { KafkaJSRequestTimeoutError, KafkaJSNonRetriableError } = require('../../errors')\nconst events = require('../instrumentationEvents')\n\nconst PRIVATE = {\n  STATE: Symbol('private:SocketRequest:state'),\n  EMIT_EVENT: Symbol('private:SocketRequest:emitEvent'),\n}\n\nconst REQUEST_STATE = {\n  PENDING: Symbol('PENDING'),\n  SENT: Symbol('SENT'),\n  COMPLETED: Symbol('COMPLETED'),\n  REJECTED: Symbol('REJECTED'),\n}\n\n/**\n * SocketRequest abstracts the life cycle of a socket request, making it easier to track\n * request durations and to have individual timeouts per request.\n *\n * @typedef {Object} SocketRequest\n * @property {number} createdAt\n * @property {number} sentAt\n * @property {number} pendingDuration\n * @property {number} duration\n * @property {number} requestTimeout\n * @property {string} broker\n * @property {string} clientId\n * @property {RequestEntry} entry\n * @property {boolean} expectResponse\n * @property {Function} send\n * @property {Function} timeout\n *\n * @typedef {Object} RequestEntry\n * @property {string} apiKey\n * @property {string} apiName\n * @property {number} apiVersion\n * @property {number} correlationId\n * @property {Function} resolve\n * @property {Function} reject\n */\nmodule.exports = class SocketRequest {\n  /**\n   * @param {Object} options\n   * @param {number} options.requestTimeout\n   * @param {string} options.broker - e.g: 127.0.0.1:9092\n   * @param {string} options.clientId\n   * @param {RequestEntry} options.entry\n   * @param {boolean} options.expectResponse\n   * @param {Function} options.send\n   * @param {() => void} options.timeout\n   * @param {import(\"../../instrumentation/emitter\")} [options.instrumentationEmitter=null]\n   */\n  constructor({\n    requestTimeout,\n    broker,\n    clientId,\n    entry,\n    expectResponse,\n    send,\n    timeout,\n    instrumentationEmitter = null,\n  }) {\n    this.createdAt = Date.now()\n    this.requestTimeout = requestTimeout\n    this.broker = broker\n    this.clientId = clientId\n    this.entry = entry\n    this.correlationId = entry.correlationId\n    this.expectResponse = expectResponse\n    this.sendRequest = send\n    this.timeoutHandler = timeout\n\n    this.sentAt = null\n    this.duration = null\n    this.pendingDuration = null\n\n    this[PRIVATE.STATE] = REQUEST_STATE.PENDING\n    this[PRIVATE.EMIT_EVENT] = (eventName, payload) =>\n      instrumentationEmitter && instrumentationEmitter.emit(eventName, payload)\n  }\n\n  send() {\n    this.throwIfInvalidState({\n      accepted: [REQUEST_STATE.PENDING],\n      next: REQUEST_STATE.SENT,\n    })\n\n    this.sendRequest()\n    this.sentAt = Date.now()\n    this.pendingDuration = this.sentAt - this.createdAt\n    this[PRIVATE.STATE] = REQUEST_STATE.SENT\n  }\n\n  timeoutRequest() {\n    const { apiName, apiKey, apiVersion } = this.entry\n    const requestInfo = `${apiName}(key: ${apiKey}, version: ${apiVersion})`\n    const eventData = {\n      broker: this.broker,\n      clientId: this.clientId,\n      correlationId: this.correlationId,\n      createdAt: this.createdAt,\n      sentAt: this.sentAt,\n      pendingDuration: this.pendingDuration,\n    }\n\n    this.timeoutHandler()\n    this.rejected(new KafkaJSRequestTimeoutError(`Request ${requestInfo} timed out`, eventData))\n    this[PRIVATE.EMIT_EVENT](events.NETWORK_REQUEST_TIMEOUT, {\n      ...eventData,\n      apiName,\n      apiKey,\n      apiVersion,\n    })\n  }\n\n  completed({ size, payload }) {\n    this.throwIfInvalidState({\n      accepted: [REQUEST_STATE.SENT],\n      next: REQUEST_STATE.COMPLETED,\n    })\n\n    const { entry, correlationId, broker, clientId, createdAt, sentAt, pendingDuration } = this\n\n    this[PRIVATE.STATE] = REQUEST_STATE.COMPLETED\n    this.duration = Date.now() - this.sentAt\n    entry.resolve({ correlationId, entry, size, payload })\n\n    this[PRIVATE.EMIT_EVENT](events.NETWORK_REQUEST, {\n      broker,\n      clientId,\n      correlationId,\n      size,\n      createdAt,\n      sentAt,\n      pendingDuration,\n      duration: this.duration,\n      apiName: entry.apiName,\n      apiKey: entry.apiKey,\n      apiVersion: entry.apiVersion,\n    })\n  }\n\n  rejected(error) {\n    this.throwIfInvalidState({\n      accepted: [REQUEST_STATE.PENDING, REQUEST_STATE.SENT],\n      next: REQUEST_STATE.REJECTED,\n    })\n\n    this[PRIVATE.STATE] = REQUEST_STATE.REJECTED\n    this.duration = Date.now() - this.sentAt\n    this.entry.reject(error)\n  }\n\n  /**\n   * @private\n   */\n  throwIfInvalidState({ accepted, next }) {\n    if (accepted.includes(this[PRIVATE.STATE])) {\n      return\n    }\n\n    const current = this[PRIVATE.STATE].toString()\n\n    throw new KafkaJSNonRetriableError(\n      `Invalid state, can't transition from ${current} to ${next.toString()}`\n    )\n  }\n}\n", "const { EventEmitter } = require('events')\nconst SocketRequest = require('./socketRequest')\nconst events = require('../instrumentationEvents')\nconst { KafkaJSInvariantViolation } = require('../../errors')\n\nconst PRIVATE = {\n  EMIT_QUEUE_SIZE_EVENT: Symbol('private:RequestQueue:emitQueueSizeEvent'),\n  EMIT_REQUEST_QUEUE_EMPTY: Symbol('private:RequestQueue:emitQueueEmpty'),\n}\n\nconst REQUEST_QUEUE_EMPTY = 'requestQueueEmpty'\nconst CHECK_PENDING_REQUESTS_INTERVAL = 10\n\nmodule.exports = class RequestQueue extends EventEmitter {\n  /**\n   * @param {Object} options\n   * @param {number} options.maxInFlightRequests\n   * @param {number} options.requestTimeout\n   * @param {boolean} options.enforceRequestTimeout\n   * @param {string} options.clientId\n   * @param {string} options.broker\n   * @param {import(\"../../../types\").Logger} options.logger\n   * @param {import(\"../../instrumentation/emitter\")} [options.instrumentationEmitter=null]\n   * @param {() => boolean} [options.isConnected]\n   */\n  constructor({\n    instrumentationEmitter = null,\n    maxInFlightRequests,\n    requestTimeout,\n    enforceRequestTimeout,\n    clientId,\n    broker,\n    logger,\n    isConnected = () => true,\n  }) {\n    super()\n    this.instrumentationEmitter = instrumentationEmitter\n    this.maxInFlightRequests = maxInFlightRequests\n    this.requestTimeout = requestTimeout\n    this.enforceRequestTimeout = enforceRequestTimeout\n    this.clientId = clientId\n    this.broker = broker\n    this.logger = logger\n    this.isConnected = isConnected\n\n    this.inflight = new Map()\n    this.pending = []\n\n    /**\n     * Until when this request queue is throttled and shouldn't send requests\n     *\n     * The value represents the timestamp of the end of the throttling in ms-since-epoch. If the value\n     * is smaller than the current timestamp no throttling is active.\n     *\n     * @type {number}\n     */\n    this.throttledUntil = -1\n\n    /**\n     * Timeout id if we have scheduled a check for pending requests due to client-side throttling\n     *\n     * @type {null|NodeJS.Timeout}\n     */\n    this.throttleCheckTimeoutId = null\n\n    this[PRIVATE.EMIT_REQUEST_QUEUE_EMPTY] = () => {\n      if (this.pending.length === 0 && this.inflight.size === 0) {\n        this.emit(REQUEST_QUEUE_EMPTY)\n      }\n    }\n\n    this[PRIVATE.EMIT_QUEUE_SIZE_EVENT] = () => {\n      instrumentationEmitter &&\n        instrumentationEmitter.emit(events.NETWORK_REQUEST_QUEUE_SIZE, {\n          broker: this.broker,\n          clientId: this.clientId,\n          queueSize: this.pending.length,\n        })\n\n      this[PRIVATE.EMIT_REQUEST_QUEUE_EMPTY]()\n    }\n  }\n\n  /**\n   * @public\n   */\n  scheduleRequestTimeoutCheck() {\n    if (this.enforceRequestTimeout) {\n      this.destroy()\n\n      this.requestTimeoutIntervalId = setInterval(() => {\n        this.inflight.forEach(request => {\n          if (Date.now() - request.sentAt > request.requestTimeout) {\n            request.timeoutRequest()\n          }\n        })\n\n        if (!this.isConnected()) {\n          this.destroy()\n        }\n      }, Math.min(this.requestTimeout, 100))\n    }\n  }\n\n  maybeThrottle(clientSideThrottleTime) {\n    if (clientSideThrottleTime !== null && clientSideThrottleTime > 0) {\n      this.logger.debug(`Client side throttling in effect for ${clientSideThrottleTime}ms`)\n      const minimumThrottledUntil = Date.now() + clientSideThrottleTime\n      this.throttledUntil = Math.max(minimumThrottledUntil, this.throttledUntil)\n    }\n  }\n\n  createSocketRequest(pushedRequest) {\n    const { correlationId } = pushedRequest.entry\n    const defaultRequestTimeout = this.requestTimeout\n    const customRequestTimeout = pushedRequest.requestTimeout\n\n    // Some protocol requests have custom request timeouts (e.g JoinGroup, Fetch, etc). The custom\n    // timeouts are influenced by user configurations, which can be lower than the default requestTimeout\n    const requestTimeout = Math.max(defaultRequestTimeout, customRequestTimeout || 0)\n\n    const socketRequest = new SocketRequest({\n      entry: pushedRequest.entry,\n      expectResponse: pushedRequest.expectResponse,\n      broker: this.broker,\n      clientId: this.clientId,\n      instrumentationEmitter: this.instrumentationEmitter,\n      requestTimeout,\n      send: () => {\n        if (this.inflight.has(correlationId)) {\n          throw new KafkaJSInvariantViolation('Correlation id already exists')\n        }\n        this.inflight.set(correlationId, socketRequest)\n        pushedRequest.sendRequest()\n      },\n      timeout: () => {\n        this.inflight.delete(correlationId)\n        this.checkPendingRequests()\n        // Try to emit REQUEST_QUEUE_EMPTY. Otherwise, waitForPendingRequests may stuck forever\n        this[PRIVATE.EMIT_REQUEST_QUEUE_EMPTY]()\n      },\n    })\n\n    return socketRequest\n  }\n\n  /**\n   * @typedef {Object} PushedRequest\n   * @property {import(\"./socketRequest\").RequestEntry} entry\n   * @property {boolean} expectResponse\n   * @property {Function} sendRequest\n   * @property {number} [requestTimeout]\n   *\n   * @public\n   * @param {PushedRequest} pushedRequest\n   */\n  push(pushedRequest) {\n    const { correlationId } = pushedRequest.entry\n    const socketRequest = this.createSocketRequest(pushedRequest)\n\n    if (this.canSendSocketRequestImmediately()) {\n      this.sendSocketRequest(socketRequest)\n      return\n    }\n\n    this.pending.push(socketRequest)\n    this.scheduleCheckPendingRequests()\n\n    this.logger.debug(`Request enqueued`, {\n      clientId: this.clientId,\n      broker: this.broker,\n      correlationId,\n    })\n\n    this[PRIVATE.EMIT_QUEUE_SIZE_EVENT]()\n  }\n\n  /**\n   * @param {SocketRequest} socketRequest\n   */\n  sendSocketRequest(socketRequest) {\n    socketRequest.send()\n\n    if (!socketRequest.expectResponse) {\n      this.logger.debug(`Request does not expect a response, resolving immediately`, {\n        clientId: this.clientId,\n        broker: this.broker,\n        correlationId: socketRequest.correlationId,\n      })\n\n      this.inflight.delete(socketRequest.correlationId)\n      socketRequest.completed({ size: 0, payload: null })\n    }\n  }\n\n  /**\n   * @public\n   * @param {object} response\n   * @param {number} response.correlationId\n   * @param {Buffer} response.payload\n   * @param {number} response.size\n   */\n  fulfillRequest({ correlationId, payload, size }) {\n    const socketRequest = this.inflight.get(correlationId)\n    this.inflight.delete(correlationId)\n    this.checkPendingRequests()\n\n    if (socketRequest) {\n      socketRequest.completed({ size, payload })\n    } else {\n      this.logger.warn(`Response without match`, {\n        clientId: this.clientId,\n        broker: this.broker,\n        correlationId,\n      })\n    }\n\n    this[PRIVATE.EMIT_REQUEST_QUEUE_EMPTY]()\n  }\n\n  /**\n   * @public\n   * @param {Error} error\n   */\n  rejectAll(error) {\n    const requests = [...this.inflight.values(), ...this.pending]\n\n    for (const socketRequest of requests) {\n      socketRequest.rejected(error)\n      this.inflight.delete(socketRequest.correlationId)\n    }\n\n    this.pending = []\n    this.inflight.clear()\n    this[PRIVATE.EMIT_QUEUE_SIZE_EVENT]()\n  }\n\n  /**\n   * @public\n   */\n  waitForPendingRequests() {\n    return new Promise(resolve => {\n      if (this.pending.length === 0 && this.inflight.size === 0) {\n        return resolve()\n      }\n\n      this.logger.debug('Waiting for pending requests', {\n        clientId: this.clientId,\n        broker: this.broker,\n        currentInflightRequests: this.inflight.size,\n        currentPendingQueueSize: this.pending.length,\n      })\n\n      this.once(REQUEST_QUEUE_EMPTY, () => resolve())\n    })\n  }\n\n  /**\n   * @public\n   */\n  destroy() {\n    clearInterval(this.requestTimeoutIntervalId)\n    clearTimeout(this.throttleCheckTimeoutId)\n    this.throttleCheckTimeoutId = null\n  }\n\n  canSendSocketRequestImmediately() {\n    const shouldEnqueue =\n      (this.maxInFlightRequests != null && this.inflight.size >= this.maxInFlightRequests) ||\n      this.throttledUntil > Date.now()\n\n    return !shouldEnqueue\n  }\n\n  /**\n   * Check and process pending requests either now or in the future\n   *\n   * This function will send out as many pending requests as possible taking throttling and\n   * in-flight limits into account.\n   */\n  checkPendingRequests() {\n    while (this.pending.length > 0 && this.canSendSocketRequestImmediately()) {\n      const pendingRequest = this.pending.shift() // first in first out\n      this.sendSocketRequest(pendingRequest)\n\n      this.logger.debug(`Consumed pending request`, {\n        clientId: this.clientId,\n        broker: this.broker,\n        correlationId: pendingRequest.correlationId,\n        pendingDuration: pendingRequest.pendingDuration,\n        currentPendingQueueSize: this.pending.length,\n      })\n\n      this[PRIVATE.EMIT_QUEUE_SIZE_EVENT]()\n    }\n\n    this.scheduleCheckPendingRequests()\n  }\n\n  /**\n   * Ensure that pending requests will be checked in the future\n   *\n   * If there is a client-side throttling in place this will ensure that we will check\n   * the pending request queue eventually.\n   */\n  scheduleCheckPendingRequests() {\n    // If we're throttled: Schedule checkPendingRequests when the throttle\n    // should be resolved. If there is already something scheduled we assume that that\n    // will be fine, and potentially fix up a new timeout if needed at that time.\n    // Note that if we're merely \"overloaded\" by having too many inflight requests\n    // we will anyways check the queue when one of them gets fulfilled.\n    let scheduleAt = this.throttledUntil - Date.now()\n    if (!this.throttleCheckTimeoutId) {\n      if (this.pending.length > 0) {\n        scheduleAt = scheduleAt > 0 ? scheduleAt : CHECK_PENDING_REQUESTS_INTERVAL\n      }\n      this.throttleCheckTimeoutId = setTimeout(() => {\n        this.throttleCheckTimeoutId = null\n        this.checkPendingRequests()\n      }, scheduleAt)\n    }\n  }\n}\n", "const CONNECTION_STATUS = {\n  CONNECTED: 'connected',\n  DISCONNECTING: 'disconnecting',\n  DISCONNECTED: 'disconnected',\n}\n\nconst CONNECTED_STATUS = [CONNECTION_STATUS.CONNECTED, CONNECTION_STATUS.DISCONNECTING]\n\nmodule.exports = {\n  CONNECTION_STATUS,\n  CONNECTED_STATUS,\n}\n", "/**\n * http://www.ietf.org/rfc/rfc2595.txt\n *\n * The mechanism consists of a single message from the client to the\n * server.  The client sends the authorization identity (identity to\n * login as), followed by a US-ASCII NUL character, followed by the\n * authentication identity (identity whose password will be used),\n * followed by a US-ASCII NUL character, followed by the clear-text\n * password.  The client may leave the authorization identity empty to\n * indicate that it is the same as the authentication identity.\n *\n * The server will verify the authentication identity and password with\n * the system authentication database and verify that the authentication\n * credentials permit the client to login as the authorization identity.\n * If both steps succeed, the user is logged in.\n */\n\nconst Encoder = require('../../encoder')\n\nconst US_ASCII_NULL_CHAR = '\\u0000'\n\nmodule.exports = ({ authorizationIdentity = null, username, password }) => ({\n  encode: async () => {\n    return new Encoder().writeBytes(\n      [authorizationIdentity, username, password].join(US_ASCII_NULL_CHAR)\n    ).buffer\n  },\n})\n", "module.exports = {\n  decode: async () => true,\n  parse: async () => true,\n}\n", "module.exports = {\n  request: require('./request'),\n  response: require('./response'),\n}\n", "const { request, response } = require('../../protocol/sasl/plain')\nconst { KafkaJSSASLAuthenticationError } = require('../../errors')\n\nconst plainAuthenticatorProvider = sasl => ({ host, port, logger, saslAuthenticate }) => {\n  return {\n    authenticate: async () => {\n      if (sasl.username == null || sasl.password == null) {\n        throw new KafkaJSSASLAuthenticationError('SASL Plain: Invalid username or password')\n      }\n\n      const broker = `${host}:${port}`\n\n      try {\n        logger.debug('Authenticate with SASL PLAIN', { broker })\n        await saslAuthenticate({ request: request(sasl), response })\n        logger.debug('SASL PLAIN authentication successful', { broker })\n      } catch (e) {\n        const error = new KafkaJSSASLAuthenticationError(\n          `SASL PLAIN authentication failed: ${e.message}`\n        )\n        logger.error(error.message, { broker })\n        throw error\n      }\n    },\n  }\n}\n\nmodule.exports = plainAuthenticatorProvider\n", "/**\n * https://tools.ietf.org/html/rfc5802\n *\n * First, the client sends the \"client-first-message\" containing:\n *\n *  -> a GS2 header consisting of a flag indicating whether channel\n * binding is supported-but-not-used, not supported, or used, and an\n * optional SASL authorization identity;\n *\n *  -> SCRAM username and a random, unique nonce attributes.\n *\n * Note that the client's first message will always start with \"n\", \"y\",\n * or \"p\"; otherwise, the message is invalid and authentication MUST\n * fail.  This is important, as it allows for GS2 extensibility (e.g.,\n * to add support for security layers).\n */\n\nconst Encoder = require('../../../encoder')\n\nmodule.exports = ({ clientFirstMessage }) => ({\n  encode: async () => new Encoder().writeBytes(clientFirstMessage).buffer,\n})\n", "/* eslint no-unused-vars: [\"error\", { \"varsIgnorePattern\": \"_\" }] */\n\nconst Decoder = require('../../../decoder')\n\nconst ENTRY_REGEX = /^([rsiev])=(.*)$/\n\nmodule.exports = {\n  decode: async rawData => {\n    return new Decoder(rawData).readBytes()\n  },\n  parse: async data => {\n    const processed = data\n      .toString()\n      .split(',')\n      .map(str => {\n        const [_, key, value] = str.match(ENTRY_REGEX)\n        return [key, value]\n      })\n      .reduce((obj, entry) => ({ ...obj, [entry[0]]: entry[1] }), {})\n\n    return { original: data.toString(), ...processed }\n  },\n}\n", "const Encoder = require('../../../encoder')\n\nmodule.exports = ({ finalMessage }) => ({\n  encode: async () => new Encoder().writeBytes(finalMessage).buffer,\n})\n", "module.exports = require('../firstMessage/response')\n", "module.exports = {\n  firstMessage: {\n    request: require('./firstMessage/request'),\n    response: require('./firstMessage/response'),\n  },\n  finalMessage: {\n    request: require('./finalMessage/request'),\n    response: require('./finalMessage/response'),\n  },\n}\n", "const crypto = require('crypto')\nconst scram = require('../../protocol/sasl/scram')\nconst { KafkaJSSASLAuthenticationError, KafkaJSNonRetriableError } = require('../../errors')\n\nconst GS2_HEADER = 'n,,'\n\nconst EQUAL_SIGN_REGEX = /=/g\nconst COMMA_SIGN_REGEX = /,/g\n\nconst URLSAFE_BASE64_PLUS_REGEX = /\\+/g\nconst URLSAFE_BASE64_SLASH_REGEX = /\\//g\nconst URLSAFE_BASE64_TRAILING_EQUAL_REGEX = /=+$/\n\nconst HMAC_CLIENT_KEY = 'Client Key'\nconst HMAC_SERVER_KEY = 'Server Key'\n\nconst DIGESTS = {\n  SHA256: {\n    length: 32,\n    type: 'sha256',\n    minIterations: 4096,\n  },\n  SHA512: {\n    length: 64,\n    type: 'sha512',\n    minIterations: 4096,\n  },\n}\n\nconst encode64 = str => Buffer.from(str).toString('base64')\n\nclass SCRAM {\n  /**\n   * From https://tools.ietf.org/html/rfc5802#section-5.1\n   *\n   * The characters ',' or '=' in usernames are sent as '=2C' and\n   * '=3D' respectively.  If the server receives a username that\n   * contains '=' not followed by either '2C' or '3D', then the\n   * server MUST fail the authentication.\n   *\n   * @returns {String}\n   */\n  static sanitizeString(str) {\n    return str.replace(EQUAL_SIGN_REGEX, '=3D').replace(COMMA_SIGN_REGEX, '=2C')\n  }\n\n  /**\n   * In cryptography, a nonce is an arbitrary number that can be used just once.\n   * It is similar in spirit to a nonce * word, hence the name. It is often a random or pseudo-random\n   * number issued in an authentication protocol to * ensure that old communications cannot be reused\n   * in replay attacks.\n   *\n   * @returns {String}\n   */\n  static nonce() {\n    return crypto\n      .randomBytes(16)\n      .toString('base64')\n      .replace(URLSAFE_BASE64_PLUS_REGEX, '-') // make it url safe\n      .replace(URLSAFE_BASE64_SLASH_REGEX, '_')\n      .replace(URLSAFE_BASE64_TRAILING_EQUAL_REGEX, '')\n      .toString('ascii')\n  }\n\n  /**\n   * Hi() is, essentially, PBKDF2 [RFC2898] with HMAC() as the\n   * pseudorandom function (PRF) and with dkLen == output length of\n   * HMAC() == output length of H()\n   *\n   * @returns {Promise<Buffer>}\n   */\n  static hi(password, salt, iterations, digestDefinition) {\n    return new Promise((resolve, reject) => {\n      crypto.pbkdf2(\n        password,\n        salt,\n        iterations,\n        digestDefinition.length,\n        digestDefinition.type,\n        (err, derivedKey) => (err ? reject(err) : resolve(derivedKey))\n      )\n    })\n  }\n\n  /**\n   * Apply the exclusive-or operation to combine the octet string\n   * on the left of this operator with the octet string on the right of\n   * this operator.  The length of the output and each of the two\n   * inputs will be the same for this use\n   *\n   * @returns {Buffer}\n   */\n  static xor(left, right) {\n    const bufferA = Buffer.from(left)\n    const bufferB = Buffer.from(right)\n    const length = Buffer.byteLength(bufferA)\n\n    if (length !== Buffer.byteLength(bufferB)) {\n      throw new KafkaJSNonRetriableError('Buffers must be of the same length')\n    }\n\n    const result = []\n    for (let i = 0; i < length; i++) {\n      result.push(bufferA[i] ^ bufferB[i])\n    }\n\n    return Buffer.from(result)\n  }\n\n  /**\n   * @param {SASLOptions} sasl\n   * @param {Logger} logger\n   * @param {Function} saslAuthenticate\n   * @param {DigestDefinition} digestDefinition\n   */\n  constructor(sasl, host, port, logger, saslAuthenticate, digestDefinition) {\n    this.sasl = sasl\n    this.host = host\n    this.port = port\n    this.logger = logger\n    this.saslAuthenticate = saslAuthenticate\n    this.digestDefinition = digestDefinition\n\n    const digestType = digestDefinition.type.toUpperCase()\n    this.PREFIX = `SASL SCRAM ${digestType} authentication`\n\n    this.currentNonce = SCRAM.nonce()\n  }\n\n  async authenticate() {\n    const { PREFIX } = this\n    const broker = `${this.host}:${this.port}`\n\n    if (this.sasl.username == null || this.sasl.password == null) {\n      throw new KafkaJSSASLAuthenticationError(`${this.PREFIX}: Invalid username or password`)\n    }\n\n    try {\n      this.logger.debug('Exchanging first client message', { broker })\n      const clientMessageResponse = await this.sendClientFirstMessage()\n\n      this.logger.debug('Sending final message', { broker })\n      const finalResponse = await this.sendClientFinalMessage(clientMessageResponse)\n\n      if (finalResponse.e) {\n        throw new Error(finalResponse.e)\n      }\n\n      const serverKey = await this.serverKey(clientMessageResponse)\n      const serverSignature = this.serverSignature(serverKey, clientMessageResponse)\n\n      if (finalResponse.v !== serverSignature) {\n        throw new Error('Invalid server signature in server final message')\n      }\n\n      this.logger.debug(`${PREFIX} successful`, { broker })\n    } catch (e) {\n      const error = new KafkaJSSASLAuthenticationError(`${PREFIX} failed: ${e.message}`)\n      this.logger.error(error.message, { broker })\n      throw error\n    }\n  }\n\n  /**\n   * @private\n   */\n  async sendClientFirstMessage() {\n    const clientFirstMessage = `${GS2_HEADER}${this.firstMessageBare()}`\n    const request = scram.firstMessage.request({ clientFirstMessage })\n    const response = scram.firstMessage.response\n\n    return this.saslAuthenticate({\n      request,\n      response,\n    })\n  }\n\n  /**\n   * @private\n   */\n  async sendClientFinalMessage(clientMessageResponse) {\n    const { PREFIX } = this\n    const iterations = parseInt(clientMessageResponse.i, 10)\n    const { minIterations } = this.digestDefinition\n\n    if (!clientMessageResponse.r.startsWith(this.currentNonce)) {\n      throw new KafkaJSSASLAuthenticationError(\n        `${PREFIX} failed: Invalid server nonce, it does not start with the client nonce`\n      )\n    }\n\n    if (iterations < minIterations) {\n      throw new KafkaJSSASLAuthenticationError(\n        `${PREFIX} failed: Requested iterations ${iterations} is less than the minimum ${minIterations}`\n      )\n    }\n\n    const finalMessageWithoutProof = this.finalMessageWithoutProof(clientMessageResponse)\n    const clientProof = await this.clientProof(clientMessageResponse)\n    const finalMessage = `${finalMessageWithoutProof},p=${clientProof}`\n    const request = scram.finalMessage.request({ finalMessage })\n    const response = scram.finalMessage.response\n\n    return this.saslAuthenticate({\n      request,\n      response,\n    })\n  }\n\n  /**\n   * @private\n   */\n  async clientProof(clientMessageResponse) {\n    const clientKey = await this.clientKey(clientMessageResponse)\n    const storedKey = this.H(clientKey)\n    const clientSignature = this.clientSignature(storedKey, clientMessageResponse)\n    return encode64(SCRAM.xor(clientKey, clientSignature))\n  }\n\n  /**\n   * @private\n   */\n  async clientKey(clientMessageResponse) {\n    const saltedPassword = await this.saltPassword(clientMessageResponse)\n    return this.HMAC(saltedPassword, HMAC_CLIENT_KEY)\n  }\n\n  /**\n   * @private\n   */\n  async serverKey(clientMessageResponse) {\n    const saltedPassword = await this.saltPassword(clientMessageResponse)\n    return this.HMAC(saltedPassword, HMAC_SERVER_KEY)\n  }\n\n  /**\n   * @private\n   */\n  clientSignature(storedKey, clientMessageResponse) {\n    return this.HMAC(storedKey, this.authMessage(clientMessageResponse))\n  }\n\n  /**\n   * @private\n   */\n  serverSignature(serverKey, clientMessageResponse) {\n    return encode64(this.HMAC(serverKey, this.authMessage(clientMessageResponse)))\n  }\n\n  /**\n   * @private\n   */\n  authMessage(clientMessageResponse) {\n    return [\n      this.firstMessageBare(),\n      clientMessageResponse.original,\n      this.finalMessageWithoutProof(clientMessageResponse),\n    ].join(',')\n  }\n\n  /**\n   * @private\n   */\n  async saltPassword(clientMessageResponse) {\n    const salt = Buffer.from(clientMessageResponse.s, 'base64')\n    const iterations = parseInt(clientMessageResponse.i, 10)\n    return SCRAM.hi(this.encodedPassword(), salt, iterations, this.digestDefinition)\n  }\n\n  /**\n   * @private\n   */\n  firstMessageBare() {\n    return `n=${this.encodedUsername()},r=${this.currentNonce}`\n  }\n\n  /**\n   * @private\n   */\n  finalMessageWithoutProof(clientMessageResponse) {\n    const rnonce = clientMessageResponse.r\n    return `c=${encode64(GS2_HEADER)},r=${rnonce}`\n  }\n\n  /**\n   * @private\n   */\n  encodedUsername() {\n    const { username } = this.sasl\n    return SCRAM.sanitizeString(username).toString('utf-8')\n  }\n\n  /**\n   * @private\n   */\n  encodedPassword() {\n    const { password } = this.sasl\n    return password.toString('utf-8')\n  }\n\n  /**\n   * @private\n   */\n  H(data) {\n    return crypto\n      .createHash(this.digestDefinition.type)\n      .update(data)\n      .digest()\n  }\n\n  /**\n   * @private\n   */\n  HMAC(key, data) {\n    return crypto\n      .createHmac(this.digestDefinition.type, key)\n      .update(data)\n      .digest()\n  }\n}\n\nmodule.exports = {\n  DIGESTS,\n  SCRAM,\n}\n", "const { SCRAM, DIGESTS } = require('./scram')\n\nconst scram256AuthenticatorProvider = sasl => ({ host, port, logger, saslAuthenticate }) => {\n  const scram = new SCRAM(sasl, host, port, logger, saslAuthenticate, DIGESTS.SHA256)\n  return {\n    authenticate: async () => await scram.authenticate(),\n  }\n}\n\nmodule.exports = scram256AuthenticatorProvider\n", "const { SCRAM, DIGESTS } = require('./scram')\n\nconst scram512AuthenticatorProvider = sasl => ({ host, port, logger, saslAuthenticate }) => {\n  const scram = new SCRAM(sasl, host, port, logger, saslAuthenticate, DIGESTS.SHA512)\n  return {\n    authenticate: async () => await scram.authenticate(),\n  }\n}\n\nmodule.exports = scram512AuthenticatorProvider\n", "const Encoder = require('../../encoder')\n\nconst US_ASCII_NULL_CHAR = '\\u0000'\n\nmodule.exports = ({ authorizationIdentity, accessKeyId, secretAccessKey, sessionToken = '' }) => ({\n  encode: async () => {\n    return new Encoder().writeBytes(\n      [authorizationIdentity, accessKeyId, secretAccessKey, sessionToken].join(US_ASCII_NULL_CHAR)\n    ).buffer\n  },\n})\n", "module.exports = {\n  decode: async () => true,\n  parse: async () => true,\n}\n", "module.exports = {\n  request: require('./request'),\n  response: require('./response'),\n}\n", "const { request, response } = require('../../protocol/sasl/awsIam')\nconst { KafkaJSSASLAuthenticationError } = require('../../errors')\n\nconst awsIAMAuthenticatorProvider = sasl => ({ host, port, logger, saslAuthenticate }) => {\n  return {\n    authenticate: async () => {\n      if (!sasl.authorizationIdentity) {\n        throw new KafkaJSSASLAuthenticationError('SASL AWS-IAM: Missing authorizationIdentity')\n      }\n      if (!sasl.accessKeyId) {\n        throw new KafkaJSSASLAuthenticationError('SASL AWS-IAM: Missing accessKeyId')\n      }\n      if (!sasl.secretAccessKey) {\n        throw new KafkaJSSASLAuthenticationError('SASL AWS-IAM: Missing secretAccessKey')\n      }\n      if (!sasl.sessionToken) {\n        sasl.sessionToken = ''\n      }\n\n      const broker = `${host}:${port}`\n\n      try {\n        logger.debug('Authenticate with SASL AWS-IAM', { broker })\n        await saslAuthenticate({ request: request(sasl), response })\n        logger.debug('SASL AWS-IAM authentication successful', { broker })\n      } catch (e) {\n        const error = new KafkaJSSASLAuthenticationError(\n          `SASL AWS-IAM authentication failed: ${e.message}`\n        )\n        logger.error(error.message, { broker })\n        throw error\n      }\n    },\n  }\n}\n\nmodule.exports = awsIAMAuthenticatorProvider\n", "/**\n * http://www.ietf.org/rfc/rfc5801.txt\n *\n * See org.apache.kafka.common.security.oauthbearer.internals.OAuthBearerClientInitialResponse\n * for official Java client implementation.\n *\n * The mechanism consists of a message from the client to the server.\n * The client sends the \"n,\"\" GS header, followed by the authorizationIdentitty\n * prefixed by \"a=\" (if present), followed by \",\", followed by a US-ASCII SOH\n * character, followed by \"auth=Bearer \", followed by the token value, followed\n * by US-ASCII SOH character, followed by SASL extensions in OAuth \"friendly\"\n * format and then closed by two additionals US-ASCII SOH characters.\n *\n * SASL extensions are optional an must be expressed as key-value pairs in an\n * object. Each expression is converted as, the extension entry key, followed\n * by \"=\", followed by extension entry value. Each extension is separated by a\n * US-ASCII SOH character. If extensions are not present, their relative part\n * in the message, including the US-ASCII SOH character, is omitted.\n *\n * The client may leave the authorization identity empty to\n * indicate that it is the same as the authentication identity.\n *\n * The server will verify the authentication token and verify that the\n * authentication credentials permit the client to login as the authorization\n * identity. If both steps succeed, the user is logged in.\n */\n\nconst Encoder = require('../../encoder')\n\nconst SEPARATOR = '\\u0001' // SOH - Start Of Header ASCII\n\nfunction formatExtensions(extensions) {\n  let msg = ''\n\n  if (extensions == null) {\n    return msg\n  }\n\n  let prefix = ''\n  for (const k in extensions) {\n    msg += `${prefix}${k}=${extensions[k]}`\n    prefix = SEPARATOR\n  }\n\n  return msg\n}\n\nmodule.exports = async ({ authorizationIdentity = null }, oauthBearerToken) => {\n  const authzid = authorizationIdentity == null ? '' : `\"a=${authorizationIdentity}`\n  let ext = formatExtensions(oauthBearerToken.extensions)\n  if (ext.length > 0) {\n    ext = `${SEPARATOR}${ext}`\n  }\n\n  const oauthMsg = `n,${authzid},${SEPARATOR}auth=Bearer ${oauthBearerToken.value}${ext}${SEPARATOR}${SEPARATOR}`\n\n  return {\n    encode: async () => {\n      return new Encoder().writeBytes(Buffer.from(oauthMsg)).buffer\n    },\n  }\n}\n", "module.exports = {\n  decode: async () => true,\n  parse: async () => true,\n}\n", "module.exports = {\n  request: require('./request'),\n  response: require('./response'),\n}\n", "/**\n * The sasl object must include a property named oauthBearerProvider, an\n * async function that is used to return the OAuth bearer token.\n *\n * The OAuth bearer token must be an object with properties value and\n * (optionally) extensions, that will be sent during the SASL/OAUTHBEARER\n * request.\n *\n * The implementation of the oauthBearerProvider must take care that tokens are\n * reused and refreshed when appropriate.\n */\n\nconst { request } = require('../../protocol/sasl/oauthBearer')\nconst { KafkaJSSASLAuthenticationError } = require('../../errors')\n\nconst oauthBearerAuthenticatorProvider = sasl => ({ host, port, logger, saslAuthenticate }) => {\n  return {\n    authenticate: async () => {\n      const { oauthBearerProvider } = sasl\n\n      if (oauthBearerProvider == null) {\n        throw new KafkaJSSASLAuthenticationError(\n          'SASL OAUTHBEARER: Missing OAuth bearer token provider'\n        )\n      }\n\n      const oauthBearerToken = await oauthBearerProvider()\n\n      if (oauthBearerToken.value == null) {\n        throw new KafkaJSSASLAuthenticationError('SASL OAUTHBEARER: Invalid OAuth bearer token')\n      }\n\n      const broker = `${host}:${port}`\n\n      try {\n        logger.debug('Authenticate with SASL OAUTHBEARER', { broker })\n        await saslAuthenticate({ request: await request(sasl, oauthBearerToken) })\n        logger.debug('SASL OAUTHBEARER authentication successful', { broker })\n      } catch (e) {\n        const error = new KafkaJSSASLAuthenticationError(\n          `SASL OAUTHBEARER authentication failed: ${e.message}`\n        )\n        logger.error(error.message, { broker })\n        throw error\n      }\n    },\n  }\n}\n\nmodule.exports = oauthBearerAuthenticatorProvider\n", "const { requests, lookup } = require('../../protocol/requests')\nconst apiKeys = require('../../protocol/requests/apiKeys')\nconst plainAuthenticatorProvider = require('./plain')\nconst scram256AuthenticatorProvider = require('./scram256')\nconst scram512AuthenticatorProvider = require('./scram512')\nconst awsIAMAuthenticatorProvider = require('./awsIam')\nconst oauthBearerAuthenticatorProvider = require('./oauthBearer')\nconst { KafkaJSSASLAuthenticationError } = require('../../errors')\n\nconst BUILT_IN_AUTHENTICATION_PROVIDERS = {\n  AWS: awsIAMAuthenticatorProvider,\n  PLAIN: plainAuthenticatorProvider,\n  OAUTHBEARER: oauthBearerAuthenticatorProvider,\n  'SCRAM-SHA-256': scram256AuthenticatorProvider,\n  'SCRAM-SHA-512': scram512AuthenticatorProvider,\n}\n\nconst UNLIMITED_SESSION_LIFETIME = '0'\n\nmodule.exports = class SASLAuthenticator {\n  constructor(connection, logger, versions, supportAuthenticationProtocol) {\n    this.connection = connection\n    this.logger = logger\n    this.sessionLifetime = UNLIMITED_SESSION_LIFETIME\n\n    const lookupRequest = lookup(versions)\n    this.saslHandshake = lookupRequest(apiKeys.SaslHandshake, requests.SaslHandshake)\n    this.protocolAuthentication = supportAuthenticationProtocol\n      ? lookupRequest(apiKeys.SaslAuthenticate, requests.SaslAuthenticate)\n      : null\n  }\n\n  async authenticate() {\n    const mechanism = this.connection.sasl.mechanism.toUpperCase()\n    const handshake = await this.connection.send(this.saslHandshake({ mechanism }))\n    if (!handshake.enabledMechanisms.includes(mechanism)) {\n      throw new KafkaJSSASLAuthenticationError(\n        `SASL ${mechanism} mechanism is not supported by the server`\n      )\n    }\n\n    const saslAuthenticate = async ({ request, response }) => {\n      if (this.protocolAuthentication) {\n        const requestAuthBytes = await request.encode()\n        const authResponse = await this.connection.send(\n          this.protocolAuthentication({ authBytes: requestAuthBytes })\n        )\n\n        // `0` is a string because `sessionLifetimeMs` is an int64 encoded as string.\n        // This is not present in SaslAuthenticateV0, so we default to `\"0\"`\n        this.sessionLifetime = authResponse.sessionLifetimeMs || UNLIMITED_SESSION_LIFETIME\n\n        if (!response) {\n          return\n        }\n\n        const { authBytes: responseAuthBytes } = authResponse\n        const payloadDecoded = await response.decode(responseAuthBytes)\n        return response.parse(payloadDecoded)\n      }\n\n      return this.connection.sendAuthRequest({ request, response })\n    }\n\n    if (\n      !this.connection.sasl.authenticationProvider &&\n      Object.keys(BUILT_IN_AUTHENTICATION_PROVIDERS).includes(mechanism)\n    ) {\n      this.connection.sasl.authenticationProvider = BUILT_IN_AUTHENTICATION_PROVIDERS[mechanism](\n        this.connection.sasl\n      )\n    }\n    await this.connection.sasl\n      .authenticationProvider({\n        host: this.connection.host,\n        port: this.connection.port,\n        logger: this.logger.namespace(`SaslAuthenticator-${mechanism}`),\n        saslAuthenticate,\n      })\n      .authenticate()\n  }\n}\n", "const createSocket = require('./socket')\nconst createRequest = require('../protocol/request')\nconst Decoder = require('../protocol/decoder')\nconst { KafkaJSConnectionError, KafkaJSConnectionClosedError } = require('../errors')\nconst { INT_32_MAX_VALUE } = require('../constants')\nconst getEnv = require('../env')\nconst RequestQueue = require('./requestQueue')\nconst { CONNECTION_STATUS, CONNECTED_STATUS } = require('./connectionStatus')\nconst sharedPromiseTo = require('../utils/sharedPromiseTo')\nconst Long = require('../utils/long')\nconst SASLAuthenticator = require('../broker/saslAuthenticator')\nconst apiKeys = require('../protocol/requests/apiKeys')\n\nconst requestInfo = ({ apiName, apiKey, apiVersion }) =>\n  `${apiName}(key: ${apiKey}, version: ${apiVersion})`\n\n/**\n * @param request - request from protocol\n * @returns {boolean}\n */\nconst isAuthenticatedRequest = request => {\n  return ![apiKeys.ApiVersions, apiKeys.SaslHandshake, apiKeys.SaslAuthenticate].includes(\n    request.apiKey\n  )\n}\n\nconst PRIVATE = {\n  SHOULD_REAUTHENTICATE: Symbol('private:Connection:shouldReauthenticate'),\n  AUTHENTICATE: Symbol('private:Connection:authenticate'),\n}\n\nmodule.exports = class Connection {\n  /**\n   * @param {Object} options\n   * @param {string} options.host\n   * @param {number} options.port\n   * @param {import(\"../../types\").Logger} options.logger\n   * @param {import(\"../../types\").ISocketFactory} options.socketFactory\n   * @param {string} [options.clientId='kafkajs']\n   * @param {number} options.requestTimeout The maximum amount of time the client will wait for the response of a request,\n   *                                in milliseconds\n   * @param {string} [options.rack=null]\n   * @param {Object} [options.ssl=null] Options for the TLS Secure Context. It accepts all options,\n   *                            usually \"cert\", \"key\" and \"ca\". More information at\n   *                            https://nodejs.org/api/tls.html#tls_tls_createsecurecontext_options\n   * @param {Object} [options.sasl=null] Attributes used for SASL authentication. Options based on the\n   *                             key \"mechanism\". Connection is not actively using the SASL attributes\n   *                             but acting as a data object for this information\n   * @param {number} [options.reauthenticationThreshold=10000]\n   * @param {number} options.connectionTimeout The connection timeout, in milliseconds\n   * @param {boolean} [options.enforceRequestTimeout]\n   * @param {number} [options.maxInFlightRequests=null] The maximum number of unacknowledged requests on a connection before\n   *                                            enqueuing\n   * @param {import(\"../instrumentation/emitter\")} [options.instrumentationEmitter=null]\n   */\n  constructor({\n    host,\n    port,\n    logger,\n    socketFactory,\n    requestTimeout,\n    reauthenticationThreshold = 10000,\n    rack = null,\n    ssl = null,\n    sasl = null,\n    clientId = 'kafkajs',\n    connectionTimeout,\n    enforceRequestTimeout = true,\n    maxInFlightRequests = null,\n    instrumentationEmitter = null,\n  }) {\n    this.host = host\n    this.port = port\n    this.rack = rack\n    this.clientId = clientId\n    this.broker = `${this.host}:${this.port}`\n    this.logger = logger.namespace('Connection')\n\n    this.socketFactory = socketFactory\n    this.ssl = ssl\n    this.sasl = sasl\n\n    this.requestTimeout = requestTimeout\n    this.connectionTimeout = connectionTimeout\n    this.reauthenticationThreshold = reauthenticationThreshold\n\n    this.bytesBuffered = 0\n    this.bytesNeeded = Decoder.int32Size()\n    this.chunks = []\n\n    this.connectionStatus = CONNECTION_STATUS.DISCONNECTED\n    this.correlationId = 0\n    this.requestQueue = new RequestQueue({\n      instrumentationEmitter,\n      maxInFlightRequests,\n      requestTimeout,\n      enforceRequestTimeout,\n      clientId,\n      broker: this.broker,\n      logger: logger.namespace('RequestQueue'),\n      isConnected: () => this.isConnected(),\n    })\n\n    this.versions = null\n\n    this.authHandlers = null\n    this.authExpectResponse = false\n\n    const log = level => (message, extra = {}) => {\n      const logFn = this.logger[level]\n      logFn(message, { broker: this.broker, clientId, ...extra })\n    }\n\n    this.logDebug = log('debug')\n    this.logError = log('error')\n\n    const env = getEnv()\n    this.shouldLogBuffers = env.KAFKAJS_DEBUG_PROTOCOL_BUFFERS === '1'\n    this.shouldLogFetchBuffer =\n      this.shouldLogBuffers && env.KAFKAJS_DEBUG_EXTENDED_PROTOCOL_BUFFERS === '1'\n\n    this.authenticatedAt = null\n    this.sessionLifetime = Long.ZERO\n    this.supportAuthenticationProtocol = null\n\n    /**\n     * @private\n     * @returns {Promise}\n     */\n    this[PRIVATE.AUTHENTICATE] = sharedPromiseTo(async () => {\n      if (this.sasl && !this.isAuthenticated()) {\n        const authenticator = new SASLAuthenticator(\n          this,\n          this.logger,\n          this.versions,\n          this.supportAuthenticationProtocol\n        )\n\n        await authenticator.authenticate()\n        this.authenticatedAt = process.hrtime()\n        this.sessionLifetime = Long.fromValue(authenticator.sessionLifetime)\n      }\n    })\n  }\n\n  getSupportAuthenticationProtocol() {\n    return this.supportAuthenticationProtocol\n  }\n\n  setSupportAuthenticationProtocol(isSupported) {\n    this.supportAuthenticationProtocol = isSupported\n  }\n\n  setVersions(versions) {\n    this.versions = versions\n  }\n\n  isConnected() {\n    return CONNECTED_STATUS.includes(this.connectionStatus)\n  }\n\n  /**\n   * @public\n   * @returns {Promise}\n   */\n  connect() {\n    return new Promise((resolve, reject) => {\n      if (this.isConnected()) {\n        return resolve(true)\n      }\n\n      this.authenticatedAt = null\n\n      let timeoutId\n\n      const onConnect = () => {\n        clearTimeout(timeoutId)\n        this.connectionStatus = CONNECTION_STATUS.CONNECTED\n        this.requestQueue.scheduleRequestTimeoutCheck()\n        resolve(true)\n      }\n\n      const onData = data => {\n        this.processData(data)\n      }\n\n      const onEnd = async () => {\n        clearTimeout(timeoutId)\n\n        const wasConnected = this.isConnected()\n\n        if (this.authHandlers) {\n          this.authHandlers.onError()\n        } else if (wasConnected) {\n          this.logDebug('Kafka server has closed connection')\n          this.rejectRequests(\n            new KafkaJSConnectionClosedError('Closed connection', {\n              host: this.host,\n              port: this.port,\n            })\n          )\n        }\n\n        await this.disconnect()\n      }\n\n      const onError = async e => {\n        clearTimeout(timeoutId)\n\n        const error = new KafkaJSConnectionError(`Connection error: ${e.message}`, {\n          broker: `${this.host}:${this.port}`,\n          code: e.code,\n        })\n\n        this.logError(error.message, { stack: e.stack })\n        this.rejectRequests(error)\n        await this.disconnect()\n\n        reject(error)\n      }\n\n      const onTimeout = async () => {\n        const error = new KafkaJSConnectionError('Connection timeout', {\n          broker: `${this.host}:${this.port}`,\n        })\n\n        this.logError(error.message)\n        this.rejectRequests(error)\n        await this.disconnect()\n        reject(error)\n      }\n\n      this.logDebug(`Connecting`, {\n        ssl: !!this.ssl,\n        sasl: !!this.sasl,\n      })\n\n      try {\n        timeoutId = setTimeout(onTimeout, this.connectionTimeout)\n        this.socket = createSocket({\n          socketFactory: this.socketFactory,\n          host: this.host,\n          port: this.port,\n          ssl: this.ssl,\n          onConnect,\n          onData,\n          onEnd,\n          onError,\n          onTimeout,\n        })\n      } catch (e) {\n        clearTimeout(timeoutId)\n        reject(\n          new KafkaJSConnectionError(`Failed to connect: ${e.message}`, {\n            broker: `${this.host}:${this.port}`,\n          })\n        )\n      }\n    })\n  }\n\n  /**\n   * @public\n   * @returns {Promise}\n   */\n  async disconnect() {\n    this.authenticatedAt = null\n    this.connectionStatus = CONNECTION_STATUS.DISCONNECTING\n    this.logDebug('disconnecting...')\n\n    await this.requestQueue.waitForPendingRequests()\n    this.requestQueue.destroy()\n\n    if (this.socket) {\n      this.socket.end()\n      this.socket.unref()\n    }\n\n    this.connectionStatus = CONNECTION_STATUS.DISCONNECTED\n    this.logDebug('disconnected')\n    return true\n  }\n\n  /**\n   * @public\n   * @returns {boolean}\n   */\n  isAuthenticated() {\n    return this.authenticatedAt != null && !this[PRIVATE.SHOULD_REAUTHENTICATE]()\n  }\n\n  /***\n   * @private\n   */\n  [PRIVATE.SHOULD_REAUTHENTICATE]() {\n    if (this.sessionLifetime.equals(Long.ZERO)) {\n      return false\n    }\n\n    if (this.authenticatedAt == null) {\n      return true\n    }\n\n    const [secondsSince, remainingNanosSince] = process.hrtime(this.authenticatedAt)\n    const millisSince = Long.fromValue(secondsSince)\n      .multiply(1000)\n      .add(Long.fromValue(remainingNanosSince).divide(1000000))\n\n    const reauthenticateAt = millisSince.add(this.reauthenticationThreshold)\n    return reauthenticateAt.greaterThanOrEqual(this.sessionLifetime)\n  }\n\n  /** @public */\n  async authenticate() {\n    await this[PRIVATE.AUTHENTICATE]()\n  }\n\n  /**\n   * @public\n   * @returns {Promise}\n   */\n  sendAuthRequest({ request, response }) {\n    this.authExpectResponse = !!response\n\n    /**\n     * TODO: rewrite removing the async promise executor\n     */\n\n    /* eslint-disable no-async-promise-executor */\n    return new Promise(async (resolve, reject) => {\n      this.authHandlers = {\n        onSuccess: rawData => {\n          this.authHandlers = null\n          this.authExpectResponse = false\n\n          response\n            .decode(rawData)\n            .then(data => response.parse(data))\n            .then(resolve)\n            .catch(reject)\n        },\n        onError: () => {\n          this.authHandlers = null\n          this.authExpectResponse = false\n\n          reject(\n            new KafkaJSConnectionError('Connection closed by the server', {\n              broker: `${this.host}:${this.port}`,\n            })\n          )\n        },\n      }\n\n      try {\n        const requestPayload = await request.encode()\n\n        this.failIfNotConnected()\n        this.socket.write(requestPayload, 'binary')\n      } catch (e) {\n        reject(e)\n      }\n    })\n  }\n\n  /**\n   * @public\n   * @param {object} protocol\n   * @param {object} protocol.request It is defined by the protocol and consists of an object with \"apiKey\",\n   *                         \"apiVersion\", \"apiName\" and an \"encode\" function. The encode function\n   *                         must return an instance of Encoder\n   *\n   * @param {object} protocol.response It is defined by the protocol and consists of an object with two functions:\n   *                          \"decode\" and \"parse\"\n   *\n   * @param {number} [protocol.requestTimeout=null] Override for the default requestTimeout\n   * @param {boolean} [protocol.logResponseError=true] Whether to log errors\n   * @returns {Promise<data>} where data is the return of \"response#parse\"\n   */\n  async send({ request, response, requestTimeout = null, logResponseError = true }) {\n    if (!this.isAuthenticated() && isAuthenticatedRequest(request)) {\n      await this[PRIVATE.AUTHENTICATE]()\n    }\n\n    this.failIfNotConnected()\n\n    const expectResponse = !request.expectResponse || request.expectResponse()\n    const sendRequest = async () => {\n      const { clientId } = this\n      const correlationId = this.nextCorrelationId()\n\n      const requestPayload = await createRequest({ request, correlationId, clientId })\n      const { apiKey, apiName, apiVersion } = request\n      this.logDebug(`Request ${requestInfo(request)}`, {\n        correlationId,\n        expectResponse,\n        size: Buffer.byteLength(requestPayload.buffer),\n      })\n\n      return new Promise((resolve, reject) => {\n        try {\n          this.failIfNotConnected()\n          const entry = { apiKey, apiName, apiVersion, correlationId, resolve, reject }\n\n          this.requestQueue.push({\n            entry,\n            expectResponse,\n            requestTimeout,\n            sendRequest: () => {\n              this.socket.write(requestPayload.buffer, 'binary')\n            },\n          })\n        } catch (e) {\n          reject(e)\n        }\n      })\n    }\n\n    const { correlationId, size, entry, payload } = await sendRequest()\n\n    if (!expectResponse) {\n      return\n    }\n\n    try {\n      const payloadDecoded = await response.decode(payload)\n\n      /**\n       * @see KIP-219\n       * If the response indicates that the client-side needs to throttle, do that.\n       */\n      this.requestQueue.maybeThrottle(payloadDecoded.clientSideThrottleTime)\n\n      const data = await response.parse(payloadDecoded)\n      const isFetchApi = entry.apiName === 'Fetch'\n      this.logDebug(`Response ${requestInfo(entry)}`, {\n        correlationId,\n        size,\n        data: isFetchApi && !this.shouldLogFetchBuffer ? '[filtered]' : data,\n      })\n\n      return data\n    } catch (e) {\n      if (logResponseError) {\n        this.logError(`Response ${requestInfo(entry)}`, {\n          error: e.message,\n          correlationId,\n          size,\n        })\n      }\n\n      const isBuffer = Buffer.isBuffer(payload)\n      this.logDebug(`Response ${requestInfo(entry)}`, {\n        error: e.message,\n        correlationId,\n        payload:\n          isBuffer && !this.shouldLogBuffers ? { type: 'Buffer', data: '[filtered]' } : payload,\n      })\n\n      throw e\n    }\n  }\n\n  /**\n   * @private\n   */\n  failIfNotConnected() {\n    if (!this.isConnected()) {\n      throw new KafkaJSConnectionError('Not connected', {\n        broker: `${this.host}:${this.port}`,\n      })\n    }\n  }\n\n  /**\n   * @private\n   */\n  nextCorrelationId() {\n    if (this.correlationId >= INT_32_MAX_VALUE) {\n      this.correlationId = 0\n    }\n\n    return this.correlationId++\n  }\n\n  /**\n   * @private\n   */\n  processData(rawData) {\n    if (this.authHandlers && !this.authExpectResponse) {\n      return this.authHandlers.onSuccess(rawData)\n    }\n\n    // Accumulate the new chunk\n    this.chunks.push(rawData)\n    this.bytesBuffered += Buffer.byteLength(rawData)\n\n    // Process data if there are enough bytes to read the expected response size,\n    // otherwise keep buffering\n    while (this.bytesNeeded <= this.bytesBuffered) {\n      const buffer = this.chunks.length > 1 ? Buffer.concat(this.chunks) : this.chunks[0]\n      const decoder = new Decoder(buffer)\n      const expectedResponseSize = decoder.readInt32()\n\n      // Return early if not enough bytes to read the full response\n      if (!decoder.canReadBytes(expectedResponseSize)) {\n        this.chunks = [buffer]\n        this.bytesBuffered = Buffer.byteLength(buffer)\n        this.bytesNeeded = Decoder.int32Size() + expectedResponseSize\n        return\n      }\n\n      const response = new Decoder(decoder.readBytes(expectedResponseSize))\n\n      // Reset the buffered chunks as the rest of the bytes\n      const remainderBuffer = decoder.readAll()\n      this.chunks = [remainderBuffer]\n      this.bytesBuffered = Buffer.byteLength(remainderBuffer)\n      this.bytesNeeded = Decoder.int32Size()\n\n      if (this.authHandlers) {\n        const rawResponseSize = Decoder.int32Size() + expectedResponseSize\n        const rawResponseBuffer = buffer.slice(0, rawResponseSize)\n        return this.authHandlers.onSuccess(rawResponseBuffer)\n      }\n\n      const correlationId = response.readInt32()\n      const payload = response.readAll()\n\n      this.requestQueue.fulfillRequest({\n        size: expectedResponseSize,\n        correlationId,\n        payload,\n      })\n    }\n  }\n\n  /**\n   * @private\n   */\n  rejectRequests(error) {\n    this.requestQueue.rejectAll(error)\n  }\n}\n", "const apiKeys = require('../protocol/requests/apiKeys')\nconst Connection = require('./connection')\n\nmodule.exports = class ConnectionPool {\n  /**\n   * @param {ConstructorParameters<typeof Connection>[0]} options\n   */\n  constructor(options) {\n    this.logger = options.logger.namespace('ConnectionPool')\n    this.connectionTimeout = options.connectionTimeout\n    this.host = options.host\n    this.port = options.port\n    this.rack = options.rack\n    this.ssl = options.ssl\n    this.sasl = options.sasl\n    this.clientId = options.clientId\n    this.socketFactory = options.socketFactory\n\n    this.pool = new Array(2).fill().map(() => new Connection(options))\n  }\n\n  isConnected() {\n    return this.pool.some(c => c.isConnected())\n  }\n\n  isAuthenticated() {\n    return this.pool.some(c => c.isAuthenticated())\n  }\n\n  setSupportAuthenticationProtocol(isSupported) {\n    this.map(c => c.setSupportAuthenticationProtocol(isSupported))\n  }\n\n  setVersions(versions) {\n    this.map(c => c.setVersions(versions))\n  }\n\n  map(callback) {\n    return this.pool.map(c => callback(c))\n  }\n\n  async send(protocolRequest) {\n    const connection = await this.getConnectionByRequest(protocolRequest)\n    return connection.send(protocolRequest)\n  }\n\n  getConnectionByRequest({ request: { apiKey } }) {\n    const index = { [apiKeys.Fetch]: 1 }[apiKey] || 0\n    return this.getConnection(index)\n  }\n\n  async getConnection(index = 0) {\n    const connection = this.pool[index]\n\n    if (!connection.isConnected()) {\n      await connection.connect()\n    }\n\n    return connection\n  }\n\n  async destroy() {\n    await Promise.all(this.map(c => c.disconnect()))\n  }\n}\n", "const { KafkaJSConnectionError, KafkaJSNonRetriableError } = require('../errors')\nconst ConnectionPool = require('../network/connectionPool')\n\n/**\n * @typedef {Object} ConnectionPoolBuilder\n * @property {(destination?: { host?: string, port?: number, rack?: string }) => Promise<ConnectionPool>} build\n */\n\n/**\n * @param {Object} options\n * @param {import(\"../../types\").ISocketFactory} [options.socketFactory]\n * @param {string[]|(() => string[])} options.brokers\n * @param {Object} [options.ssl]\n * @param {Object} [options.sasl]\n * @param {string} options.clientId\n * @param {number} options.requestTimeout\n * @param {boolean} [options.enforceRequestTimeout]\n * @param {number} [options.connectionTimeout]\n * @param {number} [options.maxInFlightRequests]\n * @param {import(\"../../types\").RetryOptions} [options.retry]\n * @param {import(\"../../types\").Logger} options.logger\n * @param {import(\"../instrumentation/emitter\")} [options.instrumentationEmitter]\n * @param {number} [options.reauthenticationThreshold]\n * @returns {ConnectionPoolBuilder}\n */\nmodule.exports = ({\n  socketFactory,\n  brokers,\n  ssl,\n  sasl,\n  clientId,\n  requestTimeout,\n  enforceRequestTimeout,\n  connectionTimeout,\n  maxInFlightRequests,\n  logger,\n  instrumentationEmitter = null,\n  reauthenticationThreshold,\n}) => {\n  let index = 0\n\n  const isValidBroker = broker => {\n    return broker && typeof broker === 'string' && broker.length > 0\n  }\n\n  const validateBrokers = brokers => {\n    if (!brokers) {\n      throw new KafkaJSNonRetriableError(`Failed to connect: brokers should not be null`)\n    }\n\n    if (Array.isArray(brokers)) {\n      if (!brokers.length) {\n        throw new KafkaJSNonRetriableError(`Failed to connect: brokers array is empty`)\n      }\n\n      brokers.forEach((broker, index) => {\n        if (!isValidBroker(broker)) {\n          throw new KafkaJSNonRetriableError(\n            `Failed to connect: broker at index ${index} is invalid \"${typeof broker}\"`\n          )\n        }\n      })\n    }\n  }\n\n  const getBrokers = async () => {\n    let list\n\n    if (typeof brokers === 'function') {\n      try {\n        list = await brokers()\n      } catch (e) {\n        const wrappedError = new KafkaJSConnectionError(\n          `Failed to connect: \"config.brokers\" threw: ${e.message}`\n        )\n        wrappedError.stack = `${wrappedError.name}\\n  Caused by: ${e.stack}`\n        throw wrappedError\n      }\n    } else {\n      list = brokers\n    }\n\n    validateBrokers(list)\n\n    return list\n  }\n\n  return {\n    build: async ({ host, port, rack } = {}) => {\n      if (!host) {\n        const list = await getBrokers()\n\n        const randomBroker = list[index++ % list.length]\n\n        host = randomBroker.split(':')[0]\n        port = Number(randomBroker.split(':')[1])\n      }\n\n      return new ConnectionPool({\n        host,\n        port,\n        rack,\n        sasl,\n        ssl,\n        clientId,\n        socketFactory,\n        connectionTimeout,\n        requestTimeout,\n        enforceRequestTimeout,\n        maxInFlightRequests,\n        instrumentationEmitter,\n        logger,\n        reauthenticationThreshold,\n      })\n    },\n  }\n}\n", "const BrokerPool = require('./brokerPool')\nconst Lock = require('../utils/lock')\nconst sharedPromiseTo = require('../utils/sharedPromiseTo')\nconst createRetry = require('../retry')\nconst connectionPoolBuilder = require('./connectionPoolBuilder')\nconst { EARLIEST_OFFSET, LATEST_OFFSET } = require('../constants')\nconst {\n  KafkaJSError,\n  KafkaJSBrokerNotFound,\n  KafkaJSMetadataNotLoaded,\n  KafkaJSTopicMetadataNotLoaded,\n  KafkaJSGroupCoordinatorNotFound,\n} = require('../errors')\nconst COORDINATOR_TYPES = require('../protocol/coordinatorTypes')\n\nconst { keys } = Object\n\nconst mergeTopics = (obj, { topic, partitions }) => ({\n  ...obj,\n  [topic]: [...(obj[topic] || []), ...partitions],\n})\n\nconst PRIVATE = {\n  CONNECT: Symbol('private:Cluster:connect'),\n  REFRESH_METADATA: Symbol('private:Cluster:refreshMetadata'),\n  REFRESH_METADATA_IF_NECESSARY: Symbol('private:Cluster:refreshMetadataIfNecessary'),\n  FIND_CONTROLLER_BROKER: Symbol('private:Cluster:findControllerBroker'),\n}\n\nmodule.exports = class Cluster {\n  /**\n   * @param {Object} options\n   * @param {Array<string>} options.brokers example: ['127.0.0.1:9092', '127.0.0.1:9094']\n   * @param {Object} options.ssl\n   * @param {Object} options.sasl\n   * @param {string} options.clientId\n   * @param {number} options.connectionTimeout - in milliseconds\n   * @param {number} options.authenticationTimeout - in milliseconds\n   * @param {number} options.reauthenticationThreshold - in milliseconds\n   * @param {number} [options.requestTimeout=30000] - in milliseconds\n   * @param {boolean} [options.enforceRequestTimeout]\n   * @param {number} options.metadataMaxAge - in milliseconds\n   * @param {boolean} options.allowAutoTopicCreation\n   * @param {number} options.maxInFlightRequests\n   * @param {number} options.isolationLevel\n   * @param {import(\"../../types\").RetryOptions} options.retry\n   * @param {import(\"../../types\").Logger} options.logger\n   * @param {import(\"../../types\").ISocketFactory} options.socketFactory\n   * @param {Map} [options.offsets]\n   * @param {import(\"../instrumentation/emitter\")} [options.instrumentationEmitter=null]\n   */\n  constructor({\n    logger: rootLogger,\n    socketFactory,\n    brokers,\n    ssl,\n    sasl,\n    clientId,\n    connectionTimeout,\n    authenticationTimeout,\n    reauthenticationThreshold,\n    requestTimeout = 30000,\n    enforceRequestTimeout,\n    metadataMaxAge,\n    retry,\n    allowAutoTopicCreation,\n    maxInFlightRequests,\n    isolationLevel,\n    instrumentationEmitter = null,\n    offsets = new Map(),\n  }) {\n    this.rootLogger = rootLogger\n    this.logger = rootLogger.namespace('Cluster')\n    this.retrier = createRetry(retry)\n    this.connectionPoolBuilder = connectionPoolBuilder({\n      logger: rootLogger,\n      instrumentationEmitter,\n      socketFactory,\n      brokers,\n      ssl,\n      sasl,\n      clientId,\n      connectionTimeout,\n      requestTimeout,\n      enforceRequestTimeout,\n      maxInFlightRequests,\n      reauthenticationThreshold,\n    })\n\n    this.targetTopics = new Set()\n    this.mutatingTargetTopics = new Lock({\n      description: `updating target topics`,\n      timeout: requestTimeout,\n    })\n    this.isolationLevel = isolationLevel\n    this.brokerPool = new BrokerPool({\n      connectionPoolBuilder: this.connectionPoolBuilder,\n      logger: this.rootLogger,\n      retry,\n      allowAutoTopicCreation,\n      authenticationTimeout,\n      metadataMaxAge,\n    })\n    this.committedOffsetsByGroup = offsets\n\n    this[PRIVATE.CONNECT] = sharedPromiseTo(async () => {\n      return await this.brokerPool.connect()\n    })\n\n    this[PRIVATE.REFRESH_METADATA] = sharedPromiseTo(async () => {\n      return await this.brokerPool.refreshMetadata(Array.from(this.targetTopics))\n    })\n\n    this[PRIVATE.REFRESH_METADATA_IF_NECESSARY] = sharedPromiseTo(async () => {\n      return await this.brokerPool.refreshMetadataIfNecessary(Array.from(this.targetTopics))\n    })\n\n    this[PRIVATE.FIND_CONTROLLER_BROKER] = sharedPromiseTo(async () => {\n      const { metadata } = this.brokerPool\n\n      if (!metadata || metadata.controllerId == null) {\n        throw new KafkaJSMetadataNotLoaded('Topic metadata not loaded')\n      }\n\n      const broker = await this.findBroker({ nodeId: metadata.controllerId })\n\n      if (!broker) {\n        throw new KafkaJSBrokerNotFound(\n          `Controller broker with id ${metadata.controllerId} not found in the cached metadata`\n        )\n      }\n\n      return broker\n    })\n  }\n\n  isConnected() {\n    return this.brokerPool.hasConnectedBrokers()\n  }\n\n  /**\n   * @public\n   * @returns {Promise<void>}\n   */\n  async connect() {\n    await this[PRIVATE.CONNECT]()\n  }\n\n  /**\n   * @public\n   * @returns {Promise<void>}\n   */\n  async disconnect() {\n    await this.brokerPool.disconnect()\n  }\n\n  /**\n   * @public\n   * @param {object} destination\n   * @param {String} destination.host\n   * @param {Number} destination.port\n   */\n  removeBroker({ host, port }) {\n    this.brokerPool.removeBroker({ host, port })\n  }\n\n  /**\n   * @public\n   * @returns {Promise<void>}\n   */\n  async refreshMetadata() {\n    await this[PRIVATE.REFRESH_METADATA]()\n  }\n\n  /**\n   * @public\n   * @returns {Promise<void>}\n   */\n  async refreshMetadataIfNecessary() {\n    await this[PRIVATE.REFRESH_METADATA_IF_NECESSARY]()\n  }\n\n  /**\n   * @public\n   * @returns {Promise<import(\"../../types\").BrokerMetadata>}\n   */\n  async metadata({ topics = [] } = {}) {\n    return this.retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await this.brokerPool.refreshMetadataIfNecessary(topics)\n        return this.brokerPool.withBroker(async ({ broker }) => broker.metadata(topics))\n      } catch (e) {\n        if (e.type === 'LEADER_NOT_AVAILABLE') {\n          throw e\n        }\n\n        bail(e)\n      }\n    })\n  }\n\n  /**\n   * @public\n   * @param {string} topic\n   * @return {Promise}\n   */\n  async addTargetTopic(topic) {\n    return this.addMultipleTargetTopics([topic])\n  }\n\n  /**\n   * @public\n   * @param {string[]} topics\n   * @return {Promise}\n   */\n  async addMultipleTargetTopics(topics) {\n    await this.mutatingTargetTopics.acquire()\n\n    try {\n      const previousSize = this.targetTopics.size\n      const previousTopics = new Set(this.targetTopics)\n      for (const topic of topics) {\n        this.targetTopics.add(topic)\n      }\n\n      const hasChanged = previousSize !== this.targetTopics.size || !this.brokerPool.metadata\n\n      if (hasChanged) {\n        try {\n          await this.refreshMetadata()\n        } catch (e) {\n          if (\n            e.type === 'INVALID_TOPIC_EXCEPTION' ||\n            e.type === 'UNKNOWN_TOPIC_OR_PARTITION' ||\n            e.type === 'TOPIC_AUTHORIZATION_FAILED'\n          ) {\n            this.targetTopics = previousTopics\n          }\n\n          throw e\n        }\n      }\n    } finally {\n      await this.mutatingTargetTopics.release()\n    }\n  }\n\n  /** @type {() => string[]} */\n  getNodeIds() {\n    return this.brokerPool.getNodeIds()\n  }\n\n  /**\n   * @public\n   * @param {object} options\n   * @param {string} options.nodeId\n   * @returns {Promise<import(\"../../types\").Broker>}\n   */\n  async findBroker({ nodeId }) {\n    try {\n      return await this.brokerPool.findBroker({ nodeId })\n    } catch (e) {\n      // The client probably has stale metadata\n      if (\n        e.name === 'KafkaJSBrokerNotFound' ||\n        e.name === 'KafkaJSLockTimeout' ||\n        e.name === 'KafkaJSConnectionError'\n      ) {\n        await this.refreshMetadata()\n      }\n\n      throw e\n    }\n  }\n\n  /**\n   * @public\n   * @returns {Promise<import(\"../../types\").Broker>}\n   */\n  async findControllerBroker() {\n    return await this[PRIVATE.FIND_CONTROLLER_BROKER]()\n  }\n\n  /**\n   * @public\n   * @param {string} topic\n   * @returns {import(\"../../types\").PartitionMetadata[]} Example:\n   *                   [{\n   *                     isr: [2],\n   *                     leader: 2,\n   *                     partitionErrorCode: 0,\n   *                     partitionId: 0,\n   *                     replicas: [2],\n   *                   }]\n   */\n  findTopicPartitionMetadata(topic) {\n    const { metadata } = this.brokerPool\n    if (!metadata || !metadata.topicMetadata) {\n      throw new KafkaJSTopicMetadataNotLoaded('Topic metadata not loaded', { topic })\n    }\n\n    const topicMetadata = metadata.topicMetadata.find(t => t.topic === topic)\n    return topicMetadata ? topicMetadata.partitionMetadata : []\n  }\n\n  /**\n   * @public\n   * @param {string} topic\n   * @param {(number|string)[]} partitions\n   * @returns {Object} Object with leader and partitions. For partitions 0 and 5\n   *                   the result could be:\n   *                     { '0': [0], '2': [5] }\n   *\n   *                   where the key is the nodeId.\n   */\n  findLeaderForPartitions(topic, partitions) {\n    const partitionMetadata = this.findTopicPartitionMetadata(topic)\n    return partitions.reduce((result, id) => {\n      const partitionId = parseInt(id, 10)\n      const metadata = partitionMetadata.find(p => p.partitionId === partitionId)\n\n      if (!metadata) {\n        return result\n      }\n\n      if (metadata.leader === null || metadata.leader === undefined) {\n        throw new KafkaJSError('Invalid partition metadata', { topic, partitionId, metadata })\n      }\n\n      const { leader } = metadata\n      const current = result[leader] || []\n      return { ...result, [leader]: [...current, partitionId] }\n    }, {})\n  }\n\n  /**\n   * @public\n   * @param {object} params\n   * @param {string} params.groupId\n   * @param {import(\"../protocol/coordinatorTypes\").CoordinatorType} [params.coordinatorType=0]\n   * @returns {Promise<import(\"../../types\").Broker>}\n   */\n  async findGroupCoordinator({ groupId, coordinatorType = COORDINATOR_TYPES.GROUP }) {\n    return this.retrier(async (bail, retryCount, retryTime) => {\n      try {\n        const { coordinator } = await this.findGroupCoordinatorMetadata({\n          groupId,\n          coordinatorType,\n        })\n        return await this.findBroker({ nodeId: coordinator.nodeId })\n      } catch (e) {\n        // A new broker can join the cluster before we have the chance\n        // to refresh metadata\n        if (e.name === 'KafkaJSBrokerNotFound' || e.type === 'GROUP_COORDINATOR_NOT_AVAILABLE') {\n          this.logger.debug(`${e.message}, refreshing metadata and trying again...`, {\n            groupId,\n            retryCount,\n            retryTime,\n          })\n\n          await this.refreshMetadata()\n          throw e\n        }\n\n        if (e.code === 'ECONNREFUSED') {\n          // During maintenance the current coordinator can go down; findBroker will\n          // refresh metadata and re-throw the error. findGroupCoordinator has to re-throw\n          // the error to go through the retry cycle.\n          throw e\n        }\n\n        bail(e)\n      }\n    })\n  }\n\n  /**\n   * @public\n   * @param {object} params\n   * @param {string} params.groupId\n   * @param {import(\"../protocol/coordinatorTypes\").CoordinatorType} [params.coordinatorType=0]\n   * @returns {Promise<Object>}\n   */\n  async findGroupCoordinatorMetadata({ groupId, coordinatorType }) {\n    const brokerMetadata = await this.brokerPool.withBroker(async ({ nodeId, broker }) => {\n      return await this.retrier(async (bail, retryCount, retryTime) => {\n        try {\n          const brokerMetadata = await broker.findGroupCoordinator({ groupId, coordinatorType })\n          this.logger.debug('Found group coordinator', {\n            broker: brokerMetadata.host,\n            nodeId: brokerMetadata.coordinator.nodeId,\n          })\n          return brokerMetadata\n        } catch (e) {\n          this.logger.debug('Tried to find group coordinator', {\n            nodeId,\n            groupId,\n            error: e,\n          })\n\n          if (e.type === 'GROUP_COORDINATOR_NOT_AVAILABLE') {\n            this.logger.debug('Group coordinator not available, retrying...', {\n              nodeId,\n              retryCount,\n              retryTime,\n            })\n\n            throw e\n          }\n\n          bail(e)\n        }\n      })\n    })\n\n    if (brokerMetadata) {\n      return brokerMetadata\n    }\n\n    throw new KafkaJSGroupCoordinatorNotFound('Failed to find group coordinator')\n  }\n\n  /**\n   * @param {object} topicConfiguration\n   * @returns {number}\n   */\n  defaultOffset({ fromBeginning }) {\n    return fromBeginning ? EARLIEST_OFFSET : LATEST_OFFSET\n  }\n\n  /**\n   * @public\n   * @param {Array<Object>} topics\n   *                          [\n   *                            {\n   *                              topic: 'my-topic-name',\n   *                              partitions: [{ partition: 0 }],\n   *                              fromBeginning: false\n   *                            }\n   *                          ]\n   * @returns {Promise<import(\"../../types\").TopicOffsets[]>} example:\n   *                          [\n   *                            {\n   *                              topic: 'my-topic-name',\n   *                              partitions: [\n   *                                { partition: 0, offset: '1' },\n   *                                { partition: 1, offset: '2' },\n   *                                { partition: 2, offset: '1' },\n   *                              ],\n   *                            },\n   *                          ]\n   */\n  async fetchTopicsOffset(topics) {\n    const partitionsPerBroker = {}\n    const topicConfigurations = {}\n\n    const addDefaultOffset = topic => partition => {\n      const { timestamp } = topicConfigurations[topic]\n      return { ...partition, timestamp }\n    }\n\n    // Index all topics and partitions per leader (nodeId)\n    for (const topicData of topics) {\n      const { topic, partitions, fromBeginning, fromTimestamp } = topicData\n      const partitionsPerLeader = this.findLeaderForPartitions(\n        topic,\n        partitions.map(p => p.partition)\n      )\n      const timestamp =\n        fromTimestamp != null ? fromTimestamp : this.defaultOffset({ fromBeginning })\n\n      topicConfigurations[topic] = { timestamp }\n\n      keys(partitionsPerLeader).forEach(nodeId => {\n        partitionsPerBroker[nodeId] = partitionsPerBroker[nodeId] || {}\n        partitionsPerBroker[nodeId][topic] = partitions.filter(p =>\n          partitionsPerLeader[nodeId].includes(p.partition)\n        )\n      })\n    }\n\n    // Create a list of requests to fetch the offset of all partitions\n    const requests = keys(partitionsPerBroker).map(async nodeId => {\n      const broker = await this.findBroker({ nodeId })\n      const partitions = partitionsPerBroker[nodeId]\n\n      const { responses: topicOffsets } = await broker.listOffsets({\n        isolationLevel: this.isolationLevel,\n        topics: keys(partitions).map(topic => ({\n          topic,\n          partitions: partitions[topic].map(addDefaultOffset(topic)),\n        })),\n      })\n\n      return topicOffsets\n    })\n\n    // Execute all requests, merge and normalize the responses\n    const responses = await Promise.all(requests)\n    const partitionsPerTopic = responses.flat().reduce(mergeTopics, {})\n\n    return keys(partitionsPerTopic).map(topic => ({\n      topic,\n      partitions: partitionsPerTopic[topic].map(({ partition, offset }) => ({\n        partition,\n        offset,\n      })),\n    }))\n  }\n\n  /**\n   * Retrieve the object mapping for committed offsets for a single consumer group\n   * @param {object} options\n   * @param {string} options.groupId\n   * @returns {Object}\n   */\n  committedOffsets({ groupId }) {\n    if (!this.committedOffsetsByGroup.has(groupId)) {\n      this.committedOffsetsByGroup.set(groupId, {})\n    }\n\n    return this.committedOffsetsByGroup.get(groupId)\n  }\n\n  /**\n   * Mark offset as committed for a single consumer group's topic-partition\n   * @param {object} options\n   * @param {string} options.groupId\n   * @param {string} options.topic\n   * @param {string|number} options.partition\n   * @param {string} options.offset\n   */\n  markOffsetAsCommitted({ groupId, topic, partition, offset }) {\n    const committedOffsets = this.committedOffsets({ groupId })\n\n    committedOffsets[topic] = committedOffsets[topic] || {}\n    committedOffsets[topic][partition] = offset\n  }\n}\n", "/* eslint-disable */\nconst Long = require('../../../utils/long')\n\n// Based on the kafka client 0.10.2 murmur2 implementation\n// https://github.com/apache/kafka/blob/0.10.2/clients/src/main/java/org/apache/kafka/common/utils/Utils.java#L364\n\nconst SEED = Long.fromValue(0x9747b28c)\n\n// 'm' and 'r' are mixing constants generated offline.\n// They're not really 'magic', they just happen to work well.\nconst M = Long.fromValue(0x5bd1e995)\nconst R = Long.fromValue(24)\n\nmodule.exports = key => {\n  const data = Buffer.isBuffer(key) ? key : Buffer.from(String(key))\n  const length = data.length\n\n  // Initialize the hash to a random value\n  let h = Long.fromValue(SEED.xor(length))\n  let length4 = Math.floor(length / 4)\n\n  for (let i = 0; i < length4; i++) {\n    const i4 = i * 4\n    let k =\n      (data[i4 + 0] & 0xff) +\n      ((data[i4 + 1] & 0xff) << 8) +\n      ((data[i4 + 2] & 0xff) << 16) +\n      ((data[i4 + 3] & 0xff) << 24)\n    k = Long.fromValue(k)\n    k = k.multiply(M)\n    k = k.xor(k.toInt() >>> R)\n    k = Long.fromValue(k).multiply(M)\n    h = h.multiply(M)\n    h = h.xor(k)\n  }\n\n  // Handle the last few bytes of the input array\n  switch (length % 4) {\n    case 3:\n      h = h.xor((data[(length & ~3) + 2] & 0xff) << 16)\n    case 2:\n      h = h.xor((data[(length & ~3) + 1] & 0xff) << 8)\n    case 1:\n      h = h.xor(data[length & ~3] & 0xff)\n      h = h.multiply(M)\n  }\n\n  h = h.xor(h.toInt() >>> 13)\n  h = h.multiply(M)\n  h = h.xor(h.toInt() >>> 15)\n\n  return h.toInt()\n}\n", "const { KafkaJSNonRetriableError } = require('../../../errors')\n\nconst toNodeCompatible = crypto => ({\n  randomBytes: size => crypto.getRandomValues(Buffer.allocUnsafe(size)),\n})\n\nlet cryptoImplementation = null\nif (global && global.crypto) {\n  cryptoImplementation =\n    global.crypto.randomBytes === undefined ? toNodeCompatible(global.crypto) : global.crypto\n} else if (global && global.msCrypto) {\n  cryptoImplementation = toNodeCompatible(global.msCrypto)\n} else if (global && !global.crypto) {\n  cryptoImplementation = require('crypto')\n}\n\nconst MAX_BYTES = 65536\n\nmodule.exports = size => {\n  if (size > MAX_BYTES) {\n    throw new KafkaJSNonRetriableError(\n      `Byte length (${size}) exceeds the max number of bytes of entropy available (${MAX_BYTES})`\n    )\n  }\n\n  if (!cryptoImplementation) {\n    throw new KafkaJSNonRetriableError('No available crypto implementation')\n  }\n\n  return cryptoImplementation.randomBytes(size)\n}\n", "const randomBytes = require('./randomBytes')\n\n// Based on the java client 0.10.2\n// https://github.com/apache/kafka/blob/0.10.2/clients/src/main/java/org/apache/kafka/clients/producer/internals/DefaultPartitioner.java\n\n/**\n * A cheap way to deterministically convert a number to a positive value. When the input is\n * positive, the original value is returned. When the input number is negative, the returned\n * positive value is the original value bit AND against 0x7fffffff which is not its absolutely\n * value.\n */\nconst toPositive = x => x & 0x7fffffff\n\n/**\n * The default partitioning strategy:\n *  - If a partition is specified in the message, use it\n *  - If no partition is specified but a key is present choose a partition based on a hash of the key\n *  - If no partition or key is present choose a partition in a round-robin fashion\n */\nmodule.exports = murmur2 => () => {\n  const counters = {}\n\n  return ({ topic, partitionMetadata, message }) => {\n    if (!(topic in counters)) {\n      counters[topic] = randomBytes(32).readUInt32BE(0)\n    }\n    const numPartitions = partitionMetadata.length\n    const availablePartitions = partitionMetadata.filter(p => p.leader >= 0)\n    const numAvailablePartitions = availablePartitions.length\n\n    if (message.partition !== null && message.partition !== undefined) {\n      return message.partition\n    }\n\n    if (message.key !== null && message.key !== undefined) {\n      return toPositive(murmur2(message.key)) % numPartitions\n    }\n\n    if (numAvailablePartitions > 0) {\n      const i = toPositive(++counters[topic]) % numAvailablePartitions\n      return availablePartitions[i].partitionId\n    }\n\n    // no partitions are available, give a non-available partition\n    return toPositive(++counters[topic]) % numPartitions\n  }\n}\n", "const murmur2 = require('./murmur2')\nconst createDefaultPartitioner = require('../legacy/partitioner')\n\nmodule.exports = createDefaultPartitioner(murmur2)\n", "/* eslint-disable */\n\n// Based on the kafka client 0.10.2 murmur2 implementation\n// https://github.com/apache/kafka/blob/0.10.2/clients/src/main/java/org/apache/kafka/common/utils/Utils.java#L364\n\nconst SEED = 0x9747b28c\n\n// 'm' and 'r' are mixing constants generated offline.\n// They're not really 'magic', they just happen to work well.\nconst M = 0x5bd1e995\nconst R = 24\n\nmodule.exports = key => {\n  const data = Buffer.isBuffer(key) ? key : Buffer.from(String(key))\n  const length = data.length\n\n  // Initialize the hash to a random value\n  let h = SEED ^ length\n  let length4 = length / 4\n\n  for (let i = 0; i < length4; i++) {\n    const i4 = i * 4\n    let k =\n      (data[i4 + 0] & 0xff) +\n      ((data[i4 + 1] & 0xff) << 8) +\n      ((data[i4 + 2] & 0xff) << 16) +\n      ((data[i4 + 3] & 0xff) << 24)\n    k *= M\n    k ^= k >>> R\n    k *= M\n    h *= M\n    h ^= k\n  }\n\n  // Handle the last few bytes of the input array\n  switch (length % 4) {\n    case 3:\n      h ^= (data[(length & ~3) + 2] & 0xff) << 16\n    case 2:\n      h ^= (data[(length & ~3) + 1] & 0xff) << 8\n    case 1:\n      h ^= data[length & ~3] & 0xff\n      h *= M\n  }\n\n  h ^= h >>> 13\n  h *= M\n  h ^= h >>> 15\n\n  return h\n}\n", "const murmur2 = require('./murmur2')\nconst createLegacyPartitioner = require('./partitioner')\n\nmodule.exports = createLegacyPartitioner(murmur2)\n", "const DefaultPartitioner = require('./default')\nconst LegacyPartitioner = require('./legacy')\n\nmodule.exports = {\n  DefaultPartitioner,\n  LegacyPartitioner,\n  /**\n   * @deprecated Use DefaultPartitioner instead\n   *\n   * The JavaCompatiblePartitioner was renamed DefaultPartitioner\n   * and made to be the default in 2.0.0.\n   */\n  JavaCompatiblePartitioner: DefaultPartitioner,\n}\n", "module.exports = {\n  UNINITIALIZED: 'UNINITIALIZED',\n  READY: 'READY',\n  TRANSACTING: 'TRANSACTING',\n  COMMITTING: 'COMMITTING',\n  ABORTING: 'ABORTING',\n}\n", "const { EventEmitter } = require('events')\nconst { KafkaJSNonRetriableError } = require('../../errors')\nconst STATES = require('./transactionStates')\n\nconst VALID_STATE_TRANSITIONS = {\n  [STATES.UNINITIALIZED]: [STATES.READY],\n  [STATES.READY]: [STATES.READY, STATES.TRANSACTING],\n  [STATES.TRANSACTING]: [STATES.COMMITTING, STATES.ABORTING],\n  [STATES.COMMITTING]: [STATES.READY],\n  [STATES.ABORTING]: [STATES.READY],\n}\n\nmodule.exports = ({ logger, initialState = STATES.UNINITIALIZED }) => {\n  let currentState = initialState\n\n  const guard = (object, method, { legalStates, async: isAsync = true }) => {\n    if (!object[method]) {\n      throw new KafkaJSNonRetriableError(`Cannot add guard on missing method \"${method}\"`)\n    }\n\n    return (...args) => {\n      const fn = object[method]\n\n      if (!legalStates.includes(currentState)) {\n        const error = new KafkaJSNonRetriableError(\n          `Transaction state exception: Cannot call \"${method}\" in state \"${currentState}\"`\n        )\n\n        if (isAsync) {\n          return Promise.reject(error)\n        } else {\n          throw error\n        }\n      }\n\n      return fn.apply(object, args)\n    }\n  }\n\n  const stateMachine = Object.assign(new EventEmitter(), {\n    /**\n     * Create a clone of \"object\" where we ensure state machine is in correct state\n     * prior to calling any of the configured methods\n     * @param {Object} object The object whose methods we will guard\n     * @param {Object} methodStateMapping Keys are method names on \"object\"\n     * @param {string[]} methodStateMapping.legalStates Legal states for this method\n     * @param {boolean=true} methodStateMapping.async Whether this method is async (throw vs reject)\n     */\n    createGuarded(object, methodStateMapping) {\n      const guardedMethods = Object.keys(methodStateMapping).reduce((guards, method) => {\n        guards[method] = guard(object, method, methodStateMapping[method])\n        return guards\n      }, {})\n\n      return { ...object, ...guardedMethods }\n    },\n    /**\n     * Transition safely to a new state\n     */\n    transitionTo(state) {\n      logger.debug(`Transaction state transition ${currentState} --> ${state}`)\n\n      if (!VALID_STATE_TRANSITIONS[currentState].includes(state)) {\n        throw new KafkaJSNonRetriableError(\n          `Transaction state exception: Invalid transition ${currentState} --> ${state}`\n        )\n      }\n\n      stateMachine.emit('transition', { to: state, from: currentState })\n      currentState = state\n    },\n\n    state() {\n      return currentState\n    },\n  })\n\n  return stateMachine\n}\n", "const createRetry = require('../../retry')\nconst Lock = require('../../utils/lock')\nconst { KafkaJSNonRetriableError } = require('../../errors')\nconst COORDINATOR_TYPES = require('../../protocol/coordinatorTypes')\nconst createStateMachine = require('./transactionStateMachine')\nconst { INT_32_MAX_VALUE } = require('../../constants')\nconst assert = require('assert')\n\nconst STATES = require('./transactionStates')\nconst NO_PRODUCER_ID = -1\nconst SEQUENCE_START = 0\nconst INIT_PRODUCER_RETRIABLE_PROTOCOL_ERRORS = [\n  'NOT_COORDINATOR_FOR_GROUP',\n  'GROUP_COORDINATOR_NOT_AVAILABLE',\n  'GROUP_LOAD_IN_PROGRESS',\n  /**\n   * The producer might have crashed and never committed the transaction; retry the\n   * request so Kafka can abort the current transaction\n   * @see https://github.com/apache/kafka/blob/201da0542726472d954080d54bc585b111aaf86f/clients/src/main/java/org/apache/kafka/clients/producer/internals/TransactionManager.java#L1001-L1002\n   */\n  'CONCURRENT_TRANSACTIONS',\n]\nconst COMMIT_RETRIABLE_PROTOCOL_ERRORS = [\n  'UNKNOWN_TOPIC_OR_PARTITION',\n  'COORDINATOR_LOAD_IN_PROGRESS',\n]\nconst COMMIT_STALE_COORDINATOR_PROTOCOL_ERRORS = ['COORDINATOR_NOT_AVAILABLE', 'NOT_COORDINATOR']\n\n/**\n * @typedef {Object} EosManager\n */\n\n/**\n * Manage behavior for an idempotent producer and transactions.\n *\n * @returns {EosManager}\n */\nmodule.exports = ({\n  logger,\n  cluster,\n  transactionTimeout = 60000,\n  transactional,\n  transactionalId,\n}) => {\n  if (transactional && !transactionalId) {\n    throw new KafkaJSNonRetriableError('Cannot manage transactions without a transactionalId')\n  }\n\n  const retrier = createRetry(cluster.retry)\n\n  /**\n   * Current producer ID\n   */\n  let producerId = NO_PRODUCER_ID\n\n  /**\n   * Current producer epoch\n   */\n  let producerEpoch = 0\n\n  /**\n   * Idempotent production requires that the producer track the sequence number of messages.\n   *\n   * Sequences are sent with every Record Batch and tracked per Topic-Partition\n   */\n  let producerSequence = {}\n\n  /**\n   * Idempotent production requires a mutex lock per broker to serialize requests with sequence number handling\n   */\n  let brokerMutexLocks = {}\n\n  /**\n   * Topic partitions already participating in the transaction\n   */\n  let transactionTopicPartitions = {}\n\n  /**\n   * Offsets have been added to the transaction\n   */\n  let hasOffsetsAddedToTransaction = false\n\n  const stateMachine = createStateMachine({ logger })\n  stateMachine.on('transition', ({ to }) => {\n    if (to === STATES.READY) {\n      transactionTopicPartitions = {}\n      hasOffsetsAddedToTransaction = false\n    }\n  })\n\n  const findTransactionCoordinator = () => {\n    return cluster.findGroupCoordinator({\n      groupId: transactionalId,\n      coordinatorType: COORDINATOR_TYPES.TRANSACTION,\n    })\n  }\n\n  const transactionalGuard = () => {\n    if (!transactional) {\n      throw new KafkaJSNonRetriableError('Method unavailable if non-transactional')\n    }\n  }\n\n  /**\n   * A transaction is ongoing when offsets or partitions added to it\n   *\n   * @returns {boolean}\n   */\n  const isOngoing = () => {\n    return (\n      hasOffsetsAddedToTransaction ||\n      Object.entries(transactionTopicPartitions).some(([, partitions]) => {\n        return Object.entries(partitions).some(\n          ([, isPartitionAddedToTransaction]) => isPartitionAddedToTransaction\n        )\n      })\n    )\n  }\n\n  const eosManager = stateMachine.createGuarded(\n    {\n      /**\n       * Get the current producer id\n       * @returns {number}\n       */\n      getProducerId() {\n        return producerId\n      },\n\n      /**\n       * Get the current producer epoch\n       * @returns {number}\n       */\n      getProducerEpoch() {\n        return producerEpoch\n      },\n\n      getTransactionalId() {\n        return transactionalId\n      },\n\n      /**\n       * Initialize the idempotent producer by making an `InitProducerId` request.\n       * Overwrites any existing state in this transaction manager\n       */\n      async initProducerId() {\n        return retrier(async (bail, retryCount, retryTime) => {\n          try {\n            await cluster.refreshMetadataIfNecessary()\n\n            // If non-transactional we can request the PID from any broker\n            const broker = await (transactional\n              ? findTransactionCoordinator()\n              : cluster.findControllerBroker())\n\n            const result = await broker.initProducerId({\n              transactionalId: transactional ? transactionalId : undefined,\n              transactionTimeout,\n            })\n\n            stateMachine.transitionTo(STATES.READY)\n            producerId = result.producerId\n            producerEpoch = result.producerEpoch\n            producerSequence = {}\n            brokerMutexLocks = {}\n\n            logger.debug('Initialized producer id & epoch', { producerId, producerEpoch })\n          } catch (e) {\n            if (INIT_PRODUCER_RETRIABLE_PROTOCOL_ERRORS.includes(e.type)) {\n              if (e.type === 'CONCURRENT_TRANSACTIONS') {\n                logger.debug('There is an ongoing transaction on this transactionId, retrying', {\n                  error: e.message,\n                  stack: e.stack,\n                  transactionalId,\n                  retryCount,\n                  retryTime,\n                })\n              }\n\n              throw e\n            }\n\n            bail(e)\n          }\n        })\n      },\n\n      /**\n       * Get the current sequence for a given Topic-Partition. Defaults to 0.\n       *\n       * @param {string} topic\n       * @param {string} partition\n       * @returns {number}\n       */\n      getSequence(topic, partition) {\n        if (!eosManager.isInitialized()) {\n          return SEQUENCE_START\n        }\n\n        producerSequence[topic] = producerSequence[topic] || {}\n        producerSequence[topic][partition] = producerSequence[topic][partition] || SEQUENCE_START\n\n        return producerSequence[topic][partition]\n      },\n\n      /**\n       * Update the sequence for a given Topic-Partition.\n       *\n       * Do nothing if not yet initialized (not idempotent)\n       * @param {string} topic\n       * @param {string} partition\n       * @param {number} increment\n       */\n      updateSequence(topic, partition, increment) {\n        if (!eosManager.isInitialized()) {\n          return\n        }\n\n        const previous = eosManager.getSequence(topic, partition)\n        let sequence = previous + increment\n\n        // Sequence is defined as Int32 in the Record Batch,\n        // so theoretically should need to rotate here\n        if (sequence >= INT_32_MAX_VALUE) {\n          logger.debug(\n            `Sequence for ${topic} ${partition} exceeds max value (${sequence}). Rotating to 0.`\n          )\n          sequence = 0\n        }\n\n        producerSequence[topic][partition] = sequence\n      },\n\n      /**\n       * Begin a transaction\n       */\n      beginTransaction() {\n        transactionalGuard()\n        stateMachine.transitionTo(STATES.TRANSACTING)\n      },\n\n      /**\n       * Add partitions to a transaction if they are not already marked as participating.\n       *\n       * Should be called prior to sending any messages during a transaction\n       * @param {TopicData[]} topicData\n       *\n       * @typedef {Object} TopicData\n       * @property {string} topic\n       * @property {object[]} partitions\n       * @property {number} partitions[].partition\n       */\n      async addPartitionsToTransaction(topicData) {\n        transactionalGuard()\n        const newTopicPartitions = {}\n\n        topicData.forEach(({ topic, partitions }) => {\n          transactionTopicPartitions[topic] = transactionTopicPartitions[topic] || {}\n\n          partitions.forEach(({ partition }) => {\n            if (!transactionTopicPartitions[topic][partition]) {\n              newTopicPartitions[topic] = newTopicPartitions[topic] || []\n              newTopicPartitions[topic].push(partition)\n            }\n          })\n        })\n\n        const topics = Object.keys(newTopicPartitions).map(topic => ({\n          topic,\n          partitions: newTopicPartitions[topic],\n        }))\n\n        if (topics.length) {\n          const broker = await findTransactionCoordinator()\n          await broker.addPartitionsToTxn({ transactionalId, producerId, producerEpoch, topics })\n        }\n\n        topics.forEach(({ topic, partitions }) => {\n          partitions.forEach(partition => {\n            transactionTopicPartitions[topic][partition] = true\n          })\n        })\n      },\n\n      /**\n       * Commit the ongoing transaction\n       */\n      async commit() {\n        transactionalGuard()\n        stateMachine.transitionTo(STATES.COMMITTING)\n\n        if (!isOngoing()) {\n          logger.debug('No partitions or offsets registered, not sending EndTxn')\n\n          stateMachine.transitionTo(STATES.READY)\n          return\n        }\n\n        const broker = await findTransactionCoordinator()\n        await broker.endTxn({\n          producerId,\n          producerEpoch,\n          transactionalId,\n          transactionResult: true,\n        })\n\n        stateMachine.transitionTo(STATES.READY)\n      },\n\n      /**\n       * Abort the ongoing transaction\n       */\n      async abort() {\n        transactionalGuard()\n        stateMachine.transitionTo(STATES.ABORTING)\n\n        if (!isOngoing()) {\n          logger.debug('No partitions or offsets registered, not sending EndTxn')\n\n          stateMachine.transitionTo(STATES.READY)\n          return\n        }\n\n        const broker = await findTransactionCoordinator()\n        await broker.endTxn({\n          producerId,\n          producerEpoch,\n          transactionalId,\n          transactionResult: false,\n        })\n\n        stateMachine.transitionTo(STATES.READY)\n      },\n\n      /**\n       * Whether the producer id has already been initialized\n       */\n      isInitialized() {\n        return producerId !== NO_PRODUCER_ID\n      },\n\n      isTransactional() {\n        return transactional\n      },\n\n      isInTransaction() {\n        return stateMachine.state() === STATES.TRANSACTING\n      },\n\n      async acquireBrokerLock(broker) {\n        if (this.isInitialized()) {\n          brokerMutexLocks[broker.nodeId] =\n            brokerMutexLocks[broker.nodeId] || new Lock({ timeout: 0xffff })\n          await brokerMutexLocks[broker.nodeId].acquire()\n        }\n      },\n\n      releaseBrokerLock(broker) {\n        if (this.isInitialized()) brokerMutexLocks[broker.nodeId].release()\n      },\n\n      /**\n       * Mark the provided offsets as participating in the transaction for the given consumer group.\n       *\n       * This allows us to commit an offset as consumed only if the transaction passes.\n       * @param {string} consumerGroupId The unique group identifier\n       * @param {OffsetCommitTopic[]} topics The unique group identifier\n       * @returns {Promise}\n       *\n       * @typedef {Object} OffsetCommitTopic\n       * @property {string} topic\n       * @property {OffsetCommitTopicPartition[]} partitions\n       *\n       * @typedef {Object} OffsetCommitTopicPartition\n       * @property {number} partition\n       * @property {number} offset\n       */\n      async sendOffsets({ consumerGroupId, topics }) {\n        assert(consumerGroupId, 'Missing consumerGroupId')\n        assert(topics, 'Missing offset topics')\n\n        const transactionCoordinator = await findTransactionCoordinator()\n\n        // Do we need to add offsets if we've already done so for this consumer group?\n        await transactionCoordinator.addOffsetsToTxn({\n          transactionalId,\n          producerId,\n          producerEpoch,\n          groupId: consumerGroupId,\n        })\n\n        hasOffsetsAddedToTransaction = true\n\n        let groupCoordinator = await cluster.findGroupCoordinator({\n          groupId: consumerGroupId,\n          coordinatorType: COORDINATOR_TYPES.GROUP,\n        })\n\n        return retrier(async (bail, retryCount, retryTime) => {\n          try {\n            await groupCoordinator.txnOffsetCommit({\n              transactionalId,\n              producerId,\n              producerEpoch,\n              groupId: consumerGroupId,\n              topics,\n            })\n          } catch (e) {\n            if (COMMIT_RETRIABLE_PROTOCOL_ERRORS.includes(e.type)) {\n              logger.debug('Group coordinator is not ready yet, retrying', {\n                error: e.message,\n                stack: e.stack,\n                transactionalId,\n                retryCount,\n                retryTime,\n              })\n\n              throw e\n            }\n\n            if (\n              COMMIT_STALE_COORDINATOR_PROTOCOL_ERRORS.includes(e.type) ||\n              e.code === 'ECONNREFUSED'\n            ) {\n              logger.debug(\n                'Invalid group coordinator, finding new group coordinator and retrying',\n                {\n                  error: e.message,\n                  stack: e.stack,\n                  transactionalId,\n                  retryCount,\n                  retryTime,\n                }\n              )\n\n              groupCoordinator = await cluster.findGroupCoordinator({\n                groupId: consumerGroupId,\n                coordinatorType: COORDINATOR_TYPES.GROUP,\n              })\n\n              throw e\n            }\n\n            bail(e)\n          }\n        })\n      },\n    },\n\n    /**\n     * Transaction state guards\n     */\n    {\n      initProducerId: { legalStates: [STATES.UNINITIALIZED, STATES.READY] },\n      beginTransaction: { legalStates: [STATES.READY], async: false },\n      addPartitionsToTransaction: { legalStates: [STATES.TRANSACTING] },\n      sendOffsets: { legalStates: [STATES.TRANSACTING] },\n      commit: { legalStates: [STATES.TRANSACTING] },\n      abort: { legalStates: [STATES.TRANSACTING] },\n    }\n  )\n\n  return eosManager\n}\n", "module.exports = ({ topic, partitionMetadata, messages, partitioner }) => {\n  if (partitionMetadata.length === 0) {\n    return {}\n  }\n\n  return messages.reduce((result, message) => {\n    const partition = partitioner({ topic, partitionMetadata, message })\n    const current = result[partition] || []\n    return Object.assign(result, { [partition]: [...current, message] })\n  }, {})\n}\n", "module.exports = topicDataForBroker => {\n  return topicDataForBroker.map(\n    ({ topic, partitions, messagesPerPartition, sequencePerPartition }) => ({\n      topic,\n      partitions: partitions.map(partition => ({\n        partition,\n        messages: messagesPerPartition[partition],\n      })),\n    })\n  )\n}\n", "module.exports = ({ topics }) =>\n  topics.flatMap(({ topicName, partitions }) =>\n    partitions.map(partition => ({ topicName, ...partition }))\n  )\n", "const { KafkaJSMetadataNotLoaded } = require('../errors')\nconst { staleMetadata } = require('../protocol/error')\nconst groupMessagesPerPartition = require('./groupMessagesPerPartition')\nconst createTopicData = require('./createTopicData')\nconst responseSerializer = require('./responseSerializer')\n\nconst { keys } = Object\n\n/**\n * @param {Object} options\n * @param {import(\"../../types\").Logger} options.logger\n * @param {import(\"../../types\").Cluster} options.cluster\n * @param {ReturnType<import(\"../../types\").ICustomPartitioner>} options.partitioner\n * @param {import(\"./eosManager\").EosManager} options.eosManager\n * @param {import(\"../retry\").Retrier} options.retrier\n */\nmodule.exports = ({ logger, cluster, partitioner, eosManager, retrier }) => {\n  return async ({ acks, timeout, compression, topicMessages }) => {\n    /** @type {Map<import(\"../../types\").Broker, any[]>} */\n    const responsePerBroker = new Map()\n\n    /** @param {Map<import(\"../../types\").Broker, any[]>} responsePerBroker */\n    const createProducerRequests = async responsePerBroker => {\n      const topicMetadata = new Map()\n\n      await cluster.refreshMetadataIfNecessary()\n\n      for (const { topic, messages } of topicMessages) {\n        const partitionMetadata = cluster.findTopicPartitionMetadata(topic)\n\n        if (partitionMetadata.length === 0) {\n          logger.debug('Producing to topic without metadata', {\n            topic,\n            targetTopics: Array.from(cluster.targetTopics),\n          })\n\n          throw new KafkaJSMetadataNotLoaded('Producing to topic without metadata')\n        }\n\n        const messagesPerPartition = groupMessagesPerPartition({\n          topic,\n          partitionMetadata,\n          messages,\n          partitioner,\n        })\n\n        const partitions = keys(messagesPerPartition)\n        const partitionsPerLeader = cluster.findLeaderForPartitions(topic, partitions)\n        const leaders = keys(partitionsPerLeader)\n\n        topicMetadata.set(topic, {\n          partitionsPerLeader,\n          messagesPerPartition,\n        })\n\n        for (const nodeId of leaders) {\n          const broker = await cluster.findBroker({ nodeId })\n          if (!responsePerBroker.has(broker)) {\n            responsePerBroker.set(broker, null)\n          }\n        }\n      }\n\n      const brokers = Array.from(responsePerBroker.keys())\n      const brokersWithoutResponse = brokers.filter(broker => !responsePerBroker.get(broker))\n\n      return brokersWithoutResponse.map(async broker => {\n        const entries = Array.from(topicMetadata.entries())\n        const topicDataForBroker = entries\n          .filter(([_, { partitionsPerLeader }]) => !!partitionsPerLeader[broker.nodeId])\n          .map(([topic, { partitionsPerLeader, messagesPerPartition, sequencePerPartition }]) => ({\n            topic,\n            partitions: partitionsPerLeader[broker.nodeId],\n            messagesPerPartition,\n          }))\n\n        const topicData = createTopicData(topicDataForBroker)\n\n        await eosManager.acquireBrokerLock(broker)\n        try {\n          if (eosManager.isTransactional()) {\n            await eosManager.addPartitionsToTransaction(topicData)\n          }\n\n          topicData.forEach(({ topic, partitions }) => {\n            partitions.forEach(entry => {\n              entry['firstSequence'] = eosManager.getSequence(topic, entry.partition)\n              eosManager.updateSequence(topic, entry.partition, entry.messages.length)\n            })\n          })\n\n          let response\n          try {\n            response = await broker.produce({\n              transactionalId: eosManager.isTransactional()\n                ? eosManager.getTransactionalId()\n                : undefined,\n              producerId: eosManager.getProducerId(),\n              producerEpoch: eosManager.getProducerEpoch(),\n              acks,\n              timeout,\n              compression,\n              topicData,\n            })\n          } catch (e) {\n            topicData.forEach(({ topic, partitions }) => {\n              partitions.forEach(entry => {\n                eosManager.updateSequence(topic, entry.partition, -entry.messages.length)\n              })\n            })\n            throw e\n          }\n\n          const expectResponse = acks !== 0\n          const formattedResponse = expectResponse ? responseSerializer(response) : []\n\n          responsePerBroker.set(broker, formattedResponse)\n        } catch (e) {\n          responsePerBroker.delete(broker)\n          throw e\n        } finally {\n          await eosManager.releaseBrokerLock(broker)\n        }\n      })\n    }\n\n    return retrier(async (bail, retryCount, retryTime) => {\n      const topics = topicMessages.map(({ topic }) => topic)\n      await cluster.addMultipleTargetTopics(topics)\n\n      try {\n        const requests = await createProducerRequests(responsePerBroker)\n        await Promise.all(requests)\n        return Array.from(responsePerBroker.values()).flat()\n      } catch (e) {\n        if (e.name === 'KafkaJSConnectionClosedError') {\n          cluster.removeBroker({ host: e.host, port: e.port })\n        }\n\n        if (!cluster.isConnected()) {\n          logger.debug(`Cluster has disconnected, reconnecting: ${e.message}`, {\n            retryCount,\n            retryTime,\n          })\n          await cluster.connect()\n          await cluster.refreshMetadata()\n          throw e\n        }\n\n        // This is necessary in case the metadata is stale and the number of partitions\n        // for this topic has increased in the meantime\n        if (\n          staleMetadata(e) ||\n          e.name === 'KafkaJSMetadataNotLoaded' ||\n          e.name === 'KafkaJSConnectionError' ||\n          e.name === 'KafkaJSConnectionClosedError' ||\n          (e.name === 'KafkaJSProtocolError' && e.retriable)\n        ) {\n          logger.error(`Failed to send messages: ${e.message}`, { retryCount, retryTime })\n          await cluster.refreshMetadata()\n          throw e\n        }\n\n        logger.error(`${e.message}`, { retryCount, retryTime })\n        if (e.retriable) throw e\n        bail(e)\n      }\n    })\n  }\n}\n", "const createSendMessages = require('./sendMessages')\nconst { KafkaJSError, KafkaJSNonRetriableError } = require('../errors')\nconst { CONNECTION_STATUS } = require('../network/connectionStatus')\n\nmodule.exports = ({\n  logger,\n  cluster,\n  partitioner,\n  eosManager,\n  idempotent,\n  retrier,\n  getConnectionStatus,\n}) => {\n  const sendMessages = createSendMessages({\n    logger,\n    cluster,\n    retrier,\n    partitioner,\n    eosManager,\n  })\n\n  const validateConnectionStatus = () => {\n    const connectionStatus = getConnectionStatus()\n\n    switch (connectionStatus) {\n      case CONNECTION_STATUS.DISCONNECTING:\n        throw new KafkaJSNonRetriableError(\n          `The producer is disconnecting; therefore, it can't safely accept messages anymore`\n        )\n      case CONNECTION_STATUS.DISCONNECTED:\n        throw new KafkaJSError('The producer is disconnected')\n    }\n  }\n\n  /**\n   * @typedef {Object} TopicMessages\n   * @property {string} topic\n   * @property {Array} messages An array of objects with \"key\" and \"value\", example:\n   *                         [{ key: 'my-key', value: 'my-value'}]\n   *\n   * @typedef {Object} SendBatchRequest\n   * @property {Array<TopicMessages>} topicMessages\n   * @property {number} [acks=-1] Control the number of required acks.\n   *                           -1 = all replicas must acknowledge\n   *                            0 = no acknowledgments\n   *                            1 = only waits for the leader to acknowledge\n   *\n   * @property {number} [timeout=30000] The time to await a response in ms\n   * @property {Compression.Types} [compression=Compression.Types.None] Compression codec\n   *\n   * @param {SendBatchRequest}\n   * @returns {Promise}\n   */\n  const sendBatch = async ({ acks = -1, timeout, compression, topicMessages = [] }) => {\n    if (topicMessages.some(({ topic }) => !topic)) {\n      throw new KafkaJSNonRetriableError(`Invalid topic`)\n    }\n\n    if (idempotent && acks !== -1) {\n      throw new KafkaJSNonRetriableError(\n        `Not requiring ack for all messages invalidates the idempotent producer's EoS guarantees`\n      )\n    }\n\n    for (const { topic, messages } of topicMessages) {\n      if (!messages) {\n        throw new KafkaJSNonRetriableError(\n          `Invalid messages array [${messages}] for topic \"${topic}\"`\n        )\n      }\n\n      const messageWithoutValue = messages.find(message => message.value === undefined)\n      if (messageWithoutValue) {\n        throw new KafkaJSNonRetriableError(\n          `Invalid message without value for topic \"${topic}\": ${JSON.stringify(\n            messageWithoutValue\n          )}`\n        )\n      }\n    }\n\n    validateConnectionStatus()\n    const mergedTopicMessages = topicMessages.reduce((merged, { topic, messages }) => {\n      const index = merged.findIndex(({ topic: mergedTopic }) => topic === mergedTopic)\n\n      if (index === -1) {\n        merged.push({ topic, messages })\n      } else {\n        merged[index].messages = [...merged[index].messages, ...messages]\n      }\n\n      return merged\n    }, [])\n\n    return await sendMessages({\n      acks,\n      timeout,\n      compression,\n      topicMessages: mergedTopicMessages,\n    })\n  }\n\n  /**\n   * @param {ProduceRequest} ProduceRequest\n   * @returns {Promise}\n   *\n   * @typedef {Object} ProduceRequest\n   * @property {string} topic\n   * @property {Array} messages An array of objects with \"key\" and \"value\", example:\n   *                         [{ key: 'my-key', value: 'my-value'}]\n   * @property {number} [acks=-1] Control the number of required acks.\n   *                           -1 = all replicas must acknowledge\n   *                            0 = no acknowledgments\n   *                            1 = only waits for the leader to acknowledge\n   * @property {number} [timeout=30000] The time to await a response in ms\n   * @property {Compression.Types} [compression=Compression.Types.None] Compression codec\n   */\n  const send = async ({ acks, timeout, compression, topic, messages }) => {\n    const topicMessage = { topic, messages }\n    return sendBatch({\n      acks,\n      timeout,\n      compression,\n      topicMessages: [topicMessage],\n    })\n  }\n\n  return {\n    send,\n    sendBatch,\n  }\n}\n", "const { keys } = Object\nmodule.exports = object =>\n  keys(object).reduce((result, key) => ({ ...result, [object[key]]: key }), {})\n", "const swapObject = require('../utils/swapObject')\nconst networkEvents = require('../network/instrumentationEvents')\nconst InstrumentationEventType = require('../instrumentation/eventType')\nconst producerType = InstrumentationEventType('producer')\n\nconst events = {\n  CONNECT: producerType('connect'),\n  DISCONNECT: producerType('disconnect'),\n  REQUEST: producerType(networkEvents.NETWORK_REQUEST),\n  REQUEST_TIMEOUT: producerType(networkEvents.NETWORK_REQUEST_TIMEOUT),\n  REQUEST_QUEUE_SIZE: producerType(networkEvents.NETWORK_REQUEST_QUEUE_SIZE),\n}\n\nconst wrappedEvents = {\n  [events.REQUEST]: networkEvents.NETWORK_REQUEST,\n  [events.REQUEST_TIMEOUT]: networkEvents.NETWORK_REQUEST_TIMEOUT,\n  [events.REQUEST_QUEUE_SIZE]: networkEvents.NETWORK_REQUEST_QUEUE_SIZE,\n}\n\nconst reversedWrappedEvents = swapObject(wrappedEvents)\nconst unwrap = eventName => wrappedEvents[eventName] || eventName\nconst wrap = eventName => reversedWrappedEvents[eventName] || eventName\n\nmodule.exports = {\n  events,\n  wrap,\n  unwrap,\n}\n", "const createRetry = require('../retry')\nconst { CONNECTION_STATUS } = require('../network/connectionStatus')\nconst { DefaultPartitioner } = require('./partitioners/')\nconst InstrumentationEventEmitter = require('../instrumentation/emitter')\nconst createEosManager = require('./eosManager')\nconst createMessageProducer = require('./messageProducer')\nconst { events, wrap: wrapEvent, unwrap: unwrapEvent } = require('./instrumentationEvents')\nconst { KafkaJSNonRetriableError } = require('../errors')\n\nconst { values, keys } = Object\nconst eventNames = values(events)\nconst eventKeys = keys(events)\n  .map(key => `producer.events.${key}`)\n  .join(', ')\n\nconst { CONNECT, DISCONNECT } = events\n\n/**\n *\n * @param {Object} params\n * @param {import('../../types').Cluster} params.cluster\n * @param {import('../../types').Logger} params.logger\n * @param {import('../../types').ICustomPartitioner} [params.createPartitioner]\n * @param {import('../../types').RetryOptions} [params.retry]\n * @param {boolean} [params.idempotent]\n * @param {string} [params.transactionalId]\n * @param {number} [params.transactionTimeout]\n * @param {InstrumentationEventEmitter} [params.instrumentationEmitter]\n *\n * @returns {import('../../types').Producer}\n */\nmodule.exports = ({\n  cluster,\n  logger: rootLogger,\n  createPartitioner = DefaultPartitioner,\n  retry,\n  idempotent = false,\n  transactionalId,\n  transactionTimeout,\n  instrumentationEmitter: rootInstrumentationEmitter,\n}) => {\n  let connectionStatus = CONNECTION_STATUS.DISCONNECTED\n  retry = retry || { retries: idempotent ? Number.MAX_SAFE_INTEGER : 5 }\n\n  if (idempotent && retry.retries < 1) {\n    throw new KafkaJSNonRetriableError(\n      'Idempotent producer must allow retries to protect against transient errors'\n    )\n  }\n\n  const logger = rootLogger.namespace('Producer')\n\n  if (idempotent && retry.retries < Number.MAX_SAFE_INTEGER) {\n    logger.warn('Limiting retries for the idempotent producer may invalidate EoS guarantees')\n  }\n\n  const partitioner = createPartitioner()\n  const retrier = createRetry(Object.assign({}, cluster.retry, retry))\n  const instrumentationEmitter = rootInstrumentationEmitter || new InstrumentationEventEmitter()\n  const idempotentEosManager = createEosManager({\n    logger,\n    cluster,\n    transactionTimeout,\n    transactional: false,\n    transactionalId,\n  })\n\n  const { send, sendBatch } = createMessageProducer({\n    logger,\n    cluster,\n    partitioner,\n    eosManager: idempotentEosManager,\n    idempotent,\n    retrier,\n    getConnectionStatus: () => connectionStatus,\n  })\n\n  let transactionalEosManager\n\n  /** @type {import(\"../../types\").Producer[\"on\"]} */\n  const on = (eventName, listener) => {\n    if (!eventNames.includes(eventName)) {\n      throw new KafkaJSNonRetriableError(`Event name should be one of ${eventKeys}`)\n    }\n\n    return instrumentationEmitter.addListener(unwrapEvent(eventName), event => {\n      event.type = wrapEvent(event.type)\n      Promise.resolve(listener(event)).catch(e => {\n        logger.error(`Failed to execute listener: ${e.message}`, {\n          eventName,\n          stack: e.stack,\n        })\n      })\n    })\n  }\n\n  /**\n   * Begin a transaction. The returned object contains methods to send messages\n   * to the transaction and end the transaction by committing or aborting.\n   *\n   * Only messages sent on the transaction object will participate in the transaction.\n   *\n   * Calling any of the transactional methods after the transaction has ended\n   * will raise an exception (use `isActive` to ascertain if ended).\n   * @returns {Promise<Transaction>}\n   *\n   * @typedef {Object} Transaction\n   * @property {Function} send  Identical to the producer \"send\" method\n   * @property {Function} sendBatch Identical to the producer \"sendBatch\" method\n   * @property {Function} abort Abort the transaction\n   * @property {Function} commit  Commit the transaction\n   * @property {Function} isActive  Whether the transaction is active\n   */\n  const transaction = async () => {\n    if (!transactionalId) {\n      throw new KafkaJSNonRetriableError('Must provide transactional id for transactional producer')\n    }\n\n    let transactionDidEnd = false\n    transactionalEosManager =\n      transactionalEosManager ||\n      createEosManager({\n        logger,\n        cluster,\n        transactionTimeout,\n        transactional: true,\n        transactionalId,\n      })\n\n    if (transactionalEosManager.isInTransaction()) {\n      throw new KafkaJSNonRetriableError(\n        'There is already an ongoing transaction for this producer. Please end the transaction before beginning another.'\n      )\n    }\n\n    // We only initialize the producer id once\n    if (!transactionalEosManager.isInitialized()) {\n      await transactionalEosManager.initProducerId()\n    }\n    transactionalEosManager.beginTransaction()\n\n    const { send: sendTxn, sendBatch: sendBatchTxn } = createMessageProducer({\n      logger,\n      cluster,\n      partitioner,\n      retrier,\n      eosManager: transactionalEosManager,\n      idempotent: true,\n      getConnectionStatus: () => connectionStatus,\n    })\n\n    const isActive = () => transactionalEosManager.isInTransaction() && !transactionDidEnd\n\n    const transactionGuard = fn => (...args) => {\n      if (!isActive()) {\n        return Promise.reject(\n          new KafkaJSNonRetriableError('Cannot continue to use transaction once ended')\n        )\n      }\n\n      return fn(...args)\n    }\n\n    return {\n      sendBatch: transactionGuard(sendBatchTxn),\n      send: transactionGuard(sendTxn),\n      /**\n       * Abort the ongoing transaction.\n       *\n       * @throws {KafkaJSNonRetriableError} If transaction has ended\n       */\n      abort: transactionGuard(async () => {\n        await transactionalEosManager.abort()\n        transactionDidEnd = true\n      }),\n      /**\n       * Commit the ongoing transaction.\n       *\n       * @throws {KafkaJSNonRetriableError} If transaction has ended\n       */\n      commit: transactionGuard(async () => {\n        await transactionalEosManager.commit()\n        transactionDidEnd = true\n      }),\n      /**\n       * Sends a list of specified offsets to the consumer group coordinator, and also marks those offsets as part of the current transaction.\n       *\n       * @throws {KafkaJSNonRetriableError} If transaction has ended\n       */\n      sendOffsets: transactionGuard(async ({ consumerGroupId, topics }) => {\n        await transactionalEosManager.sendOffsets({ consumerGroupId, topics })\n\n        for (const topicOffsets of topics) {\n          const { topic, partitions } = topicOffsets\n          for (const { partition, offset } of partitions) {\n            cluster.markOffsetAsCommitted({\n              groupId: consumerGroupId,\n              topic,\n              partition,\n              offset,\n            })\n          }\n        }\n      }),\n      isActive,\n    }\n  }\n\n  /**\n   * @returns {Object} logger\n   */\n  const getLogger = () => logger\n\n  return {\n    /**\n     * @returns {Promise}\n     */\n    connect: async () => {\n      await cluster.connect()\n      connectionStatus = CONNECTION_STATUS.CONNECTED\n      instrumentationEmitter.emit(CONNECT)\n\n      if (idempotent && !idempotentEosManager.isInitialized()) {\n        await idempotentEosManager.initProducerId()\n      }\n    },\n    /**\n     * @return {Promise}\n     */\n    disconnect: async () => {\n      connectionStatus = CONNECTION_STATUS.DISCONNECTING\n      await cluster.disconnect()\n      connectionStatus = CONNECTION_STATUS.DISCONNECTED\n      instrumentationEmitter.emit(DISCONNECT)\n    },\n    isIdempotent: () => {\n      return idempotent\n    },\n    events,\n    on,\n    send,\n    sendBatch,\n    transaction,\n    logger: getLogger,\n  }\n}\n", "module.exports = timeInMs =>\n  new Promise(resolve => {\n    setTimeout(resolve, timeInMs)\n  })\n", "const Long = require('../../utils/long')\n\nmodule.exports = offset => (!offset && offset !== 0) || Long.fromValue(offset).isNegative()\n", "const isInvalidOffset = require('./isInvalidOffset')\nconst { keys, assign } = Object\n\nconst indexPartitions = (obj, { partition, offset }) => assign(obj, { [partition]: offset })\nconst indexTopics = (obj, { topic, partitions }) =>\n  assign(obj, { [topic]: partitions.reduce(indexPartitions, {}) })\n\nmodule.exports = (consumerOffsets, topicOffsets) => {\n  const indexedConsumerOffsets = consumerOffsets.reduce(indexTopics, {})\n  const indexedTopicOffsets = topicOffsets.reduce(indexTopics, {})\n\n  return keys(indexedConsumerOffsets).map(topic => {\n    const partitions = indexedConsumerOffsets[topic]\n    return {\n      topic,\n      partitions: keys(partitions).map(partition => {\n        const offset = partitions[partition]\n        const resolvedOffset = isInvalidOffset(offset)\n          ? indexedTopicOffsets[topic][partition]\n          : offset\n\n        return { partition: Number(partition), offset: resolvedOffset }\n      }),\n    }\n  })\n}\n", "const swapObject = require('../utils/swapObject')\nconst InstrumentationEventType = require('../instrumentation/eventType')\nconst networkEvents = require('../network/instrumentationEvents')\nconst consumerType = InstrumentationEventType('consumer')\n\n/** @type {import('types').ConsumerEvents} */\nconst events = {\n  HEARTBEAT: consumerType('heartbeat'),\n  COMMIT_OFFSETS: consumerType('commit_offsets'),\n  GROUP_JOIN: consumerType('group_join'),\n  FETCH: consumerType('fetch'),\n  FETCH_START: consumerType('fetch_start'),\n  START_BATCH_PROCESS: consumerType('start_batch_process'),\n  END_BATCH_PROCESS: consumerType('end_batch_process'),\n  CONNECT: consumerType('connect'),\n  DISCONNECT: consumerType('disconnect'),\n  STOP: consumerType('stop'),\n  CRASH: consumerType('crash'),\n  REBALANCING: consumerType('rebalancing'),\n  RECEIVED_UNSUBSCRIBED_TOPICS: consumerType('received_unsubscribed_topics'),\n  REQUEST: consumerType(networkEvents.NETWORK_REQUEST),\n  REQUEST_TIMEOUT: consumerType(networkEvents.NETWORK_REQUEST_TIMEOUT),\n  REQUEST_QUEUE_SIZE: consumerType(networkEvents.NETWORK_REQUEST_QUEUE_SIZE),\n}\n\nconst wrappedEvents = {\n  [events.REQUEST]: networkEvents.NETWORK_REQUEST,\n  [events.REQUEST_TIMEOUT]: networkEvents.NETWORK_REQUEST_TIMEOUT,\n  [events.REQUEST_QUEUE_SIZE]: networkEvents.NETWORK_REQUEST_QUEUE_SIZE,\n}\n\nconst reversedWrappedEvents = swapObject(wrappedEvents)\nconst unwrap = eventName => wrappedEvents[eventName] || eventName\nconst wrap = eventName => reversedWrappedEvents[eventName] || eventName\n\nmodule.exports = {\n  events,\n  wrap,\n  unwrap,\n}\n", "const Long = require('../../utils/long')\nconst isInvalidOffset = require('./isInvalidOffset')\nconst initializeConsumerOffsets = require('./initializeConsumerOffsets')\nconst {\n  events: { COMMIT_OFFSETS },\n} = require('../instrumentationEvents')\n\nconst { keys, assign } = Object\nconst indexTopics = topics => topics.reduce((obj, topic) => assign(obj, { [topic]: {} }), {})\n\nconst PRIVATE = {\n  COMMITTED_OFFSETS: Symbol('private:OffsetManager:committedOffsets'),\n}\nmodule.exports = class OffsetManager {\n  /**\n   * @param {Object} options\n   * @param {import(\"../../../types\").Cluster} options.cluster\n   * @param {import(\"../../../types\").Broker} options.coordinator\n   * @param {import(\"../../../types\").IMemberAssignment} options.memberAssignment\n   * @param {boolean} options.autoCommit\n   * @param {number | null} options.autoCommitInterval\n   * @param {number | null} options.autoCommitThreshold\n   * @param {{[topic: string]: { fromBeginning: boolean }}} options.topicConfigurations\n   * @param {import(\"../../instrumentation/emitter\")} options.instrumentationEmitter\n   * @param {string} options.groupId\n   * @param {number} options.generationId\n   * @param {string} options.memberId\n   */\n  constructor({\n    cluster,\n    coordinator,\n    memberAssignment,\n    autoCommit,\n    autoCommitInterval,\n    autoCommitThreshold,\n    topicConfigurations,\n    instrumentationEmitter,\n    groupId,\n    generationId,\n    memberId,\n  }) {\n    this.cluster = cluster\n    this.coordinator = coordinator\n\n    // memberAssignment format:\n    // {\n    //   'topic1': [0, 1, 2, 3],\n    //   'topic2': [0, 1, 2, 3, 4, 5],\n    // }\n    this.memberAssignment = memberAssignment\n\n    this.topicConfigurations = topicConfigurations\n    this.instrumentationEmitter = instrumentationEmitter\n    this.groupId = groupId\n    this.generationId = generationId\n    this.memberId = memberId\n\n    this.autoCommit = autoCommit\n    this.autoCommitInterval = autoCommitInterval\n    this.autoCommitThreshold = autoCommitThreshold\n    this.lastCommit = Date.now()\n\n    this.topics = keys(memberAssignment)\n    this.clearAllOffsets()\n  }\n\n  /**\n   * @param {string} topic\n   * @param {number} partition\n   * @returns {Long}\n   */\n  nextOffset(topic, partition) {\n    if (!this.resolvedOffsets[topic][partition]) {\n      this.resolvedOffsets[topic][partition] = this.committedOffsets()[topic][partition]\n    }\n\n    let offset = this.resolvedOffsets[topic][partition]\n    if (isInvalidOffset(offset)) {\n      offset = '0'\n    }\n\n    return Long.fromValue(offset)\n  }\n\n  /**\n   * @returns {Promise<import(\"../../../types\").Broker>}\n   */\n  async getCoordinator() {\n    if (!this.coordinator.isConnected()) {\n      this.coordinator = await this.cluster.findBroker(this.coordinator)\n    }\n\n    return this.coordinator\n  }\n\n  /**\n   * @param {import(\"../../../types\").TopicPartition} topicPartition\n   */\n  resetOffset({ topic, partition }) {\n    this.resolvedOffsets[topic][partition] = this.committedOffsets()[topic][partition]\n  }\n\n  /**\n   * @param {import(\"../../../types\").TopicPartitionOffset} topicPartitionOffset\n   */\n  resolveOffset({ topic, partition, offset }) {\n    this.resolvedOffsets[topic][partition] = Long.fromValue(offset)\n      .add(1)\n      .toString()\n  }\n\n  /**\n   * @returns {Long}\n   */\n  countResolvedOffsets() {\n    const committedOffsets = this.committedOffsets()\n\n    const subtractOffsets = (resolvedOffset, committedOffset) => {\n      const resolvedOffsetLong = Long.fromValue(resolvedOffset)\n      return isInvalidOffset(committedOffset)\n        ? resolvedOffsetLong\n        : resolvedOffsetLong.subtract(Long.fromValue(committedOffset))\n    }\n\n    const subtractPartitionOffsets = (resolvedTopicOffsets, committedTopicOffsets) =>\n      keys(resolvedTopicOffsets).map(partition =>\n        subtractOffsets(resolvedTopicOffsets[partition], committedTopicOffsets[partition])\n      )\n\n    const subtractTopicOffsets = topic =>\n      subtractPartitionOffsets(this.resolvedOffsets[topic], committedOffsets[topic])\n\n    const offsetsDiff = this.topics.flatMap(subtractTopicOffsets)\n    return offsetsDiff.reduce((sum, offset) => sum.add(offset), Long.fromValue(0))\n  }\n\n  /**\n   * @param {import(\"../../../types\").TopicPartition} topicPartition\n   */\n  async setDefaultOffset({ topic, partition }) {\n    const { groupId, generationId, memberId } = this\n    const defaultOffset = this.cluster.defaultOffset(this.topicConfigurations[topic])\n    const coordinator = await this.getCoordinator()\n\n    await coordinator.offsetCommit({\n      groupId,\n      memberId,\n      groupGenerationId: generationId,\n      topics: [\n        {\n          topic,\n          partitions: [{ partition, offset: defaultOffset }],\n        },\n      ],\n    })\n\n    this.clearOffsets({ topic, partition })\n  }\n\n  /**\n   * Commit the given offset to the topic/partition. If the consumer isn't assigned to the given\n   * topic/partition this method will be a NO-OP.\n   *\n   * @param {import(\"../../../types\").TopicPartitionOffset} topicPartitionOffset\n   */\n  async seek({ topic, partition, offset }) {\n    if (!this.memberAssignment[topic] || !this.memberAssignment[topic].includes(partition)) {\n      return\n    }\n\n    if (!this.autoCommit) {\n      this.resolveOffset({\n        topic,\n        partition,\n        offset: Long.fromValue(offset)\n          .subtract(1)\n          .toString(),\n      })\n      return\n    }\n\n    const { groupId, generationId, memberId } = this\n    const coordinator = await this.getCoordinator()\n\n    await coordinator.offsetCommit({\n      groupId,\n      memberId,\n      groupGenerationId: generationId,\n      topics: [\n        {\n          topic,\n          partitions: [{ partition, offset }],\n        },\n      ],\n    })\n\n    this.clearOffsets({ topic, partition })\n  }\n\n  async commitOffsetsIfNecessary() {\n    const now = Date.now()\n\n    const timeoutReached =\n      this.autoCommitInterval != null && now >= this.lastCommit + this.autoCommitInterval\n\n    const thresholdReached =\n      this.autoCommitThreshold != null &&\n      this.countResolvedOffsets().gte(Long.fromValue(this.autoCommitThreshold))\n\n    if (timeoutReached || thresholdReached) {\n      return this.commitOffsets()\n    }\n  }\n\n  /**\n   * Return all locally resolved offsets which are not marked as committed, by topic-partition.\n   * @returns {import('../../../types').OffsetsByTopicPartition}\n   */\n  uncommittedOffsets() {\n    const offsets = topic => keys(this.resolvedOffsets[topic])\n    const emptyPartitions = ({ partitions }) => partitions.length > 0\n    const toPartitions = topic => partition => ({\n      partition,\n      offset: this.resolvedOffsets[topic][partition],\n    })\n    const changedOffsets = topic => ({ partition, offset }) => {\n      return (\n        offset !== this.committedOffsets()[topic][partition] &&\n        Long.fromValue(offset).greaterThanOrEqual(0)\n      )\n    }\n\n    // Select and format updated partitions\n    const topicsWithPartitionsToCommit = this.topics\n      .map(topic => ({\n        topic,\n        partitions: offsets(topic)\n          .map(toPartitions(topic))\n          .filter(changedOffsets(topic)),\n      }))\n      .filter(emptyPartitions)\n\n    return { topics: topicsWithPartitionsToCommit }\n  }\n\n  async commitOffsets(offsets = {}) {\n    const { groupId, generationId, memberId } = this\n    const { topics = this.uncommittedOffsets().topics } = offsets\n\n    if (topics.length === 0) {\n      this.lastCommit = Date.now()\n      return\n    }\n\n    const payload = {\n      groupId,\n      memberId,\n      groupGenerationId: generationId,\n      topics,\n    }\n\n    try {\n      const coordinator = await this.getCoordinator()\n      await coordinator.offsetCommit(payload)\n      this.instrumentationEmitter.emit(COMMIT_OFFSETS, payload)\n\n      // Update local reference of committed offsets\n      topics.forEach(({ topic, partitions }) => {\n        const updatedOffsets = partitions.reduce(\n          (obj, { partition, offset }) => assign(obj, { [partition]: offset }),\n          {}\n        )\n\n        this[PRIVATE.COMMITTED_OFFSETS][topic] = assign(\n          {},\n          this.committedOffsets()[topic],\n          updatedOffsets\n        )\n      })\n\n      this.lastCommit = Date.now()\n    } catch (e) {\n      // metadata is stale, the coordinator has changed due to a restart or\n      // broker reassignment\n      if (e.type === 'NOT_COORDINATOR_FOR_GROUP') {\n        await this.cluster.refreshMetadata()\n      }\n\n      throw e\n    }\n  }\n\n  async resolveOffsets() {\n    const { groupId } = this\n    const invalidOffset = topic => partition => {\n      return isInvalidOffset(this.committedOffsets()[topic][partition])\n    }\n\n    const pendingPartitions = this.topics\n      .map(topic => ({\n        topic,\n        partitions: this.memberAssignment[topic]\n          .filter(invalidOffset(topic))\n          .map(partition => ({ partition })),\n      }))\n      .filter(t => t.partitions.length > 0)\n\n    if (pendingPartitions.length === 0) {\n      return\n    }\n\n    const coordinator = await this.getCoordinator()\n    const { responses: consumerOffsets } = await coordinator.offsetFetch({\n      groupId,\n      topics: pendingPartitions,\n    })\n\n    const unresolvedPartitions = consumerOffsets.map(({ topic, partitions }) =>\n      assign(\n        {\n          topic,\n          partitions: partitions\n            .filter(({ offset }) => isInvalidOffset(offset))\n            .map(({ partition }) => assign({ partition })),\n        },\n        this.topicConfigurations[topic]\n      )\n    )\n\n    const indexPartitions = (obj, { partition, offset }) => {\n      return assign(obj, { [partition]: offset })\n    }\n\n    const hasUnresolvedPartitions = () => unresolvedPartitions.some(t => t.partitions.length > 0)\n\n    let offsets = consumerOffsets\n    if (hasUnresolvedPartitions()) {\n      const topicOffsets = await this.cluster.fetchTopicsOffset(unresolvedPartitions)\n      offsets = initializeConsumerOffsets(consumerOffsets, topicOffsets)\n    }\n\n    offsets.forEach(({ topic, partitions }) => {\n      this.committedOffsets()[topic] = partitions.reduce(indexPartitions, {\n        ...this.committedOffsets()[topic],\n      })\n    })\n  }\n\n  /**\n   * @private\n   * @param {import(\"../../../types\").TopicPartition} topicPartition\n   */\n  clearOffsets({ topic, partition }) {\n    delete this.committedOffsets()[topic][partition]\n    delete this.resolvedOffsets[topic][partition]\n  }\n\n  /**\n   * @private\n   */\n  clearAllOffsets() {\n    const committedOffsets = this.committedOffsets()\n\n    for (const topic in committedOffsets) {\n      delete committedOffsets[topic]\n    }\n\n    for (const topic of this.topics) {\n      committedOffsets[topic] = {}\n    }\n\n    this.resolvedOffsets = indexTopics(this.topics)\n  }\n\n  committedOffsets() {\n    if (!this[PRIVATE.COMMITTED_OFFSETS]) {\n      this[PRIVATE.COMMITTED_OFFSETS] = this.groupId\n        ? this.cluster.committedOffsets({ groupId: this.groupId })\n        : {}\n    }\n\n    return this[PRIVATE.COMMITTED_OFFSETS]\n  }\n}\n", "const Long = require('../utils/long')\nconst ABORTED_MESSAGE_KEY = Buffer.from([0, 0, 0, 0])\n\nconst isAbortMarker = ({ key }) => {\n  // Handle null/undefined keys.\n  if (!key) return false\n  // Cast key to buffer defensively\n  return Buffer.from(key).equals(ABORTED_MESSAGE_KEY)\n}\n\n/**\n * Remove messages marked as aborted according to the aborted transactions list.\n *\n * Start of an aborted transaction is determined by message offset.\n * End of an aborted transaction is determined by control messages.\n * @param {Message[]} messages\n * @param {Transaction[]} [abortedTransactions]\n * @returns {Message[]} Messages which did not participate in an aborted transaction\n *\n * @typedef {object} Message\n * @param {Buffer} key\n * @param {lastOffset} key  Int64\n * @param {RecordBatch}  batchContext\n *\n * @typedef {object} Transaction\n * @param {string} firstOffset  Int64\n * @param {string} producerId  Int64\n *\n * @typedef {object} RecordBatch\n * @param {string}  producerId  Int64\n * @param {boolean}  inTransaction\n */\nmodule.exports = ({ messages, abortedTransactions }) => {\n  const currentAbortedTransactions = new Map()\n\n  if (!abortedTransactions || !abortedTransactions.length) {\n    return messages\n  }\n\n  const remainingAbortedTransactions = [...abortedTransactions]\n\n  return messages.filter(message => {\n    // If the message offset is GTE the first offset of the next aborted transaction\n    // then we have stepped into an aborted transaction.\n    if (\n      remainingAbortedTransactions.length &&\n      Long.fromValue(message.offset).gte(remainingAbortedTransactions[0].firstOffset)\n    ) {\n      const { producerId } = remainingAbortedTransactions.shift()\n      currentAbortedTransactions.set(producerId, true)\n    }\n\n    const { producerId, inTransaction } = message.batchContext\n\n    if (isAbortMarker(message)) {\n      // Transaction is over, we no longer need to ignore messages from this producer\n      currentAbortedTransactions.delete(producerId)\n    } else if (currentAbortedTransactions.has(producerId) && inTransaction) {\n      return false\n    }\n\n    return true\n  })\n}\n", "const Long = require('../utils/long')\nconst filterAbortedMessages = require('./filterAbortedMessages')\n\n/**\n * A batch collects messages returned from a single fetch call.\n *\n * A batch could contain _multiple_ Kafka RecordBatches.\n */\nmodule.exports = class Batch {\n  constructor(topic, fetchedOffset, partitionData) {\n    this.fetchedOffset = fetchedOffset\n    const longFetchedOffset = Long.fromValue(this.fetchedOffset)\n    const { abortedTransactions, messages } = partitionData\n\n    this.topic = topic\n    this.partition = partitionData.partition\n    this.highWatermark = partitionData.highWatermark\n\n    this.rawMessages = messages\n    // Apparently fetch can return different offsets than the target offset provided to the fetch API.\n    // Discard messages that are not in the requested offset\n    // https://github.com/apache/kafka/blob/bf237fa7c576bd141d78fdea9f17f65ea269c290/clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java#L912\n    this.messagesWithinOffset = this.rawMessages.filter(message =>\n      Long.fromValue(message.offset).gte(longFetchedOffset)\n    )\n\n    // 1. Don't expose aborted messages\n    // 2. Don't expose control records\n    // @see https://kafka.apache.org/documentation/#controlbatch\n    this.messages = filterAbortedMessages({\n      messages: this.messagesWithinOffset,\n      abortedTransactions,\n    }).filter(message => !message.isControlRecord)\n  }\n\n  isEmpty() {\n    return this.messages.length === 0\n  }\n\n  isEmptyIncludingFiltered() {\n    return this.messagesWithinOffset.length === 0\n  }\n\n  /**\n   * If the batch contained raw messages (i.e was not truly empty) but all messages were filtered out due to\n   * log compaction, control records or other reasons\n   */\n  isEmptyDueToFiltering() {\n    return this.isEmpty() && this.rawMessages.length > 0\n  }\n\n  isEmptyControlRecord() {\n    return (\n      this.isEmpty() && this.messagesWithinOffset.some(({ isControlRecord }) => isControlRecord)\n    )\n  }\n\n  /**\n   * With compressed messages, it's possible for the returned messages to have offsets smaller than the starting offset.\n   * These messages will be filtered out (i.e. they are not even included in this.messagesWithinOffset)\n   * If these are the only messages, the batch will appear as an empty batch.\n   *\n   * isEmpty() and isEmptyIncludingFiltered() will always return true if the batch is empty,\n   * but this method will only return true if the batch is empty due to log compacted messages.\n   *\n   * @returns boolean True if the batch is empty, because of log compacted messages in the partition.\n   */\n  isEmptyDueToLogCompactedMessages() {\n    const hasMessages = this.rawMessages.length > 0\n    return hasMessages && this.isEmptyIncludingFiltered()\n  }\n\n  firstOffset() {\n    return this.isEmptyIncludingFiltered() ? null : this.messagesWithinOffset[0].offset\n  }\n\n  lastOffset() {\n    if (this.isEmptyDueToLogCompactedMessages()) {\n      return this.fetchedOffset\n    }\n\n    if (this.isEmptyIncludingFiltered()) {\n      return Long.fromValue(this.highWatermark)\n        .add(-1)\n        .toString()\n    }\n\n    return this.messagesWithinOffset[this.messagesWithinOffset.length - 1].offset\n  }\n\n  /**\n   * Returns the lag based on the last offset in the batch (also known as \"high\")\n   */\n  offsetLag() {\n    const lastOffsetOfPartition = Long.fromValue(this.highWatermark).add(-1)\n    const lastConsumedOffset = Long.fromValue(this.lastOffset())\n    return lastOffsetOfPartition.add(lastConsumedOffset.multiply(-1)).toString()\n  }\n\n  /**\n   * Returns the lag based on the first offset in the batch\n   */\n  offsetLagLow() {\n    if (this.isEmptyIncludingFiltered()) {\n      return '0'\n    }\n\n    const lastOffsetOfPartition = Long.fromValue(this.highWatermark).add(-1)\n    const firstConsumedOffset = Long.fromValue(this.firstOffset())\n    return lastOffsetOfPartition.add(firstConsumedOffset.multiply(-1)).toString()\n  }\n}\n", "module.exports = class SeekOffsets extends Map {\n  getKey(topic, partition) {\n    return JSON.stringify([topic, partition])\n  }\n\n  set(topic, partition, offset) {\n    const key = this.getKey(topic, partition)\n    super.set(key, offset)\n  }\n\n  has(topic, partition) {\n    const key = this.getKey(topic, partition)\n    return super.has(key)\n  }\n\n  pop(topic, partition) {\n    if (this.size === 0 || !this.has(topic, partition)) {\n      return\n    }\n\n    const key = this.getKey(topic, partition)\n    const offset = this.get(key)\n\n    this.delete(key)\n    return { topic, partition, offset }\n  }\n}\n", "const createState = topic => ({\n  topic,\n  paused: new Set(),\n  pauseAll: false,\n  resumed: new Set(),\n})\n\nmodule.exports = class SubscriptionState {\n  constructor() {\n    this.assignedPartitionsByTopic = {}\n    this.subscriptionStatesByTopic = {}\n  }\n\n  /**\n   * Replace the current assignment with a new set of assignments\n   *\n   * @param {Array<TopicPartitions>} topicPartitions Example: [{ topic: 'topic-name', partitions: [1, 2] }]\n   */\n  assign(topicPartitions = []) {\n    this.assignedPartitionsByTopic = topicPartitions.reduce(\n      (assigned, { topic, partitions = [] }) => {\n        return { ...assigned, [topic]: { topic, partitions } }\n      },\n      {}\n    )\n  }\n\n  /**\n   * @param {Array<TopicPartitions>} topicPartitions Example: [{ topic: 'topic-name', partitions: [1, 2] }]\n   */\n  pause(topicPartitions = []) {\n    topicPartitions.forEach(({ topic, partitions }) => {\n      const state = this.subscriptionStatesByTopic[topic] || createState(topic)\n\n      if (typeof partitions === 'undefined') {\n        state.paused.clear()\n        state.resumed.clear()\n        state.pauseAll = true\n      } else if (Array.isArray(partitions)) {\n        partitions.forEach(partition => {\n          state.paused.add(partition)\n          state.resumed.delete(partition)\n        })\n        state.pauseAll = false\n      }\n\n      this.subscriptionStatesByTopic[topic] = state\n    })\n  }\n\n  /**\n   * @param {Array<TopicPartitions>} topicPartitions Example: [{ topic: 'topic-name', partitions: [1, 2] }]\n   */\n  resume(topicPartitions = []) {\n    topicPartitions.forEach(({ topic, partitions }) => {\n      const state = this.subscriptionStatesByTopic[topic] || createState(topic)\n\n      if (typeof partitions === 'undefined') {\n        state.paused.clear()\n        state.resumed.clear()\n        state.pauseAll = false\n      } else if (Array.isArray(partitions)) {\n        partitions.forEach(partition => {\n          state.paused.delete(partition)\n\n          if (state.pauseAll) {\n            state.resumed.add(partition)\n          }\n        })\n      }\n\n      this.subscriptionStatesByTopic[topic] = state\n    })\n  }\n\n  /**\n   * @returns {Array<import(\"../../types\").TopicPartitions>} topicPartitions\n   * Example: [{ topic: 'topic-name', partitions: [1, 2] }]\n   */\n  assigned() {\n    return Object.values(this.assignedPartitionsByTopic).map(({ topic, partitions }) => ({\n      topic,\n      partitions: partitions.sort(),\n    }))\n  }\n\n  /**\n   * @returns {Array<import(\"../../types\").TopicPartitions>} topicPartitions\n   * Example: [{ topic: 'topic-name', partitions: [1, 2] }]\n   */\n  active() {\n    return Object.values(this.assignedPartitionsByTopic).map(({ topic, partitions }) => ({\n      topic,\n      partitions: partitions.filter(partition => !this.isPaused(topic, partition)).sort(),\n    }))\n  }\n\n  /**\n   * @returns {Array<import(\"../../types\").TopicPartitions>} topicPartitions\n   * Example: [{ topic: 'topic-name', partitions: [1, 2] }]\n   */\n  paused() {\n    return Object.values(this.assignedPartitionsByTopic)\n      .map(({ topic, partitions }) => ({\n        topic,\n        partitions: partitions.filter(partition => this.isPaused(topic, partition)).sort(),\n      }))\n      .filter(({ partitions }) => partitions.length !== 0)\n  }\n\n  isPaused(topic, partition) {\n    const state = this.subscriptionStatesByTopic[topic]\n\n    if (!state) {\n      return false\n    }\n\n    const partitionResumed = state.resumed.has(partition)\n    const partitionPaused = state.paused.has(partition)\n\n    return (state.pauseAll && !partitionResumed) || partitionPaused\n  }\n}\n", "const Encoder = require('../protocol/encoder')\nconst Decoder = require('../protocol/decoder')\n\nconst MemberMetadata = {\n  /**\n   * @param {Object} metadata\n   * @param {number} metadata.version\n   * @param {Array<string>} metadata.topics\n   * @param {Buffer} [metadata.userData=Buffer.alloc(0)]\n   *\n   * @returns Buffer\n   */\n  encode({ version, topics, userData = Buffer.alloc(0) }) {\n    return new Encoder()\n      .writeInt16(version)\n      .writeArray(topics)\n      .writeBytes(userData).buffer\n  },\n\n  /**\n   * @param {Buffer} buffer\n   * @returns {Object}\n   */\n  decode(buffer) {\n    const decoder = new Decoder(buffer)\n    return {\n      version: decoder.readInt16(),\n      topics: decoder.readArray(d => d.readString()),\n      userData: decoder.readBytes(),\n    }\n  },\n}\n\nconst MemberAssignment = {\n  /**\n   * @param {object} options\n   * @param {number} options.version\n   * @param {Object<String,Array>} options.assignment, example:\n   *                               {\n   *                                 'topic-A': [0, 2, 4, 6],\n   *                                 'topic-B': [0, 2],\n   *                               }\n   * @param {Buffer} [options.userData=Buffer.alloc(0)]\n   *\n   * @returns Buffer\n   */\n  encode({ version, assignment, userData = Buffer.alloc(0) }) {\n    return new Encoder()\n      .writeInt16(version)\n      .writeArray(\n        Object.keys(assignment).map(topic =>\n          new Encoder().writeString(topic).writeArray(assignment[topic])\n        )\n      )\n      .writeBytes(userData).buffer\n  },\n\n  /**\n   * @param {Buffer} buffer\n   * @returns {Object|null}\n   */\n  decode(buffer) {\n    const decoder = new Decoder(buffer)\n    const decodePartitions = d => d.readInt32()\n    const decodeAssignment = d => ({\n      topic: d.readString(),\n      partitions: d.readArray(decodePartitions),\n    })\n    const indexAssignment = (obj, { topic, partitions }) =>\n      Object.assign(obj, { [topic]: partitions })\n\n    if (!decoder.canReadInt16()) {\n      return null\n    }\n\n    return {\n      version: decoder.readInt16(),\n      assignment: decoder.readArray(decodeAssignment).reduce(indexAssignment, {}),\n      userData: decoder.readBytes(),\n    }\n  },\n}\n\nmodule.exports = {\n  MemberMetadata,\n  MemberAssignment,\n}\n", "const sleep = require('../utils/sleep')\nconst websiteUrl = require('../utils/websiteUrl')\nconst arrayDiff = require('../utils/arrayDiff')\nconst createRetry = require('../retry')\nconst sharedPromiseTo = require('../utils/sharedPromiseTo')\n\nconst OffsetManager = require('./offsetManager')\nconst Batch = require('./batch')\nconst SeekOffsets = require('./seekOffsets')\nconst SubscriptionState = require('./subscriptionState')\nconst {\n  events: { GROUP_JOIN, HEARTBEAT, CONNECT, RECEIVED_UNSUBSCRIBED_TOPICS },\n} = require('./instrumentationEvents')\nconst { MemberAssignment } = require('./assignerProtocol')\nconst {\n  KafkaJSError,\n  KafkaJSNonRetriableError,\n  KafkaJSStaleTopicMetadataAssignment,\n  isRebalancing,\n} = require('../errors')\n\nconst { keys } = Object\n\nconst STALE_METADATA_ERRORS = [\n  'LEADER_NOT_AVAILABLE',\n  // Fetch before v9 uses NOT_LEADER_FOR_PARTITION\n  'NOT_LEADER_FOR_PARTITION',\n  // Fetch after v9 uses {FENCED,UNKNOWN}_LEADER_EPOCH\n  'FENCED_LEADER_EPOCH',\n  'UNKNOWN_LEADER_EPOCH',\n  'UNKNOWN_TOPIC_OR_PARTITION',\n]\n\nconst PRIVATE = {\n  JOIN: Symbol('private:ConsumerGroup:join'),\n  SYNC: Symbol('private:ConsumerGroup:sync'),\n  SHARED_HEARTBEAT: Symbol('private:ConsumerGroup:sharedHeartbeat'),\n}\n\nmodule.exports = class ConsumerGroup {\n  /**\n   * @param {object} options\n   * @param {import('../../types').RetryOptions} options.retry\n   * @param {import('../../types').Cluster} options.cluster\n   * @param {string} options.groupId\n   * @param {string[]} options.topics\n   * @param {Record<string, { fromBeginning?: boolean }>} options.topicConfigurations\n   * @param {import('../../types').Logger} options.logger\n   * @param {import('../instrumentation/emitter')} options.instrumentationEmitter\n   * @param {import('../../types').Assigner[]} options.assigners\n   * @param {number} options.sessionTimeout\n   * @param {number} options.rebalanceTimeout\n   * @param {number} options.maxBytesPerPartition\n   * @param {number} options.minBytes\n   * @param {number} options.maxBytes\n   * @param {number} options.maxWaitTimeInMs\n   * @param {boolean} options.autoCommit\n   * @param {number} options.autoCommitInterval\n   * @param {number} options.autoCommitThreshold\n   * @param {number} options.isolationLevel\n   * @param {string} options.rackId\n   * @param {number} options.metadataMaxAge\n   */\n  constructor({\n    retry,\n    cluster,\n    groupId,\n    topics,\n    topicConfigurations,\n    logger,\n    instrumentationEmitter,\n    assigners,\n    sessionTimeout,\n    rebalanceTimeout,\n    maxBytesPerPartition,\n    minBytes,\n    maxBytes,\n    maxWaitTimeInMs,\n    autoCommit,\n    autoCommitInterval,\n    autoCommitThreshold,\n    isolationLevel,\n    rackId,\n    metadataMaxAge,\n  }) {\n    /** @type {import(\"../../types\").Cluster} */\n    this.cluster = cluster\n    this.groupId = groupId\n    this.topics = topics\n    this.topicsSubscribed = topics\n    this.topicConfigurations = topicConfigurations\n    this.logger = logger.namespace('ConsumerGroup')\n    this.instrumentationEmitter = instrumentationEmitter\n    this.retrier = createRetry(Object.assign({}, retry))\n    this.assigners = assigners\n    this.sessionTimeout = sessionTimeout\n    this.rebalanceTimeout = rebalanceTimeout\n    this.maxBytesPerPartition = maxBytesPerPartition\n    this.minBytes = minBytes\n    this.maxBytes = maxBytes\n    this.maxWaitTime = maxWaitTimeInMs\n    this.autoCommit = autoCommit\n    this.autoCommitInterval = autoCommitInterval\n    this.autoCommitThreshold = autoCommitThreshold\n    this.isolationLevel = isolationLevel\n    this.rackId = rackId\n    this.metadataMaxAge = metadataMaxAge\n\n    this.seekOffset = new SeekOffsets()\n    this.coordinator = null\n    this.generationId = null\n    this.leaderId = null\n    this.memberId = null\n    this.members = null\n    this.groupProtocol = null\n\n    this.partitionsPerSubscribedTopic = null\n    /**\n     * Preferred read replica per topic and partition\n     *\n     * Each of the partitions tracks the preferred read replica (`nodeId`) and a timestamp\n     * until when that preference is valid.\n     *\n     * @type {{[topicName: string]: {[partition: number]: {nodeId: number, expireAt: number}}}}\n     */\n    this.preferredReadReplicasPerTopicPartition = {}\n    this.offsetManager = null\n    this.subscriptionState = new SubscriptionState()\n\n    this.lastRequest = Date.now()\n\n    this[PRIVATE.SHARED_HEARTBEAT] = sharedPromiseTo(async ({ interval }) => {\n      const { groupId, generationId, memberId } = this\n      const now = Date.now()\n\n      if (memberId && now >= this.lastRequest + interval) {\n        const payload = {\n          groupId,\n          memberId,\n          groupGenerationId: generationId,\n        }\n\n        await this.coordinator.heartbeat(payload)\n        this.instrumentationEmitter.emit(HEARTBEAT, payload)\n        this.lastRequest = Date.now()\n      }\n    })\n  }\n\n  isLeader() {\n    return this.leaderId && this.memberId === this.leaderId\n  }\n\n  getNodeIds() {\n    return this.cluster.getNodeIds()\n  }\n\n  async connect() {\n    await this.cluster.connect()\n    this.instrumentationEmitter.emit(CONNECT)\n    await this.cluster.refreshMetadataIfNecessary()\n  }\n\n  async [PRIVATE.JOIN]() {\n    const { groupId, sessionTimeout, rebalanceTimeout } = this\n\n    this.coordinator = await this.cluster.findGroupCoordinator({ groupId })\n\n    const groupData = await this.coordinator.joinGroup({\n      groupId,\n      sessionTimeout,\n      rebalanceTimeout,\n      memberId: this.memberId || '',\n      groupProtocols: this.assigners.map(assigner =>\n        assigner.protocol({\n          topics: this.topicsSubscribed,\n        })\n      ),\n    })\n\n    this.generationId = groupData.generationId\n    this.leaderId = groupData.leaderId\n    this.memberId = groupData.memberId\n    this.members = groupData.members\n    this.groupProtocol = groupData.groupProtocol\n  }\n\n  async leave() {\n    const { groupId, memberId } = this\n    if (memberId) {\n      await this.coordinator.leaveGroup({ groupId, memberId })\n      this.memberId = null\n    }\n  }\n\n  async [PRIVATE.SYNC]() {\n    let assignment = []\n    const {\n      groupId,\n      generationId,\n      memberId,\n      members,\n      groupProtocol,\n      topics,\n      topicsSubscribed,\n      coordinator,\n    } = this\n\n    if (this.isLeader()) {\n      this.logger.debug('Chosen as group leader', { groupId, generationId, memberId, topics })\n      const assigner = this.assigners.find(({ name }) => name === groupProtocol)\n\n      if (!assigner) {\n        throw new KafkaJSNonRetriableError(\n          `Unsupported partition assigner \"${groupProtocol}\", the assigner wasn't found in the assigners list`\n        )\n      }\n\n      await this.cluster.refreshMetadata()\n      assignment = await assigner.assign({ members, topics: topicsSubscribed })\n\n      this.logger.debug('Group assignment', {\n        groupId,\n        generationId,\n        groupProtocol,\n        assignment,\n        topics: topicsSubscribed,\n      })\n    }\n\n    // Keep track of the partitions for the subscribed topics\n    this.partitionsPerSubscribedTopic = this.generatePartitionsPerSubscribedTopic()\n    const { memberAssignment } = await this.coordinator.syncGroup({\n      groupId,\n      generationId,\n      memberId,\n      groupAssignment: assignment,\n    })\n\n    const decodedMemberAssignment = MemberAssignment.decode(memberAssignment)\n    const decodedAssignment =\n      decodedMemberAssignment != null ? decodedMemberAssignment.assignment : {}\n\n    this.logger.debug('Received assignment', {\n      groupId,\n      generationId,\n      memberId,\n      memberAssignment: decodedAssignment,\n    })\n\n    const assignedTopics = keys(decodedAssignment)\n    const topicsNotSubscribed = arrayDiff(assignedTopics, topicsSubscribed)\n\n    if (topicsNotSubscribed.length > 0) {\n      const payload = {\n        groupId,\n        generationId,\n        memberId,\n        assignedTopics,\n        topicsSubscribed,\n        topicsNotSubscribed,\n      }\n\n      this.instrumentationEmitter.emit(RECEIVED_UNSUBSCRIBED_TOPICS, payload)\n      this.logger.warn('Consumer group received unsubscribed topics', {\n        ...payload,\n        helpUrl: websiteUrl(\n          'docs/faq',\n          'why-am-i-receiving-messages-for-topics-i-m-not-subscribed-to'\n        ),\n      })\n    }\n\n    // Remove unsubscribed topics from the list\n    const safeAssignment = arrayDiff(assignedTopics, topicsNotSubscribed)\n    const currentMemberAssignment = safeAssignment.map(topic => ({\n      topic,\n      partitions: decodedAssignment[topic],\n    }))\n\n    // Check if the consumer is aware of all assigned partitions\n    for (const assignment of currentMemberAssignment) {\n      const { topic, partitions: assignedPartitions } = assignment\n      const knownPartitions = this.partitionsPerSubscribedTopic.get(topic)\n      const isAwareOfAllAssignedPartitions = assignedPartitions.every(partition =>\n        knownPartitions.includes(partition)\n      )\n\n      if (!isAwareOfAllAssignedPartitions) {\n        this.logger.warn('Consumer is not aware of all assigned partitions, refreshing metadata', {\n          groupId,\n          generationId,\n          memberId,\n          topic,\n          knownPartitions,\n          assignedPartitions,\n        })\n\n        // If the consumer is not aware of all assigned partitions, refresh metadata\n        // and update the list of partitions per subscribed topic. It's enough to perform\n        // this operation once since refresh metadata will update metadata for all topics\n        await this.cluster.refreshMetadata()\n        this.partitionsPerSubscribedTopic = this.generatePartitionsPerSubscribedTopic()\n        break\n      }\n    }\n\n    this.topics = currentMemberAssignment.map(({ topic }) => topic)\n    this.subscriptionState.assign(currentMemberAssignment)\n    this.offsetManager = new OffsetManager({\n      cluster: this.cluster,\n      topicConfigurations: this.topicConfigurations,\n      instrumentationEmitter: this.instrumentationEmitter,\n      memberAssignment: currentMemberAssignment.reduce(\n        (partitionsByTopic, { topic, partitions }) => ({\n          ...partitionsByTopic,\n          [topic]: partitions,\n        }),\n        {}\n      ),\n      autoCommit: this.autoCommit,\n      autoCommitInterval: this.autoCommitInterval,\n      autoCommitThreshold: this.autoCommitThreshold,\n      coordinator,\n      groupId,\n      generationId,\n      memberId,\n    })\n  }\n\n  joinAndSync() {\n    const startJoin = Date.now()\n    return this.retrier(async bail => {\n      try {\n        await this[PRIVATE.JOIN]()\n        await this[PRIVATE.SYNC]()\n\n        const memberAssignment = this.assigned().reduce(\n          (result, { topic, partitions }) => ({ ...result, [topic]: partitions }),\n          {}\n        )\n\n        const payload = {\n          groupId: this.groupId,\n          memberId: this.memberId,\n          leaderId: this.leaderId,\n          isLeader: this.isLeader(),\n          memberAssignment,\n          groupProtocol: this.groupProtocol,\n          duration: Date.now() - startJoin,\n        }\n\n        this.instrumentationEmitter.emit(GROUP_JOIN, payload)\n        this.logger.info('Consumer has joined the group', payload)\n      } catch (e) {\n        if (isRebalancing(e)) {\n          // Rebalance in progress isn't a retriable protocol error since the consumer\n          // has to go through find coordinator and join again before it can\n          // actually retry the operation. We wrap the original error in a retriable error\n          // here instead in order to restart the join + sync sequence using the retrier.\n          throw new KafkaJSError(e)\n        }\n\n        if (e.type === 'UNKNOWN_MEMBER_ID') {\n          this.memberId = null\n          throw new KafkaJSError(e)\n        }\n\n        bail(e)\n      }\n    })\n  }\n\n  /**\n   * @param {import(\"../../types\").TopicPartition} topicPartition\n   */\n  resetOffset({ topic, partition }) {\n    this.offsetManager.resetOffset({ topic, partition })\n  }\n\n  /**\n   * @param {import(\"../../types\").TopicPartitionOffset} topicPartitionOffset\n   */\n  resolveOffset({ topic, partition, offset }) {\n    this.offsetManager.resolveOffset({ topic, partition, offset })\n  }\n\n  /**\n   * Update the consumer offset for the given topic/partition. This will be used\n   * on the next fetch. If this API is invoked for the same topic/partition more\n   * than once, the latest offset will be used on the next fetch.\n   *\n   * @param {import(\"../../types\").TopicPartitionOffset} topicPartitionOffset\n   */\n  seek({ topic, partition, offset }) {\n    this.seekOffset.set(topic, partition, offset)\n  }\n\n  pause(topicPartitions) {\n    this.logger.info(`Pausing fetching from ${topicPartitions.length} topics`, {\n      topicPartitions,\n    })\n    this.subscriptionState.pause(topicPartitions)\n  }\n\n  resume(topicPartitions) {\n    this.logger.info(`Resuming fetching from ${topicPartitions.length} topics`, {\n      topicPartitions,\n    })\n    this.subscriptionState.resume(topicPartitions)\n  }\n\n  assigned() {\n    return this.subscriptionState.assigned()\n  }\n\n  paused() {\n    return this.subscriptionState.paused()\n  }\n\n  /**\n   * @param {string} topic\n   * @param {string} partition\n   * @returns {boolean} whether the specified topic-partition are paused or not\n   */\n  isPaused(topic, partition) {\n    return this.subscriptionState.isPaused(topic, partition)\n  }\n\n  async commitOffsetsIfNecessary() {\n    await this.offsetManager.commitOffsetsIfNecessary()\n  }\n\n  async commitOffsets(offsets) {\n    await this.offsetManager.commitOffsets(offsets)\n  }\n\n  uncommittedOffsets() {\n    return this.offsetManager.uncommittedOffsets()\n  }\n\n  async heartbeat({ interval }) {\n    return this[PRIVATE.SHARED_HEARTBEAT]({ interval })\n  }\n\n  async fetch(nodeId) {\n    try {\n      await this.cluster.refreshMetadataIfNecessary()\n      this.checkForStaleAssignment()\n\n      let topicPartitions = this.subscriptionState.assigned()\n      topicPartitions = this.filterPartitionsByNode(nodeId, topicPartitions)\n\n      await this.seekOffsets(topicPartitions)\n\n      const committedOffsets = this.offsetManager.committedOffsets()\n      const activeTopicPartitions = this.getActiveTopicPartitions()\n\n      const requests = topicPartitions\n        .map(({ topic, partitions }) => ({\n          topic,\n          partitions: partitions\n            .filter(\n              partition =>\n                /**\n                 * When recovering from OffsetOutOfRange, each partition can recover\n                 * concurrently, which invalidates resolved and committed offsets as part\n                 * of the recovery mechanism (see OffsetManager.clearOffsets). In concurrent\n                 * scenarios this can initiate a new fetch with invalid offsets.\n                 *\n                 * This was further highlighted by https://github.com/tulios/kafkajs/pull/570,\n                 * which increased concurrency, making this more likely to happen.\n                 *\n                 * This is solved by only making requests for partitions with initialized offsets.\n                 *\n                 * See the following pull request which explains the context of the problem:\n                 * @issue https://github.com/tulios/kafkajs/pull/578\n                 */\n                committedOffsets[topic][partition] != null &&\n                activeTopicPartitions[topic].has(partition)\n            )\n            .map(partition => ({\n              partition,\n              fetchOffset: this.offsetManager.nextOffset(topic, partition).toString(),\n              maxBytes: this.maxBytesPerPartition,\n            })),\n        }))\n        .filter(({ partitions }) => partitions.length)\n\n      if (!requests.length) {\n        await sleep(this.maxWaitTime)\n        return []\n      }\n\n      const broker = await this.cluster.findBroker({ nodeId })\n\n      const { responses } = await broker.fetch({\n        maxWaitTime: this.maxWaitTime,\n        minBytes: this.minBytes,\n        maxBytes: this.maxBytes,\n        isolationLevel: this.isolationLevel,\n        topics: requests,\n        rackId: this.rackId,\n      })\n\n      return responses.flatMap(({ topicName, partitions }) => {\n        const topicRequestData = requests.find(({ topic }) => topic === topicName)\n\n        let preferredReadReplicas = this.preferredReadReplicasPerTopicPartition[topicName]\n        if (!preferredReadReplicas) {\n          this.preferredReadReplicasPerTopicPartition[topicName] = preferredReadReplicas = {}\n        }\n\n        return partitions\n          .filter(\n            ({ partition }) =>\n              !this.seekOffset.has(topicName, partition) &&\n              !this.subscriptionState.isPaused(topicName, partition)\n          )\n          .map(partitionData => {\n            const { partition, preferredReadReplica } = partitionData\n\n            if (preferredReadReplica != null && preferredReadReplica !== -1) {\n              const { nodeId: currentPreferredReadReplica } = preferredReadReplicas[partition] || {}\n              if (currentPreferredReadReplica !== preferredReadReplica) {\n                this.logger.info(`Preferred read replica is now ${preferredReadReplica}`, {\n                  groupId: this.groupId,\n                  memberId: this.memberId,\n                  topic: topicName,\n                  partition,\n                })\n              }\n              preferredReadReplicas[partition] = {\n                nodeId: preferredReadReplica,\n                expireAt: Date.now() + this.metadataMaxAge,\n              }\n            }\n\n            const partitionRequestData = topicRequestData.partitions.find(\n              ({ partition }) => partition === partitionData.partition\n            )\n\n            const fetchedOffset = partitionRequestData.fetchOffset\n            return new Batch(topicName, fetchedOffset, partitionData)\n          })\n      })\n    } catch (e) {\n      await this.recoverFromFetch(e)\n      return []\n    }\n  }\n\n  async recoverFromFetch(e) {\n    if (STALE_METADATA_ERRORS.includes(e.type) || e.name === 'KafkaJSTopicMetadataNotLoaded') {\n      this.logger.debug('Stale cluster metadata, refreshing...', {\n        groupId: this.groupId,\n        memberId: this.memberId,\n        error: e.message,\n      })\n\n      await this.cluster.refreshMetadata()\n      await this.joinAndSync()\n      return\n    }\n\n    if (e.name === 'KafkaJSStaleTopicMetadataAssignment') {\n      this.logger.warn(`${e.message}, resync group`, {\n        groupId: this.groupId,\n        memberId: this.memberId,\n        topic: e.topic,\n        unknownPartitions: e.unknownPartitions,\n      })\n\n      await this.joinAndSync()\n      return\n    }\n\n    if (e.name === 'KafkaJSOffsetOutOfRange') {\n      await this.recoverFromOffsetOutOfRange(e)\n      return\n    }\n\n    if (e.name === 'KafkaJSConnectionClosedError') {\n      this.cluster.removeBroker({ host: e.host, port: e.port })\n      return\n    }\n\n    if (e.name === 'KafkaJSBrokerNotFound' || e.name === 'KafkaJSConnectionClosedError') {\n      this.logger.debug(`${e.message}, refreshing metadata and retrying...`)\n      await this.cluster.refreshMetadata()\n      return\n    }\n\n    throw e\n  }\n\n  async recoverFromOffsetOutOfRange(e) {\n    // If we are fetching from a follower try with the leader before resetting offsets\n    const preferredReadReplicas = this.preferredReadReplicasPerTopicPartition[e.topic]\n    if (preferredReadReplicas && typeof preferredReadReplicas[e.partition] === 'number') {\n      this.logger.info('Offset out of range while fetching from follower, retrying with leader', {\n        topic: e.topic,\n        partition: e.partition,\n        groupId: this.groupId,\n        memberId: this.memberId,\n      })\n      delete preferredReadReplicas[e.partition]\n    } else {\n      this.logger.error('Offset out of range, resetting to default offset', {\n        topic: e.topic,\n        partition: e.partition,\n        groupId: this.groupId,\n        memberId: this.memberId,\n      })\n\n      await this.offsetManager.setDefaultOffset({\n        topic: e.topic,\n        partition: e.partition,\n      })\n    }\n  }\n\n  generatePartitionsPerSubscribedTopic() {\n    const map = new Map()\n\n    for (const topic of this.topicsSubscribed) {\n      const partitions = this.cluster\n        .findTopicPartitionMetadata(topic)\n        .map(m => m.partitionId)\n        .sort()\n\n      map.set(topic, partitions)\n    }\n\n    return map\n  }\n\n  checkForStaleAssignment() {\n    if (!this.partitionsPerSubscribedTopic) {\n      return\n    }\n\n    const newPartitionsPerSubscribedTopic = this.generatePartitionsPerSubscribedTopic()\n\n    for (const [topic, partitions] of newPartitionsPerSubscribedTopic) {\n      const diff = arrayDiff(partitions, this.partitionsPerSubscribedTopic.get(topic))\n\n      if (diff.length > 0) {\n        throw new KafkaJSStaleTopicMetadataAssignment('Topic has been updated', {\n          topic,\n          unknownPartitions: diff,\n        })\n      }\n    }\n  }\n\n  async seekOffsets(topicPartitions) {\n    for (const { topic, partitions } of topicPartitions) {\n      for (const partition of partitions) {\n        const seekEntry = this.seekOffset.pop(topic, partition)\n        if (!seekEntry) {\n          continue\n        }\n\n        this.logger.debug('Seek offset', {\n          groupId: this.groupId,\n          memberId: this.memberId,\n          seek: seekEntry,\n        })\n        await this.offsetManager.seek(seekEntry)\n      }\n    }\n\n    await this.offsetManager.resolveOffsets()\n  }\n\n  hasSeekOffset({ topic, partition }) {\n    return this.seekOffset.has(topic, partition)\n  }\n\n  /**\n   * For each of the partitions find the best nodeId to read it from\n   *\n   * @param {string} topic\n   * @param {number[]} partitions\n   * @returns {{[nodeId: number]: number[]}} per-node assignment of partitions\n   * @see Cluster~findLeaderForPartitions\n   */\n  // Invariant: The resulting object has each partition referenced exactly once\n  findReadReplicaForPartitions(topic, partitions) {\n    const partitionMetadata = this.cluster.findTopicPartitionMetadata(topic)\n    const preferredReadReplicas = this.preferredReadReplicasPerTopicPartition[topic]\n    return partitions.reduce((result, id) => {\n      const partitionId = parseInt(id, 10)\n      const metadata = partitionMetadata.find(p => p.partitionId === partitionId)\n      if (!metadata) {\n        return result\n      }\n\n      if (metadata.leader == null) {\n        throw new KafkaJSError('Invalid partition metadata', { topic, partitionId, metadata })\n      }\n\n      // Pick the preferred replica if there is one, and it isn't known to be offline, otherwise the leader.\n      let nodeId = metadata.leader\n      if (preferredReadReplicas) {\n        const { nodeId: preferredReadReplica, expireAt } = preferredReadReplicas[partitionId] || {}\n        if (Date.now() >= expireAt) {\n          this.logger.debug('Preferred read replica information has expired, using leader', {\n            topic,\n            partitionId,\n            groupId: this.groupId,\n            memberId: this.memberId,\n            preferredReadReplica,\n            leader: metadata.leader,\n          })\n          // Drop the entry\n          delete preferredReadReplicas[partitionId]\n        } else if (preferredReadReplica != null) {\n          // Valid entry, check whether it is not offline\n          // Note that we don't delete the preference here, and rather hope that eventually that replica comes online again\n          const offlineReplicas = metadata.offlineReplicas\n          if (Array.isArray(offlineReplicas) && offlineReplicas.includes(nodeId)) {\n            this.logger.debug('Preferred read replica is offline, using leader', {\n              topic,\n              partitionId,\n              groupId: this.groupId,\n              memberId: this.memberId,\n              preferredReadReplica,\n              leader: metadata.leader,\n            })\n          } else {\n            nodeId = preferredReadReplica\n          }\n        }\n      }\n      const current = result[nodeId] || []\n      return { ...result, [nodeId]: [...current, partitionId] }\n    }, {})\n  }\n\n  filterPartitionsByNode(nodeId, topicPartitions) {\n    return topicPartitions.map(({ topic, partitions }) => ({\n      topic,\n      partitions: this.findReadReplicaForPartitions(topic, partitions)[nodeId] || [],\n    }))\n  }\n\n  getActiveTopicPartitions() {\n    const activeSubscriptionState = this.subscriptionState.active()\n\n    const activeTopicPartitions = {}\n    activeSubscriptionState.forEach(({ topic, partitions }) => {\n      activeTopicPartitions[topic] = new Set(partitions)\n    })\n\n    return activeTopicPartitions\n  }\n}\n", "/**\n * @param {number} count\n * @param {(index: number) => T} [callback]\n * @template T\n */\nconst seq = (count, callback = x => x) =>\n  new Array(count).fill(0).map((_, index) => callback(index))\n\nmodule.exports = seq\n", "const EventEmitter = require('events')\n\n/**\n * Fetches data from all assigned nodes, waits for workerQueue to drain and repeats.\n *\n * @param {object} options\n * @param {number} options.nodeId\n * @param {import('./workerQueue').WorkerQueue} options.workerQueue\n * @param {Map<string, string[]>} options.partitionAssignments\n * @param {(nodeId: number) => Promise<T[]>} options.fetch\n * @param {import('../../types').Logger} options.logger\n * @template T\n */\nconst createFetcher = ({\n  nodeId,\n  workerQueue,\n  partitionAssignments,\n  fetch,\n  logger: rootLogger,\n}) => {\n  const logger = rootLogger.namespace(`Fetcher ${nodeId}`)\n  const emitter = new EventEmitter()\n  let isRunning = false\n\n  const getWorkerQueue = () => workerQueue\n  const assignmentKey = ({ topic, partition }) => `${topic}|${partition}`\n  const getAssignedFetcher = batch => partitionAssignments.get(assignmentKey(batch))\n  const assignTopicPartition = batch => partitionAssignments.set(assignmentKey(batch), nodeId)\n  const unassignTopicPartition = batch => partitionAssignments.delete(assignmentKey(batch))\n  const filterUnassignedBatches = batches =>\n    batches.filter(batch => {\n      const assignedFetcher = getAssignedFetcher(batch)\n      if (assignedFetcher != null && assignedFetcher !== nodeId) {\n        logger.info(\n          'Filtering out batch due to partition already being processed by another fetcher',\n          {\n            topic: batch.topic,\n            partition: batch.partition,\n            assignedFetcher: assignedFetcher,\n            fetcher: nodeId,\n          }\n        )\n        return false\n      }\n\n      return true\n    })\n\n  const start = async () => {\n    if (isRunning) return\n    isRunning = true\n\n    while (isRunning) {\n      try {\n        const batches = await fetch(nodeId)\n        if (isRunning) {\n          const availableBatches = filterUnassignedBatches(batches)\n\n          if (availableBatches.length > 0) {\n            availableBatches.forEach(assignTopicPartition)\n            try {\n              await workerQueue.push(...availableBatches)\n            } finally {\n              availableBatches.forEach(unassignTopicPartition)\n            }\n          }\n        }\n      } catch (error) {\n        isRunning = false\n        emitter.emit('end')\n        throw error\n      }\n    }\n    emitter.emit('end')\n  }\n\n  const stop = async () => {\n    if (!isRunning) return\n    isRunning = false\n    await new Promise(resolve => emitter.once('end', () => resolve()))\n  }\n\n  return { start, stop, getWorkerQueue }\n}\n\nmodule.exports = createFetcher\n", "/**\n * @typedef {(batch: T, metadata: { workerId: number }) => Promise<void>} Handler\n * @template T\n *\n * @typedef {ReturnType<typeof createWorker>} Worker\n */\n\nconst sharedPromiseTo = require('../utils/sharedPromiseTo')\n\n/**\n * @param {{ handler: Handler<T>, workerId: number }} options\n * @template T\n */\nconst createWorker = ({ handler, workerId }) => {\n  /**\n   * Takes batches from next() until it returns undefined.\n   *\n   * @param {{ next: () => { batch: T, resolve: () => void, reject: (e: Error) => void } | undefined }} param0\n   * @returns {Promise<void>}\n   */\n  const run = sharedPromiseTo(async ({ next }) => {\n    while (true) {\n      const item = next()\n      if (!item) break\n\n      const { batch, resolve, reject } = item\n\n      try {\n        await handler(batch, { workerId })\n        resolve()\n      } catch (error) {\n        reject(error)\n      }\n    }\n  })\n\n  return { run }\n}\n\nmodule.exports = createWorker\n", "/**\n * @typedef {ReturnType<typeof createWorkerQueue>} WorkerQueue\n */\n\n/**\n * @param {object} options\n * @param {import('./worker').Worker<T>[]} options.workers\n * @template T\n */\nconst createWorkerQueue = ({ workers }) => {\n  /** @type {{ batch: T, resolve: (value?: any) => void, reject: (e: Error) => void}[]} */\n  const queue = []\n\n  const getWorkers = () => workers\n\n  /**\n   * Waits until workers have processed all batches in the queue.\n   *\n   * @param {...T} batches\n   * @returns {Promise<void>}\n   */\n  const push = async (...batches) => {\n    const promises = batches.map(\n      batch => new Promise((resolve, reject) => queue.push({ batch, resolve, reject }))\n    )\n\n    workers.forEach(worker => worker.run({ next: () => queue.shift() }))\n\n    const results = await Promise.allSettled(promises)\n    const rejected = results.find(result => result.status === 'rejected')\n    if (rejected) {\n      // @ts-ignore\n      throw rejected.reason\n    }\n  }\n\n  return { push, getWorkers }\n}\n\nmodule.exports = createWorkerQueue\n", "const seq = require('../utils/seq')\nconst createFetcher = require('./fetcher')\nconst createWorker = require('./worker')\nconst createWorkerQueue = require('./workerQueue')\nconst { KafkaJSFetcherRebalanceError, KafkaJSNoBrokerAvailableError } = require('../errors')\n\n/** @typedef {ReturnType<typeof createFetchManager>} FetchManager */\n\n/**\n * @param {object} options\n * @param {import('../../types').Logger} options.logger\n * @param {() => number[]} options.getNodeIds\n * @param {(nodeId: number) => Promise<import('../../types').Batch[]>} options.fetch\n * @param {import('./worker').Handler<T>} options.handler\n * @param {number} [options.concurrency]\n * @template T\n */\nconst createFetchManager = ({\n  logger: rootLogger,\n  getNodeIds,\n  fetch,\n  handler,\n  concurrency = 1,\n}) => {\n  const logger = rootLogger.namespace('FetchManager')\n  const workers = seq(concurrency, workerId => createWorker({ handler, workerId }))\n  const workerQueue = createWorkerQueue({ workers })\n\n  let fetchers = []\n\n  const getFetchers = () => fetchers\n\n  const createFetchers = () => {\n    const nodeIds = getNodeIds()\n    const partitionAssignments = new Map()\n\n    if (nodeIds.length === 0) {\n      throw new KafkaJSNoBrokerAvailableError()\n    }\n\n    const validateShouldRebalance = () => {\n      const current = getNodeIds()\n      const hasChanged =\n        nodeIds.length !== current.length || nodeIds.some(nodeId => !current.includes(nodeId))\n      if (hasChanged && current.length !== 0) {\n        throw new KafkaJSFetcherRebalanceError()\n      }\n    }\n\n    const fetchers = nodeIds.map(nodeId =>\n      createFetcher({\n        nodeId,\n        workerQueue,\n        partitionAssignments,\n        fetch: async nodeId => {\n          validateShouldRebalance()\n          return fetch(nodeId)\n        },\n        logger,\n      })\n    )\n\n    logger.debug(`Created ${fetchers.length} fetchers`, { nodeIds, concurrency })\n    return fetchers\n  }\n\n  const start = async () => {\n    logger.debug('Starting...')\n\n    while (true) {\n      fetchers = createFetchers()\n\n      try {\n        await Promise.all(fetchers.map(fetcher => fetcher.start()))\n      } catch (error) {\n        await stop()\n\n        if (error instanceof KafkaJSFetcherRebalanceError) {\n          logger.debug('Rebalancing fetchers...')\n          continue\n        }\n\n        throw error\n      }\n\n      break\n    }\n  }\n\n  const stop = async () => {\n    logger.debug('Stopping fetchers...')\n    await Promise.all(fetchers.map(fetcher => fetcher.stop()))\n    logger.debug('Stopped fetchers')\n  }\n\n  return { start, stop, getFetchers }\n}\n\nmodule.exports = createFetchManager\n", "const { EventEmitter } = require('events')\nconst Long = require('../utils/long')\nconst createRetry = require('../retry')\nconst { isKafkaJSError, isRebalancing } = require('../errors')\n\nconst {\n  events: { FETCH, FETCH_START, START_BATCH_PROCESS, END_BATCH_PROCESS, REBALANCING },\n} = require('./instrumentationEvents')\nconst createFetchManager = require('./fetchManager')\n\nconst isSameOffset = (offsetA, offsetB) => Long.fromValue(offsetA).equals(Long.fromValue(offsetB))\nconst CONSUMING_START = 'consuming-start'\nconst CONSUMING_STOP = 'consuming-stop'\n\nmodule.exports = class Runner extends EventEmitter {\n  /**\n   * @param {object} options\n   * @param {import(\"../../types\").Logger} options.logger\n   * @param {import(\"./consumerGroup\")} options.consumerGroup\n   * @param {import(\"../instrumentation/emitter\")} options.instrumentationEmitter\n   * @param {boolean} [options.eachBatchAutoResolve=true]\n   * @param {number} options.concurrency\n   * @param {(payload: import(\"../../types\").EachBatchPayload) => Promise<void>} [options.eachBatch]\n   * @param {(payload: import(\"../../types\").EachMessagePayload) => Promise<void>} [options.eachMessage]\n   * @param {number} [options.heartbeatInterval]\n   * @param {(reason: Error) => void} options.onCrash\n   * @param {import(\"../../types\").RetryOptions} [options.retry]\n   * @param {boolean} [options.autoCommit=true]\n   */\n  constructor({\n    logger,\n    consumerGroup,\n    instrumentationEmitter,\n    eachBatchAutoResolve = true,\n    concurrency,\n    eachBatch,\n    eachMessage,\n    heartbeatInterval,\n    onCrash,\n    retry,\n    autoCommit = true,\n  }) {\n    super()\n    this.logger = logger.namespace('Runner')\n    this.consumerGroup = consumerGroup\n    this.instrumentationEmitter = instrumentationEmitter\n    this.eachBatchAutoResolve = eachBatchAutoResolve\n    this.eachBatch = eachBatch\n    this.eachMessage = eachMessage\n    this.heartbeatInterval = heartbeatInterval\n    this.retrier = createRetry(Object.assign({}, retry))\n    this.onCrash = onCrash\n    this.autoCommit = autoCommit\n    this.fetchManager = createFetchManager({\n      logger: this.logger,\n      getNodeIds: () => this.consumerGroup.getNodeIds(),\n      fetch: nodeId => this.fetch(nodeId),\n      handler: batch => this.handleBatch(batch),\n      concurrency,\n    })\n\n    this.running = false\n    this.consuming = false\n  }\n\n  get consuming() {\n    return this._consuming\n  }\n\n  set consuming(value) {\n    if (this._consuming !== value) {\n      this._consuming = value\n      this.emit(value ? CONSUMING_START : CONSUMING_STOP)\n    }\n  }\n\n  async start() {\n    if (this.running) {\n      return\n    }\n\n    try {\n      await this.consumerGroup.connect()\n      await this.consumerGroup.joinAndSync()\n    } catch (e) {\n      return this.onCrash(e)\n    }\n\n    this.running = true\n    this.scheduleFetchManager()\n  }\n\n  scheduleFetchManager() {\n    if (!this.running) {\n      this.consuming = false\n\n      this.logger.info('consumer not running, exiting', {\n        groupId: this.consumerGroup.groupId,\n        memberId: this.consumerGroup.memberId,\n      })\n\n      return\n    }\n\n    this.consuming = true\n\n    this.retrier(async (bail, retryCount, retryTime) => {\n      if (!this.running) {\n        return\n      }\n\n      try {\n        await this.fetchManager.start()\n      } catch (e) {\n        if (isRebalancing(e)) {\n          this.logger.warn('The group is rebalancing, re-joining', {\n            groupId: this.consumerGroup.groupId,\n            memberId: this.consumerGroup.memberId,\n            error: e.message,\n          })\n\n          this.instrumentationEmitter.emit(REBALANCING, {\n            groupId: this.consumerGroup.groupId,\n            memberId: this.consumerGroup.memberId,\n          })\n\n          await this.consumerGroup.joinAndSync()\n          return\n        }\n\n        if (e.type === 'UNKNOWN_MEMBER_ID') {\n          this.logger.error('The coordinator is not aware of this member, re-joining the group', {\n            groupId: this.consumerGroup.groupId,\n            memberId: this.consumerGroup.memberId,\n            error: e.message,\n          })\n\n          this.consumerGroup.memberId = null\n          await this.consumerGroup.joinAndSync()\n          return\n        }\n\n        if (e.name === 'KafkaJSNotImplemented') {\n          return bail(e)\n        }\n\n        if (e.name === 'KafkaJSNoBrokerAvailableError') {\n          return bail(e)\n        }\n\n        this.logger.debug('Error while scheduling fetch manager, trying again...', {\n          groupId: this.consumerGroup.groupId,\n          memberId: this.consumerGroup.memberId,\n          error: e.message,\n          stack: e.stack,\n          retryCount,\n          retryTime,\n        })\n\n        throw e\n      }\n    })\n      .then(() => {\n        this.scheduleFetchManager()\n      })\n      .catch(e => {\n        this.onCrash(e)\n        this.consuming = false\n        this.running = false\n      })\n  }\n\n  async stop() {\n    if (!this.running) {\n      return\n    }\n\n    this.logger.debug('stop consumer group', {\n      groupId: this.consumerGroup.groupId,\n      memberId: this.consumerGroup.memberId,\n    })\n\n    this.running = false\n\n    try {\n      await this.fetchManager.stop()\n      await this.waitForConsumer()\n      await this.consumerGroup.leave()\n    } catch (e) {}\n  }\n\n  waitForConsumer() {\n    return new Promise(resolve => {\n      if (!this.consuming) {\n        return resolve()\n      }\n\n      this.logger.debug('waiting for consumer to finish...', {\n        groupId: this.consumerGroup.groupId,\n        memberId: this.consumerGroup.memberId,\n      })\n\n      this.once(CONSUMING_STOP, () => resolve())\n    })\n  }\n\n  async heartbeat() {\n    try {\n      await this.consumerGroup.heartbeat({ interval: this.heartbeatInterval })\n    } catch (e) {\n      if (isRebalancing(e)) {\n        await this.autoCommitOffsets()\n      }\n      throw e\n    }\n  }\n\n  async processEachMessage(batch) {\n    const { topic, partition } = batch\n\n    const pause = () => {\n      this.consumerGroup.pause([{ topic, partitions: [partition] }])\n      return () => this.consumerGroup.resume([{ topic, partitions: [partition] }])\n    }\n    for (const message of batch.messages) {\n      if (!this.running || this.consumerGroup.hasSeekOffset({ topic, partition })) {\n        break\n      }\n\n      try {\n        await this.eachMessage({\n          topic,\n          partition,\n          message,\n          heartbeat: () => this.heartbeat(),\n          pause,\n        })\n      } catch (e) {\n        if (!isKafkaJSError(e)) {\n          this.logger.error(`Error when calling eachMessage`, {\n            topic,\n            partition,\n            offset: message.offset,\n            stack: e.stack,\n            error: e,\n          })\n        }\n\n        // In case of errors, commit the previously consumed offsets unless autoCommit is disabled\n        await this.autoCommitOffsets()\n        throw e\n      }\n\n      this.consumerGroup.resolveOffset({ topic, partition, offset: message.offset })\n      await this.heartbeat()\n      await this.autoCommitOffsetsIfNecessary()\n\n      if (this.consumerGroup.isPaused(topic, partition)) {\n        break\n      }\n    }\n  }\n\n  async processEachBatch(batch) {\n    const { topic, partition } = batch\n    const lastFilteredMessage = batch.messages[batch.messages.length - 1]\n\n    const pause = () => {\n      this.consumerGroup.pause([{ topic, partitions: [partition] }])\n      return () => this.consumerGroup.resume([{ topic, partitions: [partition] }])\n    }\n\n    try {\n      await this.eachBatch({\n        batch,\n        resolveOffset: offset => {\n          /**\n           * The transactional producer generates a control record after committing the transaction.\n           * The control record is the last record on the RecordBatch, and it is filtered before it\n           * reaches the eachBatch callback. When disabling auto-resolve, the user-land code won't\n           * be able to resolve the control record offset, since it never reaches the callback,\n           * causing stuck consumers as the consumer will never move the offset marker.\n           *\n           * When the last offset of the batch is resolved, we should automatically resolve\n           * the control record offset as this entry doesn't have any meaning to the user-land code,\n           * and won't interfere with the stream processing.\n           *\n           * @see https://github.com/apache/kafka/blob/9aa660786e46c1efbf5605a6a69136a1dac6edb9/clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java#L1499-L1505\n           */\n          const offsetToResolve =\n            lastFilteredMessage && isSameOffset(offset, lastFilteredMessage.offset)\n              ? batch.lastOffset()\n              : offset\n\n          this.consumerGroup.resolveOffset({ topic, partition, offset: offsetToResolve })\n        },\n        heartbeat: () => this.heartbeat(),\n        /**\n         * Pause consumption for the current topic-partition being processed\n         */\n        pause,\n        /**\n         * Commit offsets if provided. Otherwise commit most recent resolved offsets\n         * if the autoCommit conditions are met.\n         *\n         * @param {import('../../types').OffsetsByTopicPartition} [offsets] Optional.\n         */\n        commitOffsetsIfNecessary: async offsets => {\n          return offsets\n            ? this.consumerGroup.commitOffsets(offsets)\n            : this.consumerGroup.commitOffsetsIfNecessary()\n        },\n        uncommittedOffsets: () => this.consumerGroup.uncommittedOffsets(),\n        isRunning: () => this.running,\n        isStale: () => this.consumerGroup.hasSeekOffset({ topic, partition }),\n      })\n    } catch (e) {\n      if (!isKafkaJSError(e)) {\n        this.logger.error(`Error when calling eachBatch`, {\n          topic,\n          partition,\n          offset: batch.firstOffset(),\n          stack: e.stack,\n          error: e,\n        })\n      }\n\n      // eachBatch has a special resolveOffset which can be used\n      // to keep track of the messages\n      await this.autoCommitOffsets()\n      throw e\n    }\n\n    // resolveOffset for the last offset can be disabled to allow the users of eachBatch to\n    // stop their consumers without resolving unprocessed offsets (issues/18)\n    if (this.eachBatchAutoResolve) {\n      this.consumerGroup.resolveOffset({ topic, partition, offset: batch.lastOffset() })\n    }\n  }\n\n  async fetch(nodeId) {\n    if (!this.running) {\n      this.logger.debug('consumer not running, exiting', {\n        groupId: this.consumerGroup.groupId,\n        memberId: this.consumerGroup.memberId,\n      })\n\n      return []\n    }\n\n    const startFetch = Date.now()\n\n    this.instrumentationEmitter.emit(FETCH_START, { nodeId })\n\n    const batches = await this.consumerGroup.fetch(nodeId)\n\n    this.instrumentationEmitter.emit(FETCH, {\n      /**\n       * PR #570 removed support for the number of batches in this instrumentation event;\n       * The new implementation uses an async generation to deliver the batches, which makes\n       * this number impossible to get. The number is set to 0 to keep the event backward\n       * compatible until we bump KafkaJS to version 2, following the end of node 8 LTS.\n       *\n       * @since 2019-11-29\n       */\n      numberOfBatches: 0,\n      duration: Date.now() - startFetch,\n      nodeId,\n    })\n\n    if (batches.length === 0) {\n      await this.heartbeat()\n    }\n\n    return batches\n  }\n\n  async handleBatch(batch) {\n    if (!this.running) {\n      this.logger.debug('consumer not running, exiting', {\n        groupId: this.consumerGroup.groupId,\n        memberId: this.consumerGroup.memberId,\n      })\n\n      return\n    }\n\n    /** @param {import('./batch')} batch */\n    const onBatch = async batch => {\n      const startBatchProcess = Date.now()\n      const payload = {\n        topic: batch.topic,\n        partition: batch.partition,\n        highWatermark: batch.highWatermark,\n        offsetLag: batch.offsetLag(),\n        /**\n         * @since 2019-06-24 (>= 1.8.0)\n         *\n         * offsetLag returns the lag based on the latest offset in the batch, to\n         * keep the event backward compatible we just introduced \"offsetLagLow\"\n         * which calculates the lag based on the first offset in the batch\n         */\n        offsetLagLow: batch.offsetLagLow(),\n        batchSize: batch.messages.length,\n        firstOffset: batch.firstOffset(),\n        lastOffset: batch.lastOffset(),\n      }\n\n      /**\n       * If the batch contained only control records or only aborted messages then we still\n       * need to resolve and auto-commit to ensure the consumer can move forward.\n       *\n       * We also need to emit batch instrumentation events to allow any listeners keeping\n       * track of offsets to know about the latest point of consumption.\n       *\n       * Added in #1256\n       *\n       * @see https://github.com/apache/kafka/blob/9aa660786e46c1efbf5605a6a69136a1dac6edb9/clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java#L1499-L1505\n       */\n      if (batch.isEmptyDueToFiltering()) {\n        this.instrumentationEmitter.emit(START_BATCH_PROCESS, payload)\n\n        this.consumerGroup.resolveOffset({\n          topic: batch.topic,\n          partition: batch.partition,\n          offset: batch.lastOffset(),\n        })\n        await this.autoCommitOffsetsIfNecessary()\n\n        this.instrumentationEmitter.emit(END_BATCH_PROCESS, {\n          ...payload,\n          duration: Date.now() - startBatchProcess,\n        })\n\n        await this.heartbeat()\n        return\n      }\n\n      if (batch.isEmpty()) {\n        await this.heartbeat()\n        return\n      }\n\n      this.instrumentationEmitter.emit(START_BATCH_PROCESS, payload)\n\n      if (this.eachMessage) {\n        await this.processEachMessage(batch)\n      } else if (this.eachBatch) {\n        await this.processEachBatch(batch)\n      }\n\n      this.instrumentationEmitter.emit(END_BATCH_PROCESS, {\n        ...payload,\n        duration: Date.now() - startBatchProcess,\n      })\n\n      await this.autoCommitOffsets()\n      await this.heartbeat()\n    }\n\n    await onBatch(batch)\n  }\n\n  autoCommitOffsets() {\n    if (this.autoCommit) {\n      return this.consumerGroup.commitOffsets()\n    }\n  }\n\n  autoCommitOffsetsIfNecessary() {\n    if (this.autoCommit) {\n      return this.consumerGroup.commitOffsetsIfNecessary()\n    }\n  }\n\n  commitOffsets(offsets) {\n    if (!this.running) {\n      this.logger.debug('consumer not running, exiting', {\n        groupId: this.consumerGroup.groupId,\n        memberId: this.consumerGroup.memberId,\n        offsets,\n      })\n      return\n    }\n\n    return this.retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await this.consumerGroup.commitOffsets(offsets)\n      } catch (e) {\n        if (!this.running) {\n          this.logger.debug('consumer not running, exiting', {\n            error: e.message,\n            groupId: this.consumerGroup.groupId,\n            memberId: this.consumerGroup.memberId,\n            offsets,\n          })\n          return\n        }\n\n        if (e.name === 'KafkaJSNotImplemented') {\n          return bail(e)\n        }\n\n        this.logger.debug('Error while committing offsets, trying again...', {\n          groupId: this.consumerGroup.groupId,\n          memberId: this.consumerGroup.memberId,\n          error: e.message,\n          stack: e.stack,\n          retryCount,\n          retryTime,\n          offsets,\n        })\n\n        throw e\n      }\n    })\n  }\n}\n", "const { MemberMetadata, MemberAssignment } = require('../../assignerProtocol')\n\n/**\n * RoundRobinAssigner\n * @type {import('types').PartitionAssigner}\n */\nmodule.exports = ({ cluster }) => ({\n  name: 'RoundRobinAssigner',\n  version: 0,\n\n  /**\n   * Assign the topics to the provided members.\n   *\n   * The members array contains information about each member, `memberMetadata` is the result of the\n   * `protocol` operation.\n   *\n   * @param {object} group\n   * @param {import('types').GroupMember[]} group.members array of members, e.g:\n                              [{ memberId: 'test-5f93f5a3', memberMetadata: Buffer }]\n   * @param {string[]} group.topics\n   * @returns {Promise<import('types').GroupMemberAssignment[]>} object partitions per topic per member, e.g:\n   *                   [\n   *                     {\n   *                       memberId: 'test-5f93f5a3',\n   *                       memberAssignment: {\n   *                         'topic-A': [0, 2, 4, 6],\n   *                         'topic-B': [1],\n   *                       },\n   *                     },\n   *                     {\n   *                       memberId: 'test-3d3d5341',\n   *                       memberAssignment: {\n   *                         'topic-A': [1, 3, 5],\n   *                         'topic-B': [0, 2],\n   *                       },\n   *                     }\n   *                   ]\n   */\n  async assign({ members, topics }) {\n    const membersCount = members.length\n    const sortedMembers = members.map(({ memberId }) => memberId).sort()\n    const assignment = {}\n\n    const topicsPartitions = topics.flatMap(topic => {\n      const partitionMetadata = cluster.findTopicPartitionMetadata(topic)\n      return partitionMetadata.map(m => ({ topic: topic, partitionId: m.partitionId }))\n    })\n\n    topicsPartitions.forEach((topicPartition, i) => {\n      const assignee = sortedMembers[i % membersCount]\n\n      if (!assignment[assignee]) {\n        assignment[assignee] = Object.create(null)\n      }\n\n      if (!assignment[assignee][topicPartition.topic]) {\n        assignment[assignee][topicPartition.topic] = []\n      }\n\n      assignment[assignee][topicPartition.topic].push(topicPartition.partitionId)\n    })\n\n    return Object.keys(assignment).map(memberId => ({\n      memberId,\n      memberAssignment: MemberAssignment.encode({\n        version: this.version,\n        assignment: assignment[memberId],\n      }),\n    }))\n  },\n\n  protocol({ topics }) {\n    return {\n      name: this.name,\n      metadata: MemberMetadata.encode({\n        version: this.version,\n        topics,\n      }),\n    }\n  },\n})\n", "const roundRobin = require('./roundRobinAssigner')\n\nmodule.exports = {\n  roundRobin,\n}\n", "const Long = require('../utils/long')\nconst createRetry = require('../retry')\nconst { initialRetryTime } = require('../retry/defaults')\nconst ConsumerGroup = require('./consumerGroup')\nconst Runner = require('./runner')\nconst { events, wrap: wrapEvent, unwrap: unwrapEvent } = require('./instrumentationEvents')\nconst InstrumentationEventEmitter = require('../instrumentation/emitter')\nconst { KafkaJSNonRetriableError } = require('../errors')\nconst { roundRobin } = require('./assigners')\nconst { EARLIEST_OFFSET, LATEST_OFFSET } = require('../constants')\nconst ISOLATION_LEVEL = require('../protocol/isolationLevel')\nconst sharedPromiseTo = require('../utils/sharedPromiseTo')\n\nconst { keys, values } = Object\nconst { CONNECT, DISCONNECT, STOP, CRASH } = events\n\nconst eventNames = values(events)\nconst eventKeys = keys(events)\n  .map(key => `consumer.events.${key}`)\n  .join(', ')\n\nconst specialOffsets = [\n  Long.fromValue(EARLIEST_OFFSET).toString(),\n  Long.fromValue(LATEST_OFFSET).toString(),\n]\n\n/**\n * @param {Object} params\n * @param {import(\"../../types\").Cluster} params.cluster\n * @param {String} params.groupId\n * @param {import('../../types').RetryOptions} [params.retry]\n * @param {import('../../types').Logger} params.logger\n * @param {import('../../types').PartitionAssigner[]} [params.partitionAssigners]\n * @param {number} [params.sessionTimeout]\n * @param {number} [params.rebalanceTimeout]\n * @param {number} [params.heartbeatInterval]\n * @param {number} [params.maxBytesPerPartition]\n * @param {number} [params.minBytes]\n * @param {number} [params.maxBytes]\n * @param {number} [params.maxWaitTimeInMs]\n * @param {number} [params.isolationLevel]\n * @param {string} [params.rackId]\n * @param {InstrumentationEventEmitter} [params.instrumentationEmitter]\n * @param {number} params.metadataMaxAge\n *\n * @returns {import(\"../../types\").Consumer}\n */\nmodule.exports = ({\n  cluster,\n  groupId,\n  retry,\n  logger: rootLogger,\n  partitionAssigners = [roundRobin],\n  sessionTimeout = 30000,\n  rebalanceTimeout = 60000,\n  heartbeatInterval = 3000,\n  maxBytesPerPartition = 1048576, // 1MB\n  minBytes = 1,\n  maxBytes = 10485760, // 10MB\n  maxWaitTimeInMs = 5000,\n  isolationLevel = ISOLATION_LEVEL.READ_COMMITTED,\n  rackId = '',\n  instrumentationEmitter: rootInstrumentationEmitter,\n  metadataMaxAge,\n}) => {\n  if (!groupId) {\n    throw new KafkaJSNonRetriableError('Consumer groupId must be a non-empty string.')\n  }\n\n  const logger = rootLogger.namespace('Consumer')\n  const instrumentationEmitter = rootInstrumentationEmitter || new InstrumentationEventEmitter()\n  const assigners = partitionAssigners.map(createAssigner =>\n    createAssigner({ groupId, logger, cluster })\n  )\n\n  /** @type {Record<string, { fromBeginning?: boolean }>} */\n  const topics = {}\n  let runner = null\n  /** @type {ConsumerGroup} */\n  let consumerGroup = null\n  let restartTimeout = null\n\n  if (heartbeatInterval >= sessionTimeout) {\n    throw new KafkaJSNonRetriableError(\n      `Consumer heartbeatInterval (${heartbeatInterval}) must be lower than sessionTimeout (${sessionTimeout}). It is recommended to set heartbeatInterval to approximately a third of the sessionTimeout.`\n    )\n  }\n\n  /** @type {import(\"../../types\").Consumer[\"connect\"]} */\n  const connect = async () => {\n    await cluster.connect()\n    instrumentationEmitter.emit(CONNECT)\n  }\n\n  /** @type {import(\"../../types\").Consumer[\"disconnect\"]} */\n  const disconnect = async () => {\n    try {\n      await stop()\n      logger.debug('consumer has stopped, disconnecting', { groupId })\n      await cluster.disconnect()\n      instrumentationEmitter.emit(DISCONNECT)\n    } catch (e) {\n      logger.error(`Caught error when disconnecting the consumer: ${e.message}`, {\n        stack: e.stack,\n        groupId,\n      })\n      throw e\n    }\n  }\n\n  /** @type {import(\"../../types\").Consumer[\"stop\"]} */\n  const stop = sharedPromiseTo(async () => {\n    try {\n      if (runner) {\n        await runner.stop()\n        runner = null\n        consumerGroup = null\n        instrumentationEmitter.emit(STOP)\n      }\n\n      clearTimeout(restartTimeout)\n      logger.info('Stopped', { groupId })\n    } catch (e) {\n      logger.error(`Caught error when stopping the consumer: ${e.message}`, {\n        stack: e.stack,\n        groupId,\n      })\n\n      throw e\n    }\n  })\n\n  /** @type {import(\"../../types\").Consumer[\"subscribe\"]} */\n  const subscribe = async ({ topic, topics: subscriptionTopics, fromBeginning = false }) => {\n    if (consumerGroup) {\n      throw new KafkaJSNonRetriableError('Cannot subscribe to topic while consumer is running')\n    }\n\n    if (!topic && !subscriptionTopics) {\n      throw new KafkaJSNonRetriableError('Missing required argument \"topics\"')\n    }\n\n    if (subscriptionTopics != null && !Array.isArray(subscriptionTopics)) {\n      throw new KafkaJSNonRetriableError('Argument \"topics\" must be an array')\n    }\n\n    const subscriptions = subscriptionTopics || [topic]\n\n    for (const subscription of subscriptions) {\n      if (typeof subscription !== 'string' && !(subscription instanceof RegExp)) {\n        throw new KafkaJSNonRetriableError(\n          `Invalid topic ${subscription} (${typeof subscription}), the topic name has to be a String or a RegExp`\n        )\n      }\n    }\n\n    const hasRegexSubscriptions = subscriptions.some(subscription => subscription instanceof RegExp)\n    const metadata = hasRegexSubscriptions ? await cluster.metadata() : undefined\n\n    const topicsToSubscribe = []\n    for (const subscription of subscriptions) {\n      const isRegExp = subscription instanceof RegExp\n      if (isRegExp) {\n        const topicRegExp = subscription\n        const matchedTopics = metadata.topicMetadata\n          .map(({ topic: topicName }) => topicName)\n          .filter(topicName => topicRegExp.test(topicName))\n\n        logger.debug('Subscription based on RegExp', {\n          groupId,\n          topicRegExp: topicRegExp.toString(),\n          matchedTopics,\n        })\n\n        topicsToSubscribe.push(...matchedTopics)\n      } else {\n        topicsToSubscribe.push(subscription)\n      }\n    }\n\n    for (const t of topicsToSubscribe) {\n      topics[t] = { fromBeginning }\n    }\n\n    await cluster.addMultipleTargetTopics(topicsToSubscribe)\n  }\n\n  /** @type {import(\"../../types\").Consumer[\"run\"]} */\n  const run = async ({\n    autoCommit = true,\n    autoCommitInterval = null,\n    autoCommitThreshold = null,\n    eachBatchAutoResolve = true,\n    partitionsConsumedConcurrently: concurrency = 1,\n    eachBatch = null,\n    eachMessage = null,\n  } = {}) => {\n    if (consumerGroup) {\n      logger.warn('consumer#run was called, but the consumer is already running', { groupId })\n      return\n    }\n\n    const start = async onCrash => {\n      logger.info('Starting', { groupId })\n\n      consumerGroup = new ConsumerGroup({\n        logger: rootLogger,\n        topics: keys(topics),\n        topicConfigurations: topics,\n        retry,\n        cluster,\n        groupId,\n        assigners,\n        sessionTimeout,\n        rebalanceTimeout,\n        maxBytesPerPartition,\n        minBytes,\n        maxBytes,\n        maxWaitTimeInMs,\n        instrumentationEmitter,\n        isolationLevel,\n        rackId,\n        metadataMaxAge,\n        autoCommit,\n        autoCommitInterval,\n        autoCommitThreshold,\n      })\n\n      runner = new Runner({\n        logger: rootLogger,\n        consumerGroup,\n        instrumentationEmitter,\n        heartbeatInterval,\n        retry,\n        autoCommit,\n        eachBatchAutoResolve,\n        eachBatch,\n        eachMessage,\n        onCrash,\n        concurrency,\n      })\n\n      await runner.start()\n    }\n\n    const onCrash = async e => {\n      logger.error(`Crash: ${e.name}: ${e.message}`, {\n        groupId,\n        retryCount: e.retryCount,\n        stack: e.stack,\n      })\n\n      if (e.name === 'KafkaJSConnectionClosedError') {\n        cluster.removeBroker({ host: e.host, port: e.port })\n      }\n\n      await disconnect()\n\n      const getOriginalCause = error => {\n        if (error.cause) {\n          return getOriginalCause(error.cause)\n        }\n\n        return error\n      }\n\n      const isErrorRetriable =\n        e.name === 'KafkaJSNumberOfRetriesExceeded' || getOriginalCause(e).retriable === true\n      const shouldRestart =\n        isErrorRetriable &&\n        (!retry ||\n          !retry.restartOnFailure ||\n          (await retry.restartOnFailure(e).catch(error => {\n            logger.error(\n              'Caught error when invoking user-provided \"restartOnFailure\" callback. Defaulting to restarting.',\n              {\n                error: error.message || error,\n                cause: e.message || e,\n                groupId,\n              }\n            )\n\n            return true\n          })))\n\n      instrumentationEmitter.emit(CRASH, {\n        error: e,\n        groupId,\n        restart: shouldRestart,\n      })\n\n      if (shouldRestart) {\n        const retryTime = e.retryTime || (retry && retry.initialRetryTime) || initialRetryTime\n        logger.error(`Restarting the consumer in ${retryTime}ms`, {\n          retryCount: e.retryCount,\n          retryTime,\n          groupId,\n        })\n\n        restartTimeout = setTimeout(() => start(onCrash), retryTime)\n      }\n    }\n\n    await start(onCrash)\n  }\n\n  /** @type {import(\"../../types\").Consumer[\"on\"]} */\n  const on = (eventName, listener) => {\n    if (!eventNames.includes(eventName)) {\n      throw new KafkaJSNonRetriableError(`Event name should be one of ${eventKeys}`)\n    }\n\n    return instrumentationEmitter.addListener(unwrapEvent(eventName), event => {\n      event.type = wrapEvent(event.type)\n      Promise.resolve(listener(event)).catch(e => {\n        logger.error(`Failed to execute listener: ${e.message}`, {\n          eventName,\n          stack: e.stack,\n        })\n      })\n    })\n  }\n\n  /**\n   * @type {import(\"../../types\").Consumer[\"commitOffsets\"]}\n   * @param topicPartitions\n   *   Example: [{ topic: 'topic-name', partition: 0, offset: '1', metadata: 'event-id-3' }]\n   */\n  const commitOffsets = async (topicPartitions = []) => {\n    const commitsByTopic = topicPartitions.reduce(\n      (payload, { topic, partition, offset, metadata = null }) => {\n        if (!topic) {\n          throw new KafkaJSNonRetriableError(`Invalid topic ${topic}`)\n        }\n\n        if (isNaN(partition)) {\n          throw new KafkaJSNonRetriableError(\n            `Invalid partition, expected a number received ${partition}`\n          )\n        }\n\n        let commitOffset\n        try {\n          commitOffset = Long.fromValue(offset)\n        } catch (_) {\n          throw new KafkaJSNonRetriableError(`Invalid offset, expected a long received ${offset}`)\n        }\n\n        if (commitOffset.lessThan(0)) {\n          throw new KafkaJSNonRetriableError('Offset must not be a negative number')\n        }\n\n        if (metadata !== null && typeof metadata !== 'string') {\n          throw new KafkaJSNonRetriableError(\n            `Invalid offset metadata, expected string or null, received ${metadata}`\n          )\n        }\n\n        const topicCommits = payload[topic] || []\n\n        topicCommits.push({ partition, offset: commitOffset, metadata })\n\n        return { ...payload, [topic]: topicCommits }\n      },\n      {}\n    )\n\n    if (!consumerGroup) {\n      throw new KafkaJSNonRetriableError(\n        'Consumer group was not initialized, consumer#run must be called first'\n      )\n    }\n\n    const topics = Object.keys(commitsByTopic)\n\n    return runner.commitOffsets({\n      topics: topics.map(topic => {\n        return {\n          topic,\n          partitions: commitsByTopic[topic],\n        }\n      }),\n    })\n  }\n\n  /** @type {import(\"../../types\").Consumer[\"seek\"]} */\n  const seek = ({ topic, partition, offset }) => {\n    if (!topic) {\n      throw new KafkaJSNonRetriableError(`Invalid topic ${topic}`)\n    }\n\n    if (isNaN(partition)) {\n      throw new KafkaJSNonRetriableError(\n        `Invalid partition, expected a number received ${partition}`\n      )\n    }\n\n    let seekOffset\n    try {\n      seekOffset = Long.fromValue(offset)\n    } catch (_) {\n      throw new KafkaJSNonRetriableError(`Invalid offset, expected a long received ${offset}`)\n    }\n\n    if (seekOffset.lessThan(0) && !specialOffsets.includes(seekOffset.toString())) {\n      throw new KafkaJSNonRetriableError('Offset must not be a negative number')\n    }\n\n    if (!consumerGroup) {\n      throw new KafkaJSNonRetriableError(\n        'Consumer group was not initialized, consumer#run must be called first'\n      )\n    }\n\n    consumerGroup.seek({ topic, partition, offset: seekOffset.toString() })\n  }\n\n  /** @type {import(\"../../types\").Consumer[\"describeGroup\"]} */\n  const describeGroup = async () => {\n    const coordinator = await cluster.findGroupCoordinator({ groupId })\n    const retrier = createRetry(retry)\n    return retrier(async () => {\n      const { groups } = await coordinator.describeGroups({ groupIds: [groupId] })\n      return groups.find(group => group.groupId === groupId)\n    })\n  }\n\n  /**\n   * @type {import(\"../../types\").Consumer[\"pause\"]}\n   * @param topicPartitions\n   *   Example: [{ topic: 'topic-name', partitions: [1, 2] }]\n   */\n  const pause = (topicPartitions = []) => {\n    for (const topicPartition of topicPartitions) {\n      if (!topicPartition || !topicPartition.topic) {\n        throw new KafkaJSNonRetriableError(\n          `Invalid topic ${(topicPartition && topicPartition.topic) || topicPartition}`\n        )\n      } else if (\n        typeof topicPartition.partitions !== 'undefined' &&\n        (!Array.isArray(topicPartition.partitions) || topicPartition.partitions.some(isNaN))\n      ) {\n        throw new KafkaJSNonRetriableError(\n          `Array of valid partitions required to pause specific partitions instead of ${topicPartition.partitions}`\n        )\n      }\n    }\n\n    if (!consumerGroup) {\n      throw new KafkaJSNonRetriableError(\n        'Consumer group was not initialized, consumer#run must be called first'\n      )\n    }\n\n    consumerGroup.pause(topicPartitions)\n  }\n\n  /**\n   * Returns the list of topic partitions paused on this consumer\n   *\n   * @type {import(\"../../types\").Consumer[\"paused\"]}\n   */\n  const paused = () => {\n    if (!consumerGroup) {\n      return []\n    }\n\n    return consumerGroup.paused()\n  }\n\n  /**\n   * @type {import(\"../../types\").Consumer[\"resume\"]}\n   * @param topicPartitions\n   *  Example: [{ topic: 'topic-name', partitions: [1, 2] }]\n   */\n  const resume = (topicPartitions = []) => {\n    for (const topicPartition of topicPartitions) {\n      if (!topicPartition || !topicPartition.topic) {\n        throw new KafkaJSNonRetriableError(\n          `Invalid topic ${(topicPartition && topicPartition.topic) || topicPartition}`\n        )\n      } else if (\n        typeof topicPartition.partitions !== 'undefined' &&\n        (!Array.isArray(topicPartition.partitions) || topicPartition.partitions.some(isNaN))\n      ) {\n        throw new KafkaJSNonRetriableError(\n          `Array of valid partitions required to resume specific partitions instead of ${topicPartition.partitions}`\n        )\n      }\n    }\n\n    if (!consumerGroup) {\n      throw new KafkaJSNonRetriableError(\n        'Consumer group was not initialized, consumer#run must be called first'\n      )\n    }\n\n    consumerGroup.resume(topicPartitions)\n  }\n\n  /**\n   * @return {Object} logger\n   */\n  const getLogger = () => logger\n\n  return {\n    connect,\n    disconnect,\n    subscribe,\n    stop,\n    run,\n    commitOffsets,\n    seek,\n    describeGroup,\n    pause,\n    paused,\n    resume,\n    on,\n    events,\n    logger: getLogger,\n  }\n}\n", "const sleep = require('./sleep')\nconst { KafkaJSTimeout } = require('../errors')\n\nmodule.exports = (\n  fn,\n  { delay = 50, maxWait = 10000, timeoutMessage = 'Timeout', ignoreTimeout = false } = {}\n) => {\n  let timeoutId\n  let totalWait = 0\n  let fulfilled = false\n\n  const checkCondition = async (resolve, reject) => {\n    totalWait += delay\n    if (fulfilled) {\n      return\n    }\n\n    await sleep(delay)\n\n    try {\n      const result = await fn(totalWait)\n      if (result) {\n        fulfilled = true\n        clearTimeout(timeoutId)\n        return resolve(result)\n      }\n\n      checkCondition(resolve, reject)\n    } catch (e) {\n      fulfilled = true\n      clearTimeout(timeoutId)\n      reject(e)\n    }\n  }\n\n  return new Promise((resolve, reject) => {\n    checkCondition(resolve, reject)\n\n    if (ignoreTimeout) {\n      return\n    }\n\n    timeoutId = setTimeout(() => {\n      if (!fulfilled) {\n        fulfilled = true\n        return reject(new KafkaJSTimeout(timeoutMessage))\n      }\n    }, maxWait)\n  })\n}\n", "module.exports = async (array, groupFn) => {\n  const result = new Map()\n\n  for (const item of array) {\n    const group = await Promise.resolve(groupFn(item))\n    result.set(group, result.has(group) ? [...result.get(group), item] : [item])\n  }\n\n  return result\n}\n", "const swapObject = require('../utils/swapObject')\nconst networkEvents = require('../network/instrumentationEvents')\nconst InstrumentationEventType = require('../instrumentation/eventType')\nconst adminType = InstrumentationEventType('admin')\n\nconst events = {\n  CONNECT: adminType('connect'),\n  DISCONNECT: adminType('disconnect'),\n  REQUEST: adminType(networkEvents.NETWORK_REQUEST),\n  REQUEST_TIMEOUT: adminType(networkEvents.NETWORK_REQUEST_TIMEOUT),\n  REQUEST_QUEUE_SIZE: adminType(networkEvents.NETWORK_REQUEST_QUEUE_SIZE),\n}\n\nconst wrappedEvents = {\n  [events.REQUEST]: networkEvents.NETWORK_REQUEST,\n  [events.REQUEST_TIMEOUT]: networkEvents.NETWORK_REQUEST_TIMEOUT,\n  [events.REQUEST_QUEUE_SIZE]: networkEvents.NETWORK_REQUEST_QUEUE_SIZE,\n}\n\nconst reversedWrappedEvents = swapObject(wrappedEvents)\nconst unwrap = eventName => wrappedEvents[eventName] || eventName\nconst wrap = eventName => reversedWrappedEvents[eventName] || eventName\n\nmodule.exports = {\n  events,\n  wrap,\n  unwrap,\n}\n", "/**\n * @see https://github.com/apache/kafka/blob/a15387f34d142684859c2a57fcbef25edcdce25a/clients/src/main/java/org/apache/kafka/common/resource/ResourceType.java#L25-L31\n * @typedef {number} ACLResourceTypes\n *\n * Enum for ACL Resource Types\n * @readonly\n * @enum {ACLResourceTypes}\n */\n\nmodule.exports = {\n  /**\n   * Represents any ResourceType which this client cannot understand,\n   * perhaps because this client is too old.\n   */\n  UNKNOWN: 0,\n  /**\n   * In a filter, matches any ResourceType.\n   */\n  ANY: 1,\n  /**\n   * A Kafka topic.\n   * @see http://kafka.apache.org/documentation/#topicconfigs\n   */\n  TOPIC: 2,\n  /**\n   * A consumer group.\n   * @see http://kafka.apache.org/documentation/#consumerconfigs\n   */\n  GROUP: 3,\n  /**\n   * The cluster as a whole.\n   */\n  CLUSTER: 4,\n  /**\n   * A transactional ID.\n   */\n  TRANSACTIONAL_ID: 5,\n  /**\n   * A token ID.\n   */\n  DELEGATION_TOKEN: 6,\n}\n", "// From:\n// https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/common/acl/AclOperation.java#L44\n\n/**\n * @typedef {number} ACLOperationTypes\n *\n * Enum for ACL Operations Types\n * @readonly\n * @enum {ACLOperationTypes}\n */\nmodule.exports = {\n  /**\n   * Represents any AclOperation which this client cannot understand, perhaps because this\n   * client is too old.\n   */\n  UNKNOWN: 0,\n  /**\n   * In a filter, matches any AclOperation.\n   */\n  ANY: 1,\n  /**\n   * ALL operation.\n   */\n  ALL: 2,\n  /**\n   * READ operation.\n   */\n  READ: 3,\n  /**\n   * WRITE operation.\n   */\n  WRITE: 4,\n  /**\n   * CREATE operation.\n   */\n  CREATE: 5,\n  /**\n   * DELETE operation.\n   */\n  DELETE: 6,\n  /**\n   * ALTER operation.\n   */\n  ALTER: 7,\n  /**\n   * DESCRIBE operation.\n   */\n  DESCRIBE: 8,\n  /**\n   * CLUSTER_ACTION operation.\n   */\n  CLUSTER_ACTION: 9,\n  /**\n   * DESCRIBE_CONFIGS operation.\n   */\n  DESCRIBE_CONFIGS: 10,\n  /**\n   * ALTER_CONFIGS operation.\n   */\n  ALTER_CONFIGS: 11,\n  /**\n   * IDEMPOTENT_WRITE operation.\n   */\n  IDEMPOTENT_WRITE: 12,\n}\n", "// From:\n// https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/common/acl/AclPermissionType.java/#L31\n\n/**\n * @typedef {number} ACLPermissionTypes\n *\n * Enum for Permission Types\n * @readonly\n * @enum {ACLPermissionTypes}\n */\nmodule.exports = {\n  /**\n   * Represents any AclPermissionType which this client cannot understand,\n   * perhaps because this client is too old.\n   */\n  UNKNOWN: 0,\n  /**\n   * In a filter, matches any AclPermissionType.\n   */\n  ANY: 1,\n  /**\n   * Disallows access.\n   */\n  DENY: 2,\n  /**\n   * Grants access.\n   */\n  ALLOW: 3,\n}\n", "// From:\n// https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/common/resource/PatternType.java#L32\n\n/**\n * @typedef {number} ACLResourcePatternTypes\n *\n * Enum for ACL Resource Pattern Type\n * @readonly\n * @enum {ACLResourcePatternTypes}\n */\nmodule.exports = {\n  /**\n   * Represents any PatternType which this client cannot understand, perhaps because this client is too old.\n   */\n  UNKNOWN: 0,\n  /**\n   * In a filter, matches any resource pattern type.\n   */\n  ANY: 1,\n  /**\n   * In a filter, will perform pattern matching.\n   *\n   * e.g. Given a filter of {@code ResourcePatternFilter(TOPIC, \"payments.received\", MATCH)`}, the filter match\n   * any {@link ResourcePattern} that matches topic 'payments.received'. This might include:\n   * <ul>\n   *     <li>A Literal pattern with the same type and name, e.g. {@code ResourcePattern(TOPIC, \"payments.received\", LITERAL)}</li>\n   *     <li>A Wildcard pattern with the same type, e.g. {@code ResourcePattern(TOPIC, \"*\", LITERAL)}</li>\n   *     <li>A Prefixed pattern with the same type and where the name is a matching prefix, e.g. {@code ResourcePattern(TOPIC, \"payments.\", PREFIXED)}</li>\n   * </ul>\n   */\n  MATCH: 2,\n  /**\n   * A literal resource name.\n   *\n   * A literal name defines the full name of a resource, e.g. topic with name 'foo', or group with name 'bob'.\n   *\n   * The special wildcard character {@code *} can be used to represent a resource with any name.\n   */\n  LITERAL: 3,\n  /**\n   * A prefixed resource name.\n   *\n   * A prefixed name defines a prefix for a resource, e.g. topics with names that start with 'foo'.\n   */\n  PREFIXED: 4,\n}\n", "const createRetry = require('../retry')\nconst waitFor = require('../utils/waitFor')\nconst groupBy = require('../utils/groupBy')\nconst createConsumer = require('../consumer')\nconst InstrumentationEventEmitter = require('../instrumentation/emitter')\nconst { events, wrap: wrapEvent, unwrap: unwrapEvent } = require('./instrumentationEvents')\nconst { LEVELS } = require('../loggers')\nconst {\n  KafkaJSNonRetriableError,\n  KafkaJSDeleteGroupsError,\n  KafkaJSBrokerNotFound,\n  KafkaJSDeleteTopicRecordsError,\n  KafkaJSAggregateError,\n} = require('../errors')\nconst { staleMetadata } = require('../protocol/error')\nconst CONFIG_RESOURCE_TYPES = require('../protocol/configResourceTypes')\nconst ACL_RESOURCE_TYPES = require('../protocol/aclResourceTypes')\nconst ACL_OPERATION_TYPES = require('../protocol/aclOperationTypes')\nconst ACL_PERMISSION_TYPES = require('../protocol/aclPermissionTypes')\nconst RESOURCE_PATTERN_TYPES = require('../protocol/resourcePatternTypes')\nconst { EARLIEST_OFFSET, LATEST_OFFSET } = require('../constants')\n\nconst { CONNECT, DISCONNECT } = events\n\nconst NO_CONTROLLER_ID = -1\n\nconst { values, keys, entries } = Object\nconst eventNames = values(events)\nconst eventKeys = keys(events)\n  .map(key => `admin.events.${key}`)\n  .join(', ')\n\nconst retryOnLeaderNotAvailable = (fn, opts = {}) => {\n  const callback = async () => {\n    try {\n      return await fn()\n    } catch (e) {\n      if (e.type !== 'LEADER_NOT_AVAILABLE') {\n        throw e\n      }\n      return false\n    }\n  }\n\n  return waitFor(callback, opts)\n}\n\nconst isConsumerGroupRunning = description => ['Empty', 'Dead'].includes(description.state)\nconst findTopicPartitions = async (cluster, topic) => {\n  await cluster.addTargetTopic(topic)\n  await cluster.refreshMetadataIfNecessary()\n\n  return cluster\n    .findTopicPartitionMetadata(topic)\n    .map(({ partitionId }) => partitionId)\n    .sort()\n}\nconst indexByPartition = array =>\n  array.reduce(\n    (obj, { partition, ...props }) => Object.assign(obj, { [partition]: { ...props } }),\n    {}\n  )\n\n/**\n *\n * @param {Object} params\n * @param {import(\"../../types\").Logger} params.logger\n * @param {InstrumentationEventEmitter} [params.instrumentationEmitter]\n * @param {import('../../types').RetryOptions} params.retry\n * @param {import(\"../../types\").Cluster} params.cluster\n *\n * @returns {import(\"../../types\").Admin}\n */\nmodule.exports = ({\n  logger: rootLogger,\n  instrumentationEmitter: rootInstrumentationEmitter,\n  retry,\n  cluster,\n}) => {\n  const logger = rootLogger.namespace('Admin')\n  const instrumentationEmitter = rootInstrumentationEmitter || new InstrumentationEventEmitter()\n\n  /**\n   * @returns {Promise}\n   */\n  const connect = async () => {\n    await cluster.connect()\n    instrumentationEmitter.emit(CONNECT)\n  }\n\n  /**\n   * @return {Promise}\n   */\n  const disconnect = async () => {\n    await cluster.disconnect()\n    instrumentationEmitter.emit(DISCONNECT)\n  }\n\n  /**\n   * @return {Promise}\n   */\n  const listTopics = async () => {\n    const { topicMetadata } = await cluster.metadata()\n    const topics = topicMetadata.map(t => t.topic)\n    return topics\n  }\n\n  /**\n   * @param {Object} request\n   * @param {array} request.topics\n   * @param {boolean} [request.validateOnly=false]\n   * @param {number} [request.timeout=5000]\n   * @param {boolean} [request.waitForLeaders=true]\n   * @return {Promise}\n   */\n  const createTopics = async ({ topics, validateOnly, timeout, waitForLeaders = true }) => {\n    if (!topics || !Array.isArray(topics)) {\n      throw new KafkaJSNonRetriableError(`Invalid topics array ${topics}`)\n    }\n\n    if (topics.filter(({ topic }) => typeof topic !== 'string').length > 0) {\n      throw new KafkaJSNonRetriableError(\n        'Invalid topics array, the topic names have to be a valid string'\n      )\n    }\n\n    const topicNames = new Set(topics.map(({ topic }) => topic))\n    if (topicNames.size < topics.length) {\n      throw new KafkaJSNonRetriableError(\n        'Invalid topics array, it cannot have multiple entries for the same topic'\n      )\n    }\n\n    for (const { topic, configEntries } of topics) {\n      if (configEntries == null) {\n        continue\n      }\n\n      if (!Array.isArray(configEntries)) {\n        throw new KafkaJSNonRetriableError(\n          `Invalid configEntries for topic \"${topic}\", must be an array`\n        )\n      }\n\n      configEntries.forEach((entry, index) => {\n        if (typeof entry !== 'object' || entry == null) {\n          throw new KafkaJSNonRetriableError(\n            `Invalid configEntries for topic \"${topic}\". Entry ${index} must be an object`\n          )\n        }\n\n        for (const requiredProperty of ['name', 'value']) {\n          if (\n            !Object.prototype.hasOwnProperty.call(entry, requiredProperty) ||\n            typeof entry[requiredProperty] !== 'string'\n          ) {\n            throw new KafkaJSNonRetriableError(\n              `Invalid configEntries for topic \"${topic}\". Entry ${index} must have a valid \"${requiredProperty}\" property`\n            )\n          }\n        }\n      })\n    }\n\n    const retrier = createRetry(retry)\n\n    return retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await cluster.refreshMetadata()\n        const broker = await cluster.findControllerBroker()\n        await broker.createTopics({ topics, validateOnly, timeout })\n\n        if (waitForLeaders) {\n          const topicNamesArray = Array.from(topicNames.values())\n          await retryOnLeaderNotAvailable(async () => await broker.metadata(topicNamesArray), {\n            delay: 100,\n            maxWait: timeout,\n            timeoutMessage: 'Timed out while waiting for topic leaders',\n          })\n        }\n\n        return true\n      } catch (e) {\n        if (e.type === 'NOT_CONTROLLER') {\n          logger.warn('Could not create topics', { error: e.message, retryCount, retryTime })\n          throw e\n        }\n\n        if (e instanceof KafkaJSAggregateError) {\n          if (e.errors.every(error => error.type === 'TOPIC_ALREADY_EXISTS')) {\n            return false\n          }\n        }\n\n        bail(e)\n      }\n    })\n  }\n  /**\n   * @param {array} topicPartitions\n   * @param {boolean} [validateOnly=false]\n   * @param {number} [timeout=5000]\n   * @return {Promise<void>}\n   */\n  const createPartitions = async ({ topicPartitions, validateOnly, timeout }) => {\n    if (!topicPartitions || !Array.isArray(topicPartitions)) {\n      throw new KafkaJSNonRetriableError(`Invalid topic partitions array ${topicPartitions}`)\n    }\n    if (topicPartitions.length === 0) {\n      throw new KafkaJSNonRetriableError(`Empty topic partitions array`)\n    }\n\n    if (topicPartitions.filter(({ topic }) => typeof topic !== 'string').length > 0) {\n      throw new KafkaJSNonRetriableError(\n        'Invalid topic partitions array, the topic names have to be a valid string'\n      )\n    }\n\n    const topicNames = new Set(topicPartitions.map(({ topic }) => topic))\n    if (topicNames.size < topicPartitions.length) {\n      throw new KafkaJSNonRetriableError(\n        'Invalid topic partitions array, it cannot have multiple entries for the same topic'\n      )\n    }\n\n    const retrier = createRetry(retry)\n\n    return retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await cluster.refreshMetadata()\n        const broker = await cluster.findControllerBroker()\n        await broker.createPartitions({ topicPartitions, validateOnly, timeout })\n      } catch (e) {\n        if (e.type === 'NOT_CONTROLLER') {\n          logger.warn('Could not create topics', { error: e.message, retryCount, retryTime })\n          throw e\n        }\n\n        bail(e)\n      }\n    })\n  }\n\n  /**\n   * @param {string[]} topics\n   * @param {number} [timeout=5000]\n   * @return {Promise}\n   */\n  const deleteTopics = async ({ topics, timeout }) => {\n    if (!topics || !Array.isArray(topics)) {\n      throw new KafkaJSNonRetriableError(`Invalid topics array ${topics}`)\n    }\n\n    if (topics.filter(topic => typeof topic !== 'string').length > 0) {\n      throw new KafkaJSNonRetriableError('Invalid topics array, the names must be a valid string')\n    }\n\n    const retrier = createRetry(retry)\n\n    return retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await cluster.refreshMetadata()\n        const broker = await cluster.findControllerBroker()\n        await broker.deleteTopics({ topics, timeout })\n\n        // Remove deleted topics\n        for (const topic of topics) {\n          cluster.targetTopics.delete(topic)\n        }\n\n        await cluster.refreshMetadata()\n      } catch (e) {\n        if (['NOT_CONTROLLER', 'UNKNOWN_TOPIC_OR_PARTITION'].includes(e.type)) {\n          logger.warn('Could not delete topics', { error: e.message, retryCount, retryTime })\n          throw e\n        }\n\n        if (e.type === 'REQUEST_TIMED_OUT') {\n          logger.error(\n            'Could not delete topics, check if \"delete.topic.enable\" is set to \"true\" (the default value is \"false\") or increase the timeout',\n            {\n              error: e.message,\n              retryCount,\n              retryTime,\n            }\n          )\n        }\n\n        bail(e)\n      }\n    })\n  }\n\n  /**\n   * @param {string} topic\n   */\n\n  const fetchTopicOffsets = async topic => {\n    if (!topic || typeof topic !== 'string') {\n      throw new KafkaJSNonRetriableError(`Invalid topic ${topic}`)\n    }\n\n    const retrier = createRetry(retry)\n\n    return retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await cluster.addTargetTopic(topic)\n        await cluster.refreshMetadataIfNecessary()\n\n        const metadata = cluster.findTopicPartitionMetadata(topic)\n        const high = await cluster.fetchTopicsOffset([\n          {\n            topic,\n            fromBeginning: false,\n            partitions: metadata.map(p => ({ partition: p.partitionId })),\n          },\n        ])\n\n        const low = await cluster.fetchTopicsOffset([\n          {\n            topic,\n            fromBeginning: true,\n            partitions: metadata.map(p => ({ partition: p.partitionId })),\n          },\n        ])\n\n        const { partitions: highPartitions } = high.pop()\n        const { partitions: lowPartitions } = low.pop()\n        return highPartitions.map(({ partition, offset }) => ({\n          partition,\n          offset,\n          high: offset,\n          low: lowPartitions.find(({ partition: lowPartition }) => lowPartition === partition)\n            .offset,\n        }))\n      } catch (e) {\n        if (e.type === 'UNKNOWN_TOPIC_OR_PARTITION') {\n          await cluster.refreshMetadata()\n          throw e\n        }\n\n        bail(e)\n      }\n    })\n  }\n\n  /**\n   * @param {string} topic\n   * @param {number} [timestamp]\n   */\n\n  const fetchTopicOffsetsByTimestamp = async (topic, timestamp) => {\n    if (!topic || typeof topic !== 'string') {\n      throw new KafkaJSNonRetriableError(`Invalid topic ${topic}`)\n    }\n\n    const retrier = createRetry(retry)\n\n    return retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await cluster.addTargetTopic(topic)\n        await cluster.refreshMetadataIfNecessary()\n\n        const metadata = cluster.findTopicPartitionMetadata(topic)\n        const partitions = metadata.map(p => ({ partition: p.partitionId }))\n\n        const high = await cluster.fetchTopicsOffset([\n          {\n            topic,\n            fromBeginning: false,\n            partitions,\n          },\n        ])\n        const { partitions: highPartitions } = high.pop()\n\n        const offsets = await cluster.fetchTopicsOffset([\n          {\n            topic,\n            fromTimestamp: timestamp,\n            partitions,\n          },\n        ])\n        const { partitions: lowPartitions } = offsets.pop()\n\n        return lowPartitions.map(({ partition, offset }) => ({\n          partition,\n          offset:\n            parseInt(offset, 10) >= 0\n              ? offset\n              : highPartitions.find(({ partition: highPartition }) => highPartition === partition)\n                  .offset,\n        }))\n      } catch (e) {\n        if (e.type === 'UNKNOWN_TOPIC_OR_PARTITION') {\n          await cluster.refreshMetadata()\n          throw e\n        }\n\n        bail(e)\n      }\n    })\n  }\n\n  /**\n   * Fetch offsets for a topic or multiple topics\n   *\n   * Note: set either topic or topics but not both.\n   *\n   * @param {string} groupId\n   * @param {string[]} topics - list of topics to fetch offsets for, defaults to `[]` which fetches all topics for `groupId`.\n   * @param {boolean} [resolveOffsets=false]\n   * @return {Promise}\n   */\n  const fetchOffsets = async ({ groupId, topics, resolveOffsets = false }) => {\n    if (!groupId) {\n      throw new KafkaJSNonRetriableError(`Invalid groupId ${groupId}`)\n    }\n\n    if (!topics) {\n      topics = []\n    }\n\n    if (!Array.isArray(topics)) {\n      throw new KafkaJSNonRetriableError('Expected topics array to be set')\n    }\n\n    const coordinator = await cluster.findGroupCoordinator({ groupId })\n    const topicsToFetch = await Promise.all(\n      topics.map(async topic => {\n        const partitions = await findTopicPartitions(cluster, topic)\n        const partitionsToFetch = partitions.map(partition => ({ partition }))\n        return { topic, partitions: partitionsToFetch }\n      })\n    )\n    let { responses: consumerOffsets } = await coordinator.offsetFetch({\n      groupId,\n      topics: topicsToFetch,\n    })\n\n    if (resolveOffsets) {\n      consumerOffsets = await Promise.all(\n        consumerOffsets.map(async ({ topic, partitions }) => {\n          const indexedOffsets = indexByPartition(await fetchTopicOffsets(topic))\n          const recalculatedPartitions = partitions.map(({ offset, partition, ...props }) => {\n            let resolvedOffset = offset\n            if (Number(offset) === EARLIEST_OFFSET) {\n              resolvedOffset = indexedOffsets[partition].low\n            }\n            if (Number(offset) === LATEST_OFFSET) {\n              resolvedOffset = indexedOffsets[partition].high\n            }\n            return {\n              partition,\n              offset: resolvedOffset,\n              ...props,\n            }\n          })\n\n          await setOffsets({ groupId, topic, partitions: recalculatedPartitions })\n\n          return {\n            topic,\n            partitions: recalculatedPartitions,\n          }\n        })\n      )\n    }\n\n    return consumerOffsets.map(({ topic, partitions }) => {\n      const completePartitions = partitions.map(({ partition, offset, metadata }) => ({\n        partition,\n        offset,\n        metadata: metadata || null,\n      }))\n\n      return { topic, partitions: completePartitions }\n    })\n  }\n\n  /**\n   * @param {string} groupId\n   * @param {string} topic\n   * @param {boolean} [earliest=false]\n   * @return {Promise}\n   */\n  const resetOffsets = async ({ groupId, topic, earliest = false }) => {\n    if (!groupId) {\n      throw new KafkaJSNonRetriableError(`Invalid groupId ${groupId}`)\n    }\n\n    if (!topic) {\n      throw new KafkaJSNonRetriableError(`Invalid topic ${topic}`)\n    }\n\n    const partitions = await findTopicPartitions(cluster, topic)\n    const partitionsToSeek = partitions.map(partition => ({\n      partition,\n      offset: cluster.defaultOffset({ fromBeginning: earliest }),\n    }))\n\n    return setOffsets({ groupId, topic, partitions: partitionsToSeek })\n  }\n\n  /**\n   * @param {string} groupId\n   * @param {string} topic\n   * @param {Array<SeekEntry>} partitions\n   * @return {Promise}\n   *\n   * @typedef {Object} SeekEntry\n   * @property {number} partition\n   * @property {string} offset\n   */\n  const setOffsets = async ({ groupId, topic, partitions }) => {\n    if (!groupId) {\n      throw new KafkaJSNonRetriableError(`Invalid groupId ${groupId}`)\n    }\n\n    if (!topic) {\n      throw new KafkaJSNonRetriableError(`Invalid topic ${topic}`)\n    }\n\n    if (!partitions || partitions.length === 0) {\n      throw new KafkaJSNonRetriableError(`Invalid partitions`)\n    }\n\n    const consumer = createConsumer({\n      logger: rootLogger.namespace('Admin', LEVELS.NOTHING),\n      cluster,\n      groupId,\n    })\n\n    await consumer.subscribe({ topic, fromBeginning: true })\n    const description = await consumer.describeGroup()\n\n    if (!isConsumerGroupRunning(description)) {\n      throw new KafkaJSNonRetriableError(\n        `The consumer group must have no running instances, current state: ${description.state}`\n      )\n    }\n\n    return new Promise((resolve, reject) => {\n      consumer.on(consumer.events.FETCH, async () =>\n        consumer\n          .stop()\n          .then(resolve)\n          .catch(reject)\n      )\n\n      consumer\n        .run({\n          eachBatchAutoResolve: false,\n          eachBatch: async () => true,\n        })\n        .catch(reject)\n\n      // This consumer doesn't need to consume any data\n      consumer.pause([{ topic }])\n\n      for (const seekData of partitions) {\n        consumer.seek({ topic, ...seekData })\n      }\n    })\n  }\n\n  const isBrokerConfig = type =>\n    [CONFIG_RESOURCE_TYPES.BROKER, CONFIG_RESOURCE_TYPES.BROKER_LOGGER].includes(type)\n\n  /**\n   * Broker configs can only be returned by the target broker\n   *\n   * @see\n   * https://github.com/apache/kafka/blob/821c1ac6641845aeca96a43bc2b946ecec5cba4f/clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java#L3783\n   * https://github.com/apache/kafka/blob/821c1ac6641845aeca96a43bc2b946ecec5cba4f/clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java#L2027\n   *\n   * @param {Broker} defaultBroker. Broker used in case the configuration is not a broker config\n   */\n  const groupResourcesByBroker = ({ resources, defaultBroker }) =>\n    groupBy(resources, async ({ type, name: nodeId }) => {\n      return isBrokerConfig(type)\n        ? await cluster.findBroker({ nodeId: String(nodeId) })\n        : defaultBroker\n    })\n\n  /**\n   * @param {Array<ResourceConfigQuery>} resources\n   * @param {boolean} [includeSynonyms=false]\n   * @return {Promise}\n   *\n   * @typedef {Object} ResourceConfigQuery\n   * @property {ConfigResourceType} type\n   * @property {string} name\n   * @property {Array<String>} [configNames=[]]\n   */\n  const describeConfigs = async ({ resources, includeSynonyms }) => {\n    if (!resources || !Array.isArray(resources)) {\n      throw new KafkaJSNonRetriableError(`Invalid resources array ${resources}`)\n    }\n\n    if (resources.length === 0) {\n      throw new KafkaJSNonRetriableError('Resources array cannot be empty')\n    }\n\n    const validResourceTypes = Object.values(CONFIG_RESOURCE_TYPES)\n    const invalidType = resources.find(r => !validResourceTypes.includes(r.type))\n\n    if (invalidType) {\n      throw new KafkaJSNonRetriableError(\n        `Invalid resource type ${invalidType.type}: ${JSON.stringify(invalidType)}`\n      )\n    }\n\n    const invalidName = resources.find(r => !r.name || typeof r.name !== 'string')\n\n    if (invalidName) {\n      throw new KafkaJSNonRetriableError(\n        `Invalid resource name ${invalidName.name}: ${JSON.stringify(invalidName)}`\n      )\n    }\n\n    const invalidConfigs = resources.find(\n      r => !Array.isArray(r.configNames) && r.configNames != null\n    )\n\n    if (invalidConfigs) {\n      const { configNames } = invalidConfigs\n      throw new KafkaJSNonRetriableError(\n        `Invalid resource configNames ${configNames}: ${JSON.stringify(invalidConfigs)}`\n      )\n    }\n\n    const retrier = createRetry(retry)\n\n    return retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await cluster.refreshMetadata()\n        const controller = await cluster.findControllerBroker()\n        const resourcerByBroker = await groupResourcesByBroker({\n          resources,\n          defaultBroker: controller,\n        })\n\n        const describeConfigsAction = async broker => {\n          const targetBroker = broker || controller\n          return targetBroker.describeConfigs({\n            resources: resourcerByBroker.get(targetBroker),\n            includeSynonyms,\n          })\n        }\n\n        const brokers = Array.from(resourcerByBroker.keys())\n        const responses = await Promise.all(brokers.map(describeConfigsAction))\n        const responseResources = responses.reduce(\n          (result, { resources }) => [...result, ...resources],\n          []\n        )\n\n        return { resources: responseResources }\n      } catch (e) {\n        if (e.type === 'NOT_CONTROLLER') {\n          logger.warn('Could not describe configs', { error: e.message, retryCount, retryTime })\n          throw e\n        }\n\n        bail(e)\n      }\n    })\n  }\n\n  /**\n   * @param {Array<ResourceConfig>} resources\n   * @param {boolean} [validateOnly=false]\n   * @return {Promise}\n   *\n   * @typedef {Object} ResourceConfig\n   * @property {ConfigResourceType} type\n   * @property {string} name\n   * @property {Array<ResourceConfigEntry>} configEntries\n   *\n   * @typedef {Object} ResourceConfigEntry\n   * @property {string} name\n   * @property {string} value\n   */\n  const alterConfigs = async ({ resources, validateOnly }) => {\n    if (!resources || !Array.isArray(resources)) {\n      throw new KafkaJSNonRetriableError(`Invalid resources array ${resources}`)\n    }\n\n    if (resources.length === 0) {\n      throw new KafkaJSNonRetriableError('Resources array cannot be empty')\n    }\n\n    const validResourceTypes = Object.values(CONFIG_RESOURCE_TYPES)\n    const invalidType = resources.find(r => !validResourceTypes.includes(r.type))\n\n    if (invalidType) {\n      throw new KafkaJSNonRetriableError(\n        `Invalid resource type ${invalidType.type}: ${JSON.stringify(invalidType)}`\n      )\n    }\n\n    const invalidName = resources.find(r => !r.name || typeof r.name !== 'string')\n\n    if (invalidName) {\n      throw new KafkaJSNonRetriableError(\n        `Invalid resource name ${invalidName.name}: ${JSON.stringify(invalidName)}`\n      )\n    }\n\n    const invalidConfigs = resources.find(r => !Array.isArray(r.configEntries))\n\n    if (invalidConfigs) {\n      const { configEntries } = invalidConfigs\n      throw new KafkaJSNonRetriableError(\n        `Invalid resource configEntries ${configEntries}: ${JSON.stringify(invalidConfigs)}`\n      )\n    }\n\n    const invalidConfigValue = resources.find(r =>\n      r.configEntries.some(e => typeof e.name !== 'string' || typeof e.value !== 'string')\n    )\n\n    if (invalidConfigValue) {\n      throw new KafkaJSNonRetriableError(\n        `Invalid resource config value: ${JSON.stringify(invalidConfigValue)}`\n      )\n    }\n\n    const retrier = createRetry(retry)\n\n    return retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await cluster.refreshMetadata()\n        const controller = await cluster.findControllerBroker()\n        const resourcerByBroker = await groupResourcesByBroker({\n          resources,\n          defaultBroker: controller,\n        })\n\n        const alterConfigsAction = async broker => {\n          const targetBroker = broker || controller\n          return targetBroker.alterConfigs({\n            resources: resourcerByBroker.get(targetBroker),\n            validateOnly: !!validateOnly,\n          })\n        }\n\n        const brokers = Array.from(resourcerByBroker.keys())\n        const responses = await Promise.all(brokers.map(alterConfigsAction))\n        const responseResources = responses.reduce(\n          (result, { resources }) => [...result, ...resources],\n          []\n        )\n\n        return { resources: responseResources }\n      } catch (e) {\n        if (e.type === 'NOT_CONTROLLER') {\n          logger.warn('Could not alter configs', { error: e.message, retryCount, retryTime })\n          throw e\n        }\n\n        bail(e)\n      }\n    })\n  }\n\n  /**\n   * Fetch metadata for provided topics.\n   *\n   * If no topics are provided fetch metadata for all topics.\n   * @see https://kafka.apache.org/protocol#The_Messages_Metadata\n   *\n   * @param {Object} [options]\n   * @param {string[]} [options.topics]\n   * @return {Promise<TopicsMetadata>}\n   *\n   * @typedef {Object} TopicsMetadata\n   * @property {Array<TopicMetadata>} topics\n   *\n   * @typedef {Object} TopicMetadata\n   * @property {String} name\n   * @property {Array<PartitionMetadata>} partitions\n   *\n   * @typedef {Object} PartitionMetadata\n   * @property {number} partitionErrorCode Response error code\n   * @property {number} partitionId Topic partition id\n   * @property {number} leader  The id of the broker acting as leader for this partition.\n   * @property {Array<number>} replicas The set of all nodes that host this partition.\n   * @property {Array<number>} isr The set of nodes that are in sync with the leader for this partition.\n   */\n  const fetchTopicMetadata = async ({ topics = [] } = {}) => {\n    if (topics) {\n      topics.forEach(topic => {\n        if (!topic || typeof topic !== 'string') {\n          throw new KafkaJSNonRetriableError(`Invalid topic ${topic}`)\n        }\n      })\n    }\n\n    const metadata = await cluster.metadata({ topics })\n\n    return {\n      topics: metadata.topicMetadata.map(topicMetadata => ({\n        name: topicMetadata.topic,\n        partitions: topicMetadata.partitionMetadata,\n      })),\n    }\n  }\n\n  /**\n   * Describe cluster\n   *\n   * @return {Promise<ClusterMetadata>}\n   *\n   * @typedef {Object} ClusterMetadata\n   * @property {Array<Broker>} brokers\n   * @property {Number} controller Current controller id. Returns null if unknown.\n   * @property {String} clusterId\n   *\n   * @typedef {Object} Broker\n   * @property {Number} nodeId\n   * @property {String} host\n   * @property {Number} port\n   */\n  const describeCluster = async () => {\n    const { brokers: nodes, clusterId, controllerId } = await cluster.metadata({ topics: [] })\n    const brokers = nodes.map(({ nodeId, host, port }) => ({\n      nodeId,\n      host,\n      port,\n    }))\n    const controller =\n      controllerId == null || controllerId === NO_CONTROLLER_ID ? null : controllerId\n\n    return {\n      brokers,\n      controller,\n      clusterId,\n    }\n  }\n\n  /**\n   * List groups in a broker\n   *\n   * @return {Promise<ListGroups>}\n   *\n   * @typedef {Object} ListGroups\n   * @property {Array<ListGroup>} groups\n   *\n   * @typedef {Object} ListGroup\n   * @property {string} groupId\n   * @property {string} protocolType\n   */\n  const listGroups = async () => {\n    await cluster.refreshMetadata()\n    let groups = []\n    for (var nodeId in cluster.brokerPool.brokers) {\n      const broker = await cluster.findBroker({ nodeId })\n      const response = await broker.listGroups()\n      groups = groups.concat(response.groups)\n    }\n\n    return { groups }\n  }\n\n  /**\n   * Describe groups by group ids\n   * @param {Array<string>} groupIds\n   *\n   * @typedef {Object} GroupDescriptions\n   * @property {Array<GroupDescription>} groups\n   *\n   * @return {Promise<GroupDescriptions>}\n   */\n  const describeGroups = async groupIds => {\n    const coordinatorsForGroup = await Promise.all(\n      groupIds.map(async groupId => {\n        const coordinator = await cluster.findGroupCoordinator({ groupId })\n        return {\n          coordinator,\n          groupId,\n        }\n      })\n    )\n\n    const groupsByCoordinator = Object.values(\n      coordinatorsForGroup.reduce((coordinators, { coordinator, groupId }) => {\n        const group = coordinators[coordinator.nodeId]\n\n        if (group) {\n          coordinators[coordinator.nodeId] = {\n            ...group,\n            groupIds: [...group.groupIds, groupId],\n          }\n        } else {\n          coordinators[coordinator.nodeId] = { coordinator, groupIds: [groupId] }\n        }\n        return coordinators\n      }, {})\n    )\n\n    const responses = await Promise.all(\n      groupsByCoordinator.map(async ({ coordinator, groupIds }) => {\n        const retrier = createRetry(retry)\n        const { groups } = await retrier(() => coordinator.describeGroups({ groupIds }))\n        return groups\n      })\n    )\n\n    const groups = [].concat.apply([], responses)\n\n    return { groups }\n  }\n\n  /**\n   * Delete groups in a broker\n   *\n   * @param {string[]} [groupIds]\n   * @return {Promise<DeleteGroups>}\n   *\n   * @typedef {Array} DeleteGroups\n   * @property {string} groupId\n   * @property {number} errorCode\n   */\n  const deleteGroups = async groupIds => {\n    if (!groupIds || !Array.isArray(groupIds)) {\n      throw new KafkaJSNonRetriableError(`Invalid groupIds array ${groupIds}`)\n    }\n\n    const invalidGroupId = groupIds.some(g => typeof g !== 'string')\n\n    if (invalidGroupId) {\n      throw new KafkaJSNonRetriableError(`Invalid groupId name: ${JSON.stringify(invalidGroupId)}`)\n    }\n\n    const retrier = createRetry(retry)\n\n    let results = []\n\n    let clonedGroupIds = groupIds.slice()\n\n    return retrier(async (bail, retryCount, retryTime) => {\n      try {\n        if (clonedGroupIds.length === 0) return []\n\n        await cluster.refreshMetadata()\n\n        const brokersPerGroups = {}\n        const brokersPerNode = {}\n        for (const groupId of clonedGroupIds) {\n          const broker = await cluster.findGroupCoordinator({ groupId })\n          if (brokersPerGroups[broker.nodeId] === undefined) brokersPerGroups[broker.nodeId] = []\n          brokersPerGroups[broker.nodeId].push(groupId)\n          brokersPerNode[broker.nodeId] = broker\n        }\n\n        const res = await Promise.all(\n          Object.keys(brokersPerNode).map(\n            async nodeId => await brokersPerNode[nodeId].deleteGroups(brokersPerGroups[nodeId])\n          )\n        )\n\n        const errors = res\n          .flatMap(({ results }) =>\n            results.map(({ groupId, errorCode, error }) => {\n              return { groupId, errorCode, error }\n            })\n          )\n          .filter(({ errorCode }) => errorCode !== 0)\n\n        clonedGroupIds = errors.map(({ groupId }) => groupId)\n\n        if (errors.length > 0) throw new KafkaJSDeleteGroupsError('Error in DeleteGroups', errors)\n\n        results = res.flatMap(({ results }) => results)\n\n        return results\n      } catch (e) {\n        if (e.type === 'NOT_CONTROLLER' || e.type === 'COORDINATOR_NOT_AVAILABLE') {\n          logger.warn('Could not delete groups', { error: e.message, retryCount, retryTime })\n          throw e\n        }\n\n        bail(e)\n      }\n    })\n  }\n\n  /**\n   * Delete topic records up to the selected partition offsets\n   *\n   * @param {string} topic\n   * @param {Array<SeekEntry>} partitions\n   * @return {Promise}\n   *\n   * @typedef {Object} SeekEntry\n   * @property {number} partition\n   * @property {string} offset\n   */\n  const deleteTopicRecords = async ({ topic, partitions }) => {\n    if (!topic || typeof topic !== 'string') {\n      throw new KafkaJSNonRetriableError(`Invalid topic \"${topic}\"`)\n    }\n\n    if (!partitions || partitions.length === 0) {\n      throw new KafkaJSNonRetriableError(`Invalid partitions`)\n    }\n\n    const partitionsByBroker = cluster.findLeaderForPartitions(\n      topic,\n      partitions.map(p => p.partition)\n    )\n\n    const partitionsFound = values(partitionsByBroker).flat()\n    const topicOffsets = await fetchTopicOffsets(topic)\n\n    const leaderNotFoundErrors = []\n    partitions.forEach(({ partition, offset }) => {\n      // throw if no leader found for partition\n      if (!partitionsFound.includes(partition)) {\n        leaderNotFoundErrors.push({\n          partition,\n          offset,\n          error: new KafkaJSBrokerNotFound('Could not find the leader for the partition', {\n            retriable: false,\n          }),\n        })\n        return\n      }\n      const { low } = topicOffsets.find(p => p.partition === partition) || {\n        high: undefined,\n        low: undefined,\n      }\n      // warn in case of offset below low watermark\n      if (parseInt(offset) < parseInt(low) && parseInt(offset) !== -1) {\n        logger.warn(\n          'The requested offset is before the earliest offset maintained on the partition - no records will be deleted from this partition',\n          {\n            topic,\n            partition,\n            offset,\n          }\n        )\n      }\n    })\n\n    if (leaderNotFoundErrors.length > 0) {\n      throw new KafkaJSDeleteTopicRecordsError({ topic, partitions: leaderNotFoundErrors })\n    }\n\n    const seekEntriesByBroker = entries(partitionsByBroker).reduce(\n      (obj, [nodeId, nodePartitions]) => {\n        obj[nodeId] = {\n          topic,\n          partitions: partitions.filter(p => nodePartitions.includes(p.partition)),\n        }\n        return obj\n      },\n      {}\n    )\n\n    const retrier = createRetry(retry)\n    return retrier(async bail => {\n      try {\n        const partitionErrors = []\n\n        const brokerRequests = entries(seekEntriesByBroker).map(\n          ([nodeId, { topic, partitions }]) => async () => {\n            const broker = await cluster.findBroker({ nodeId })\n            await broker.deleteRecords({ topics: [{ topic, partitions }] })\n            // remove successful entry so it's ignored on retry\n            delete seekEntriesByBroker[nodeId]\n          }\n        )\n\n        await Promise.all(\n          brokerRequests.map(request =>\n            request().catch(e => {\n              if (e.name === 'KafkaJSDeleteTopicRecordsError') {\n                e.partitions.forEach(({ partition, offset, error }) => {\n                  partitionErrors.push({\n                    partition,\n                    offset,\n                    error,\n                  })\n                })\n              } else {\n                // then it's an unknown error, not from the broker response\n                throw e\n              }\n            })\n          )\n        )\n\n        if (partitionErrors.length > 0) {\n          throw new KafkaJSDeleteTopicRecordsError({\n            topic,\n            partitions: partitionErrors,\n          })\n        }\n      } catch (e) {\n        if (\n          e.retriable &&\n          e.partitions.some(\n            ({ error }) => staleMetadata(error) || error.name === 'KafkaJSMetadataNotLoaded'\n          )\n        ) {\n          await cluster.refreshMetadata()\n        }\n        throw e\n      }\n    })\n  }\n\n  /**\n   * @param {Array<ACLEntry>} acl\n   * @return {Promise<void>}\n   *\n   * @typedef {Object} ACLEntry\n   */\n  const createAcls = async ({ acl }) => {\n    if (!acl || !Array.isArray(acl)) {\n      throw new KafkaJSNonRetriableError(`Invalid ACL array ${acl}`)\n    }\n    if (acl.length === 0) {\n      throw new KafkaJSNonRetriableError('Empty ACL array')\n    }\n\n    // Validate principal\n    if (acl.some(({ principal }) => typeof principal !== 'string')) {\n      throw new KafkaJSNonRetriableError(\n        'Invalid ACL array, the principals have to be a valid string'\n      )\n    }\n\n    // Validate host\n    if (acl.some(({ host }) => typeof host !== 'string')) {\n      throw new KafkaJSNonRetriableError('Invalid ACL array, the hosts have to be a valid string')\n    }\n\n    // Validate resourceName\n    if (acl.some(({ resourceName }) => typeof resourceName !== 'string')) {\n      throw new KafkaJSNonRetriableError(\n        'Invalid ACL array, the resourceNames have to be a valid string'\n      )\n    }\n\n    let invalidType\n    // Validate operation\n    const validOperationTypes = Object.values(ACL_OPERATION_TYPES)\n    invalidType = acl.find(i => !validOperationTypes.includes(i.operation))\n\n    if (invalidType) {\n      throw new KafkaJSNonRetriableError(\n        `Invalid operation type ${invalidType.operation}: ${JSON.stringify(invalidType)}`\n      )\n    }\n\n    // Validate resourcePatternTypes\n    const validResourcePatternTypes = Object.values(RESOURCE_PATTERN_TYPES)\n    invalidType = acl.find(i => !validResourcePatternTypes.includes(i.resourcePatternType))\n\n    if (invalidType) {\n      throw new KafkaJSNonRetriableError(\n        `Invalid resource pattern type ${invalidType.resourcePatternType}: ${JSON.stringify(\n          invalidType\n        )}`\n      )\n    }\n\n    // Validate permissionTypes\n    const validPermissionTypes = Object.values(ACL_PERMISSION_TYPES)\n    invalidType = acl.find(i => !validPermissionTypes.includes(i.permissionType))\n\n    if (invalidType) {\n      throw new KafkaJSNonRetriableError(\n        `Invalid permission type ${invalidType.permissionType}: ${JSON.stringify(invalidType)}`\n      )\n    }\n\n    // Validate resourceTypes\n    const validResourceTypes = Object.values(ACL_RESOURCE_TYPES)\n    invalidType = acl.find(i => !validResourceTypes.includes(i.resourceType))\n\n    if (invalidType) {\n      throw new KafkaJSNonRetriableError(\n        `Invalid resource type ${invalidType.resourceType}: ${JSON.stringify(invalidType)}`\n      )\n    }\n\n    const retrier = createRetry(retry)\n\n    return retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await cluster.refreshMetadata()\n        const broker = await cluster.findControllerBroker()\n        await broker.createAcls({ acl })\n\n        return true\n      } catch (e) {\n        if (e.type === 'NOT_CONTROLLER') {\n          logger.warn('Could not create ACL', { error: e.message, retryCount, retryTime })\n          throw e\n        }\n\n        bail(e)\n      }\n    })\n  }\n\n  /**\n   * @param {ACLResourceTypes} resourceType The type of resource\n   * @param {string} resourceName The name of the resource\n   * @param {ACLResourcePatternTypes} resourcePatternType The resource pattern type filter\n   * @param {string} principal The principal name\n   * @param {string} host The hostname\n   * @param {ACLOperationTypes} operation The type of operation\n   * @param {ACLPermissionTypes} permissionType The type of permission\n   * @return {Promise<void>}\n   *\n   * @typedef {number} ACLResourceTypes\n   * @typedef {number} ACLResourcePatternTypes\n   * @typedef {number} ACLOperationTypes\n   * @typedef {number} ACLPermissionTypes\n   */\n  const describeAcls = async ({\n    resourceType,\n    resourceName,\n    resourcePatternType,\n    principal,\n    host,\n    operation,\n    permissionType,\n  }) => {\n    // Validate principal\n    if (typeof principal !== 'string' && typeof principal !== 'undefined') {\n      throw new KafkaJSNonRetriableError(\n        'Invalid principal, the principal have to be a valid string'\n      )\n    }\n\n    // Validate host\n    if (typeof host !== 'string' && typeof host !== 'undefined') {\n      throw new KafkaJSNonRetriableError('Invalid host, the host have to be a valid string')\n    }\n\n    // Validate resourceName\n    if (typeof resourceName !== 'string' && typeof resourceName !== 'undefined') {\n      throw new KafkaJSNonRetriableError(\n        'Invalid resourceName, the resourceName have to be a valid string'\n      )\n    }\n\n    // Validate operation\n    const validOperationTypes = Object.values(ACL_OPERATION_TYPES)\n    if (!validOperationTypes.includes(operation)) {\n      throw new KafkaJSNonRetriableError(`Invalid operation type ${operation}`)\n    }\n\n    // Validate resourcePatternType\n    const validResourcePatternTypes = Object.values(RESOURCE_PATTERN_TYPES)\n    if (!validResourcePatternTypes.includes(resourcePatternType)) {\n      throw new KafkaJSNonRetriableError(\n        `Invalid resource pattern filter type ${resourcePatternType}`\n      )\n    }\n\n    // Validate permissionType\n    const validPermissionTypes = Object.values(ACL_PERMISSION_TYPES)\n    if (!validPermissionTypes.includes(permissionType)) {\n      throw new KafkaJSNonRetriableError(`Invalid permission type ${permissionType}`)\n    }\n\n    // Validate resourceType\n    const validResourceTypes = Object.values(ACL_RESOURCE_TYPES)\n    if (!validResourceTypes.includes(resourceType)) {\n      throw new KafkaJSNonRetriableError(`Invalid resource type ${resourceType}`)\n    }\n\n    const retrier = createRetry(retry)\n\n    return retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await cluster.refreshMetadata()\n        const broker = await cluster.findControllerBroker()\n        const { resources } = await broker.describeAcls({\n          resourceType,\n          resourceName,\n          resourcePatternType,\n          principal,\n          host,\n          operation,\n          permissionType,\n        })\n        return { resources }\n      } catch (e) {\n        if (e.type === 'NOT_CONTROLLER') {\n          logger.warn('Could not describe ACL', { error: e.message, retryCount, retryTime })\n          throw e\n        }\n\n        bail(e)\n      }\n    })\n  }\n\n  /**\n   * @param {Array<ACLFilter>} filters\n   * @return {Promise<void>}\n   *\n   * @typedef {Object} ACLFilter\n   */\n  const deleteAcls = async ({ filters }) => {\n    if (!filters || !Array.isArray(filters)) {\n      throw new KafkaJSNonRetriableError(`Invalid ACL Filter array ${filters}`)\n    }\n\n    if (filters.length === 0) {\n      throw new KafkaJSNonRetriableError('Empty ACL Filter array')\n    }\n\n    // Validate principal\n    if (\n      filters.some(\n        ({ principal }) => typeof principal !== 'string' && typeof principal !== 'undefined'\n      )\n    ) {\n      throw new KafkaJSNonRetriableError(\n        'Invalid ACL Filter array, the principals have to be a valid string'\n      )\n    }\n\n    // Validate host\n    if (filters.some(({ host }) => typeof host !== 'string' && typeof host !== 'undefined')) {\n      throw new KafkaJSNonRetriableError(\n        'Invalid ACL Filter array, the hosts have to be a valid string'\n      )\n    }\n\n    // Validate resourceName\n    if (\n      filters.some(\n        ({ resourceName }) =>\n          typeof resourceName !== 'string' && typeof resourceName !== 'undefined'\n      )\n    ) {\n      throw new KafkaJSNonRetriableError(\n        'Invalid ACL Filter array, the resourceNames have to be a valid string'\n      )\n    }\n\n    let invalidType\n    // Validate operation\n    const validOperationTypes = Object.values(ACL_OPERATION_TYPES)\n    invalidType = filters.find(i => !validOperationTypes.includes(i.operation))\n\n    if (invalidType) {\n      throw new KafkaJSNonRetriableError(\n        `Invalid operation type ${invalidType.operation}: ${JSON.stringify(invalidType)}`\n      )\n    }\n\n    // Validate resourcePatternTypes\n    const validResourcePatternTypes = Object.values(RESOURCE_PATTERN_TYPES)\n    invalidType = filters.find(i => !validResourcePatternTypes.includes(i.resourcePatternType))\n\n    if (invalidType) {\n      throw new KafkaJSNonRetriableError(\n        `Invalid resource pattern type ${invalidType.resourcePatternType}: ${JSON.stringify(\n          invalidType\n        )}`\n      )\n    }\n\n    // Validate permissionTypes\n    const validPermissionTypes = Object.values(ACL_PERMISSION_TYPES)\n    invalidType = filters.find(i => !validPermissionTypes.includes(i.permissionType))\n\n    if (invalidType) {\n      throw new KafkaJSNonRetriableError(\n        `Invalid permission type ${invalidType.permissionType}: ${JSON.stringify(invalidType)}`\n      )\n    }\n\n    // Validate resourceTypes\n    const validResourceTypes = Object.values(ACL_RESOURCE_TYPES)\n    invalidType = filters.find(i => !validResourceTypes.includes(i.resourceType))\n\n    if (invalidType) {\n      throw new KafkaJSNonRetriableError(\n        `Invalid resource type ${invalidType.resourceType}: ${JSON.stringify(invalidType)}`\n      )\n    }\n\n    const retrier = createRetry(retry)\n\n    return retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await cluster.refreshMetadata()\n        const broker = await cluster.findControllerBroker()\n        const { filterResponses } = await broker.deleteAcls({ filters })\n        return { filterResponses }\n      } catch (e) {\n        if (e.type === 'NOT_CONTROLLER') {\n          logger.warn('Could not delete ACL', { error: e.message, retryCount, retryTime })\n          throw e\n        }\n\n        bail(e)\n      }\n    })\n  }\n\n  /**\n   * Alter the replicas partitions are assigned to for a topic\n   * @param {Object} request\n   * @param {import(\"../../types\").IPartitionReassignment[]} request.topics topics and the paritions to be reassigned\n   * @param {number} [request.timeout]\n   * @returns {Promise}\n   */\n  const alterPartitionReassignments = async ({ topics, timeout }) => {\n    if (!topics || !Array.isArray(topics)) {\n      throw new KafkaJSNonRetriableError(`Invalid topics array ${topics}`)\n    }\n\n    if (topics.filter(({ topic }) => typeof topic !== 'string').length > 0) {\n      throw new KafkaJSNonRetriableError(\n        'Invalid topics array, the topic names have to be a valid string'\n      )\n    }\n\n    const topicNames = new Set(topics.map(({ topic }) => topic))\n    if (topicNames.size < topics.length) {\n      throw new KafkaJSNonRetriableError(\n        'Invalid topics array, it cannot have multiple entries for the same topic'\n      )\n    }\n\n    for (const { topic, partitionAssignment } of topics) {\n      if (!partitionAssignment || !Array.isArray(partitionAssignment)) {\n        throw new KafkaJSNonRetriableError(\n          `Invalid partitions array: ${partitionAssignment} for topic: ${topic}`\n        )\n      }\n\n      for (const { partition, replicas } of partitionAssignment) {\n        if (\n          partition === null ||\n          partition === undefined ||\n          typeof partition !== 'number' ||\n          partition < 0\n        ) {\n          throw new KafkaJSNonRetriableError(\n            `Invalid partitions index: ${partition} for topic: ${topic}`\n          )\n        }\n\n        if (!replicas || !Array.isArray(replicas)) {\n          throw new KafkaJSNonRetriableError(\n            `Invalid replica assignment: ${replicas} for topic: ${topic} on partition: ${partition}`\n          )\n        }\n\n        if (replicas.filter(replica => typeof replica !== 'number' || replica < 0).length >= 1) {\n          throw new KafkaJSNonRetriableError(\n            `Invalid replica assignment: ${replicas} for topic: ${topic} on partition: ${partition}. Replicas must be a non negative number`\n          )\n        }\n      }\n    }\n\n    const retrier = createRetry(retry)\n\n    return retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await cluster.refreshMetadata()\n        const broker = await cluster.findControllerBroker()\n        await broker.alterPartitionReassignments({ topics, timeout })\n      } catch (e) {\n        if (e.type === 'NOT_CONTROLLER') {\n          logger.warn('Could not reassign partitions', { error: e.message, retryCount, retryTime })\n          throw e\n        }\n\n        bail(e)\n      }\n    })\n  }\n\n  /**\n   * List the partition reassignments in progress.\n   * If a partition is not going through a reassignment, its AddingReplicas and RemovingReplicas fields will simply be empty.\n   * If a partition doesn't exist, no response will be returned for it.\n   * @param {Object} request\n   * @param {import(\"../../types\").TopicPartitions[]} request.topics topics and the paritions to be returned, if this is null will return all the topics.\n   * @param {number} [request.timeout]\n   * @returns {Promise<import(\"../../types\").ListPartitionReassignmentsResponse>}\n   */\n  const listPartitionReassignments = async ({ topics = null, timeout }) => {\n    if (topics) {\n      if (!Array.isArray(topics)) {\n        throw new KafkaJSNonRetriableError(`Invalid topics array ${topics}`)\n      }\n\n      if (topics.filter(({ topic }) => typeof topic !== 'string').length > 0) {\n        throw new KafkaJSNonRetriableError(\n          'Invalid topics array, the topic names have to be a valid string'\n        )\n      }\n\n      const topicNames = new Set(topics.map(({ topic }) => topic))\n      if (topicNames.size < topics.length) {\n        throw new KafkaJSNonRetriableError(\n          'Invalid topics array, it cannot have multiple entries for the same topic'\n        )\n      }\n\n      for (const { topic, partitions } of topics) {\n        if (!partitions || !Array.isArray(partitions)) {\n          throw new KafkaJSNonRetriableError(\n            `Invalid partition array: ${partitions} for topic: ${topic}`\n          )\n        }\n\n        if (\n          partitions.filter(partition => typeof partition !== 'number' || partition < 0).length >= 1\n        ) {\n          throw new KafkaJSNonRetriableError(\n            `Invalid partition array: ${partitions} for topic: ${topic}. The partition indices have to be a valid number greater than 0.`\n          )\n        }\n      }\n    }\n\n    const retrier = createRetry(retry)\n\n    return retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await cluster.refreshMetadata()\n        const broker = await cluster.findControllerBroker()\n        const response = await broker.listPartitionReassignments({ topics, timeout })\n\n        return { topics: response.topics }\n      } catch (e) {\n        if (e.type === 'NOT_CONTROLLER') {\n          logger.warn('Could not reassign partitions', { error: e.message, retryCount, retryTime })\n          throw e\n        }\n\n        bail(e)\n      }\n    })\n  }\n\n  /** @type {import(\"../../types\").Admin[\"on\"]} */\n  const on = (eventName, listener) => {\n    if (!eventNames.includes(eventName)) {\n      throw new KafkaJSNonRetriableError(`Event name should be one of ${eventKeys}`)\n    }\n\n    return instrumentationEmitter.addListener(unwrapEvent(eventName), event => {\n      event.type = wrapEvent(event.type)\n      Promise.resolve(listener(event)).catch(e => {\n        logger.error(`Failed to execute listener: ${e.message}`, {\n          eventName,\n          stack: e.stack,\n        })\n      })\n    })\n  }\n\n  /**\n   * @return {Object} logger\n   */\n  const getLogger = () => logger\n\n  return {\n    connect,\n    disconnect,\n    listTopics,\n    createTopics,\n    deleteTopics,\n    createPartitions,\n    fetchTopicMetadata,\n    describeCluster,\n    events,\n    fetchOffsets,\n    fetchTopicOffsets,\n    fetchTopicOffsetsByTimestamp,\n    setOffsets,\n    resetOffsets,\n    describeConfigs,\n    alterConfigs,\n    on,\n    logger: getLogger,\n    listGroups,\n    describeGroups,\n    deleteGroups,\n    describeAcls,\n    deleteAcls,\n    createAcls,\n    deleteTopicRecords,\n    alterPartitionReassignments,\n    listPartitionReassignments,\n  }\n}\n", "const KEEP_ALIVE_DELAY = 60000 // in ms\n\n/**\n * @returns {import(\"../../types\").ISocketFactory}\n */\nmodule.exports = () => {\n  const net = require('net')\n  const tls = require('tls')\n\n  return ({ host, port, ssl, onConnect }) => {\n    const socket = ssl\n      ? tls.connect(\n          Object.assign({ host, port }, !net.isIP(host) ? { servername: host } : {}, ssl),\n          onConnect\n        )\n      : net.connect({ host, port }, onConnect)\n\n    socket.setKeepAlive(true, KEEP_ALIVE_DELAY)\n\n    return socket\n  }\n}\n", "module.exports = fn => {\n  let called = false\n\n  return (...args) => {\n    if (!called) {\n      called = true\n      return fn(...args)\n    }\n  }\n}\n", "const {\n  createLogger,\n  LEVELS: { INFO },\n} = require('./loggers')\n\nconst InstrumentationEventEmitter = require('./instrumentation/emitter')\nconst LoggerConsole = require('./loggers/console')\nconst Cluster = require('./cluster')\nconst createProducer = require('./producer')\nconst createConsumer = require('./consumer')\nconst createAdmin = require('./admin')\nconst ISOLATION_LEVEL = require('./protocol/isolationLevel')\nconst defaultSocketFactory = require('./network/socketFactory')\nconst once = require('./utils/once')\nconst websiteUrl = require('./utils/websiteUrl')\n\nconst PRIVATE = {\n  CREATE_CLUSTER: Symbol('private:Kafka:createCluster'),\n  CLUSTER_RETRY: Symbol('private:Kafka:clusterRetry'),\n  LOGGER: Symbol('private:Kafka:logger'),\n  OFFSETS: Symbol('private:Kafka:offsets'),\n}\n\nconst DEFAULT_METADATA_MAX_AGE = 300000\nconst warnOfDefaultPartitioner = once(logger => {\n  if (process.env.KAFKAJS_NO_PARTITIONER_WARNING == null) {\n    logger.warn(\n      `KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at ${websiteUrl(\n        'docs/migration-guide-v2.0.0',\n        'producer-new-default-partitioner'\n      )} for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\"`\n    )\n  }\n})\n\nmodule.exports = class Client {\n  /**\n   * @param {Object} options\n   * @param {Array<string>} options.brokers example: ['127.0.0.1:9092', '127.0.0.1:9094']\n   * @param {Object} options.ssl\n   * @param {Object} options.sasl\n   * @param {string} options.clientId\n   * @param {number} [options.connectionTimeout=1000] - in milliseconds\n   * @param {number} options.authenticationTimeout - in milliseconds\n   * @param {number} options.reauthenticationThreshold - in milliseconds\n   * @param {number} [options.requestTimeout=30000] - in milliseconds\n   * @param {boolean} [options.enforceRequestTimeout]\n   * @param {import(\"../types\").RetryOptions} [options.retry]\n   * @param {import(\"../types\").ISocketFactory} [options.socketFactory]\n   */\n  constructor({\n    brokers,\n    ssl,\n    sasl,\n    clientId,\n    connectionTimeout = 1000,\n    authenticationTimeout,\n    reauthenticationThreshold,\n    requestTimeout,\n    enforceRequestTimeout = true,\n    retry,\n    socketFactory = defaultSocketFactory(),\n    logLevel = INFO,\n    logCreator = LoggerConsole,\n  }) {\n    this[PRIVATE.OFFSETS] = new Map()\n    this[PRIVATE.LOGGER] = createLogger({ level: logLevel, logCreator })\n    this[PRIVATE.CLUSTER_RETRY] = retry\n    this[PRIVATE.CREATE_CLUSTER] = ({\n      metadataMaxAge,\n      allowAutoTopicCreation = true,\n      maxInFlightRequests = null,\n      instrumentationEmitter = null,\n      isolationLevel,\n    }) =>\n      new Cluster({\n        logger: this[PRIVATE.LOGGER],\n        retry: this[PRIVATE.CLUSTER_RETRY],\n        offsets: this[PRIVATE.OFFSETS],\n        socketFactory,\n        brokers,\n        ssl,\n        sasl,\n        clientId,\n        connectionTimeout,\n        authenticationTimeout,\n        reauthenticationThreshold,\n        requestTimeout,\n        enforceRequestTimeout,\n        metadataMaxAge,\n        instrumentationEmitter,\n        allowAutoTopicCreation,\n        maxInFlightRequests,\n        isolationLevel,\n      })\n  }\n\n  /**\n   * @public\n   */\n  producer({\n    createPartitioner,\n    retry,\n    metadataMaxAge = DEFAULT_METADATA_MAX_AGE,\n    allowAutoTopicCreation,\n    idempotent,\n    transactionalId,\n    transactionTimeout,\n    maxInFlightRequests,\n  } = {}) {\n    const instrumentationEmitter = new InstrumentationEventEmitter()\n    const cluster = this[PRIVATE.CREATE_CLUSTER]({\n      metadataMaxAge,\n      allowAutoTopicCreation,\n      maxInFlightRequests,\n      instrumentationEmitter,\n    })\n\n    if (createPartitioner == null) {\n      warnOfDefaultPartitioner(this[PRIVATE.LOGGER])\n    }\n\n    return createProducer({\n      retry: { ...this[PRIVATE.CLUSTER_RETRY], ...retry },\n      logger: this[PRIVATE.LOGGER],\n      cluster,\n      createPartitioner,\n      idempotent,\n      transactionalId,\n      transactionTimeout,\n      instrumentationEmitter,\n    })\n  }\n\n  /**\n   * @public\n   */\n  consumer({\n    groupId,\n    partitionAssigners,\n    metadataMaxAge = DEFAULT_METADATA_MAX_AGE,\n    sessionTimeout,\n    rebalanceTimeout,\n    heartbeatInterval,\n    maxBytesPerPartition,\n    minBytes,\n    maxBytes,\n    maxWaitTimeInMs,\n    retry = { retries: 5 },\n    allowAutoTopicCreation,\n    maxInFlightRequests,\n    readUncommitted = false,\n    rackId = '',\n  } = {}) {\n    const isolationLevel = readUncommitted\n      ? ISOLATION_LEVEL.READ_UNCOMMITTED\n      : ISOLATION_LEVEL.READ_COMMITTED\n\n    const instrumentationEmitter = new InstrumentationEventEmitter()\n    const cluster = this[PRIVATE.CREATE_CLUSTER]({\n      metadataMaxAge,\n      allowAutoTopicCreation,\n      maxInFlightRequests,\n      isolationLevel,\n      instrumentationEmitter,\n    })\n\n    return createConsumer({\n      retry: { ...this[PRIVATE.CLUSTER_RETRY], ...retry },\n      logger: this[PRIVATE.LOGGER],\n      cluster,\n      groupId,\n      partitionAssigners,\n      sessionTimeout,\n      rebalanceTimeout,\n      heartbeatInterval,\n      maxBytesPerPartition,\n      minBytes,\n      maxBytes,\n      maxWaitTimeInMs,\n      isolationLevel,\n      instrumentationEmitter,\n      rackId,\n      metadataMaxAge,\n    })\n  }\n\n  /**\n   * @public\n   */\n  admin({ retry } = {}) {\n    const instrumentationEmitter = new InstrumentationEventEmitter()\n    const cluster = this[PRIVATE.CREATE_CLUSTER]({\n      allowAutoTopicCreation: false,\n      instrumentationEmitter,\n    })\n\n    return createAdmin({\n      retry: { ...this[PRIVATE.CLUSTER_RETRY], ...retry },\n      logger: this[PRIVATE.LOGGER],\n      instrumentationEmitter,\n      cluster,\n    })\n  }\n\n  /**\n   * @public\n   */\n  logger() {\n    return this[PRIVATE.LOGGER]\n  }\n}\n", "const Kafka = require('./src')\nconst PartitionAssigners = require('./src/consumer/assigners')\nconst AssignerProtocol = require('./src/consumer/assignerProtocol')\nconst Partitioners = require('./src/producer/partitioners')\nconst Compression = require('./src/protocol/message/compression')\nconst ConfigResourceTypes = require('./src/protocol/configResourceTypes')\nconst ConfigSource = require('./src/protocol/configSource')\nconst AclResourceTypes = require('./src/protocol/aclResourceTypes')\nconst AclOperationTypes = require('./src/protocol/aclOperationTypes')\nconst AclPermissionTypes = require('./src/protocol/aclPermissionTypes')\nconst ResourcePatternTypes = require('./src/protocol/resourcePatternTypes')\nconst { isRebalancing, isKafkaJSError, ...errors } = require('./src/errors')\nconst { LEVELS } = require('./src/loggers')\n\nmodule.exports = {\n  Kafka,\n  PartitionAssigners,\n  AssignerProtocol,\n  Partitioners,\n  logLevel: LEVELS,\n  CompressionTypes: Compression.Types,\n  CompressionCodecs: Compression.Codecs,\n  ConfigResourceTypes,\n  AclResourceTypes,\n  AclOperationTypes,\n  AclPermissionTypes,\n  ResourcePatternTypes,\n  ConfigSource,\n  ...errors,\n}\n", "/* eslint-disable no-console */\nconst { ConfigResourceTypes, Kafka } = require(\"kafkajs\");\n\n/**\n * Removes topics in BigMac given the following\n * @param {*} brokerString - Comma delimited list of brokers\n * @param {*} namespace - String in the format of `--${event.project}--`, only used for temp branches for easy identification and cleanup\n */\nexports.listTopics = async function (brokerString, namespace) {\n  const brokers = brokerString.split(\",\");\n\n  const kafka = new Kafka({\n    clientId: \"admin\",\n    brokers: brokers,\n    ssl: true,\n  });\n  var admin = kafka.admin();\n\n  await admin.connect();\n\n  const currentTopics = await admin.listTopics();\n  var lingeringTopics = currentTopics.filter(\n    (topic) =>\n      topic.startsWith(namespace) ||\n      topic.startsWith(`_confluent-ksql-${namespace}`)\n  );\n\n  await admin.disconnect();\n  return lingeringTopics;\n};\n\n/**\n * Generates topics in BigMac given the following\n * @param { string[] } brokers - List of brokers\n * @param {{ topic: string, numPartitions: number, replicationFactor: number }[]}\n *   desiredTopicConfigs - array of topics to create or update.\n *   The `topic` property should include any namespace.\n */\nexports.createTopics = async function (brokers, desiredTopicConfigs) {\n  const kafka = new Kafka({\n    clientId: \"admin\",\n    brokers,\n    ssl: true,\n  });\n  var admin = kafka.admin();\n  await admin.connect();\n\n  // Fetch topic names from MSK, filtering out __ internal management topic\n  const listTopicResponse = await admin.listTopics();\n  const existingTopicNames = listTopicResponse.filter(\n    (name) => !name.startsWith(\"_\")\n  );\n\n  console.log(\"Existing topics:\", JSON.stringify(existingTopicNames, null, 2));\n\n  // Fetch the metadata for those topics from MSK\n  const fetchTopicResponse = await admin.fetchTopicMetadata({\n    topics: existingTopicNames,\n  });\n  const existingTopicConfigs = fetchTopicResponse.topics;\n  console.log(\n    \"Topics Metadata:\",\n    JSON.stringify(existingTopicConfigs, null, 2)\n  );\n\n  // Any desired topics whose names don't exist in MSK need to be created\n  const topicsToCreate = desiredTopicConfigs.filter(\n    (desired) => !existingTopicNames.includes(desired.topic)\n  );\n\n  /*\n   * Any topics which do exist, but with fewer partitions than desired,\n   * need to be updated. Partitions can't be removed, only added.\n   */\n  const topicsToUpdate = desiredTopicConfigs.filter((desired) =>\n    existingTopicConfigs.some(\n      (existing) =>\n        desired.topic === existing.name &&\n        desired.numPartitions > existing.partitions.length\n    )\n  );\n\n  // Format the request to update those topics (by creating partitions)\n  const partitionsToCreate = topicsToUpdate.map((topic) => ({\n    topic: topic.topic,\n    count: topic.numPartitions,\n  }));\n\n  // Describe existing topics for informational logs\n  let existingTopicDescriptions = [];\n  if (existingTopicConfigs.length > 0) {\n    const resourcesToDescribe = existingTopicConfigs.map((topic) => ({\n      name: topic.name,\n      type: ConfigResourceTypes.TOPIC,\n    }));\n    existingTopicDescriptions = await admin.describeConfigs({\n      resources: resourcesToDescribe,\n    });\n  }\n\n  console.log(\"Topics to Create:\", JSON.stringify(topicsToCreate, null, 2));\n  console.log(\"Topics to Update:\", JSON.stringify(topicsToUpdate, null, 2));\n  console.log(\n    \"Partitions to Create:\",\n    JSON.stringify(partitionsToCreate, null, 2)\n  );\n  console.log(\n    \"Topic configuration options:\",\n    JSON.stringify(existingTopicDescriptions, null, 2)\n  );\n\n  // Create all the new topics\n  await admin.createTopics({ topics: topicsToCreate });\n\n  // Create all the new partitions\n  if (partitionsToCreate.length > 0) {\n    await admin.createPartitions({ topicPartitions: partitionsToCreate });\n  }\n\n  await admin.disconnect();\n};\n\n/**\n * Deletes all topics for an ephemeral (`--` prefixed) namespace\n * @param { string[] } brokers - List of brokers\n * @param {string} topicNamespace\n */\nexports.deleteTopics = async function (brokers, topicNamespace) {\n  if (!topicNamespace.startsWith(\"--\")) {\n    throw \"ERROR:  The deleteTopics function only operates against topics that begin with --.\";\n  }\n\n  const kafka = new Kafka({\n    clientId: \"admin\",\n    brokers,\n    ssl: true,\n    requestTimeout: 295000, // 5s short of the lambda function's timeout\n  });\n  var admin = kafka.admin();\n\n  await admin.connect();\n\n  const existingTopicNames = await admin.listTopics();\n  console.log(`All existing topics: ${existingTopicNames}`);\n  var topicsToDelete = existingTopicNames.filter(\n    (name) =>\n      name.startsWith(topicNamespace) ||\n      name.startsWith(`_confluent-ksql-${topicNamespace}`)\n  );\n  console.log(`Deleting topics:  ${topicsToDelete}`);\n\n  await admin.deleteTopics({\n    topics: topicsToDelete,\n  });\n\n  await admin.disconnect();\n  return topicsToDelete;\n};\n", "/* eslint-disable no-console */\nconst topics = require(\"../libs/topics-lib.js\");\n\nconst brokers = process.env.brokerString?.split(\",\") ?? [];\n\n/**\n * Handler to be triggered in temporary branches by the destroy workflow, cleans up topics with the known namespace format\n * `--${event.project}--${event.stage}--`\n * @param {{ project: string | undefined, stage: string | undefined }} event\n * @param {*} _context\n * @param {*} _callback\n */\nexports.handler = async function (event, _context, _callback) {\n  console.log(\"Received event:\", JSON.stringify(event, null, 2));\n  if (!event.project || !event.stage) {\n    throw \"ERROR:  project and stage keys must be sent in the event.\";\n  }\n  const namespace = `--${event.project}--${event.stage}--`;\n  return await topics.deleteTopics(brokers, namespace);\n};\n"],
./.cdk/cdk.out/bundling-temp-ed4cfe5cd0af9b41e2be0559999179fbc66f642b8a0c973718307a474601075c-building/index.js.map:4: TODO: remove\n\n\n  get _type() {\n    return this.type;\n  }\n\n  _typeCheck(_value) {\n    return true;\n  }\n\n  clone(spec) {\n    if (this._mutate) {\n      if (spec) Object.assign(this.spec, spec);\n      return this;\n    } // if the nested value is a schema we can skip cloning, since\n    // they are already immutable\n\n\n    const next = Object.create(Object.getPrototypeOf(this)); // @ts-expect-error this is readonly\n\n    next.type = this.type;\n    next._typeError = this._typeError;\n    next._whitelistError = this._whitelistError;\n    next._blacklistError = this._blacklistError;\n    next._whitelist = this._whitelist.clone();\n    next._blacklist = this._blacklist.clone();\n    next.exclusiveTests = _extends({}, this.exclusiveTests); // @ts-expect-error this is readonly\n\n    next.deps = [...this.deps];\n    next.conditions = [...this.conditions];\n    next.tests = [...this.tests];\n    next.transforms = [...this.transforms];\n    next.spec = (0, _nanoclone.default)(_extends({}, this.spec, spec));\n    return next;\n  }\n\n  label(label) {\n    let next = this.clone();\n    next.spec.label = label;\n    return next;\n  }\n\n  meta(...args) {\n    if (args.length === 0) return this.spec.meta;\n    let next = this.clone();\n    next.spec.meta = Object.assign(next.spec.meta || {}, args[0]);\n    return next;\n  } // withContext<TContext extends AnyObject>(): BaseSchema<\n  //   TCast,\n  //   TContext,\n  //   TOutput\n  // > {\n  //   return this as any;\n  // }\n\n\n  withMutation(fn) {\n    let before = this._mutate;\n    this._mutate = true;\n    let result = fn(this);\n    this._mutate = before;\n    return result;\n  }\n\n  concat(schema) {\n    if (!schema || schema === this) return this;\n    if (schema.type !== this.type && this.type !== 'mixed') throw new TypeError(`You cannot \\`concat()\\` schema's of different types: ${this.type} and ${schema.type}`);\n    let base = this;\n    let combined = schema.clone();\n\n    const mergedSpec = _extends({}, base.spec, combined.spec); // if (combined.spec.nullable === UNSET)\n    //   mergedSpec.nullable = base.spec.nullable;\n    // if (combined.spec.presence === UNSET)\n    //   mergedSpec.presence = base.spec.presence;\n\n\n    combined.spec = mergedSpec;\n    combined._typeError || (combined._typeError = base._typeError);\n    combined._whitelistError || (combined._whitelistError = base._whitelistError);\n    combined._blacklistError || (combined._blacklistError = base._blacklistError); // manually merge the blacklist/whitelist (the other `schema` takes\n    // precedence in case of conflicts)\n\n    combined._whitelist = base._whitelist.merge(schema._whitelist, schema._blacklist);\n    combined._blacklist = base._blacklist.merge(schema._blacklist, schema._whitelist); // start with the current tests\n\n    combined.tests = base.tests;\n    combined.exclusiveTests = base.exclusiveTests; // manually add the new tests to ensure\n    // the deduping logic is consistent\n\n    combined.withMutation(next => {\n      schema.tests.forEach(fn => {\n        next.test(fn.OPTIONS);\n      });\n    });\n    combined.transforms = [...base.transforms, ...combined.transforms];\n    return combined;\n  }\n\n  isType(v) {\n    if (this.spec.nullable && v === null) return true;\n    return this._typeCheck(v);\n  }\n\n  resolve(options) {\n    let schema = this;\n\n    if (schema.conditions.length) {\n      let conditions = schema.conditions;\n      schema = schema.clone();\n      schema.conditions = [];\n      schema = conditions.reduce((schema, condition) => condition.resolve(schema, options), schema);\n      schema = schema.resolve(options);\n    }\n\n    return schema;\n  }\n  /**\n   *\n   * @param {*} value\n   * @param {Object} options\n   * @param {*=} options.parent\n   * @param {*=} options.context\n   */\n\n\n  cast(value, options = {}) {\n    let resolvedSchema = this.resolve(_extends({\n      value\n    }, options));\n\n    let result = resolvedSchema._cast(value, options);\n\n    if (value !== undefined && options.assert !== false && resolvedSchema.isType(result) !== true) {\n      let formattedValue = (0, _printValue.default)(value);\n      let formattedResult = (0, _printValue.default)(result);\n      throw new TypeError(`The value of ${options.path || 'field'} could not be cast to a value ` + `that satisfies the schema type: \"${resolvedSchema._type}\". \\n\\n` + `attempted value: ${formattedValue} \\n` + (formattedResult !== formattedValue ? `result of cast: ${formattedResult}` : ''));\n    }\n\n    return result;\n  }\n\n  _cast(rawValue, _options) {\n    let value = rawValue === undefined ? rawValue : this.transforms.reduce((value, fn) => fn.call(this, value, rawValue, this), rawValue);\n\n    if (value === undefined) {\n      value = this.getDefault();\n    }\n\n    return value;\n  }\n\n  _validate(_value, options = {}, cb) {\n    let {\n      sync,\n      path,\n      from = [],\n      originalValue = _value,\n      strict = this.spec.strict,\n      abortEarly = this.spec.abortEarly\n    } = options;\n    let value = _value;\n\n    if (!strict) {\n      // this._validating = true;\n      value = this._cast(value, _extends({\n        assert: false\n      }, options)); // this._validating = false;\n    } // value is cast, we can check if it meets type requirements\n\n\n    let args = {\n      value,\n      path,\n      options,\n      originalValue,\n      schema: this,\n      label: this.spec.label,\n      sync,\n      from\n    };\n    let initialTests = [];\n    if (this._typeError) initialTests.push(this._typeError);\n    let finalTests = [];\n    if (this._whitelistError) finalTests.push(this._whitelistError);\n    if (this._blacklistError) finalTests.push(this._blacklistError);\n    (0, _runTests.default)({\n      args,\n      value,\n      path,\n      sync,\n      tests: initialTests,\n      endEarly: abortEarly\n    }, err => {\n      if (err) return void cb(err, value);\n      (0, _runTests.default)({\n        tests: this.tests.concat(finalTests),\n        args,\n        path,\n        sync,\n        value,\n        endEarly: abortEarly\n      }, cb);\n    });\n  }\n\n  validate(value, options, maybeCb) {\n    let schema = this.resolve(_extends({}, options, {\n      value\n    })); // callback case is for nested validations\n\n    return typeof maybeCb === 'function' ? schema._validate(value, options, maybeCb) : new Promise((resolve, reject) => schema._validate(value, options, (err, value) => {\n      if (err) reject(err);else resolve(value);\n    }));\n  }\n\n  validateSync(value, options) {\n    let schema = this.resolve(_extends({}, options, {\n      value\n    }));\n    let result;\n\n    schema._validate(value, _extends({}, options, {\n      sync: true\n    }), (err, value) => {\n      if (err) throw err;\n      result = value;\n    });\n\n    return result;\n  }\n\n  isValid(value, options) {\n    return this.validate(value, options).then(() => true, err => {\n      if (_ValidationError.default.isError(err)) return false;\n      throw err;\n    });\n  }\n\n  isValidSync(value, options) {\n    try {\n      this.validateSync(value, options);\n      return true;\n    } catch (err) {\n      if (_ValidationError.default.isError(err)) return false;\n      throw err;\n    }\n  }\n\n  _getDefault() {\n    let defaultValue = this.spec.default;\n\n    if (defaultValue == null) {\n      return defaultValue;\n    }\n\n    return typeof defaultValue === 'function' ? defaultValue.call(this) : (0, _nanoclone.default)(defaultValue);\n  }\n\n  getDefault(options) {\n    let schema = this.resolve(options || {});\n    return schema._getDefault();\n  }\n\n  default(def) {\n    if (arguments.length === 0) {\n      return this._getDefault();\n    }\n\n    let next = this.clone({\n      default: def\n    });\n    return next;\n  }\n\n  strict(isStrict = true) {\n    let next = this.clone();\n    next.spec.strict = isStrict;\n    return next;\n  }\n\n  _isPresent(value) {\n    return value != null;\n  }\n\n  defined(message = _locale.mixed.defined) {\n    return this.test({\n      message,\n      name: 'defined',\n      exclusive: true,\n\n      test(value) {\n        return value !== undefined;\n      }\n\n    });\n  }\n\n  required(message = _locale.mixed.required) {\n    return this.clone({\n      presence: 'required'\n    }).withMutation(s => s.test({\n      message,\n      name: 'required',\n      exclusive: true,\n\n      test(value) {\n        return this.schema._isPresent(value);\n      }\n\n    }));\n  }\n\n  notRequired() {\n    let next = this.clone({\n      presence: 'optional'\n    });\n    next.tests = next.tests.filter(test => test.OPTIONS.name !== 'required');\n    return next;\n  }\n\n  nullable(isNullable = true) {\n    let next = this.clone({\n      nullable: isNullable !== false\n    });\n    return next;\n  }\n\n  transform(fn) {\n    let next = this.clone();\n    next.transforms.push(fn);\n    return next;\n  }\n  /**\n   * Adds a test function to the schema's queue of tests.\n   * tests can be exclusive or non-exclusive.\n   *\n   * - exclusive tests, will replace any existing tests of the same name.\n   * - non-exclusive: can be stacked\n   *\n   * If a non-exclusive test is added to a schema with an exclusive test of the same name\n   * the exclusive test is removed and further tests of the same name will be stacked.\n   *\n   * If an exclusive test is added to a schema with non-exclusive tests of the same name\n   * the previous tests are removed and further tests of the same name will replace each other.\n   */\n\n\n  test(...args) {\n    let opts;\n\n    if (args.length === 1) {\n      if (typeof args[0] === 'function') {\n        opts = {\n          test: args[0]\n        };\n      } else {\n        opts = args[0];\n      }\n    } else if (args.length === 2) {\n      opts = {\n        name: args[0],\n        test: args[1]\n      };\n    } else {\n      opts = {\n        name: args[0],\n        message: args[1],\n        test: args[2]\n      };\n    }\n\n    if (opts.message === undefined) opts.message = _locale.mixed.default;\n    if (typeof opts.test !== 'function') throw new TypeError('`test` is a required parameters');\n    let next = this.clone();\n    let validate = (0, _createValidation.default)(opts);\n    let isExclusive = opts.exclusive || opts.name && next.exclusiveTests[opts.name] === true;\n\n    if (opts.exclusive) {\n      if (!opts.name) throw new TypeError('Exclusive tests must provide a unique `name` identifying the test');\n    }\n\n    if (opts.name) next.exclusiveTests[opts.name] = !!opts.exclusive;\n    next.tests = next.tests.filter(fn => {\n      if (fn.OPTIONS.name === opts.name) {\n        if (isExclusive) return false;\n        if (fn.OPTIONS.test === validate.OPTIONS.test) return false;\n      }\n\n      return true;\n    });\n    next.tests.push(validate);\n    return next;\n  }\n\n  when(keys, options) {\n    if (!Array.isArray(keys) && typeof keys !== 'string') {\n      options = keys;\n      keys = '.';\n    }\n\n    let next = this.clone();\n    let deps = (0, _toArray.default)(keys).map(key => new _Reference.default(key));\n    deps.forEach(dep => {\n      // @ts-ignore\n      if (dep.isSibling) next.deps.push(dep.key);\n    });\n    next.conditions.push(new _Condition.default(deps, options));\n    return next;\n  }\n\n  typeError(message) {\n    let next = this.clone();\n    next._typeError = (0, _createValidation.default)({\n      message,\n      name: 'typeError',\n\n      test(value) {\n        if (value !== undefined && !this.schema.isType(value)) return this.createError({\n          params: {\n            type: this.schema._type\n          }\n        });\n        return true;\n      }\n\n    });\n    return next;\n  }\n\n  oneOf(enums, message = _locale.mixed.oneOf) {\n    let next = this.clone();\n    enums.forEach(val => {\n      next._whitelist.add(val);\n\n      next._blacklist.delete(val);\n    });\n    next._whitelistError = (0, _createValidation.default)({\n      message,\n      name: 'oneOf',\n\n      test(value) {\n        if (value === undefined) return true;\n        let valids = this.schema._whitelist;\n        let resolved = valids.resolveAll(this.resolve);\n        return resolved.includes(value) ? true : this.createError({\n          params: {\n            values: valids.toArray().join(', '),\n            resolved\n          }\n        });\n      }\n\n    });\n    return next;\n  }\n\n  notOneOf(enums, message = _locale.mixed.notOneOf) {\n    let next = this.clone();\n    enums.forEach(val => {\n      next._blacklist.add(val);\n\n      next._whitelist.delete(val);\n    });\n    next._blacklistError = (0, _createValidation.default)({\n      message,\n      name: 'notOneOf',\n\n      test(value) {\n        let invalids = this.schema._blacklist;\n        let resolved = invalids.resolveAll(this.resolve);\n        if (resolved.includes(value)) return this.createError({\n          params: {\n            values: invalids.toArray().join(', '),\n            resolved\n          }\n        });\n        return true;\n      }\n\n    });\n    return next;\n  }\n\n  strip(strip = true) {\n    let next = this.clone();\n    next.spec.strip = strip;\n    return next;\n  }\n\n  describe() {\n    const next = this.clone();\n    const {\n      label,\n      meta\n    } = next.spec;\n    const description = {\n      meta,\n      label,\n      type: next.type,\n      oneOf: next._whitelist.describe(),\n      notOneOf: next._blacklist.describe(),\n      tests: next.tests.map(fn => ({\n        name: fn.OPTIONS.name,\n        params: fn.OPTIONS.params\n      })).filter((n, idx, list) => list.findIndex(c => c.name === n.name) === idx)\n    };\n    return description;\n  }\n\n} // eslint-disable-next-line @typescript-eslint/no-unused-vars\n\n\nexports.default = BaseSchema;\n// @ts-expect-error\nBaseSchema.prototype.__isYupSchema__ = true;\n\nfor (const method of ['validate', 'validateSync']) BaseSchema.prototype[`${method}At`] = function (path, value, options = {}) {\n  const {\n    parent,\n    parentPath,\n    schema\n  } = (0, _reach.getIn)(this, path, value, options.context);\n  return schema[method](parent && parent[parentPath], _extends({}, options, {\n    parent,\n    path\n  }));\n};\n\nfor (const alias of ['equals', 'is']) BaseSchema.prototype[alias] = BaseSchema.prototype.oneOf;\n\nfor (const alias of ['not', 'nope']) BaseSchema.prototype[alias] = BaseSchema.prototype.notOneOf;\n\nBaseSchema.prototype.optional = BaseSchema.prototype.notRequired;", "\"use strict\";\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\nexports.create = create;\nexports.default = void 0;\n\nvar _schema = _interopRequireDefault(require(\"./schema\"));\n\nfunction _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { default: obj }; }\n\nconst Mixed = _schema.default;\nvar _default = Mixed;\nexports.default = _default;\n\nfunction create() {\n  return new Mixed();\n} // XXX: this is using the Base schema so that `addMethod(mixed)` works as a base class\n\n\ncreate.prototype = Mixed.prototype;", "\"use strict\";\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\nexports.default = void 0;\n\nconst isAbsent = value => value == null;\n\nvar _default = isAbsent;\nexports.default = _default;", "\"use strict\";\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\nexports.create = create;\nexports.default = void 0;\n\nvar _schema = _interopRequireDefault(require(\"./schema\"));\n\nvar _locale = require(\"./locale\");\n\nvar _isAbsent = _interopRequireDefault(require(\"./util/isAbsent\"));\n\nfunction _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { default: obj }; }\n\nfunction create() {\n  return new BooleanSchema();\n}\n\nclass BooleanSchema extends _schema.default {\n  constructor() {\n    super({\n      type: 'boolean'\n    });\n    this.withMutation(() => {\n      this.transform(function (value) {\n        if (!this.isType(value)) {\n          if (/^(true|1)$/i.test(String(value))) return true;\n          if (/^(false|0)$/i.test(String(value))) return false;\n        }\n\n        return value;\n      });\n    });\n  }\n\n  _typeCheck(v) {\n    if (v instanceof Boolean) v = v.valueOf();\n    return typeof v === 'boolean';\n  }\n\n  isTrue(message = _locale.boolean.isValue) {\n    return this.test({\n      message,\n      name: 'is-value',\n      exclusive: true,\n      params: {\n        value: 'true'\n      },\n\n      test(value) {\n        return (0, _isAbsent.default)(value) || value === true;\n      }\n\n    });\n  }\n\n  isFalse(message = _locale.boolean.isValue) {\n    return this.test({\n      message,\n      name: 'is-value',\n      exclusive: true,\n      params: {\n        value: 'false'\n      },\n\n      test(value) {\n        return (0, _isAbsent.default)(value) || value === false;\n      }\n\n    });\n  }\n\n}\n\nexports.default = BooleanSchema;\ncreate.prototype = BooleanSchema.prototype;", "\"use strict\";\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\nexports.create = create;\nexports.default = void 0;\n\nvar _locale = require(\"./locale\");\n\nvar _isAbsent = _interopRequireDefault(require(\"./util/isAbsent\"));\n\nvar _schema = _interopRequireDefault(require(\"./schema\"));\n\nfunction _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { default: obj }; }\n\n// eslint-disable-next-line\nlet rEmail = /^((([a-z]|\\d|[!#\\$%&'\\*\\+\\-\\/=\\?\\^_`{\\|}~]|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])+(\\.([a-z]|\\d|[!#\\$%&'\\*\\+\\-\\/=\\?\\^_`{\\|}~]|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])+)*)|((\\x22)((((\\x20|\\x09)*(\\x0d\\x0a))?(\\x20|\\x09)+)?(([\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x7f]|\\x21|[\\x23-\\x5b]|[\\x5d-\\x7e]|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])|(\\\\([\\x01-\\x09\\x0b\\x0c\\x0d-\\x7f]|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF]))))*(((\\x20|\\x09)*(\\x0d\\x0a))?(\\x20|\\x09)+)?(\\x22)))@((([a-z]|\\d|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])|(([a-z]|\\d|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])([a-z]|\\d|-|\\.|_|~|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])*([a-z]|\\d|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])))\\.)+(([a-z]|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])|(([a-z]|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])([a-z]|\\d|-|\\.|_|~|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])*([a-z]|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])))$/i; // eslint-disable-next-line\n\nlet rUrl = /^((https?|ftp):)?\\/\\/(((([a-z]|\\d|-|\\.|_|~|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])|(%[\\da-f]{2})|[!\\$&'\\(\\)\\*\\+,;=]|:)*@)?(((\\d|[1-9]\\d|1\\d\\d|2[0-4]\\d|25[0-5])\\.(\\d|[1-9]\\d|1\\d\\d|2[0-4]\\d|25[0-5])\\.(\\d|[1-9]\\d|1\\d\\d|2[0-4]\\d|25[0-5])\\.(\\d|[1-9]\\d|1\\d\\d|2[0-4]\\d|25[0-5]))|((([a-z]|\\d|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])|(([a-z]|\\d|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])([a-z]|\\d|-|\\.|_|~|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])*([a-z]|\\d|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])))\\.)+(([a-z]|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])|(([a-z]|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])([a-z]|\\d|-|\\.|_|~|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])*([a-z]|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])))\\.?)(:\\d*)?)(\\/((([a-z]|\\d|-|\\.|_|~|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])|(%[\\da-f]{2})|[!\\$&'\\(\\)\\*\\+,;=]|:|@)+(\\/(([a-z]|\\d|-|\\.|_|~|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])|(%[\\da-f]{2})|[!\\$&'\\(\\)\\*\\+,;=]|:|@)*)*)?)?(\\?((([a-z]|\\d|-|\\.|_|~|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])|(%[\\da-f]{2})|[!\\$&'\\(\\)\\*\\+,;=]|:|@)|[\\uE000-\\uF8FF]|\\/|\\?)*)?(\\#((([a-z]|\\d|-|\\.|_|~|[\\u00A0-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFEF])|(%[\\da-f]{2})|[!\\$&'\\(\\)\\*\\+,;=]|:|@)|\\/|\\?)*)?$/i; // eslint-disable-next-line\n\nlet rUUID = /^(?:[0-9a-f]{8}-[0-9a-f]{4}-[1-5][0-9a-f]{3}-[89ab][0-9a-f]{3}-[0-9a-f]{12}|00000000-0000-0000-0000-000000000000)$/i;\n\nlet isTrimmed = value => (0, _isAbsent.default)(value) || value === value.trim();\n\nlet objStringTag = {}.toString();\n\nfunction create() {\n  return new StringSchema();\n}\n\nclass StringSchema extends _schema.default {\n  constructor() {\n    super({\n      type: 'string'\n    });\n    this.withMutation(() => {\n      this.transform(function (value) {\n        if (this.isType(value)) return value;\n        if (Array.isArray(value)) return value;\n        const strValue = value != null && value.toString ? value.toString() : value;\n        if (strValue === objStringTag) return value;\n        return strValue;\n      });\n    });\n  }\n\n  _typeCheck(value) {\n    if (value instanceof String) value = value.valueOf();\n    return typeof value === 'string';\n  }\n\n  _isPresent(value) {\n    return super._isPresent(value) && !!value.length;\n  }\n\n  length(length, message = _locale.string.length) {\n    return this.test({\n      message,\n      name: 'length',\n      exclusive: true,\n      params: {\n        length\n      },\n\n      test(value) {\n        return (0, _isAbsent.default)(value) || value.length === this.resolve(length);\n      }\n\n    });\n  }\n\n  min(min, message = _locale.string.min) {\n    return this.test({\n      message,\n      name: 'min',\n      exclusive: true,\n      params: {\n        min\n      },\n\n      test(value) {\n        return (0, _isAbsent.default)(value) || value.length >= this.resolve(min);\n      }\n\n    });\n  }\n\n  max(max, message = _locale.string.max) {\n    return this.test({\n      name: 'max',\n      exclusive: true,\n      message,\n      params: {\n        max\n      },\n\n      test(value) {\n        return (0, _isAbsent.default)(value) || value.length <= this.resolve(max);\n      }\n\n    });\n  }\n\n  matches(regex, options) {\n    let excludeEmptyString = false;\n    let message;\n    let name;\n\n    if (options) {\n      if (typeof options === 'object') {\n        ({\n          excludeEmptyString = false,\n          message,\n          name\n        } = options);\n      } else {\n        message = options;\n      }\n    }\n\n    return this.test({\n      name: name || 'matches',\n      message: message || _locale.string.matches,\n      params: {\n        regex\n      },\n      test: value => (0, _isAbsent.default)(value) || value === '' && excludeEmptyString || value.search(regex) !== -1\n    });\n  }\n\n  email(message = _locale.string.email) {\n    return this.matches(rEmail, {\n      name: 'email',\n      message,\n      excludeEmptyString: true\n    });\n  }\n\n  url(message = _locale.string.url) {\n    return this.matches(rUrl, {\n      name: 'url',\n      message,\n      excludeEmptyString: true\n    });\n  }\n\n  uuid(message = _locale.string.uuid) {\n    return this.matches(rUUID, {\n      name: 'uuid',\n      message,\n      excludeEmptyString: false\n    });\n  } //-- transforms --\n\n\n  ensure() {\n    return this.default('').transform(val => val === null ? '' : val);\n  }\n\n  trim(message = _locale.string.trim) {\n    return this.transform(val => val != null ? val.trim() : val).test({\n      message,\n      name: 'trim',\n      test: isTrimmed\n    });\n  }\n\n  lowercase(message = _locale.string.lowercase) {\n    return this.transform(value => !(0, _isAbsent.default)(value) ? value.toLowerCase() : value).test({\n      message,\n      name: 'string_case',\n      exclusive: true,\n      test: value => (0, _isAbsent.default)(value) || value === value.toLowerCase()\n    });\n  }\n\n  uppercase(message = _locale.string.uppercase) {\n    return this.transform(value => !(0, _isAbsent.default)(value) ? value.toUpperCase() : value).test({\n      message,\n      name: 'string_case',\n      exclusive: true,\n      test: value => (0, _isAbsent.default)(value) || value === value.toUpperCase()\n    });\n  }\n\n}\n\nexports.default = StringSchema;\ncreate.prototype = StringSchema.prototype; //\n// String Interfaces\n//", "\"use strict\";\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\nexports.create = create;\nexports.default = void 0;\n\nvar _locale = require(\"./locale\");\n\nvar _isAbsent = _interopRequireDefault(require(\"./util/isAbsent\"));\n\nvar _schema = _interopRequireDefault(require(\"./schema\"));\n\nfunction _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { default: obj }; }\n\nlet isNaN = value => value != +value;\n\nfunction create() {\n  return new NumberSchema();\n}\n\nclass NumberSchema extends _schema.default {\n  constructor() {\n    super({\n      type: 'number'\n    });\n    this.withMutation(() => {\n      this.transform(function (value) {\n        let parsed = value;\n\n        if (typeof parsed === 'string') {\n          parsed = parsed.replace(/\\s/g, '');\n          if (parsed === '') return NaN; // don't use parseFloat to avoid positives on alpha-numeric strings\n\n          parsed = +parsed;\n        }\n\n        if (this.isType(parsed)) return parsed;\n        return parseFloat(parsed);\n      });\n    });\n  }\n\n  _typeCheck(value) {\n    if (value instanceof Number) value = value.valueOf();\n    return typeof value === 'number' && !isNaN(value);\n  }\n\n  min(min, message = _locale.number.min) {\n    return this.test({\n      message,\n      name: 'min',\n      exclusive: true,\n      params: {\n        min\n      },\n\n      test(value) {\n        return (0, _isAbsent.default)(value) || value >= this.resolve(min);\n      }\n\n    });\n  }\n\n  max(max, message = _locale.number.max) {\n    return this.test({\n      message,\n      name: 'max',\n      exclusive: true,\n      params: {\n        max\n      },\n\n      test(value) {\n        return (0, _isAbsent.default)(value) || value <= this.resolve(max);\n      }\n\n    });\n  }\n\n  lessThan(less, message = _locale.number.lessThan) {\n    return this.test({\n      message,\n      name: 'max',\n      exclusive: true,\n      params: {\n        less\n      },\n\n      test(value) {\n        return (0, _isAbsent.default)(value) || value < this.resolve(less);\n      }\n\n    });\n  }\n\n  moreThan(more, message = _locale.number.moreThan) {\n    return this.test({\n      message,\n      name: 'min',\n      exclusive: true,\n      params: {\n        more\n      },\n\n      test(value) {\n        return (0, _isAbsent.default)(value) || value > this.resolve(more);\n      }\n\n    });\n  }\n\n  positive(msg = _locale.number.positive) {\n    return this.moreThan(0, msg);\n  }\n\n  negative(msg = _locale.number.negative) {\n    return this.lessThan(0, msg);\n  }\n\n  integer(message = _locale.number.integer) {\n    return this.test({\n      name: 'integer',\n      message,\n      test: val => (0, _isAbsent.default)(val) || Number.isInteger(val)\n    });\n  }\n\n  truncate() {\n    return this.transform(value => !(0, _isAbsent.default)(value) ? value | 0 : value);\n  }\n\n  round(method) {\n    var _method;\n\n    let avail = ['ceil', 'floor', 'round', 'trunc'];\n    method = ((_method = method) == null ? void 0 : _method.toLowerCase()) || 'round'; // this exists for symemtry with the new Math.trunc\n\n    if (method === 'trunc') return this.truncate();\n    if (avail.indexOf(method.toLowerCase()) === -1) throw new TypeError('Only valid options for round() are: ' + avail.join(', '));\n    return this.transform(value => !(0, _isAbsent.default)(value) ? Math[method](value) : value);\n  }\n\n}\n\nexports.default = NumberSchema;\ncreate.prototype = NumberSchema.prototype; //\n// Number Interfaces\n//", "\"use strict\";\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\nexports.default = parseIsoDate;\n\n/* eslint-disable */\n\n/**\n *\n * Date.parse with progressive enhancement for ISO 8601 <https://github.com/csnover/js-iso8601>\n * NON-CONFORMANT EDITION.\n * \u00A9 2011 Colin Snover <http://zetafleet.com>\n * Released under MIT license.\n */\n//              1 YYYY                 2 MM        3 DD              4 HH     5 mm        6 ss            7 msec         8 Z 9 \u00B1    10 tzHH    11 tzmm\nvar isoReg = /^(\\d{4}|[+\\-]\\d{6})(?:-?(\\d{2})(?:-?(\\d{2}))?)?(?:[ T]?(\\d{2}):?(\\d{2})(?::?(\\d{2})(?:[,\\.](\\d{1,}))?)?(?:(Z)|([+\\-])(\\d{2})(?::?(\\d{2}))?)?)?$/;\n\nfunction parseIsoDate(date) {\n  var numericKeys = [1, 4, 5, 6, 7, 10, 11],\n      minutesOffset = 0,\n      timestamp,\n      struct;\n\n  if (struct = isoReg.exec(date)) {\n    // avoid NaN timestamps caused by \u201Cundefined\u201D values being passed to Date.UTC\n    for (var i = 0, k; k = numericKeys[i]; ++i) struct[k] = +struct[k] || 0; // allow undefined days and months\n\n\n    struct[2] = (+struct[2] || 1) - 1;\n    struct[3] = +struct[3] || 1; // allow arbitrary sub-second precision beyond milliseconds\n\n    struct[7] = struct[7] ? String(struct[7]).substr(0, 3) : 0; // timestamps without timezone identifiers should be considered local time\n\n    if ((struct[8] === undefined || struct[8] === '') && (struct[9] === undefined || struct[9] === '')) timestamp = +new Date(struct[1], struct[2], struct[3], struct[4], struct[5], struct[6], struct[7]);else {\n      if (struct[8] !== 'Z' && struct[9] !== undefined) {\n        minutesOffset = struct[10] * 60 + struct[11];\n        if (struct[9] === '+') minutesOffset = 0 - minutesOffset;\n      }\n\n      timestamp = Date.UTC(struct[1], struct[2], struct[3], struct[4], struct[5] + minutesOffset, struct[6], struct[7]);\n    }\n  } else timestamp = Date.parse ? Date.parse(date) : NaN;\n\n  return timestamp;\n}", "\"use strict\";\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\nexports.create = create;\nexports.default = void 0;\n\nvar _isodate = _interopRequireDefault(require(\"./util/isodate\"));\n\nvar _locale = require(\"./locale\");\n\nvar _isAbsent = _interopRequireDefault(require(\"./util/isAbsent\"));\n\nvar _Reference = _interopRequireDefault(require(\"./Reference\"));\n\nvar _schema = _interopRequireDefault(require(\"./schema\"));\n\nfunction _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { default: obj }; }\n\n// @ts-ignore\nlet invalidDate = new Date('');\n\nlet isDate = obj => Object.prototype.toString.call(obj) === '[object Date]';\n\nfunction create() {\n  return new DateSchema();\n}\n\nclass DateSchema extends _schema.default {\n  constructor() {\n    super({\n      type: 'date'\n    });\n    this.withMutation(() => {\n      this.transform(function (value) {\n        if (this.isType(value)) return value;\n        value = (0, _isodate.default)(value); // 0 is a valid timestamp equivalent to 1970-01-01T00:00:00Z(unix epoch) or before.\n\n        return !isNaN(value) ? new Date(value) : invalidDate;\n      });\n    });\n  }\n\n  _typeCheck(v) {\n    return isDate(v) && !isNaN(v.getTime());\n  }\n\n  prepareParam(ref, name) {\n    let param;\n\n    if (!_Reference.default.isRef(ref)) {\n      let cast = this.cast(ref);\n      if (!this._typeCheck(cast)) throw new TypeError(`\\`${name}\\` must be a Date or a value that can be \\`cast()\\` to a Date`);\n      param = cast;\n    } else {\n      param = ref;\n    }\n\n    return param;\n  }\n\n  min(min, message = _locale.date.min) {\n    let limit = this.prepareParam(min, 'min');\n    return this.test({\n      message,\n      name: 'min',\n      exclusive: true,\n      params: {\n        min\n      },\n\n      test(value) {\n        return (0, _isAbsent.default)(value) || value >= this.resolve(limit);\n      }\n\n    });\n  }\n\n  max(max, message = _locale.date.max) {\n    let limit = this.prepareParam(max, 'max');\n    return this.test({\n      message,\n      name: 'max',\n      exclusive: true,\n      params: {\n        max\n      },\n\n      test(value) {\n        return (0, _isAbsent.default)(value) || value <= this.resolve(limit);\n      }\n\n    });\n  }\n\n}\n\nexports.default = DateSchema;\nDateSchema.INVALID_DATE = invalidDate;\ncreate.prototype = DateSchema.prototype;\ncreate.INVALID_DATE = invalidDate;", "/**\n * A specialized version of `_.reduce` for arrays without support for\n * iteratee shorthands.\n *\n * @private\n * @param {Array} [array] The array to iterate over.\n * @param {Function} iteratee The function invoked per iteration.\n * @param {*} [accumulator] The initial value.\n * @param {boolean} [initAccum] Specify using the first element of `array` as\n *  the initial value.\n * @returns {*} Returns the accumulated value.\n */\nfunction arrayReduce(array, iteratee, accumulator, initAccum) {\n  var index = -1,\n      length = array == null ? 0 : array.length;\n\n  if (initAccum && length) {\n    accumulator = array[++index];\n  }\n  while (++index < length) {\n    accumulator = iteratee(accumulator, array[index], index, array);\n  }\n  return accumulator;\n}\n\nmodule.exports = arrayReduce;\n", "/**\n * The base implementation of `_.propertyOf` without support for deep paths.\n *\n * @private\n * @param {Object} object The object to query.\n * @returns {Function} Returns the new accessor function.\n */\nfunction basePropertyOf(object) {\n  return function(key) {\n    return object == null ? undefined : object[key];\n  };\n}\n\nmodule.exports = basePropertyOf;\n", "var basePropertyOf = require('./_basePropertyOf');\n\n/** Used to map Latin Unicode letters to basic Latin letters. */\nvar deburredLetters = {\n  // Latin-1 Supplement block.\n  '\\xc0': 'A',  '\\xc1': 'A', '\\xc2': 'A', '\\xc3': 'A', '\\xc4': 'A', '\\xc5': 'A',\n  '\\xe0': 'a',  '\\xe1': 'a', '\\xe2': 'a', '\\xe3': 'a', '\\xe4': 'a', '\\xe5': 'a',\n  '\\xc7': 'C',  '\\xe7': 'c',\n  '\\xd0': 'D',  '\\xf0': 'd',\n  '\\xc8': 'E',  '\\xc9': 'E', '\\xca': 'E', '\\xcb': 'E',\n  '\\xe8': 'e',  '\\xe9': 'e', '\\xea': 'e', '\\xeb': 'e',\n  '\\xcc': 'I',  '\\xcd': 'I', '\\xce': 'I', '\\xcf': 'I',\n  '\\xec': 'i',  '\\xed': 'i', '\\xee': 'i', '\\xef': 'i',\n  '\\xd1': 'N',  '\\xf1': 'n',\n  '\\xd2': 'O',  '\\xd3': 'O', '\\xd4': 'O', '\\xd5': 'O', '\\xd6': 'O', '\\xd8': 'O',\n  '\\xf2': 'o',  '\\xf3': 'o', '\\xf4': 'o', '\\xf5': 'o', '\\xf6': 'o', '\\xf8': 'o',\n  '\\xd9': 'U',  '\\xda': 'U', '\\xdb': 'U', '\\xdc': 'U',\n  '\\xf9': 'u',  '\\xfa': 'u', '\\xfb': 'u', '\\xfc': 'u',\n  '\\xdd': 'Y',  '\\xfd': 'y', '\\xff': 'y',\n  '\\xc6': 'Ae', '\\xe6': 'ae',\n  '\\xde': 'Th', '\\xfe': 'th',\n  '\\xdf': 'ss',\n  // Latin Extended-A block.\n  '\\u0100': 'A',  '\\u0102': 'A', '\\u0104': 'A',\n  '\\u0101': 'a',  '\\u0103': 'a', '\\u0105': 'a',\n  '\\u0106': 'C',  '\\u0108': 'C', '\\u010a': 'C', '\\u010c': 'C',\n  '\\u0107': 'c',  '\\u0109': 'c', '\\u010b': 'c', '\\u010d': 'c',\n  '\\u010e': 'D',  '\\u0110': 'D', '\\u010f': 'd', '\\u0111': 'd',\n  '\\u0112': 'E',  '\\u0114': 'E', '\\u0116': 'E', '\\u0118': 'E', '\\u011a': 'E',\n  '\\u0113': 'e',  '\\u0115': 'e', '\\u0117': 'e', '\\u0119': 'e', '\\u011b': 'e',\n  '\\u011c': 'G',  '\\u011e': 'G', '\\u0120': 'G', '\\u0122': 'G',\n  '\\u011d': 'g',  '\\u011f': 'g', '\\u0121': 'g', '\\u0123': 'g',\n  '\\u0124': 'H',  '\\u0126': 'H', '\\u0125': 'h', '\\u0127': 'h',\n  '\\u0128': 'I',  '\\u012a': 'I', '\\u012c': 'I', '\\u012e': 'I', '\\u0130': 'I',\n  '\\u0129': 'i',  '\\u012b': 'i', '\\u012d': 'i', '\\u012f': 'i', '\\u0131': 'i',\n  '\\u0134': 'J',  '\\u0135': 'j',\n  '\\u0136': 'K',  '\\u0137': 'k', '\\u0138': 'k',\n  '\\u0139': 'L',  '\\u013b': 'L', '\\u013d': 'L', '\\u013f': 'L', '\\u0141': 'L',\n  '\\u013a': 'l',  '\\u013c': 'l', '\\u013e': 'l', '\\u0140': 'l', '\\u0142': 'l',\n  '\\u0143': 'N',  '\\u0145': 'N', '\\u0147': 'N', '\\u014a': 'N',\n  '\\u0144': 'n',  '\\u0146': 'n', '\\u0148': 'n', '\\u014b': 'n',\n  '\\u014c': 'O',  '\\u014e': 'O', '\\u0150': 'O',\n  '\\u014d': 'o',  '\\u014f': 'o', '\\u0151': 'o',\n  '\\u0154': 'R',  '\\u0156': 'R', '\\u0158': 'R',\n  '\\u0155': 'r',  '\\u0157': 'r', '\\u0159': 'r',\n  '\\u015a': 'S',  '\\u015c': 'S', '\\u015e': 'S', '\\u0160': 'S',\n  '\\u015b': 's',  '\\u015d': 's', '\\u015f': 's', '\\u0161': 's',\n  '\\u0162': 'T',  '\\u0164': 'T', '\\u0166': 'T',\n  '\\u0163': 't',  '\\u0165': 't', '\\u0167': 't',\n  '\\u0168': 'U',  '\\u016a': 'U', '\\u016c': 'U', '\\u016e': 'U', '\\u0170': 'U', '\\u0172': 'U',\n  '\\u0169': 'u',  '\\u016b': 'u', '\\u016d': 'u', '\\u016f': 'u', '\\u0171': 'u', '\\u0173': 'u',\n  '\\u0174': 'W',  '\\u0175': 'w',\n  '\\u0176': 'Y',  '\\u0177': 'y', '\\u0178': 'Y',\n  '\\u0179': 'Z',  '\\u017b': 'Z', '\\u017d': 'Z',\n  '\\u017a': 'z',  '\\u017c': 'z', '\\u017e': 'z',\n  '\\u0132': 'IJ', '\\u0133': 'ij',\n  '\\u0152': 'Oe', '\\u0153': 'oe',\n  '\\u0149': \"'n\", '\\u017f': 's'\n};\n\n/**\n * Used by `_.deburr` to convert Latin-1 Supplement and Latin Extended-A\n * letters to basic Latin letters.\n *\n * @private\n * @param {string} letter The matched letter to deburr.\n * @returns {string} Returns the deburred letter.\n */\nvar deburrLetter = basePropertyOf(deburredLetters);\n\nmodule.exports = deburrLetter;\n", "var deburrLetter = require('./_deburrLetter'),\n    toString = require('./toString');\n\n/** Used to match Latin Unicode letters (excluding mathematical operators). */\nvar reLatin = /[\\xc0-\\xd6\\xd8-\\xf6\\xf8-\\xff\\u0100-\\u017f]/g;\n\n/** Used to compose unicode character classes. */\nvar rsComboMarksRange = '\\\\u0300-\\\\u036f',\n    reComboHalfMarksRange = '\\\\ufe20-\\\\ufe2f',\n    rsComboSymbolsRange = '\\\\u20d0-\\\\u20ff',\n    rsComboRange = rsComboMarksRange + reComboHalfMarksRange + rsComboSymbolsRange;\n\n/** Used to compose unicode capture groups. */\nvar rsCombo = '[' + rsComboRange + ']';\n\n/**\n * Used to match [combining diacritical marks](https://en.wikipedia.org/wiki/Combining_Diacritical_Marks) and\n * [combining diacritical marks for symbols](https://en.wikipedia.org/wiki/Combining_Diacritical_Marks_for_Symbols).\n */\nvar reComboMark = RegExp(rsCombo, 'g');\n\n/**\n * Deburrs `string` by converting\n * [Latin-1 Supplement](https://en.wikipedia.org/wiki/Latin-1_Supplement_(Unicode_block)#Character_table)\n * and [Latin Extended-A](https://en.wikipedia.org/wiki/Latin_Extended-A)\n * letters to basic Latin letters and removing\n * [combining diacritical marks](https://en.wikipedia.org/wiki/Combining_Diacritical_Marks).\n *\n * @static\n * @memberOf _\n * @since 3.0.0\n * @category String\n * @param {string} [string=''] The string to deburr.\n * @returns {string} Returns the deburred string.\n * @example\n *\n * _.deburr('d\u00E9j\u00E0 vu');\n * // => 'deja vu'\n */\nfunction deburr(string) {\n  string = toString(string);\n  return string && string.replace(reLatin, deburrLetter).replace(reComboMark, '');\n}\n\nmodule.exports = deburr;\n", "/** Used to match words composed of alphanumeric characters. */\nvar reAsciiWord = /[^\\x00-\\x2f\\x3a-\\x40\\x5b-\\x60\\x7b-\\x7f]+/g;\n\n/**\n * Splits an ASCII `string` into an array of its words.\n *\n * @private\n * @param {string} The string to inspect.\n * @returns {Array} Returns the words of `string`.\n */\nfunction asciiWords(string) {\n  return string.match(reAsciiWord) || [];\n}\n\nmodule.exports = asciiWords;\n", "/** Used to detect strings that need a more robust regexp to match words. */\nvar reHasUnicodeWord = /[a-z][A-Z]|[A-Z]{2}[a-z]|[0-9][a-zA-Z]|[a-zA-Z][0-9]|[^a-zA-Z0-9 ]/;\n\n/**\n * Checks if `string` contains a word composed of Unicode symbols.\n *\n * @private\n * @param {string} string The string to inspect.\n * @returns {boolean} Returns `true` if a word is found, else `false`.\n */\nfunction hasUnicodeWord(string) {\n  return reHasUnicodeWord.test(string);\n}\n\nmodule.exports = hasUnicodeWord;\n", "/** Used to compose unicode character classes. */\nvar rsAstralRange = '\\\\ud800-\\\\udfff',\n    rsComboMarksRange = '\\\\u0300-\\\\u036f',\n    reComboHalfMarksRange = '\\\\ufe20-\\\\ufe2f',\n    rsComboSymbolsRange = '\\\\u20d0-\\\\u20ff',\n    rsComboRange = rsComboMarksRange + reComboHalfMarksRange + rsComboSymbolsRange,\n    rsDingbatRange = '\\\\u2700-\\\\u27bf',\n    rsLowerRange = 'a-z\\\\xdf-\\\\xf6\\\\xf8-\\\\xff',\n    rsMathOpRange = '\\\\xac\\\\xb1\\\\xd7\\\\xf7',\n    rsNonCharRange = '\\\\x00-\\\\x2f\\\\x3a-\\\\x40\\\\x5b-\\\\x60\\\\x7b-\\\\xbf',\n    rsPunctuationRange = '\\\\u2000-\\\\u206f',\n    rsSpaceRange = ' \\\\t\\\\x0b\\\\f\\\\xa0\\\\ufeff\\\\n\\\\r\\\\u2028\\\\u2029\\\\u1680\\\\u180e\\\\u2000\\\\u2001\\\\u2002\\\\u2003\\\\u2004\\\\u2005\\\\u2006\\\\u2007\\\\u2008\\\\u2009\\\\u200a\\\\u202f\\\\u205f\\\\u3000',\n    rsUpperRange = 'A-Z\\\\xc0-\\\\xd6\\\\xd8-\\\\xde',\n    rsVarRange = '\\\\ufe0e\\\\ufe0f',\n    rsBreakRange = rsMathOpRange + rsNonCharRange + rsPunctuationRange + rsSpaceRange;\n\n/** Used to compose unicode capture groups. */\nvar rsApos = \"['\\u2019]\",\n    rsBreak = '[' + rsBreakRange + ']',\n    rsCombo = '[' + rsComboRange + ']',\n    rsDigits = '\\\\d+',\n    rsDingbat = '[' + rsDingbatRange + ']',\n    rsLower = '[' + rsLowerRange + ']',\n    rsMisc = '[^' + rsAstralRange + rsBreakRange + rsDigits + rsDingbatRange + rsLowerRange + rsUpperRange + ']',\n    rsFitz = '\\\\ud83c[\\\\udffb-\\\\udfff]',\n    rsModifier = '(?:' + rsCombo + '|' + rsFitz + ')',\n    rsNonAstral = '[^' + rsAstralRange + ']',\n    rsRegional = '(?:\\\\ud83c[\\\\udde6-\\\\uddff]){2}',\n    rsSurrPair = '[\\\\ud800-\\\\udbff][\\\\udc00-\\\\udfff]',\n    rsUpper = '[' + rsUpperRange + ']',\n    rsZWJ = '\\\\u200d';\n\n/** Used to compose unicode regexes. */\nvar rsMiscLower = '(?:' + rsLower + '|' + rsMisc + ')',\n    rsMiscUpper = '(?:' + rsUpper + '|' + rsMisc + ')',\n    rsOptContrLower = '(?:' + rsApos + '(?:d|ll|m|re|s|t|ve))?',\n    rsOptContrUpper = '(?:' + rsApos + '(?:D|LL|M|RE|S|T|VE))?',\n    reOptMod = rsModifier + '?',\n    rsOptVar = '[' + rsVarRange + ']?',\n    rsOptJoin = '(?:' + rsZWJ + '(?:' + [rsNonAstral, rsRegional, rsSurrPair].join('|') + ')' + rsOptVar + reOptMod + ')*',\n    rsOrdLower = '\\\\d*(?:1st|2nd|3rd|(?![123])\\\\dth)(?=\\\\b|[A-Z_])',\n    rsOrdUpper = '\\\\d*(?:1ST|2ND|3RD|(?![123])\\\\dTH)(?=\\\\b|[a-z_])',\n    rsSeq = rsOptVar + reOptMod + rsOptJoin,\n    rsEmoji = '(?:' + [rsDingbat, rsRegional, rsSurrPair].join('|') + ')' + rsSeq;\n\n/** Used to match complex or compound words. */\nvar reUnicodeWord = RegExp([\n  rsUpper + '?' + rsLower + '+' + rsOptContrLower + '(?=' + [rsBreak, rsUpper, '$'].join('|') + ')',\n  rsMiscUpper + '+' + rsOptContrUpper + '(?=' + [rsBreak, rsUpper + rsMiscLower, '$'].join('|') + ')',\n  rsUpper + '?' + rsMiscLower + '+' + rsOptContrLower,\n  rsUpper + '+' + rsOptContrUpper,\n  rsOrdUpper,\n  rsOrdLower,\n  rsDigits,\n  rsEmoji\n].join('|'), 'g');\n\n/**\n * Splits a Unicode `string` into an array of its words.\n *\n * @private\n * @param {string} The string to inspect.\n * @returns {Array} Returns the words of `string`.\n */\nfunction unicodeWords(string) {\n  return string.match(reUnicodeWord) || [];\n}\n\nmodule.exports = unicodeWords;\n", "var asciiWords = require('./_asciiWords'),\n    hasUnicodeWord = require('./_hasUnicodeWord'),\n    toString = require('./toString'),\n    unicodeWords = require('./_unicodeWords');\n\n/**\n * Splits `string` into an array of its words.\n *\n * @static\n * @memberOf _\n * @since 3.0.0\n * @category String\n * @param {string} [string=''] The string to inspect.\n * @param {RegExp|string} [pattern] The pattern to match words.\n * @param- {Object} [guard] Enables use as an iteratee for methods like `_.map`.\n * @returns {Array} Returns the words of `string`.\n * @example\n *\n * _.words('fred, barney, & pebbles');\n * // => ['fred', 'barney', 'pebbles']\n *\n * _.words('fred, barney, & pebbles', /[^, ]+/g);\n * // => ['fred', 'barney', '&', 'pebbles']\n */\nfunction words(string, pattern, guard) {\n  string = toString(string);\n  pattern = guard ? undefined : pattern;\n\n  if (pattern === undefined) {\n    return hasUnicodeWord(string) ? unicodeWords(string) : asciiWords(string);\n  }\n  return string.match(pattern) || [];\n}\n\nmodule.exports = words;\n", "var arrayReduce = require('./_arrayReduce'),\n    deburr = require('./deburr'),\n    words = require('./words');\n\n/** Used to compose unicode capture groups. */\nvar rsApos = \"['\\u2019]\";\n\n/** Used to match apostrophes. */\nvar reApos = RegExp(rsApos, 'g');\n\n/**\n * Creates a function like `_.camelCase`.\n *\n * @private\n * @param {Function} callback The function to combine each word.\n * @returns {Function} Returns the new compounder function.\n */\nfunction createCompounder(callback) {\n  return function(string) {\n    return arrayReduce(words(deburr(string).replace(reApos, '')), callback, '');\n  };\n}\n\nmodule.exports = createCompounder;\n", "var createCompounder = require('./_createCompounder');\n\n/**\n * Converts `string` to\n * [snake case](https://en.wikipedia.org/wiki/Snake_case).\n *\n * @static\n * @memberOf _\n * @since 3.0.0\n * @category String\n * @param {string} [string=''] The string to convert.\n * @returns {string} Returns the snake cased string.\n * @example\n *\n * _.snakeCase('Foo Bar');\n * // => 'foo_bar'\n *\n * _.snakeCase('fooBar');\n * // => 'foo_bar'\n *\n * _.snakeCase('--FOO-BAR--');\n * // => 'foo_bar'\n */\nvar snakeCase = createCompounder(function(result, word, index) {\n  return result + (index ? '_' : '') + word.toLowerCase();\n});\n\nmodule.exports = snakeCase;\n", "/**\n * The base implementation of `_.slice` without an iteratee call guard.\n *\n * @private\n * @param {Array} array The array to slice.\n * @param {number} [start=0] The start position.\n * @param {number} [end=array.length] The end position.\n * @returns {Array} Returns the slice of `array`.\n */\nfunction baseSlice(array, start, end) {\n  var index = -1,\n      length = array.length;\n\n  if (start < 0) {\n    start = -start > length ? 0 : (length + start);\n  }\n  end = end > length ? length : end;\n  if (end < 0) {\n    end += length;\n  }\n  length = start > end ? 0 : ((end - start) >>> 0);\n  start >>>= 0;\n\n  var result = Array(length);\n  while (++index < length) {\n    result[index] = array[index + start];\n  }\n  return result;\n}\n\nmodule.exports = baseSlice;\n", "var baseSlice = require('./_baseSlice');\n\n/**\n * Casts `array` to a slice if it's needed.\n *\n * @private\n * @param {Array} array The array to inspect.\n * @param {number} start The start position.\n * @param {number} [end=array.length] The end position.\n * @returns {Array} Returns the cast slice.\n */\nfunction castSlice(array, start, end) {\n  var length = array.length;\n  end = end === undefined ? length : end;\n  return (!start && end >= length) ? array : baseSlice(array, start, end);\n}\n\nmodule.exports = castSlice;\n", "/** Used to compose unicode character classes. */\nvar rsAstralRange = '\\\\ud800-\\\\udfff',\n    rsComboMarksRange = '\\\\u0300-\\\\u036f',\n    reComboHalfMarksRange = '\\\\ufe20-\\\\ufe2f',\n    rsComboSymbolsRange = '\\\\u20d0-\\\\u20ff',\n    rsComboRange = rsComboMarksRange + reComboHalfMarksRange + rsComboSymbolsRange,\n    rsVarRange = '\\\\ufe0e\\\\ufe0f';\n\n/** Used to compose unicode capture groups. */\nvar rsZWJ = '\\\\u200d';\n\n/** Used to detect strings with [zero-width joiners or code points from the astral planes](http://eev.ee/blog/2015/09/12/dark-corners-of-unicode/). */\nvar reHasUnicode = RegExp('[' + rsZWJ + rsAstralRange  + rsComboRange + rsVarRange + ']');\n\n/**\n * Checks if `string` contains Unicode symbols.\n *\n * @private\n * @param {string} string The string to inspect.\n * @returns {boolean} Returns `true` if a symbol is found, else `false`.\n */\nfunction hasUnicode(string) {\n  return reHasUnicode.test(string);\n}\n\nmodule.exports = hasUnicode;\n", "/**\n * Converts an ASCII `string` to an array.\n *\n * @private\n * @param {string} string The string to convert.\n * @returns {Array} Returns the converted array.\n */\nfunction asciiToArray(string) {\n  return string.split('');\n}\n\nmodule.exports = asciiToArray;\n", "/** Used to compose unicode character classes. */\nvar rsAstralRange = '\\\\ud800-\\\\udfff',\n    rsComboMarksRange = '\\\\u0300-\\\\u036f',\n    reComboHalfMarksRange = '\\\\ufe20-\\\\ufe2f',\n    rsComboSymbolsRange = '\\\\u20d0-\\\\u20ff',\n    rsComboRange = rsComboMarksRange + reComboHalfMarksRange + rsComboSymbolsRange,\n    rsVarRange = '\\\\ufe0e\\\\ufe0f';\n\n/** Used to compose unicode capture groups. */\nvar rsAstral = '[' + rsAstralRange + ']',\n    rsCombo = '[' + rsComboRange + ']',\n    rsFitz = '\\\\ud83c[\\\\udffb-\\\\udfff]',\n    rsModifier = '(?:' + rsCombo + '|' + rsFitz + ')',\n    rsNonAstral = '[^' + rsAstralRange + ']',\n    rsRegional = '(?:\\\\ud83c[\\\\udde6-\\\\uddff]){2}',\n    rsSurrPair = '[\\\\ud800-\\\\udbff][\\\\udc00-\\\\udfff]',\n    rsZWJ = '\\\\u200d';\n\n/** Used to compose unicode regexes. */\nvar reOptMod = rsModifier + '?',\n    rsOptVar = '[' + rsVarRange + ']?',\n    rsOptJoin = '(?:' + rsZWJ + '(?:' + [rsNonAstral, rsRegional, rsSurrPair].join('|') + ')' + rsOptVar + reOptMod + ')*',\n    rsSeq = rsOptVar + reOptMod + rsOptJoin,\n    rsSymbol = '(?:' + [rsNonAstral + rsCombo + '?', rsCombo, rsRegional, rsSurrPair, rsAstral].join('|') + ')';\n\n/** Used to match [string symbols](https://mathiasbynens.be/notes/javascript-unicode). */\nvar reUnicode = RegExp(rsFitz + '(?=' + rsFitz + ')|' + rsSymbol + rsSeq, 'g');\n\n/**\n * Converts a Unicode `string` to an array.\n *\n * @private\n * @param {string} string The string to convert.\n * @returns {Array} Returns the converted array.\n */\nfunction unicodeToArray(string) {\n  return string.match(reUnicode) || [];\n}\n\nmodule.exports = unicodeToArray;\n", "var asciiToArray = require('./_asciiToArray'),\n    hasUnicode = require('./_hasUnicode'),\n    unicodeToArray = require('./_unicodeToArray');\n\n/**\n * Converts `string` to an array.\n *\n * @private\n * @param {string} string The string to convert.\n * @returns {Array} Returns the converted array.\n */\nfunction stringToArray(string) {\n  return hasUnicode(string)\n    ? unicodeToArray(string)\n    : asciiToArray(string);\n}\n\nmodule.exports = stringToArray;\n", "var castSlice = require('./_castSlice'),\n    hasUnicode = require('./_hasUnicode'),\n    stringToArray = require('./_stringToArray'),\n    toString = require('./toString');\n\n/**\n * Creates a function like `_.lowerFirst`.\n *\n * @private\n * @param {string} methodName The name of the `String` case method to use.\n * @returns {Function} Returns the new case function.\n */\nfunction createCaseFirst(methodName) {\n  return function(string) {\n    string = toString(string);\n\n    var strSymbols = hasUnicode(string)\n      ? stringToArray(string)\n      : undefined;\n\n    var chr = strSymbols\n      ? strSymbols[0]\n      : string.charAt(0);\n\n    var trailing = strSymbols\n      ? castSlice(strSymbols, 1).join('')\n      : string.slice(1);\n\n    return chr[methodName]() + trailing;\n  };\n}\n\nmodule.exports = createCaseFirst;\n", "var createCaseFirst = require('./_createCaseFirst');\n\n/**\n * Converts the first character of `string` to upper case.\n *\n * @static\n * @memberOf _\n * @since 4.0.0\n * @category String\n * @param {string} [string=''] The string to convert.\n * @returns {string} Returns the converted string.\n * @example\n *\n * _.upperFirst('fred');\n * // => 'Fred'\n *\n * _.upperFirst('FRED');\n * // => 'FRED'\n */\nvar upperFirst = createCaseFirst('toUpperCase');\n\nmodule.exports = upperFirst;\n", "var toString = require('./toString'),\n    upperFirst = require('./upperFirst');\n\n/**\n * Converts the first character of `string` to upper case and the remaining\n * to lower case.\n *\n * @static\n * @memberOf _\n * @since 3.0.0\n * @category String\n * @param {string} [string=''] The string to capitalize.\n * @returns {string} Returns the capitalized string.\n * @example\n *\n * _.capitalize('FRED');\n * // => 'Fred'\n */\nfunction capitalize(string) {\n  return upperFirst(toString(string).toLowerCase());\n}\n\nmodule.exports = capitalize;\n", "var capitalize = require('./capitalize'),\n    createCompounder = require('./_createCompounder');\n\n/**\n * Converts `string` to [camel case](https://en.wikipedia.org/wiki/CamelCase).\n *\n * @static\n * @memberOf _\n * @since 3.0.0\n * @category String\n * @param {string} [string=''] The string to convert.\n * @returns {string} Returns the camel cased string.\n * @example\n *\n * _.camelCase('Foo Bar');\n * // => 'fooBar'\n *\n * _.camelCase('--foo-bar--');\n * // => 'fooBar'\n *\n * _.camelCase('__FOO_BAR__');\n * // => 'fooBar'\n */\nvar camelCase = createCompounder(function(result, word, index) {\n  word = word.toLowerCase();\n  return result + (index ? capitalize(word) : word);\n});\n\nmodule.exports = camelCase;\n", "var baseAssignValue = require('./_baseAssignValue'),\n    baseForOwn = require('./_baseForOwn'),\n    baseIteratee = require('./_baseIteratee');\n\n/**\n * The opposite of `_.mapValues`; this method creates an object with the\n * same values as `object` and keys generated by running each own enumerable\n * string keyed property of `object` thru `iteratee`. The iteratee is invoked\n * with three arguments: (value, key, object).\n *\n * @static\n * @memberOf _\n * @since 3.8.0\n * @category Object\n * @param {Object} object The object to iterate over.\n * @param {Function} [iteratee=_.identity] The function invoked per iteration.\n * @returns {Object} Returns the new mapped object.\n * @see _.mapValues\n * @example\n *\n * _.mapKeys({ 'a': 1, 'b': 2 }, function(value, key) {\n *   return key + value;\n * });\n * // => { 'a1': 1, 'b2': 2 }\n */\nfunction mapKeys(object, iteratee) {\n  var result = {};\n  iteratee = baseIteratee(iteratee, 3);\n\n  baseForOwn(object, function(value, key, object) {\n    baseAssignValue(result, iteratee(value, key, object), value);\n  });\n  return result;\n}\n\nmodule.exports = mapKeys;\n", "\n/**\n * Topological sorting function\n *\n * @param {Array} edges\n * @returns {Array}\n */\n\nmodule.exports = function(edges) {\n  return toposort(uniqueNodes(edges), edges)\n}\n\nmodule.exports.array = toposort\n\nfunction toposort(nodes, edges) {\n  var cursor = nodes.length\n    , sorted = new Array(cursor)\n    , visited = {}\n    , i = cursor\n    // Better data structures make algorithm much faster.\n    , outgoingEdges = makeOutgoingEdges(edges)\n    , nodesHash = makeNodesHash(nodes)\n\n  // check for unknown nodes\n  edges.forEach(function(edge) {\n    if (!nodesHash.has(edge[0]) || !nodesHash.has(edge[1])) {\n      throw new Error('Unknown node. There is an unknown node in the supplied edges.')\n    }\n  })\n\n  while (i--) {\n    if (!visited[i]) visit(nodes[i], i, new Set())\n  }\n\n  return sorted\n\n  function visit(node, i, predecessors) {\n    if(predecessors.has(node)) {\n      var nodeRep\n      try {\n        nodeRep = \", node was:\" + JSON.stringify(node)\n      } catch(e) {\n        nodeRep = \"\"\n      }\n      throw new Error('Cyclic dependency' + nodeRep)\n    }\n\n    if (!nodesHash.has(node)) {\n      throw new Error('Found unknown node. Make sure to provided all involved nodes. Unknown node: '+JSON.stringify(node))\n    }\n\n    if (visited[i]) return;\n    visited[i] = true\n\n    var outgoing = outgoingEdges.get(node) || new Set()\n    outgoing = Array.from(outgoing)\n\n    if (i = outgoing.length) {\n      predecessors.add(node)\n      do {\n        var child = outgoing[--i]\n        visit(child, nodesHash.get(child), predecessors)\n      } while (i)\n      predecessors.delete(node)\n    }\n\n    sorted[--cursor] = node\n  }\n}\n\nfunction uniqueNodes(arr){\n  var res = new Set()\n  for (var i = 0, len = arr.length; i < len; i++) {\n    var edge = arr[i]\n    res.add(edge[0])\n    res.add(edge[1])\n  }\n  return Array.from(res)\n}\n\nfunction makeOutgoingEdges(arr){\n  var edges = new Map()\n  for (var i = 0, len = arr.length; i < len; i++) {\n    var edge = arr[i]\n    if (!edges.has(edge[0])) edges.set(edge[0], new Set())\n    if (!edges.has(edge[1])) edges.set(edge[1], new Set())\n    edges.get(edge[0]).add(edge[1])\n  }\n  return edges\n}\n\nfunction makeNodesHash(arr){\n  var res = new Map()\n  for (var i = 0, len = arr.length; i < len; i++) {\n    res.set(arr[i], i)\n  }\n  return res\n}\n", "\"use strict\";\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\nexports.default = sortFields;\n\nvar _has = _interopRequireDefault(require(\"lodash/has\"));\n\nvar _toposort = _interopRequireDefault(require(\"toposort\"));\n\nvar _propertyExpr = require(\"property-expr\");\n\nvar _Reference = _interopRequireDefault(require(\"../Reference\"));\n\nvar _isSchema = _interopRequireDefault(require(\"./isSchema\"));\n\nfunction _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { default: obj }; }\n\n// @ts-expect-error\nfunction sortFields(fields, excludedEdges = []) {\n  let edges = [];\n  let nodes = new Set();\n  let excludes = new Set(excludedEdges.map(([a, b]) => `${a}-${b}`));\n\n  function addNode(depPath, key) {\n    let node = (0, _propertyExpr.split)(depPath)[0];\n    nodes.add(node);\n    if (!excludes.has(`${key}-${node}`)) edges.push([key, node]);\n  }\n\n  for (const key in fields) if ((0, _has.default)(fields, key)) {\n    let value = fields[key];\n    nodes.add(key);\n    if (_Reference.default.isRef(value) && value.isSibling) addNode(value.path, key);else if ((0, _isSchema.default)(value) && 'deps' in value) value.deps.forEach(path => addNode(path, key));\n  }\n\n  return _toposort.default.array(Array.from(nodes), edges).reverse();\n}", "\"use strict\";\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\nexports.default = sortByKeyOrder;\n\nfunction findIndex(arr, err) {\n  let idx = Infinity;\n  arr.some((key, ii) => {\n    var _err$path;\n\n    if (((_err$path = err.path) == null ? void 0 : _err$path.indexOf(key)) !== -1) {\n      idx = ii;\n      return true;\n    }\n  });\n  return idx;\n}\n\nfunction sortByKeyOrder(keys) {\n  return (a, b) => {\n    return findIndex(keys, a) - findIndex(keys, b);\n  };\n}", "\"use strict\";\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\nexports.create = create;\nexports.default = void 0;\n\nvar _has = _interopRequireDefault(require(\"lodash/has\"));\n\nvar _snakeCase = _interopRequireDefault(require(\"lodash/snakeCase\"));\n\nvar _camelCase = _interopRequireDefault(require(\"lodash/camelCase\"));\n\nvar _mapKeys = _interopRequireDefault(require(\"lodash/mapKeys\"));\n\nvar _mapValues = _interopRequireDefault(require(\"lodash/mapValues\"));\n\nvar _propertyExpr = require(\"property-expr\");\n\nvar _locale = require(\"./locale\");\n\nvar _sortFields = _interopRequireDefault(require(\"./util/sortFields\"));\n\nvar _sortByKeyOrder = _interopRequireDefault(require(\"./util/sortByKeyOrder\"));\n\nvar _runTests = _interopRequireDefault(require(\"./util/runTests\"));\n\nvar _ValidationError = _interopRequireDefault(require(\"./ValidationError\"));\n\nvar _schema = _interopRequireDefault(require(\"./schema\"));\n\nfunction _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { default: obj }; }\n\nfunction _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nlet isObject = obj => Object.prototype.toString.call(obj) === '[object Object]';\n\nfunction unknown(ctx, value) {\n  let known = Object.keys(ctx.fields);\n  return Object.keys(value).filter(key => known.indexOf(key) === -1);\n}\n\nconst defaultSort = (0, _sortByKeyOrder.default)([]);\n\nclass ObjectSchema extends _schema.default {\n  constructor(spec) {\n    super({\n      type: 'object'\n    });\n    this.fields = Object.create(null);\n    this._sortErrors = defaultSort;\n    this._nodes = [];\n    this._excludedEdges = [];\n    this.withMutation(() => {\n      this.transform(function coerce(value) {\n        if (typeof value === 'string') {\n          try {\n            value = JSON.parse(value);\n          } catch (err) {\n            value = null;\n          }\n        }\n\n        if (this.isType(value)) return value;\n        return null;\n      });\n\n      if (spec) {\n        this.shape(spec);\n      }\n    });\n  }\n\n  _typeCheck(value) {\n    return isObject(value) || typeof value === 'function';\n  }\n\n  _cast(_value, options = {}) {\n    var _options$stripUnknown;\n\n    let value = super._cast(_value, options); //should ignore nulls here\n\n\n    if (value === undefined) return this.getDefault();\n    if (!this._typeCheck(value)) return value;\n    let fields = this.fields;\n    let strip = (_options$stripUnknown = options.stripUnknown) != null ? _options$stripUnknown : this.spec.noUnknown;\n\n    let props = this._nodes.concat(Object.keys(value).filter(v => this._nodes.indexOf(v) === -1));\n\n    let intermediateValue = {}; // is filled during the transform below\n\n    let innerOptions = _extends({}, options, {\n      parent: intermediateValue,\n      __validating: options.__validating || false\n    });\n\n    let isChanged = false;\n\n    for (const prop of props) {\n      let field = fields[prop];\n      let exists = (0, _has.default)(value, prop);\n\n      if (field) {\n        let fieldValue;\n        let inputValue = value[prop]; // safe to mutate since this is fired in sequence\n\n        innerOptions.path = (options.path ? `${options.path}.` : '') + prop; // innerOptions.value = value[prop];\n\n        field = field.resolve({\n          value: inputValue,\n          context: options.context,\n          parent: intermediateValue\n        });\n        let fieldSpec = 'spec' in field ? field.spec : undefined;\n        let strict = fieldSpec == null ? void 0 : fieldSpec.strict;\n\n        if (fieldSpec == null ? void 0 : fieldSpec.strip) {\n          isChanged = isChanged || prop in value;\n          continue;\n        }\n\n        fieldValue = !options.__validating || !strict ? // TODO: use _cast, this is double resolving\n        field.cast(value[prop], innerOptions) : value[prop];\n\n        if (fieldValue !== undefined) {\n          intermediateValue[prop] = fieldValue;\n        }\n      } else if (exists && !strip) {\n        intermediateValue[prop] = value[prop];\n      }\n\n      if (intermediateValue[prop] !== value[prop]) {\n        isChanged = true;\n      }\n    }\n\n    return isChanged ? intermediateValue : value;\n  }\n\n  _validate(_value, opts = {}, callback) {\n    let errors = [];\n    let {\n      sync,\n      from = [],\n      originalValue = _value,\n      abortEarly = this.spec.abortEarly,\n      recursive = this.spec.recursive\n    } = opts;\n    from = [{\n      schema: this,\n      value: originalValue\n    }, ...from]; // this flag is needed for handling `strict` correctly in the context of\n    // validation vs just casting. e.g strict() on a field is only used when validating\n\n    opts.__validating = true;\n    opts.originalValue = originalValue;\n    opts.from = from;\n\n    super._validate(_value, opts, (err, value) => {\n      if (err) {\n        if (!_ValidationError.default.isError(err) || abortEarly) {\n          return void callback(err, value);\n        }\n\n        errors.push(err);\n      }\n\n      if (!recursive || !isObject(value)) {\n        callback(errors[0] || null, value);\n        return;\n      }\n\n      originalValue = originalValue || value;\n\n      let tests = this._nodes.map(key => (_, cb) => {\n        let path = key.indexOf('.') === -1 ? (opts.path ? `${opts.path}.` : '') + key : `${opts.path || ''}[\"${key}\"]`;\n        let field = this.fields[key];\n\n        if (field && 'validate' in field) {\n          field.validate(value[key], _extends({}, opts, {\n            // @ts-ignore\n            path,\n            from,\n            // inner fields are always strict:\n            // 1. this isn't strict so the casting will also have cast inner values\n            // 2. this is strict in which case the nested values weren't cast either\n            strict: true,\n            parent: value,\n            originalValue: originalValue[key]\n          }), cb);\n          return;\n        }\n\n        cb(null);\n      });\n\n      (0, _runTests.default)({\n        sync,\n        tests,\n        value,\n        errors,\n        endEarly: abortEarly,\n        sort: this._sortErrors,\n        path: opts.path\n      }, callback);\n    });\n  }\n\n  clone(spec) {\n    const next = super.clone(spec);\n    next.fields = _extends({}, this.fields);\n    next._nodes = this._nodes;\n    next._excludedEdges = this._excludedEdges;\n    next._sortErrors = this._sortErrors;\n    return next;\n  }\n\n  concat(schema) {\n    let next = super.concat(schema);\n    let nextFields = next.fields;\n\n    for (let [field, schemaOrRef] of Object.entries(this.fields)) {\n      const target = nextFields[field];\n\n      if (target === undefined) {\n        nextFields[field] = schemaOrRef;\n      } else if (target instanceof _schema.default && schemaOrRef instanceof _schema.default) {\n        nextFields[field] = schemaOrRef.concat(target);\n      }\n    }\n\n    return next.withMutation(() => next.shape(nextFields, this._excludedEdges));\n  }\n\n  getDefaultFromShape() {\n    let dft = {};\n\n    this._nodes.forEach(key => {\n      const field = this.fields[key];\n      dft[key] = 'default' in field ? field.getDefault() : undefined;\n    });\n\n    return dft;\n  }\n\n  _getDefault() {\n    if ('default' in this.spec) {\n      return super._getDefault();\n    } // if there is no default set invent one\n\n\n    if (!this._nodes.length) {\n      return undefined;\n    }\n\n    return this.getDefaultFromShape();\n  }\n\n  shape(additions, excludes = []) {\n    let next = this.clone();\n    let fields = Object.assign(next.fields, additions);\n    next.fields = fields;\n    next._sortErrors = (0, _sortByKeyOrder.default)(Object.keys(fields));\n\n    if (excludes.length) {\n      // this is a convenience for when users only supply a single pair\n      if (!Array.isArray(excludes[0])) excludes = [excludes];\n      next._excludedEdges = [...next._excludedEdges, ...excludes];\n    }\n\n    next._nodes = (0, _sortFields.default)(fields, next._excludedEdges);\n    return next;\n  }\n\n  pick(keys) {\n    const picked = {};\n\n    for (const key of keys) {\n      if (this.fields[key]) picked[key] = this.fields[key];\n    }\n\n    return this.clone().withMutation(next => {\n      next.fields = {};\n      return next.shape(picked);\n    });\n  }\n\n  omit(keys) {\n    const next = this.clone();\n    const fields = next.fields;\n    next.fields = {};\n\n    for (const key of keys) {\n      delete fields[key];\n    }\n\n    return next.withMutation(() => next.shape(fields));\n  }\n\n  from(from, to, alias) {\n    let fromGetter = (0, _propertyExpr.getter)(from, true);\n    return this.transform(obj => {\n      if (obj == null) return obj;\n      let newObj = obj;\n\n      if ((0, _has.default)(obj, from)) {\n        newObj = _extends({}, obj);\n        if (!alias) delete newObj[from];\n        newObj[to] = fromGetter(obj);\n      }\n\n      return newObj;\n    });\n  }\n\n  noUnknown(noAllow = true, message = _locale.object.noUnknown) {\n    if (typeof noAllow === 'string') {\n      message = noAllow;\n      noAllow = true;\n    }\n\n    let next = this.test({\n      name: 'noUnknown',\n      exclusive: true,\n      message: message,\n\n      test(value) {\n        if (value == null) return true;\n        const unknownKeys = unknown(this.schema, value);\n        return !noAllow || unknownKeys.length === 0 || this.createError({\n          params: {\n            unknown: unknownKeys.join(', ')\n          }\n        });\n      }\n\n    });\n    next.spec.noUnknown = noAllow;\n    return next;\n  }\n\n  unknown(allow = true, message = _locale.object.noUnknown) {\n    return this.noUnknown(!allow, message);\n  }\n\n  transformKeys(fn) {\n    return this.transform(obj => obj && (0, _mapKeys.default)(obj, (_, key) => fn(key)));\n  }\n\n  camelCase() {\n    return this.transformKeys(_camelCase.default);\n  }\n\n  snakeCase() {\n    return this.transformKeys(_snakeCase.default);\n  }\n\n  constantCase() {\n    return this.transformKeys(key => (0, _snakeCase.default)(key).toUpperCase());\n  }\n\n  describe() {\n    let base = super.describe();\n    base.fields = (0, _mapValues.default)(this.fields, value => value.describe());\n    return base;\n  }\n\n}\n\nexports.default = ObjectSchema;\n\nfunction create(spec) {\n  return new ObjectSchema(spec);\n}\n\ncreate.prototype = ObjectSchema.prototype;", "\"use strict\";\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\nexports.create = create;\nexports.default = void 0;\n\nvar _isAbsent = _interopRequireDefault(require(\"./util/isAbsent\"));\n\nvar _isSchema = _interopRequireDefault(require(\"./util/isSchema\"));\n\nvar _printValue = _interopRequireDefault(require(\"./util/printValue\"));\n\nvar _locale = require(\"./locale\");\n\nvar _runTests = _interopRequireDefault(require(\"./util/runTests\"));\n\nvar _ValidationError = _interopRequireDefault(require(\"./ValidationError\"));\n\nvar _schema = _interopRequireDefault(require(\"./schema\"));\n\nfunction _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { default: obj }; }\n\nfunction _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction create(type) {\n  return new ArraySchema(type);\n}\n\nclass ArraySchema extends _schema.default {\n  constructor(type) {\n    super({\n      type: 'array'\n    }); // `undefined` specifically means uninitialized, as opposed to\n    // \"no subtype\"\n\n    this.innerType = void 0;\n    this.innerType = type;\n    this.withMutation(() => {\n      this.transform(function (values) {\n        if (typeof values === 'string') try {\n          values = JSON.parse(values);\n        } catch (err) {\n          values = null;\n        }\n        return this.isType(values) ? values : null;\n      });\n    });\n  }\n\n  _typeCheck(v) {\n    return Array.isArray(v);\n  }\n\n  get _subType() {\n    return this.innerType;\n  }\n\n  _cast(_value, _opts) {\n    const value = super._cast(_value, _opts); //should ignore nulls here\n\n\n    if (!this._typeCheck(value) || !this.innerType) return value;\n    let isChanged = false;\n    const castArray = value.map((v, idx) => {\n      const castElement = this.innerType.cast(v, _extends({}, _opts, {\n        path: `${_opts.path || ''}[${idx}]`\n      }));\n\n      if (castElement !== v) {\n        isChanged = true;\n      }\n\n      return castElement;\n    });\n    return isChanged ? castArray : value;\n  }\n\n  _validate(_value, options = {}, callback) {\n    var _options$abortEarly, _options$recursive;\n\n    let errors = [];\n    let sync = options.sync;\n    let path = options.path;\n    let innerType = this.innerType;\n    let endEarly = (_options$abortEarly = options.abortEarly) != null ? _options$abortEarly : this.spec.abortEarly;\n    let recursive = (_options$recursive = options.recursive) != null ? _options$recursive : this.spec.recursive;\n    let originalValue = options.originalValue != null ? options.originalValue : _value;\n\n    super._validate(_value, options, (err, value) => {\n      if (err) {\n        if (!_ValidationError.default.isError(err) || endEarly) {\n          return void callback(err, value);\n        }\n\n        errors.push(err);\n      }\n\n      if (!recursive || !innerType || !this._typeCheck(value)) {\n        callback(errors[0] || null, value);\n        return;\n      }\n\n      originalValue = originalValue || value; // #950 Ensure that sparse array empty slots are validated\n\n      let tests = new Array(value.length);\n\n      for (let idx = 0; idx < value.length; idx++) {\n        let item = value[idx];\n        let path = `${options.path || ''}[${idx}]`; // object._validate note for isStrict explanation\n\n        let innerOptions = _extends({}, options, {\n          path,\n          strict: true,\n          parent: value,\n          index: idx,\n          originalValue: originalValue[idx]\n        });\n\n        tests[idx] = (_, cb) => innerType.validate(item, innerOptions, cb);\n      }\n\n      (0, _runTests.default)({\n        sync,\n        path,\n        value,\n        errors,\n        endEarly,\n        tests\n      }, callback);\n    });\n  }\n\n  clone(spec) {\n    const next = super.clone(spec);\n    next.innerType = this.innerType;\n    return next;\n  }\n\n  concat(schema) {\n    let next = super.concat(schema);\n    next.innerType = this.innerType;\n    if (schema.innerType) next.innerType = next.innerType ? // @ts-expect-error Lazy doesn't have concat()\n    next.innerType.concat(schema.innerType) : schema.innerType;\n    return next;\n  }\n\n  of(schema) {\n    // FIXME: this should return a new instance of array without the default to be\n    let next = this.clone();\n    if (!(0, _isSchema.default)(schema)) throw new TypeError('`array.of()` sub-schema must be a valid yup schema not: ' + (0, _printValue.default)(schema)); // FIXME(ts):\n\n    next.innerType = schema;\n    return next;\n  }\n\n  length(length, message = _locale.array.length) {\n    return this.test({\n      message,\n      name: 'length',\n      exclusive: true,\n      params: {\n        length\n      },\n\n      test(value) {\n        return (0, _isAbsent.default)(value) || value.length === this.resolve(length);\n      }\n\n    });\n  }\n\n  min(min, message) {\n    message = message || _locale.array.min;\n    return this.test({\n      message,\n      name: 'min',\n      exclusive: true,\n      params: {\n        min\n      },\n\n      // FIXME(ts): Array<typeof T>\n      test(value) {\n        return (0, _isAbsent.default)(value) || value.length >= this.resolve(min);\n      }\n\n    });\n  }\n\n  max(max, message) {\n    message = message || _locale.array.max;\n    return this.test({\n      message,\n      name: 'max',\n      exclusive: true,\n      params: {\n        max\n      },\n\n      test(value) {\n        return (0, _isAbsent.default)(value) || value.length <= this.resolve(max);\n      }\n\n    });\n  }\n\n  ensure() {\n    return this.default(() => []).transform((val, original) => {\n      // We don't want to return `null` for nullable schema\n      if (this._typeCheck(val)) return val;\n      return original == null ? [] : [].concat(original);\n    });\n  }\n\n  compact(rejector) {\n    let reject = !rejector ? v => !!v : (v, i, a) => !rejector(v, i, a);\n    return this.transform(values => values != null ? values.filter(reject) : values);\n  }\n\n  describe() {\n    let base = super.describe();\n    if (this.innerType) base.innerType = this.innerType.describe();\n    return base;\n  }\n\n  nullable(isNullable = true) {\n    return super.nullable(isNullable);\n  }\n\n  defined() {\n    return super.defined();\n  }\n\n  required(msg) {\n    return super.required(msg);\n  }\n\n}\n\nexports.default = ArraySchema;\ncreate.prototype = ArraySchema.prototype; //\n// Interfaces\n//", "\"use strict\";\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\nexports.create = create;\nexports.default = void 0;\n\nvar _isSchema = _interopRequireDefault(require(\"./util/isSchema\"));\n\nfunction _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { default: obj }; }\n\nfunction create(builder) {\n  return new Lazy(builder);\n}\n\nclass Lazy {\n  constructor(builder) {\n    this.type = 'lazy';\n    this.__isYupSchema__ = true;\n    this.__inputType = void 0;\n    this.__outputType = void 0;\n\n    this._resolve = (value, options = {}) => {\n      let schema = this.builder(value, options);\n      if (!(0, _isSchema.default)(schema)) throw new TypeError('lazy() functions must return a valid schema');\n      return schema.resolve(options);\n    };\n\n    this.builder = builder;\n  }\n\n  resolve(options) {\n    return this._resolve(options.value, options);\n  }\n\n  cast(value, options) {\n    return this._resolve(value, options).cast(value, options);\n  }\n\n  validate(value, options, maybeCb) {\n    // @ts-expect-error missing public callback on type\n    return this._resolve(value, options).validate(value, options, maybeCb);\n  }\n\n  validateSync(value, options) {\n    return this._resolve(value, options).validateSync(value, options);\n  }\n\n  validateAt(path, value, options) {\n    return this._resolve(value, options).validateAt(path, value, options);\n  }\n\n  validateSyncAt(path, value, options) {\n    return this._resolve(value, options).validateSyncAt(path, value, options);\n  }\n\n  describe() {\n    return null;\n  }\n\n  isValid(value, options) {\n    return this._resolve(value, options).isValid(value, options);\n  }\n\n  isValidSync(value, options) {\n    return this._resolve(value, options).isValidSync(value, options);\n  }\n\n}\n\nvar _default = Lazy;\nexports.default = _default;", "\"use strict\";\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\nexports.default = setLocale;\n\nvar _locale = _interopRequireDefault(require(\"./locale\"));\n\nfunction _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { default: obj }; }\n\nfunction setLocale(custom) {\n  Object.keys(custom).forEach(type => {\n    // @ts-ignore\n    Object.keys(custom[type]).forEach(method => {\n      // @ts-ignore\n      _locale.default[type][method] = custom[type][method];\n    });\n  });\n}", "\"use strict\";\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\nObject.defineProperty(exports, \"ArraySchema\", {\n  enumerable: true,\n  get: function () {\n    return _array.default;\n  }\n});\nObject.defineProperty(exports, \"BaseSchema\", {\n  enumerable: true,\n  get: function () {\n    return _schema.default;\n  }\n});\nObject.defineProperty(exports, \"BooleanSchema\", {\n  enumerable: true,\n  get: function () {\n    return _boolean.default;\n  }\n});\nObject.defineProperty(exports, \"DateSchema\", {\n  enumerable: true,\n  get: function () {\n    return _date.default;\n  }\n});\nObject.defineProperty(exports, \"MixedSchema\", {\n  enumerable: true,\n  get: function () {\n    return _mixed.default;\n  }\n});\nObject.defineProperty(exports, \"NumberSchema\", {\n  enumerable: true,\n  get: function () {\n    return _number.default;\n  }\n});\nObject.defineProperty(exports, \"ObjectSchema\", {\n  enumerable: true,\n  get: function () {\n    return _object.default;\n  }\n});\nObject.defineProperty(exports, \"StringSchema\", {\n  enumerable: true,\n  get: function () {\n    return _string.default;\n  }\n});\nObject.defineProperty(exports, \"ValidationError\", {\n  enumerable: true,\n  get: function () {\n    return _ValidationError.default;\n  }\n});\nexports.addMethod = addMethod;\nObject.defineProperty(exports, \"array\", {\n  enumerable: true,\n  get: function () {\n    return _array.create;\n  }\n});\nObject.defineProperty(exports, \"bool\", {\n  enumerable: true,\n  get: function () {\n    return _boolean.create;\n  }\n});\nObject.defineProperty(exports, \"boolean\", {\n  enumerable: true,\n  get: function () {\n    return _boolean.create;\n  }\n});\nObject.defineProperty(exports, \"date\", {\n  enumerable: true,\n  get: function () {\n    return _date.create;\n  }\n});\nObject.defineProperty(exports, \"isSchema\", {\n  enumerable: true,\n  get: function () {\n    return _isSchema.default;\n  }\n});\nObject.defineProperty(exports, \"lazy\", {\n  enumerable: true,\n  get: function () {\n    return _Lazy.create;\n  }\n});\nObject.defineProperty(exports, \"mixed\", {\n  enumerable: true,\n  get: function () {\n    return _mixed.create;\n  }\n});\nObject.defineProperty(exports, \"number\", {\n  enumerable: true,\n  get: function () {\n    return _number.create;\n  }\n});\nObject.defineProperty(exports, \"object\", {\n  enumerable: true,\n  get: function () {\n    return _object.create;\n  }\n});\nObject.defineProperty(exports, \"reach\", {\n  enumerable: true,\n  get: function () {\n    return _reach.default;\n  }\n});\nObject.defineProperty(exports, \"ref\", {\n  enumerable: true,\n  get: function () {\n    return _Reference.create;\n  }\n});\nObject.defineProperty(exports, \"setLocale\", {\n  enumerable: true,\n  get: function () {\n    return _setLocale.default;\n  }\n});\nObject.defineProperty(exports, \"string\", {\n  enumerable: true,\n  get: function () {\n    return _string.create;\n  }\n});\n\nvar _mixed = _interopRequireWildcard(require(\"./mixed\"));\n\nvar _boolean = _interopRequireWildcard(require(\"./boolean\"));\n\nvar _string = _interopRequireWildcard(require(\"./string\"));\n\nvar _number = _interopRequireWildcard(require(\"./number\"));\n\nvar _date = _interopRequireWildcard(require(\"./date\"));\n\nvar _object = _interopRequireWildcard(require(\"./object\"));\n\nvar _array = _interopRequireWildcard(require(\"./array\"));\n\nvar _Reference = require(\"./Reference\");\n\nvar _Lazy = require(\"./Lazy\");\n\nvar _ValidationError = _interopRequireDefault(require(\"./ValidationError\"));\n\nvar _reach = _interopRequireDefault(require(\"./util/reach\"));\n\nvar _isSchema = _interopRequireDefault(require(\"./util/isSchema\"));\n\nvar _setLocale = _interopRequireDefault(require(\"./setLocale\"));\n\nvar _schema = _interopRequireDefault(require(\"./schema\"));\n\nfunction _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { default: obj }; }\n\nfunction _getRequireWildcardCache(nodeInterop) { if (typeof WeakMap !== \"function\") return null; var cacheBabelInterop = new WeakMap(); var cacheNodeInterop = new WeakMap(); return (_getRequireWildcardCache = function (nodeInterop) { return nodeInterop ? cacheNodeInterop : cacheBabelInterop; })(nodeInterop); }\n\nfunction _interopRequireWildcard(obj, nodeInterop) { if (!nodeInterop && obj && obj.__esModule) { return obj; } if (obj === null || typeof obj !== \"object\" && typeof obj !== \"function\") { return { default: obj }; } var cache = _getRequireWildcardCache(nodeInterop); if (cache && cache.has(obj)) { return cache.get(obj); } var newObj = {}; var hasPropertyDescriptor = Object.defineProperty && Object.getOwnPropertyDescriptor; for (var key in obj) { if (key !== \"default\" && Object.prototype.hasOwnProperty.call(obj, key)) { var desc = hasPropertyDescriptor ? Object.getOwnPropertyDescriptor(obj, key) : null; if (desc && (desc.get || desc.set)) { Object.defineProperty(newObj, key, desc); } else { newObj[key] = obj[key]; } } } newObj.default = obj; if (cache) { cache.set(obj, newObj); } return newObj; }\n\nfunction addMethod(schemaType, name, fn) {\n  if (!schemaType || !(0, _isSchema.default)(schemaType.prototype)) throw new TypeError('You must provide a yup schema constructor function');\n  if (typeof name !== 'string') throw new TypeError('A Method name must be provided');\n  if (typeof fn !== 'function') throw new TypeError('Method function must be provided');\n  schemaType.prototype[name] = fn;\n}", "import handler from \"../handler-lib\";\nimport { randomUUID } from \"crypto\";\n// types\nimport { UserRoles } from \"../../utils/types\";\nimport { number, object, string } from \"yup\";\n// utils\nimport { putBanner } from \"../../storage/banners\";\nimport { hasPermissions } from \"../../utils/auth/authorization\";\nimport { error } from \"../../utils/constants/constants\";\nimport { validateData } from \"../../utils/validation/validation\";\nimport {\n  badRequest,\n  created,\n  forbidden,\n  internalServerError,\n} from \"../../utils/responses/response-lib\";\n\nconst validationSchema = object().shape({\n  title: string().required(),\n  description: string().required(),\n  link: string().url().notRequired(),\n  startDate: number().required(),\n  endDate: number().required(),\n});\n\nexport const createBanner = handler(async (event, _context) => {\n  if (!hasPermissions(event, [UserRoles.ADMIN])) {\n    return forbidden(error.UNAUTHORIZED);\n  }\n  const unvalidatedPayload = JSON.parse(event.body!);\n\n  let validatedPayload;\n  try {\n    validatedPayload = await validateData(validationSchema, unvalidatedPayload);\n  } catch {\n    return badRequest(error.INVALID_DATA);\n  }\n\n  const { title, description, link, startDate, endDate } = validatedPayload;\n  const currentTime = Date.now();\n\n  const newBanner = {\n    key: randomUUID(),\n    createdAt: currentTime,\n    lastAltered: currentTime,\n    lastAlteredBy: event?.headers[\"cognito-identity-id\"],\n    title,\n    description,\n    link,\n    startDate,\n    endDate,\n  };\n  try {\n    await putBanner(newBanner);\n  } catch {\n    return internalServerError(error.DYNAMO_CREATION_ERROR);\n  }\n  return created(newBanner);\n});\n", "import util from \"util\";\nimport { Logger } from \"@smithy/types\";\n\ntype LogLevel = \"trace\" | \"debug\" | \"info\" | \"warn\" | \"error\";\ntype LogEntry = {\n  date: Date;\n  level: LogLevel;\n  string: string;\n};\n\nconst logs: LogEntry[] = [];\n\nconst buildLoggerForLevel = (level: LogLevel) => {\n  return function (...content: any[]) {\n    logs.push({\n      date: new Date(),\n      level: level,\n      string: util.format.apply(null, content),\n    });\n\n    /*\n     * If we have a function logging thousands of messages,\n     * better to take the console performance hit mid-operation\n     * than to let memory usage run away as well.\n     */\n    if (logs.length > 99) {\n      flush();\n    }\n  };\n};\n\n/*\n * Individual functions are exported to support handler-lib;\n * This integrates SDK client logging with lambda logging.\n */\nexport const trace = buildLoggerForLevel(\"trace\");\nexport const debug = buildLoggerForLevel(\"debug\");\nexport const info = buildLoggerForLevel(\"info\");\nexport const warn = buildLoggerForLevel(\"warn\");\nexport const error = buildLoggerForLevel(\"error\");\n\nexport function flush() {\n  while (logs.length > 0) {\n    const { date, level, string } = logs.shift()!;\n    // eslint-disable-next-line no-console\n    console[level](date, string);\n  }\n}\n\n/*\n * This is only called at the beginning of a lambda handler,\n * so the log buffer should be empty anyway. But it doesn't\n * hurt to make sure!\n */\nexport const init = flush;\n\n/**\n * A logger suitable for passing to any AWS client constructor.\n * Note that the `trace` log level is excluded.\n *\n * This logger accumulates log messages in an internal buffer,\n * eventually flushing them to the console.\n */\nexport const logger: Logger = { debug, info, warn, error };\n", "import { SSMClient, GetParameterCommand } from \"@aws-sdk/client-ssm\";\nimport jwt_decode from \"jwt-decode\";\nimport { CognitoJwtVerifier } from \"aws-jwt-verify\";\n// types\nimport { APIGatewayProxyEvent, UserRoles } from \"../types\";\nimport { logger } from \"../debugging/debug-lib\";\n\ninterface DecodedToken {\n  \"custom:cms_roles\": UserRoles;\n  \"custom:cms_state\": string | undefined;\n}\n\nconst loadCognitoValues = async () => {\n  if (\n    process.env.COGNITO_USER_POOL_ID &&\n    process.env.COGNITO_USER_POOL_CLIENT_ID\n  ) {\n    return {\n      userPoolId: process.env.COGNITO_USER_POOL_ID,\n      userPoolClientId: process.env.COGNITO_USER_POOL_CLIENT_ID,\n    };\n  } else {\n    const ssmClient = new SSMClient({ logger });\n    const stage = process.env.stage!;\n    const getParam = async (identifier: string) => {\n      const command = new GetParameterCommand({\n        Name: `/${stage}/ui-auth/${identifier}`,\n      });\n      const result = await ssmClient.send(command);\n      return result.Parameter?.Value;\n    };\n    const userPoolId = await getParam(\"cognito_user_pool_id\");\n    const userPoolClientId = await getParam(\"cognito_user_pool_client_id\");\n    if (userPoolId && userPoolClientId) {\n      process.env[\"COGNITO_USER_POOL_ID\"] = userPoolId;\n      process.env[\"COGNITO_USER_POOL_CLIENT_ID\"] = userPoolClientId;\n      return { userPoolId, userPoolClientId };\n    } else {\n      throw new Error(\"cannot load cognito values\");\n    }\n  }\n};\n\nexport const isAuthenticated = async (event: APIGatewayProxyEvent) => {\n  const isLocalStack = event.requestContext.accountId === \"000000000000\";\n  if (isLocalStack) {\n    return true;\n  }\n\n  const cognitoValues = await loadCognitoValues();\n\n  // Verifier that expects valid access tokens:\n  const verifier = CognitoJwtVerifier.create({\n    userPoolId: cognitoValues.userPoolId,\n    tokenUse: \"id\",\n    clientId: cognitoValues.userPoolClientId,\n  });\n\n  try {\n    await verifier.verify(event?.headers?.[\"x-api-key\"]!);\n    return true;\n  } catch {\n    return false;\n  }\n};\n\nexport const hasPermissions = (\n  event: APIGatewayProxyEvent,\n  allowedRoles: UserRoles[],\n  state?: string\n) => {\n  let isAllowed = false;\n  // decode the idToken\n  if (event?.headers?.[\"x-api-key\"]) {\n    const decoded = jwt_decode(event.headers[\"x-api-key\"]) as DecodedToken;\n    const idmUserRoles = decoded[\"custom:cms_roles\"];\n    const idmUserState = decoded[\"custom:cms_state\"];\n    const mfpUserRole = idmUserRoles\n      ?.split(\",\")\n      .find((role) => role.includes(\"mdctmfp\")) as UserRoles;\n\n    isAllowed =\n      allowedRoles.includes(mfpUserRole) &&\n      (!state || idmUserState?.includes(state))!;\n  }\n\n  return isAllowed;\n};\n\nexport const isAuthorizedToFetchState = (\n  event: APIGatewayProxyEvent,\n  state: string\n) => {\n  // If this is a state user for the matching state, authorize them.\n  if (hasPermissions(event, [UserRoles.STATE_USER], state)) {\n    return true;\n  }\n\n  const nonStateUserRoles = Object.values(UserRoles).filter(\n    (role) => role !== UserRoles.STATE_USER\n  );\n\n  // If they are any other user type, they don't need to belong to this state.\n  return hasPermissions(event, nonStateUserRoles);\n};\n", "// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n// SPDX-License-Identifier: Apache-2.0\n/**\n * Base Error for all other errors in this file\n */\nexport class JwtBaseError extends Error {\n}\n/**\n * An error that is raised because an actual value does not match with the expected value\n */\nexport class FailedAssertionError extends JwtBaseError {\n    constructor(msg, actual, expected) {\n        super(msg);\n        this.failedAssertion = {\n            actual,\n            expected,\n        };\n    }\n}\n/**\n * JWT errors\n */\nexport class JwtParseError extends JwtBaseError {\n    constructor(msg, error) {\n        const message = error != null ? `${msg}: ${error}` : msg;\n        super(message);\n    }\n}\nexport class ParameterValidationError extends JwtBaseError {\n}\nexport class JwtInvalidSignatureError extends JwtBaseError {\n}\nexport class JwtInvalidSignatureAlgorithmError extends FailedAssertionError {\n}\nexport class JwtInvalidClaimError extends FailedAssertionError {\n    withRawJwt({ header, payload }) {\n        this.rawJwt = {\n            header,\n            payload,\n        };\n        return this;\n    }\n}\nexport class JwtInvalidIssuerError extends JwtInvalidClaimError {\n}\nexport class JwtInvalidAudienceError extends JwtInvalidClaimError {\n}\nexport class JwtInvalidScopeError extends JwtInvalidClaimError {\n}\nexport class JwtExpiredError extends JwtInvalidClaimError {\n}\nexport class JwtNotBeforeError extends JwtInvalidClaimError {\n}\n/**\n * Amazon Cognito specific erros\n */\nexport class CognitoJwtInvalidGroupError extends JwtInvalidClaimError {\n}\nexport class CognitoJwtInvalidTokenUseError extends JwtInvalidClaimError {\n}\nexport class CognitoJwtInvalidClientIdError extends JwtInvalidClaimError {\n}\n/**\n * ASN.1 errors\n */\nexport class Asn1DecodingError extends JwtBaseError {\n}\n/**\n * JWK errors\n */\nexport class JwksValidationError extends JwtBaseError {\n}\nexport class JwkValidationError extends JwtBaseError {\n}\nexport class JwtWithoutValidKidError extends JwtBaseError {\n}\nexport class KidNotFoundInJwksError extends JwtBaseError {\n}\nexport class WaitPeriodNotYetEndedJwkError extends JwtBaseError {\n}\nexport class JwksNotAvailableInCacheError extends JwtBaseError {\n}\nexport class JwkInvalidUseError extends FailedAssertionError {\n}\nexport class JwkInvalidKtyError extends FailedAssertionError {\n}\n/**\n * HTTPS fetch errors\n */\nexport class FetchError extends JwtBaseError {\n    constructor(uri, msg) {\n        super(`Failed to fetch ${uri}: ${msg}`);\n    }\n}\nexport class NonRetryableFetchError extends FetchError {\n}\n/**\n * Web compatibility errors\n */\nexport class NotSupportedError extends JwtBaseError {\n}\n", "// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n// SPDX-License-Identifier: Apache-2.0\n//\n// Node.js implementations for the node-web-compatibility layer\nimport { createPublicKey, createVerify } from \"crypto\";\nimport { constructPublicKeyInDerFormat } from \"./asn1.js\";\nimport { fetchJson } from \"./https-node.js\";\n/**\n * Enum to map supported JWT signature algorithms with OpenSSL message digest algorithm names\n */\nvar JwtSignatureAlgorithms;\n(function (JwtSignatureAlgorithms) {\n    JwtSignatureAlgorithms[\"RS256\"] = \"RSA-SHA256\";\n    JwtSignatureAlgorithms[\"RS384\"] = \"RSA-SHA384\";\n    JwtSignatureAlgorithms[\"RS512\"] = \"RSA-SHA512\";\n})(JwtSignatureAlgorithms || (JwtSignatureAlgorithms = {}));\nexport const nodeWebCompat = {\n    fetchJson,\n    transformJwkToKeyObjectSync: (jwk) => createPublicKey({\n        key: constructPublicKeyInDerFormat(Buffer.from(jwk.n, \"base64\"), Buffer.from(jwk.e, \"base64\")),\n        format: \"der\",\n        type: \"spki\",\n    }),\n    transformJwkToKeyObjectAsync: async (jwk) => createPublicKey({\n        key: constructPublicKeyInDerFormat(Buffer.from(jwk.n, \"base64\"), Buffer.from(jwk.e, \"base64\")),\n        format: \"der\",\n        type: \"spki\",\n    }),\n    parseB64UrlString: (b64) => Buffer.from(b64, \"base64\").toString(\"utf8\"),\n    verifySignatureSync: ({ alg, keyObject, jwsSigningInput, signature }) => \n    // eslint-disable-next-line security/detect-object-injection\n    createVerify(JwtSignatureAlgorithms[alg])\n        .update(jwsSigningInput)\n        .verify(keyObject, signature, \"base64\"),\n    verifySignatureAsync: async ({ alg, keyObject, jwsSigningInput, signature, }) => \n    // eslint-disable-next-line security/detect-object-injection\n    createVerify(JwtSignatureAlgorithms[alg])\n        .update(jwsSigningInput)\n        .verify(keyObject, signature, \"base64\"),\n    defaultFetchTimeouts: {\n        socketIdle: 500,\n        response: 1500,\n    },\n    setTimeoutUnref: (...args) => setTimeout(...args).unref(),\n};\n", "// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n// SPDX-License-Identifier: Apache-2.0\n//\n// Utility to encode RSA public keys (a pair of modulus (n) and exponent (e)) into DER-encoding, per ASN.1 specification.\nimport { Asn1DecodingError } from \"./error.js\";\n/** Enum with possible values for supported ASN.1 classes */\nvar Asn1Class;\n(function (Asn1Class) {\n    Asn1Class[Asn1Class[\"Universal\"] = 0] = \"Universal\";\n})(Asn1Class || (Asn1Class = {}));\n/** Enum with possible values for supported ASN.1 encodings */\nvar Asn1Encoding;\n(function (Asn1Encoding) {\n    Asn1Encoding[Asn1Encoding[\"Primitive\"] = 0] = \"Primitive\";\n    Asn1Encoding[Asn1Encoding[\"Constructed\"] = 1] = \"Constructed\";\n})(Asn1Encoding || (Asn1Encoding = {}));\n/** Enum with possible values for supported ASN.1 tags */\nvar Asn1Tag;\n(function (Asn1Tag) {\n    Asn1Tag[Asn1Tag[\"BitString\"] = 3] = \"BitString\";\n    Asn1Tag[Asn1Tag[\"ObjectIdentifier\"] = 6] = \"ObjectIdentifier\";\n    Asn1Tag[Asn1Tag[\"Sequence\"] = 16] = \"Sequence\";\n    Asn1Tag[Asn1Tag[\"Null\"] = 5] = \"Null\";\n    Asn1Tag[Asn1Tag[\"Integer\"] = 2] = \"Integer\";\n})(Asn1Tag || (Asn1Tag = {}));\n/**\n * Encode an ASN.1 identifier per ASN.1 spec (DER-encoding)\n * See https://www.itu.int/ITU-T/studygroups/com17/languages/X.690-0207.pdf chapter 8.1.2\n *\n * @param identifier - The ASN.1 identifier\n * @returns The buffer\n */\nfunction encodeIdentifier(identifier) {\n    const identifierAsNumber = (identifier.class << 7) |\n        (identifier.primitiveOrConstructed << 5) |\n        identifier.tag;\n    return Buffer.from([identifierAsNumber]);\n}\n/**\n * Encode the length of an ASN.1 type per ASN.1 spec (DER-encoding)\n * See https://www.itu.int/ITU-T/studygroups/com17/languages/X.690-0207.pdf chapter 8.1.3\n *\n * @param length - The length of the ASN.1 type\n * @returns The buffer\n */\nfunction encodeLength(length) {\n    if (length < 128) {\n        return Buffer.from([length]);\n    }\n    const integers = [];\n    while (length > 0) {\n        integers.push(length % 256);\n        length = length >> 8;\n    }\n    integers.reverse();\n    return Buffer.from([128 | integers.length, ...integers]);\n}\n/**\n * Encode a buffer (that represent an integer) as integer per ASN.1 spec (DER-encoding)\n * See https://www.itu.int/ITU-T/studygroups/com17/languages/X.690-0207.pdf chapter 8.3\n *\n * @param buffer - The buffer that represent an integer to encode\n * @returns The buffer\n */\nfunction encodeBufferAsInteger(buffer) {\n    return Buffer.concat([\n        encodeIdentifier({\n            class: Asn1Class.Universal,\n            primitiveOrConstructed: Asn1Encoding.Primitive,\n            tag: Asn1Tag.Integer,\n        }),\n        encodeLength(buffer.length),\n        buffer,\n    ]);\n}\n/**\n * Encode an object identifier (a string such as \"1.2.840.113549.1.1.1\") per ASN.1 spec (DER-encoding)\n * See https://www.itu.int/ITU-T/studygroups/com17/languages/X.690-0207.pdf chapter 8.19\n *\n * @param oid - The object identifier to encode\n * @returns The buffer\n */\nfunction encodeObjectIdentifier(oid) {\n    const oidComponents = oid.split(\".\").map((i) => parseInt(i));\n    const firstSubidentifier = oidComponents[0] * 40 + oidComponents[1];\n    const subsequentSubidentifiers = oidComponents\n        .slice(2)\n        .reduce((expanded, component) => {\n        const bytes = [];\n        do {\n            bytes.push(component % 128);\n            component = component >> 7;\n        } while (component);\n        return expanded.concat(bytes.map((b, index) => (index ? b + 128 : b)).reverse());\n    }, []);\n    const oidBuffer = Buffer.from([\n        firstSubidentifier,\n        ...subsequentSubidentifiers,\n    ]);\n    return Buffer.concat([\n        encodeIdentifier({\n            class: Asn1Class.Universal,\n            primitiveOrConstructed: Asn1Encoding.Primitive,\n            tag: Asn1Tag.ObjectIdentifier,\n        }),\n        encodeLength(oidBuffer.length),\n        oidBuffer,\n    ]);\n}\n/**\n * Encode a buffer as bit string per ASN.1 spec (DER-encoding)\n * See https://www.itu.int/ITU-T/studygroups/com17/languages/X.690-0207.pdf chapter 8.6\n *\n * @param buffer - The buffer to encode\n * @returns The buffer\n */\nfunction encodeBufferAsBitString(buffer) {\n    const bitString = Buffer.concat([Buffer.from([0]), buffer]);\n    return Buffer.concat([\n        encodeIdentifier({\n            class: Asn1Class.Universal,\n            primitiveOrConstructed: Asn1Encoding.Primitive,\n            tag: Asn1Tag.BitString,\n        }),\n        encodeLength(bitString.length),\n        bitString,\n    ]);\n}\n/**\n * Encode a sequence of DER-encoded items per ASN.1 spec (DER-encoding)\n * See https://www.itu.int/ITU-T/studygroups/com17/languages/X.690-0207.pdf chapter 8.9\n *\n * @param sequenceItems - The sequence of DER-encoded items\n * @returns The buffer\n */\nfunction encodeSequence(sequenceItems) {\n    const concatenated = Buffer.concat(sequenceItems);\n    return Buffer.concat([\n        encodeIdentifier({\n            class: Asn1Class.Universal,\n            primitiveOrConstructed: Asn1Encoding.Constructed,\n            tag: Asn1Tag.Sequence,\n        }),\n        encodeLength(concatenated.length),\n        concatenated,\n    ]);\n}\n/**\n * Encode null per ASN.1 spec (DER-encoding)\n * See https://www.itu.int/ITU-T/studygroups/com17/languages/X.690-0207.pdf chapter 8.8\n *\n * @returns The buffer\n */\nfunction encodeNull() {\n    return Buffer.concat([\n        encodeIdentifier({\n            class: Asn1Class.Universal,\n            primitiveOrConstructed: Asn1Encoding.Primitive,\n            tag: Asn1Tag.Null,\n        }),\n        encodeLength(0),\n    ]);\n}\n/**\n * RSA encryption object identifier constant\n *\n * From: https://tools.ietf.org/html/rfc8017\n *\n * pkcs-1    OBJECT IDENTIFIER ::= {\n *     iso(1) member-body(2) us(840) rsadsi(113549) pkcs(1) 1\n * }\n *\n * -- When rsaEncryption is used in an AlgorithmIdentifier,\n * -- the parameters MUST be present and MUST be NULL.\n * --\n * rsaEncryption    OBJECT IDENTIFIER ::= { pkcs-1 1 }\n *\n * See also: http://www.oid-info.com/get/1.2.840.113549.1.1.1\n */\nconst ALGORITHM_RSA_ENCRYPTION = encodeSequence([\n    encodeObjectIdentifier(\"1.2.840.113549.1.1.1\"),\n    encodeNull(), // parameters\n]);\n/**\n * Transform an RSA public key, which is a pair of modulus (n) and exponent (e),\n *  into a buffer per ASN.1 spec (DER-encoding)\n *\n * @param n - The modulus of the public key as buffer\n * @param e - The exponent of the public key as buffer\n * @returns The buffer, which is the public key encoded per ASN.1 spec (DER-encoding)\n */\nexport function constructPublicKeyInDerFormat(n, e) {\n    return encodeSequence([\n        ALGORITHM_RSA_ENCRYPTION,\n        encodeBufferAsBitString(encodeSequence([encodeBufferAsInteger(n), encodeBufferAsInteger(e)])),\n    ]);\n}\n/**\n * Decode an ASN.1 identifier (a number) into its parts: class, primitiveOrConstructed, tag\n * See https://www.itu.int/ITU-T/studygroups/com17/languages/X.690-0207.pdf chapter 8.1.2\n *\n * @param identifier - The identifier\n * @returns An object with properties class, primitiveOrConstructed, tag\n */\nfunction decodeIdentifier(identifier) {\n    if (identifier >> 3 === 0b11111) {\n        throw new Asn1DecodingError(\"Decoding of identifier with tag > 30 not implemented\");\n    }\n    return {\n        class: identifier >> 6,\n        primitiveOrConstructed: (identifier >> 5) & 0b001,\n        tag: identifier & 0b11111, // bit 1-5\n    };\n}\n/**\n * Decode an ASN.1 block of length value combinations,\n * and return the length and byte range of the first length value combination.\n * See https://www.itu.int/ITU-T/studygroups/com17/languages/X.690-0207.pdf chapter 8.1.3 - 8.1.5\n *\n * @param blockOfLengthValues - The ASN.1 length value\n * @returns The length and byte range of the first included length value\n */\nfunction decodeLengthValue(blockOfLengthValues) {\n    if (!(blockOfLengthValues[0] & 0b10000000)) {\n        return {\n            length: blockOfLengthValues[0],\n            firstByteOffset: 1,\n            lastByteOffset: 1 + blockOfLengthValues[0],\n        };\n    }\n    const nrLengthOctets = blockOfLengthValues[0] & 0b01111111;\n    const length = Buffer.from(blockOfLengthValues.subarray(1, 1 + 1 + nrLengthOctets)).readUIntBE(0, nrLengthOctets);\n    return {\n        length,\n        firstByteOffset: 1 + nrLengthOctets,\n        lastByteOffset: 1 + nrLengthOctets + length,\n    };\n}\n/**\n * Decode an ASN.1 sequence into its constituent parts, each part being an identifier-length-value triplet\n * See https://www.itu.int/ITU-T/studygroups/com17/languages/X.690-0207.pdf chapter 8.9\n *\n * @param sequenceValue - The ASN.1 sequence value\n * @returns Array of identifier-length-value triplets\n */\nfunction decodeSequence(sequence) {\n    const { tag } = decodeIdentifier(sequence[0]);\n    if (tag !== Asn1Tag.Sequence) {\n        throw new Asn1DecodingError(`Expected a sequence to decode, but got tag ${tag}`);\n    }\n    const { firstByteOffset, lastByteOffset } = decodeLengthValue(sequence.subarray(1));\n    const sequenceValue = sequence.subarray(1 + firstByteOffset, 1 + 1 + lastByteOffset);\n    const parts = [];\n    let offset = 0;\n    while (offset < sequenceValue.length) {\n        // Silence false postive: accessing an octet in a Buffer at a particular index\n        // is to be done with index operator: [index]\n        // eslint-disable-next-line security/detect-object-injection\n        const identifier = decodeIdentifier(sequenceValue[offset]);\n        const next = decodeLengthValue(sequenceValue.subarray(offset + 1));\n        const value = sequenceValue.subarray(offset + 1 + next.firstByteOffset, offset + 1 + next.lastByteOffset);\n        parts.push({ identifier, length: next.length, value });\n        offset += 1 + next.lastByteOffset;\n    }\n    return parts;\n}\n/**\n * Decode an ASN.1 sequence that is wrapped in a bit string\n * (Which is the way RSA public keys are encoded in ASN.1 DER-encoding)\n * See https://www.itu.int/ITU-T/studygroups/com17/languages/X.690-0207.pdf chapter 8.6 and 8.9\n *\n * @param bitStringValue - The ASN.1 bit string value\n * @returns Array of identifier-length-value triplets\n */\nfunction decodeBitStringWrappedSequenceValue(bitStringValue) {\n    const wrappedSequence = bitStringValue.subarray(1);\n    return decodeSequence(wrappedSequence);\n}\n/**\n * Decode an ASN.1 DER-encoded public key, into its modulus (n) and exponent (e)\n *\n * @param publicKey - The ASN.1 DER-encoded public key\n * @returns Object with modulus (n) and exponent (e)\n */\nexport function deconstructPublicKeyInDerFormat(publicKey) {\n    const [, pubkeyinfo] = decodeSequence(publicKey);\n    const [n, e] = decodeBitStringWrappedSequenceValue(pubkeyinfo.value);\n    return { n: n.value, e: e.value };\n}\n", "// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n// SPDX-License-Identifier: Apache-2.0\n//\n// NodeJS implementation for fetching JSON documents over HTTPS\nimport { request } from \"https\";\nimport { validateHttpsJsonResponse } from \"./https-common.js\";\nimport { pipeline } from \"stream\";\nimport { TextDecoder } from \"util\";\nimport { safeJsonParse } from \"./safe-json-parse.js\";\nimport { FetchError, NonRetryableFetchError } from \"./error.js\";\n/**\n * Execute a HTTPS request\n * @param uri - The URI\n * @param requestOptions - The RequestOptions to use\n * @param data - Data to send to the URI (e.g. POST data)\n * @returns - The response as parsed JSON\n */\nexport async function fetchJson(uri, requestOptions, data) {\n    let responseTimeout;\n    return new Promise((resolve, reject) => {\n        const req = request(uri, {\n            method: \"GET\",\n            ...requestOptions,\n        }, (response) => {\n            // Capture response data\n            // @types/node is incomplete so cast to any\n            // eslint-disable-next-line @typescript-eslint/no-explicit-any\n            pipeline([\n                response,\n                getJsonDestination(uri, response.statusCode, response.headers),\n            ], done);\n        });\n        if (requestOptions?.responseTimeout) {\n            responseTimeout = setTimeout(() => done(new FetchError(uri, `Response time-out (after ${requestOptions.responseTimeout} ms.)`)), requestOptions.responseTimeout);\n            responseTimeout.unref(); // Don't block Node from exiting\n        }\n        function done(...args) {\n            if (responseTimeout)\n                clearTimeout(responseTimeout);\n            if (args[0] == null) {\n                resolve(args[1]);\n                return;\n            }\n            // In case of errors, let the Agent (if any) know to abandon the socket\n            // This is probably best, because the socket may have become stale\n            /* istanbul ignore next */\n            req.socket?.emit(\"agentRemove\");\n            // Turn error into FetchError so the URI is nicely captured in the message\n            let error = args[0];\n            if (!(error instanceof FetchError)) {\n                error = new FetchError(uri, error.message);\n            }\n            req.destroy();\n            reject(error);\n        }\n        // Handle errors while sending request\n        req.on(\"error\", done);\n        // Signal end of request (include optional data)\n        req.end(data);\n    });\n}\n/**\n * Ensures the HTTPS response contains valid JSON\n *\n * @param uri - The URI you were requesting\n * @param statusCode - The response status code to your HTTPS request\n * @param headers - The response headers to your HTTPS request\n *\n * @returns - Async function that can be used as destination in a stream.pipeline, it will return the JSON, if valid, or throw an error otherwise\n */\nfunction getJsonDestination(uri, statusCode, headers) {\n    return async (responseIterable) => {\n        validateHttpsJsonResponse(uri, statusCode, headers[\"content-type\"]);\n        const collected = [];\n        for await (const chunk of responseIterable) {\n            collected.push(chunk);\n        }\n        try {\n            return safeJsonParse(new TextDecoder(\"utf8\", { fatal: true, ignoreBOM: true }).decode(Buffer.concat(collected)));\n        }\n        catch (err) {\n            throw new NonRetryableFetchError(uri, err);\n        }\n    };\n}\n", "// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n// SPDX-License-Identifier: Apache-2.0\n//\n// Lower-level HTTPS functionalities, common for Node.js and Web\nimport { FetchError, NonRetryableFetchError } from \"./error.js\";\n/**\n * Sanity check a HTTPS response where we expect to get JSON data back\n *\n * @param uri the uri that was being requested\n * @param statusCode the HTTP status code, should be 200\n * @param contentType the value of the \"Content-Type\" header in the response, should start with \"application/json\"\n * @returns void - throws an error if the status code or content type aren't as expected\n */\nexport function validateHttpsJsonResponse(uri, statusCode, contentType) {\n    if (statusCode === 429) {\n        throw new FetchError(uri, \"Too many requests\");\n    }\n    else if (statusCode !== 200) {\n        throw new NonRetryableFetchError(uri, `Status code is ${statusCode}, expected 200`);\n    }\n    if (!contentType ||\n        !contentType.toLowerCase().startsWith(\"application/json\")) {\n        throw new NonRetryableFetchError(uri, `Content-type is \"${contentType}\", expected \"application/json\"`);\n    }\n}\n", "// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n// SPDX-License-Identifier: Apache-2.0\n//\n// Utility to parse JSON safely\n/**\n * Check if a piece of JSON is a JSON object, and not e.g. a mere string or null\n *\n * @param j - the JSON\n */\nexport function isJsonObject(j) {\n    // It is not enough to check that `typeof j === \"object\"`\n    // because in JS `typeof null` is also \"object\", and so is `typeof []`.\n    // So we need to check that j is an object, and not null, and not an array\n    return typeof j === \"object\" && !Array.isArray(j) && j !== null;\n}\n/**\n * Parse a string as JSON, while removing __proto__ and constructor, so JS prototype pollution is prevented\n *\n * @param s - the string to JSON parse\n */\nexport function safeJsonParse(s) {\n    return JSON.parse(s, (_, value) => {\n        if (typeof value === \"object\" && !Array.isArray(value) && value !== null) {\n            delete value.__proto__;\n            delete value.constructor;\n        }\n        return value;\n    });\n}\n", "// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n// SPDX-License-Identifier: Apache-2.0\n//\n// Utilities for fetching the JWKS URI, to get the public keys with which to verify JWTs\nimport { NonRetryableFetchError } from \"./error.js\";\nimport { nodeWebCompat } from \"#node-web-compat\";\n/**\n * Execute a HTTPS request\n * @param uri - The URI\n * @param requestOptions - The RequestOptions to use (depending on the runtime context, either Node.js RequestOptions or Web Fetch init)\n * @param data - Data to send to the URI (e.g. POST data)\n * @returns - The response as parsed JSON\n */\nexport const fetchJson = nodeWebCompat.fetchJson;\n/**\n * HTTPS Fetcher for URIs with JSON body\n *\n * @param defaultRequestOptions - The default RequestOptions to use on individual HTTPS requests\n */\nexport class SimpleJsonFetcher {\n    constructor(props) {\n        this.defaultRequestOptions = {\n            timeout: nodeWebCompat.defaultFetchTimeouts.socketIdle,\n            responseTimeout: nodeWebCompat.defaultFetchTimeouts.response,\n            ...props?.defaultRequestOptions,\n        };\n    }\n    /**\n     * Execute a HTTPS request (with 1 immediate retry in case of errors)\n     * @param uri - The URI\n     * @param requestOptions - The RequestOptions to use\n     * @param data - Data to send to the URI (e.g. POST data)\n     * @returns - The response as parsed JSON\n     */\n    async fetch(uri, requestOptions, data) {\n        requestOptions = { ...this.defaultRequestOptions, ...requestOptions };\n        try {\n            return await fetchJson(uri, requestOptions, data);\n        }\n        catch (err) {\n            if (err instanceof NonRetryableFetchError) {\n                throw err;\n            }\n            // Retry once, immediately\n            return fetchJson(uri, requestOptions, data);\n        }\n    }\n}\n", "// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n// SPDX-License-Identifier: Apache-2.0\n//\n// Utilities to assert that supplied values match with expected values\nimport { FailedAssertionError } from \"./error.js\";\n/**\n * Assert value is a non-empty string and equal to the expected value,\n * or throw an error otherwise\n *\n * @param name - Name for the value being checked\n * @param actual - The value to check\n * @param expected - The expected value\n * @param errorConstructor - Constructor for the concrete error to be thrown\n */\nexport function assertStringEquals(name, actual, expected, errorConstructor = FailedAssertionError) {\n    if (!actual) {\n        throw new errorConstructor(`Missing ${name}. Expected: ${expected}`, actual, expected);\n    }\n    if (typeof actual !== \"string\") {\n        throw new errorConstructor(`${name} is not of type string`, actual, expected);\n    }\n    if (expected !== actual) {\n        throw new errorConstructor(`${name} not allowed: ${actual}. Expected: ${expected}`, actual, expected);\n    }\n}\n/**\n * Assert value is a non-empty string and is indeed one of the expected values,\n * or throw an error otherwise\n *\n * @param name - Name for the value being checked\n * @param actual - The value to check\n * @param expected - The array of expected values. For your convenience you can provide\n * @param errorConstructor - Constructor for the concrete error to be thrown\n * a string here as well, which will mean an array with just that string\n */\nexport function assertStringArrayContainsString(name, actual, expected, errorConstructor = FailedAssertionError) {\n    if (!actual) {\n        throw new errorConstructor(`Missing ${name}. ${expectationMessage(expected)}`, actual, expected);\n    }\n    if (typeof actual !== \"string\") {\n        throw new errorConstructor(`${name} is not of type string`, actual, expected);\n    }\n    return assertStringArraysOverlap(name, actual, expected, errorConstructor);\n}\n/**\n * Assert value is an array of strings, where at least one of the strings is indeed one of the expected values,\n * or throw an error otherwise\n *\n * @param name - Name for the value being checked\n * @param actual - The value to check, must be an array of strings, or a single string (which will be treated\n * as an array with just that string)\n * @param expected - The array of expected values. For your convenience you can provide\n * a string here as well, which will mean an array with just that string\n * @param errorConstructor - Constructor for the concrete error to be thrown\n */\nexport function assertStringArraysOverlap(name, actual, expected, errorConstructor = FailedAssertionError) {\n    if (!actual) {\n        throw new errorConstructor(`Missing ${name}. ${expectationMessage(expected)}`, actual, expected);\n    }\n    const expectedAsSet = new Set(Array.isArray(expected) ? expected : [expected]);\n    if (typeof actual === \"string\") {\n        actual = [actual];\n    }\n    if (!Array.isArray(actual)) {\n        throw new errorConstructor(`${name} is not an array`, actual, expected);\n    }\n    const overlaps = actual.some((actualItem) => {\n        if (typeof actualItem !== \"string\") {\n            throw new errorConstructor(`${name} includes elements that are not of type string`, actual, expected);\n        }\n        return expectedAsSet.has(actualItem);\n    });\n    if (!overlaps) {\n        throw new errorConstructor(`${name} not allowed: ${actual.join(\", \")}. ${expectationMessage(expected)}`, actual, expected);\n    }\n}\n/**\n * Get a nicely readable message regarding an expectation\n *\n * @param expected - The expected value.\n */\nfunction expectationMessage(expected) {\n    if (Array.isArray(expected)) {\n        if (expected.length > 1) {\n            return `Expected one of: ${expected.join(\", \")}`;\n        }\n        return `Expected: ${expected[0]}`;\n    }\n    return `Expected: ${expected}`;\n}\n/**\n * Assert value is not a promise, or throw an error otherwise\n *\n * @param actual - The value to check\n * @param errorFactory - Function that returns the error to be thrown\n */\nexport function assertIsNotPromise(actual, errorFactory) {\n    if (actual && typeof actual.then === \"function\") {\n        throw errorFactory();\n    }\n}\n", "// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n// SPDX-License-Identifier: Apache-2.0\nimport { SimpleJsonFetcher, fetchJson } from \"./https.js\";\nimport { isJsonObject } from \"./safe-json-parse.js\";\nimport { JwkValidationError, JwksNotAvailableInCacheError, JwksValidationError, KidNotFoundInJwksError, WaitPeriodNotYetEndedJwkError, JwtWithoutValidKidError, JwkInvalidUseError, JwkInvalidKtyError, } from \"./error.js\";\nimport { nodeWebCompat } from \"#node-web-compat\";\nimport { assertStringEquals } from \"./assert.js\";\nconst optionalJwkFieldNames = [\n    \"use\",\n    \"alg\",\n    \"kid\",\n    \"n\",\n    \"e\", // https://datatracker.ietf.org/doc/html/rfc7518#section-6.3.1.2\n];\nconst mandatoryJwkFieldNames = [\n    \"kty\", // https://datatracker.ietf.org/doc/html/rfc7517#section-4.1\n];\nexport function findJwkInJwks(jwks, kid) {\n    return jwks.keys.find((jwk) => jwk.kid != null && jwk.kid === kid);\n}\nexport async function fetchJwks(jwksUri) {\n    const jwks = await fetchJson(jwksUri);\n    assertIsJwks(jwks);\n    return jwks;\n}\nexport async function fetchJwk(jwksUri, decomposedJwt) {\n    if (!decomposedJwt.header.kid) {\n        throw new JwtWithoutValidKidError(\"JWT header does not have valid kid claim\");\n    }\n    const jwks = await fetchJwks(jwksUri);\n    const jwk = findJwkInJwks(jwks, decomposedJwt.header.kid);\n    if (!jwk) {\n        throw new KidNotFoundInJwksError(`JWK for kid \"${decomposedJwt.header.kid}\" not found in the JWKS`);\n    }\n    return jwk;\n}\nexport function assertIsJwks(jwks) {\n    if (!jwks) {\n        throw new JwksValidationError(\"JWKS empty\");\n    }\n    if (!isJsonObject(jwks)) {\n        throw new JwksValidationError(\"JWKS should be an object\");\n    }\n    if (!Object.keys(jwks).includes(\"keys\")) {\n        throw new JwksValidationError(\"JWKS does not include keys\");\n    }\n    if (!Array.isArray(jwks.keys)) {\n        throw new JwksValidationError(\"JWKS keys should be an array\");\n    }\n    for (const jwk of jwks.keys) {\n        assertIsJwk(jwk);\n    }\n}\nexport function assertIsRsaSignatureJwk(jwk) {\n    // Check JWK use\n    assertStringEquals(\"JWK use\", jwk.use, \"sig\", JwkInvalidUseError);\n    // Check JWK kty\n    assertStringEquals(\"JWK kty\", jwk.kty, \"RSA\", JwkInvalidKtyError);\n    // Check modulus (n) has a value\n    if (!jwk.n)\n        throw new JwkValidationError(\"Missing modulus (n)\");\n    // Check exponent (e) has a value\n    if (!jwk.e)\n        throw new JwkValidationError(\"Missing exponent (e)\");\n}\nexport function assertIsJwk(jwk) {\n    if (!jwk) {\n        throw new JwkValidationError(\"JWK empty\");\n    }\n    if (!isJsonObject(jwk)) {\n        throw new JwkValidationError(\"JWK should be an object\");\n    }\n    for (const field of mandatoryJwkFieldNames) {\n        // disable eslint rule because `field` is trusted\n        // eslint-disable-next-line security/detect-object-injection\n        if (typeof jwk[field] !== \"string\") {\n            throw new JwkValidationError(`JWK ${field} should be a string`);\n        }\n    }\n    for (const field of optionalJwkFieldNames) {\n        // disable eslint rule because `field` is trusted\n        // eslint-disable-next-line security/detect-object-injection\n        if (field in jwk && typeof jwk[field] !== \"string\") {\n            throw new JwkValidationError(`JWK ${field} should be a string`);\n        }\n    }\n}\nexport function isJwks(jwks) {\n    try {\n        assertIsJwks(jwks);\n        return true;\n    }\n    catch {\n        return false;\n    }\n}\nexport function isJwk(jwk) {\n    try {\n        assertIsJwk(jwk);\n        return true;\n    }\n    catch {\n        return false;\n    }\n}\nexport class SimplePenaltyBox {\n    constructor(props) {\n        this.waitingUris = new Map();\n        this.waitSeconds = props?.waitSeconds ?? 10;\n    }\n    async wait(jwksUri) {\n        // SimplePenaltyBox does not actually wait but bluntly throws an error\n        // Any waiting and retries are expected to be done upstream (e.g. in the browser / app)\n        if (this.waitingUris.has(jwksUri)) {\n            throw new WaitPeriodNotYetEndedJwkError(\"Not allowed to fetch JWKS yet, still waiting for back off period to end\");\n        }\n    }\n    release(jwksUri) {\n        const i = this.waitingUris.get(jwksUri);\n        if (i) {\n            clearTimeout(i);\n            this.waitingUris.delete(jwksUri);\n        }\n    }\n    registerFailedAttempt(jwksUri) {\n        const i = nodeWebCompat.setTimeoutUnref(() => {\n            this.waitingUris.delete(jwksUri);\n        }, this.waitSeconds * 1000);\n        this.waitingUris.set(jwksUri, i);\n    }\n    registerSuccessfulAttempt(jwksUri) {\n        this.release(jwksUri);\n    }\n}\nexport class SimpleJwksCache {\n    constructor(props) {\n        this.jwksCache = new Map();\n        this.fetchingJwks = new Map();\n        this.penaltyBox = props?.penaltyBox ?? new SimplePenaltyBox();\n        this.fetcher = props?.fetcher ?? new SimpleJsonFetcher();\n    }\n    addJwks(jwksUri, jwks) {\n        this.jwksCache.set(jwksUri, jwks);\n    }\n    async getJwks(jwksUri) {\n        const existingFetch = this.fetchingJwks.get(jwksUri);\n        if (existingFetch) {\n            return existingFetch;\n        }\n        const jwksPromise = this.fetcher.fetch(jwksUri).then((res) => {\n            assertIsJwks(res);\n            return res;\n        });\n        this.fetchingJwks.set(jwksUri, jwksPromise);\n        let jwks;\n        try {\n            jwks = await jwksPromise;\n        }\n        finally {\n            this.fetchingJwks.delete(jwksUri);\n        }\n        this.jwksCache.set(jwksUri, jwks);\n        return jwks;\n    }\n    getCachedJwk(jwksUri, decomposedJwt) {\n        if (typeof decomposedJwt.header.kid !== \"string\") {\n            throw new JwtWithoutValidKidError(\"JWT header does not have valid kid claim\");\n        }\n        if (!this.jwksCache.has(jwksUri)) {\n            throw new JwksNotAvailableInCacheError(`JWKS for uri ${jwksUri} not yet available in cache`);\n        }\n        const jwk = findJwkInJwks(this.jwksCache.get(jwksUri), decomposedJwt.header.kid);\n        if (!jwk) {\n            throw new KidNotFoundInJwksError(`JWK for kid ${decomposedJwt.header.kid} not found in the JWKS`);\n        }\n        return jwk;\n    }\n    async getJwk(jwksUri, decomposedJwt) {\n        if (typeof decomposedJwt.header.kid !== \"string\") {\n            throw new JwtWithoutValidKidError(\"JWT header does not have valid kid claim\");\n        }\n        // Try to get JWK from cache:\n        const cachedJwks = this.jwksCache.get(jwksUri);\n        if (cachedJwks) {\n            const cachedJwk = findJwkInJwks(cachedJwks, decomposedJwt.header.kid);\n            if (cachedJwk) {\n                return cachedJwk;\n            }\n        }\n        // Await any wait period that is currently in effect\n        // This prevents us from flooding the JWKS URI with requests\n        await this.penaltyBox.wait(jwksUri, decomposedJwt.header.kid);\n        // Fetch the JWKS and (try to) locate the JWK\n        const jwks = await this.getJwks(jwksUri);\n        const jwk = findJwkInJwks(jwks, decomposedJwt.header.kid);\n        // If the JWK could not be located, someone might be messing around with us\n        // Register the failed attempt with the penaltyBox, so it can enforce a wait period\n        // before trying again next time (instead of flooding the JWKS URI with requests)\n        if (!jwk) {\n            this.penaltyBox.registerFailedAttempt(jwksUri, decomposedJwt.header.kid);\n            throw new KidNotFoundInJwksError(`JWK for kid \"${decomposedJwt.header.kid}\" not found in the JWKS`);\n        }\n        else {\n            this.penaltyBox.registerSuccessfulAttempt(jwksUri, decomposedJwt.header.kid);\n        }\n        return jwk;\n    }\n}\n", "// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n// SPDX-License-Identifier: Apache-2.0\nexport const supportedSignatureAlgorithms = [\n    \"RS256\",\n    \"RS384\",\n    \"RS512\",\n];\n", "// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n// SPDX-License-Identifier: Apache-2.0\nimport { assertStringArrayContainsString, assertStringArraysOverlap, } from \"./assert.js\";\nimport { safeJsonParse, isJsonObject } from \"./safe-json-parse.js\";\nimport { JwtExpiredError, JwtNotBeforeError, JwtInvalidIssuerError, JwtInvalidAudienceError, JwtInvalidScopeError, JwtParseError, ParameterValidationError, } from \"./error.js\";\nimport { nodeWebCompat } from \"#node-web-compat\";\n/**\n * Assert that the argument is a valid JWT header object.\n * Throws an error in case it is not.\n *\n * @param header\n * @returns void\n */\nfunction assertJwtHeader(header) {\n    if (!isJsonObject(header)) {\n        throw new JwtParseError(\"JWT header is not an object\");\n    }\n    if (header.alg !== undefined && typeof header.alg !== \"string\") {\n        throw new JwtParseError(\"JWT header alg claim is not a string\");\n    }\n    if (header.kid !== undefined && typeof header.kid !== \"string\") {\n        throw new JwtParseError(\"JWT header kid claim is not a string\");\n    }\n}\n/**\n * Assert that the argument is a valid JWT payload object.\n * Throws an error in case it is not.\n *\n * @param payload\n * @returns void\n */\nfunction assertJwtPayload(payload) {\n    if (!isJsonObject(payload)) {\n        throw new JwtParseError(\"JWT payload is not an object\");\n    }\n    if (payload.exp !== undefined && !Number.isFinite(payload.exp)) {\n        throw new JwtParseError(\"JWT payload exp claim is not a number\");\n    }\n    if (payload.iss !== undefined && typeof payload.iss !== \"string\") {\n        throw new JwtParseError(\"JWT payload iss claim is not a string\");\n    }\n    if (payload.aud !== undefined &&\n        typeof payload.aud !== \"string\" &&\n        (!Array.isArray(payload.aud) ||\n            payload.aud.some((aud) => typeof aud !== \"string\"))) {\n        throw new JwtParseError(\"JWT payload aud claim is not a string or array of strings\");\n    }\n    if (payload.nbf !== undefined && !Number.isFinite(payload.nbf)) {\n        throw new JwtParseError(\"JWT payload nbf claim is not a number\");\n    }\n    if (payload.iat !== undefined && !Number.isFinite(payload.iat)) {\n        throw new JwtParseError(\"JWT payload iat claim is not a number\");\n    }\n    if (payload.scope !== undefined && typeof payload.scope !== \"string\") {\n        throw new JwtParseError(\"JWT payload scope claim is not a string\");\n    }\n    if (payload.jti !== undefined && typeof payload.jti !== \"string\") {\n        throw new JwtParseError(\"JWT payload jti claim is not a string\");\n    }\n}\n/**\n * Sanity check, decompose and JSON parse a JWT string into its constituent parts:\n * - header object\n * - payload object\n * - signature string\n *\n * @param jwt The JWT (as string)\n * @returns the decomposed JWT\n */\nexport function decomposeJwt(jwt) {\n    // Sanity checks on JWT\n    if (!jwt) {\n        throw new JwtParseError(\"Empty JWT\");\n    }\n    if (typeof jwt !== \"string\") {\n        throw new JwtParseError(\"JWT is not a string\");\n    }\n    if (!jwt.match(/^[A-Za-z0-9_-]+\\.[A-Za-z0-9_-]+\\.[A-Za-z0-9_-]+$/)) {\n        throw new JwtParseError(\"JWT string does not consist of exactly 3 parts (header, payload, signature)\");\n    }\n    const [headerB64, payloadB64, signatureB64] = jwt.split(\".\");\n    // B64 decode header and payload\n    const [headerString, payloadString] = [headerB64, payloadB64].map(nodeWebCompat.parseB64UrlString);\n    // Parse header\n    let header;\n    try {\n        header = safeJsonParse(headerString);\n    }\n    catch (err) {\n        throw new JwtParseError(\"Invalid JWT. Header is not a valid JSON object\", err);\n    }\n    assertJwtHeader(header);\n    // parse payload\n    let payload;\n    try {\n        payload = safeJsonParse(payloadString);\n    }\n    catch (err) {\n        throw new JwtParseError(\"Invalid JWT. Payload is not a valid JSON object\", err);\n    }\n    assertJwtPayload(payload);\n    return {\n        header,\n        headerB64,\n        payload,\n        payloadB64,\n        signatureB64,\n    };\n}\n/**\n * Validate JWT payload fields. Throws an error in case there's any validation issue.\n *\n * @param payload The (JSON parsed) JWT payload\n * @param options The options to use during validation\n * @returns void\n */\nexport function validateJwtFields(payload, options) {\n    // Check expiry\n    if (payload.exp !== undefined) {\n        if (payload.exp + (options.graceSeconds ?? 0) < Date.now() / 1000) {\n            throw new JwtExpiredError(`Token expired at ${new Date(payload.exp * 1000).toISOString()}`, payload.exp);\n        }\n    }\n    // Check not before\n    if (payload.nbf !== undefined) {\n        if (payload.nbf - (options.graceSeconds ?? 0) > Date.now() / 1000) {\n            throw new JwtNotBeforeError(`Token can't be used before ${new Date(payload.nbf * 1000).toISOString()}`, payload.nbf);\n        }\n    }\n    // Check JWT issuer\n    if (options.issuer !== null) {\n        if (options.issuer === undefined) {\n            throw new ParameterValidationError(\"issuer must be provided or set to null explicitly\");\n        }\n        assertStringArrayContainsString(\"Issuer\", payload.iss, options.issuer, JwtInvalidIssuerError);\n    }\n    // Check audience\n    if (options.audience !== null) {\n        if (options.audience === undefined) {\n            throw new ParameterValidationError(\"audience must be provided or set to null explicitly\");\n        }\n        assertStringArraysOverlap(\"Audience\", payload.aud, options.audience, JwtInvalidAudienceError);\n    }\n    // Check scope\n    if (options.scope != null) {\n        assertStringArraysOverlap(\"Scope\", payload.scope?.split(\" \"), options.scope, JwtInvalidScopeError);\n    }\n}\n", "// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n// SPDX-License-Identifier: Apache-2.0\nimport { SimpleJwksCache, isJwk, isJwks, fetchJwk, assertIsRsaSignatureJwk, findJwkInJwks, } from \"./jwk.js\";\nimport { assertIsNotPromise, assertStringArrayContainsString, assertStringEquals, } from \"./assert.js\";\nimport { supportedSignatureAlgorithms, } from \"./jwt-model.js\";\nimport { decomposeJwt, validateJwtFields } from \"./jwt.js\";\nimport { JwtInvalidClaimError, JwtInvalidIssuerError, JwtInvalidSignatureAlgorithmError, JwtInvalidSignatureError, KidNotFoundInJwksError, ParameterValidationError, } from \"./error.js\";\nimport { nodeWebCompat } from \"#node-web-compat\";\n/**\n * Sanity check the JWT header and the selected JWK\n *\n * @param header: the JWT header (decoded and JSON parsed)\n * @param jwk: the JWK\n */\nfunction validateJwtHeaderAndJwk(header, jwk) {\n    // Check that the JWK is in fact a JWK for RSA signatures\n    assertIsRsaSignatureJwk(jwk);\n    // Check that JWT signature algorithm matches JWK\n    if (jwk.alg) {\n        assertStringEquals(\"JWT signature algorithm\", header.alg, jwk.alg, JwtInvalidSignatureAlgorithmError);\n    }\n    // Check JWT signature algorithm is one of the supported signature algorithms\n    assertStringArrayContainsString(\"JWT signature algorithm\", header.alg, supportedSignatureAlgorithms, JwtInvalidSignatureAlgorithmError);\n}\n/**\n * Verify a JWT asynchronously (thus allowing for the JWKS to be fetched from the JWKS URI)\n *\n * @param jwt The JWT\n * @param jwksUri The JWKS URI, where the JWKS can be fetched from\n * @param options Verification options\n * @returns Promise that resolves to the payload of the JWT\u2013\u2013if the JWT is valid, otherwise the promise rejects\n */\nexport async function verifyJwt(jwt, jwksUri, options) {\n    return verifyDecomposedJwt(decomposeJwt(jwt), jwksUri, options);\n}\n/**\n * Verify (asynchronously) a JWT that is already decomposed (by function `decomposeJwt`)\n *\n * @param decomposedJwt The decomposed JWT\n * @param jwksUri The JWKS URI, where the JWKS can be fetched from\n * @param options Verification options\n * @param jwkFetcher A function that can execute the fetch of the JWKS from the JWKS URI\n * @param transformJwkToKeyObjectFn A function that can transform a JWK into a crypto native key object\n * @returns Promise that resolves to the payload of the JWT\u2013\u2013if the JWT is valid, otherwise the promise rejects\n */\nasync function verifyDecomposedJwt(decomposedJwt, jwksUri, options, jwkFetcher = fetchJwk, transformJwkToKeyObjectFn = nodeWebCompat.transformJwkToKeyObjectAsync) {\n    const { header, headerB64, payload, payloadB64, signatureB64 } = decomposedJwt;\n    const jwk = await jwkFetcher(jwksUri, decomposedJwt);\n    validateJwtHeaderAndJwk(decomposedJwt.header, jwk);\n    // Transform the JWK to native key format, that can be used with verifySignature\n    const keyObject = await transformJwkToKeyObjectFn(jwk, header.alg, payload.iss);\n    // Verify the JWT signature\n    const valid = await nodeWebCompat.verifySignatureAsync({\n        jwsSigningInput: `${headerB64}.${payloadB64}`,\n        signature: signatureB64,\n        alg: header.alg,\n        keyObject,\n    });\n    if (!valid) {\n        throw new JwtInvalidSignatureError(\"Invalid signature\");\n    }\n    try {\n        validateJwtFields(payload, options);\n        if (options.customJwtCheck) {\n            await options.customJwtCheck({ header, payload, jwk });\n        }\n    }\n    catch (err) {\n        if (options.includeRawJwtInErrors && err instanceof JwtInvalidClaimError) {\n            throw err.withRawJwt(decomposedJwt);\n        }\n        throw err;\n    }\n    return payload;\n}\n/**\n * Verify a JWT synchronously, using a JWKS or JWK that has already been fetched\n *\n * @param jwt The JWT\n * @param jwkOrJwks The JWKS that includes the right JWK (indexed by kid). Alternatively, provide the right JWK directly\n * @param options Verification options\n * @param transformJwkToKeyObjectFn A function that can transform a JWK into a crypto native key object\n * @returns The (JSON parsed) payload of the JWT\u2013\u2013if the JWT is valid, otherwise an error is thrown\n */\nexport function verifyJwtSync(jwt, jwkOrJwks, options, transformJwkToKeyObjectFn = nodeWebCompat.transformJwkToKeyObjectSync) {\n    return verifyDecomposedJwtSync(decomposeJwt(jwt), jwkOrJwks, options, transformJwkToKeyObjectFn);\n}\n/**\n * Verify (synchronously) a JWT that is already decomposed (by function `decomposeJwt`)\n *\n * @param decomposedJwt The decomposed JWT\n * @param jwkOrJwks The JWKS that includes the right JWK (indexed by kid). Alternatively, provide the right JWK directly\n * @param options Verification options\n * @param transformJwkToKeyObjectFn A function that can transform a JWK into a crypto native key object\n * @returns The (JSON parsed) payload of the JWT\u2013\u2013if the JWT is valid, otherwise an error is thrown\n */\nfunction verifyDecomposedJwtSync(decomposedJwt, jwkOrJwks, options, transformJwkToKeyObjectFn) {\n    const { header, headerB64, payload, payloadB64, signatureB64 } = decomposedJwt;\n    let jwk;\n    if (isJwk(jwkOrJwks)) {\n        jwk = jwkOrJwks;\n    }\n    else if (isJwks(jwkOrJwks)) {\n        const locatedJwk = header.kid\n            ? findJwkInJwks(jwkOrJwks, header.kid)\n            : undefined;\n        if (!locatedJwk) {\n            throw new KidNotFoundInJwksError(`JWK for kid ${header.kid} not found in the JWKS`);\n        }\n        jwk = locatedJwk;\n    }\n    else {\n        throw new ParameterValidationError([\n            `Expected a valid JWK or JWKS (parsed as JavaScript object), but received: ${jwkOrJwks}.`,\n            \"If you're passing a JWKS URI, use the async verify() method instead, it will download and parse the JWKS for you\",\n        ].join());\n    }\n    validateJwtHeaderAndJwk(decomposedJwt.header, jwk);\n    // Transform the JWK to native key format, that can be used with verifySignature\n    const keyObject = transformJwkToKeyObjectFn(jwk, header.alg, payload.iss);\n    // Verify the JWT signature (JWS)\n    const valid = nodeWebCompat.verifySignatureSync({\n        jwsSigningInput: `${headerB64}.${payloadB64}`,\n        signature: signatureB64,\n        alg: header.alg,\n        keyObject,\n    });\n    if (!valid) {\n        throw new JwtInvalidSignatureError(\"Invalid signature\");\n    }\n    try {\n        validateJwtFields(payload, options);\n        if (options.customJwtCheck) {\n            const res = options.customJwtCheck({ header, payload, jwk });\n            assertIsNotPromise(res, () => new ParameterValidationError(\"Custom JWT checks must be synchronous but a promise was returned\"));\n        }\n    }\n    catch (err) {\n        if (options.includeRawJwtInErrors && err instanceof JwtInvalidClaimError) {\n            throw err.withRawJwt(decomposedJwt);\n        }\n        throw err;\n    }\n    return payload;\n}\n/**\n * Abstract class representing a verifier for JWTs signed with RSA (e.g. RS256, RS384, RS512)\n *\n * A class is used, because there is state:\n * - The JWKS is fetched (downloaded) from the JWKS URI and cached in memory\n * - Verification properties at verifier level, are used as default options for individual verify calls\n *\n * When instantiating this class, relevant type parameters should be provided, for your concrete case:\n * @param StillToProvide The verification options that you want callers of verify to provide on individual verify calls\n * @param SpecificVerifyProperties The verification options that you'll use\n * @param IssuerConfig The issuer config that you'll use (config options are used as default verification options)\n * @param MultiIssuer Verify multiple issuers (true) or just a single one (false)\n */\nexport class JwtRsaVerifierBase {\n    constructor(verifyProperties, jwksCache = new SimpleJwksCache()) {\n        this.jwksCache = jwksCache;\n        this.issuersConfig = new Map();\n        this.publicKeyCache = new KeyObjectCache();\n        if (Array.isArray(verifyProperties)) {\n            if (!verifyProperties.length) {\n                throw new ParameterValidationError(\"Provide at least one issuer configuration\");\n            }\n            for (const prop of verifyProperties) {\n                if (this.issuersConfig.has(prop.issuer)) {\n                    throw new ParameterValidationError(`issuer ${prop.issuer} supplied multiple times`);\n                }\n                this.issuersConfig.set(prop.issuer, this.withJwksUri(prop));\n            }\n        }\n        else {\n            this.issuersConfig.set(verifyProperties.issuer, this.withJwksUri(verifyProperties));\n        }\n    }\n    get expectedIssuers() {\n        return Array.from(this.issuersConfig.keys());\n    }\n    getIssuerConfig(issuer) {\n        if (!issuer) {\n            if (this.issuersConfig.size !== 1) {\n                throw new ParameterValidationError(\"issuer must be provided\");\n            }\n            issuer = this.issuersConfig.keys().next().value;\n        }\n        const config = this.issuersConfig.get(issuer);\n        if (!config) {\n            throw new ParameterValidationError(`issuer not configured: ${issuer}`);\n        }\n        return config;\n    }\n    /**\n     * This method loads a JWKS that you provide, into the JWKS cache, so that it is\n     * available for JWT verification. Use this method to speed up the first JWT verification\n     * (when the JWKS would otherwise have to be downloaded from the JWKS uri), or to provide the JWKS\n     * in case the JwtVerifier does not have internet access to download the JWKS\n     *\n     * @param jwksThe JWKS\n     * @param issuer The issuer for which you want to cache the JWKS\n     *  Supply this field, if you instantiated the JwtVerifier with multiple issuers\n     * @returns void\n     */\n    cacheJwks(...[jwks, issuer]) {\n        const issuerConfig = this.getIssuerConfig(issuer);\n        this.jwksCache.addJwks(issuerConfig.jwksUri, jwks);\n        this.publicKeyCache.clearCache(issuerConfig.issuer);\n    }\n    /**\n     * Hydrate the JWKS cache for (all of) the configured issuer(s).\n     * This will fetch and cache the latest and greatest JWKS for concerned issuer(s).\n     *\n     * @param issuer The issuer to fetch the JWKS for\n     * @returns void\n     */\n    async hydrate() {\n        const jwksFetches = this.expectedIssuers\n            .map((issuer) => this.getIssuerConfig(issuer).jwksUri)\n            .map((jwksUri) => this.jwksCache.getJwks(jwksUri));\n        await Promise.all(jwksFetches);\n    }\n    /**\n     * Verify (synchronously) a JWT that is signed using RS256 / RS384 / RS512.\n     *\n     * @param jwt The JWT, as string\n     * @param props Verification properties\n     * @returns The payload of the JWT\u2013\u2013if the JWT is valid, otherwise an error is thrown\n     */\n    verifySync(...[jwt, properties]) {\n        const { decomposedJwt, jwksUri, verifyProperties } = this.getVerifyParameters(jwt, properties);\n        return this.verifyDecomposedJwtSync(decomposedJwt, jwksUri, verifyProperties);\n    }\n    /**\n     * Verify (synchronously) an already decomposed JWT, that is signed using RS256 / RS384 / RS512.\n     *\n     * @param decomposedJwt The decomposed Jwt\n     * @param jwk The JWK to verify the JWTs signature with\n     * @param verifyProperties The properties to use for verification\n     * @returns The payload of the JWT\u2013\u2013if the JWT is valid, otherwise an error is thrown\n     */\n    verifyDecomposedJwtSync(decomposedJwt, jwksUri, verifyProperties) {\n        const jwk = this.jwksCache.getCachedJwk(jwksUri, decomposedJwt);\n        return verifyDecomposedJwtSync(decomposedJwt, jwk, verifyProperties, this.publicKeyCache.transformJwkToKeyObjectSync.bind(this.publicKeyCache));\n    }\n    /**\n     * Verify (asynchronously) a JWT that is signed using RS256 / RS384 / RS512.\n     * This call is asynchronous, and the JWKS will be fetched from the JWKS uri,\n     * in case it is not yet available in the cache.\n     *\n     * @param jwt The JWT, as string\n     * @param props Verification properties\n     * @returns Promise that resolves to the payload of the JWT\u2013\u2013if the JWT is valid, otherwise the promise rejects\n     */\n    async verify(...[jwt, properties]) {\n        const { decomposedJwt, jwksUri, verifyProperties } = this.getVerifyParameters(jwt, properties);\n        return this.verifyDecomposedJwt(decomposedJwt, jwksUri, verifyProperties);\n    }\n    /**\n     * Verify (asynchronously) an already decomposed JWT, that is signed using RS256 / RS384 / RS512.\n     *\n     * @param decomposedJwt The decomposed Jwt\n     * @param jwk The JWK to verify the JWTs signature with\n     * @param verifyProperties The properties to use for verification\n     * @returns The payload of the JWT\u2013\u2013if the JWT is valid, otherwise an error is thrown\n     */\n    verifyDecomposedJwt(decomposedJwt, jwksUri, verifyProperties) {\n        return verifyDecomposedJwt(decomposedJwt, jwksUri, verifyProperties, this.jwksCache.getJwk.bind(this.jwksCache), this.publicKeyCache.transformJwkToKeyObjectAsync.bind(this.publicKeyCache));\n    }\n    /**\n     * Get the verification parameters to use, by merging the issuer configuration,\n     * with the overriding properties that are now provided\n     *\n     * @param jwt: the JWT that is going to be verified\n     * @param verifyProperties: the overriding properties, that override the issuer configuration\n     * @returns The merged verification parameters\n     */\n    getVerifyParameters(jwt, verifyProperties) {\n        const decomposedJwt = decomposeJwt(jwt);\n        assertStringArrayContainsString(\"Issuer\", decomposedJwt.payload.iss, this.expectedIssuers, JwtInvalidIssuerError);\n        const issuerConfig = this.getIssuerConfig(decomposedJwt.payload.iss);\n        return {\n            decomposedJwt,\n            jwksUri: issuerConfig.jwksUri,\n            verifyProperties: {\n                ...issuerConfig,\n                ...verifyProperties,\n            },\n        };\n    }\n    /**\n     * Get issuer config with JWKS URI, by adding a default JWKS URI if needed\n     *\n     * @param config: the issuer config.\n     * @returns The config with JWKS URI\n     */\n    withJwksUri(config) {\n        if (config.jwksUri) {\n            return config;\n        }\n        const issuerUri = new URL(config.issuer).pathname.replace(/\\/$/, \"\");\n        return {\n            jwksUri: new URL(`${issuerUri}/.well-known/jwks.json`, config.issuer)\n                .href,\n            ...config,\n        };\n    }\n}\n/**\n * Class representing a verifier for JWTs signed with RSA (e.g. RS256 / RS384 / RS512)\n */\nexport class JwtRsaVerifier extends JwtRsaVerifierBase {\n    // eslint-disable-next-line @typescript-eslint/explicit-module-boundary-types\n    static create(verifyProperties, additionalProperties) {\n        return new this(verifyProperties, additionalProperties?.jwksCache);\n    }\n}\n/**\n * Class representing a cache of RSA public keys in native key object format\n *\n * Because it takes a bit of compute time to turn a JWK into native key object format,\n * we want to cache this computation.\n */\nexport class KeyObjectCache {\n    constructor(transformJwkToKeyObjectSyncFn = nodeWebCompat.transformJwkToKeyObjectSync, transformJwkToKeyObjectAsyncFn = nodeWebCompat.transformJwkToKeyObjectAsync) {\n        this.transformJwkToKeyObjectSyncFn = transformJwkToKeyObjectSyncFn;\n        this.transformJwkToKeyObjectAsyncFn = transformJwkToKeyObjectAsyncFn;\n        this.publicKeys = new Map();\n    }\n    /**\n     * Transform the JWK into an RSA public key in native key object format.\n     * If the transformed JWK is already in the cache, it is returned from the cache instead.\n     *\n     * @param jwk: the JWK\n     * @param jwtHeaderAlg: the alg from the JWT header (used if absent on JWK)\n     * @param issuer: the issuer that uses the JWK for signing JWTs (used for caching the transformation)\n     * @returns the RSA public key in native key object format\n     */\n    transformJwkToKeyObjectSync(jwk, jwtHeaderAlg, issuer) {\n        const alg = jwk.alg ?? jwtHeaderAlg;\n        if (!issuer || !jwk.kid || !alg) {\n            return this.transformJwkToKeyObjectSyncFn(jwk, alg, issuer);\n        }\n        const fromCache = this.publicKeys.get(issuer)?.get(jwk.kid)?.get(alg);\n        if (fromCache)\n            return fromCache;\n        const publicKey = this.transformJwkToKeyObjectSyncFn(jwk, alg, issuer);\n        this.putKeyObjectInCache(issuer, jwk.kid, alg, publicKey);\n        return publicKey;\n    }\n    /**\n     * Transform the JWK into an RSA public key in native key object format (async).\n     * If the transformed JWK is already in the cache, it is returned from the cache instead.\n     *\n     * @param jwk: the JWK\n     * @param jwtHeaderAlg: the alg from the JWT header (used if absent on JWK)\n     * @param issuer: the issuer that uses the JWK for signing JWTs (used for caching the transformation)\n     * @returns the RSA public key in native key object format\n     */\n    async transformJwkToKeyObjectAsync(jwk, jwtHeaderAlg, issuer) {\n        const alg = jwk.alg ?? jwtHeaderAlg;\n        if (!issuer || !jwk.kid || !alg) {\n            return this.transformJwkToKeyObjectAsyncFn(jwk, alg, issuer);\n        }\n        const fromCache = this.publicKeys.get(issuer)?.get(jwk.kid)?.get(alg);\n        if (fromCache)\n            return fromCache;\n        const publicKey = await this.transformJwkToKeyObjectAsyncFn(jwk, alg, issuer);\n        this.putKeyObjectInCache(issuer, jwk.kid, alg, publicKey);\n        return publicKey;\n    }\n    putKeyObjectInCache(issuer, kid, alg, publicKey) {\n        const cachedIssuer = this.publicKeys.get(issuer);\n        const cachedIssuerKid = cachedIssuer?.get(kid);\n        if (cachedIssuerKid) {\n            cachedIssuerKid.set(alg, publicKey);\n        }\n        else if (cachedIssuer) {\n            cachedIssuer.set(kid, new Map([[alg, publicKey]]));\n        }\n        else {\n            this.publicKeys.set(issuer, new Map([[kid, new Map([[alg, publicKey]])]]));\n        }\n    }\n    clearCache(issuer) {\n        this.publicKeys.delete(issuer);\n    }\n}\n", "// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n// SPDX-License-Identifier: Apache-2.0\nimport { CognitoJwtInvalidClientIdError, CognitoJwtInvalidGroupError, CognitoJwtInvalidTokenUseError, JwtInvalidClaimError, ParameterValidationError, } from \"./error.js\";\nimport { JwtRsaVerifierBase } from \"./jwt-rsa.js\";\nimport { assertStringArrayContainsString, assertStringEquals, assertStringArraysOverlap, } from \"./assert.js\";\n/**\n * Validate claims of a decoded Cognito JWT.\n * This function throws an error in case there's any validation issue.\n *\n * @param payload - The JSON parsed payload of the Cognito JWT\n * @param options - Validation options\n * @param options.groups - The cognito groups, of which at least one must be present in the JWT's cognito:groups claim\n * @param options.tokenUse - The required token use of the JWT: \"id\" or \"access\"\n * @param options.clientId - The required clientId of the JWT. May be an array of string, of which at least one must match\n * @returns void\n */\nexport function validateCognitoJwtFields(payload, options) {\n    // Check groups\n    if (options.groups != null) {\n        assertStringArraysOverlap(\"Cognito group\", payload[\"cognito:groups\"], options.groups, CognitoJwtInvalidGroupError);\n    }\n    // Check token use\n    assertStringArrayContainsString(\"Token use\", payload.token_use, [\"id\", \"access\"], CognitoJwtInvalidTokenUseError);\n    if (options.tokenUse !== null) {\n        if (options.tokenUse === undefined) {\n            throw new ParameterValidationError(\"tokenUse must be provided or set to null explicitly\");\n        }\n        assertStringEquals(\"Token use\", payload.token_use, options.tokenUse, CognitoJwtInvalidTokenUseError);\n    }\n    // Check clientId aka audience\n    if (options.clientId !== null) {\n        if (options.clientId === undefined) {\n            throw new ParameterValidationError(\"clientId must be provided or set to null explicitly\");\n        }\n        if (payload.token_use === \"id\") {\n            assertStringArrayContainsString('Client ID (\"audience\")', payload.aud, options.clientId, CognitoJwtInvalidClientIdError);\n        }\n        else {\n            assertStringArrayContainsString(\"Client ID\", payload.client_id, options.clientId, CognitoJwtInvalidClientIdError);\n        }\n    }\n}\n/**\n * Class representing a verifier for JWTs signed by Amazon Cognito\n */\nexport class CognitoJwtVerifier extends JwtRsaVerifierBase {\n    constructor(props, jwksCache) {\n        const issuerConfig = Array.isArray(props)\n            ? props.map((p) => ({\n                ...p,\n                ...CognitoJwtVerifier.parseUserPoolId(p.userPoolId),\n                audience: null, // checked instead by validateCognitoJwtFields\n            }))\n            : {\n                ...props,\n                ...CognitoJwtVerifier.parseUserPoolId(props.userPoolId),\n                audience: null, // checked instead by validateCognitoJwtFields\n            };\n        super(issuerConfig, jwksCache);\n    }\n    /**\n     * Parse a User Pool ID, to extract the issuer and JWKS URI\n     *\n     * @param userPoolId The User Pool ID\n     * @returns The issuer and JWKS URI for the User Pool\n     */\n    static parseUserPoolId(userPoolId) {\n        // Disable safe regexp check as userPoolId is provided by developer, i.e. is not user input\n        // eslint-disable-next-line security/detect-unsafe-regex\n        const match = userPoolId.match(/^(?<region>(\\w+-)?\\w+-\\w+-\\d)+_\\w+$/);\n        if (!match) {\n            throw new ParameterValidationError(`Invalid Cognito User Pool ID: ${userPoolId}`);\n        }\n        const region = match.groups.region;\n        const issuer = `https://cognito-idp.${region}.amazonaws.com/${userPoolId}`;\n        return {\n            issuer,\n            jwksUri: `${issuer}/.well-known/jwks.json`,\n        };\n    }\n    // eslint-disable-next-line @typescript-eslint/explicit-module-boundary-types\n    static create(verifyProperties, additionalProperties) {\n        return new this(verifyProperties, additionalProperties?.jwksCache);\n    }\n    /**\n     * Verify (synchronously) a JWT that is signed by Amazon Cognito.\n     *\n     * @param jwt The JWT, as string\n     * @param props Verification properties\n     * @returns The payload of the JWT\u2013\u2013if the JWT is valid, otherwise an error is thrown\n     */\n    verifySync(...[jwt, properties]) {\n        const { decomposedJwt, jwksUri, verifyProperties } = this.getVerifyParameters(jwt, properties);\n        this.verifyDecomposedJwtSync(decomposedJwt, jwksUri, verifyProperties);\n        try {\n            validateCognitoJwtFields(decomposedJwt.payload, verifyProperties);\n        }\n        catch (err) {\n            if (verifyProperties.includeRawJwtInErrors &&\n                err instanceof JwtInvalidClaimError) {\n                throw err.withRawJwt(decomposedJwt);\n            }\n            throw err;\n        }\n        return decomposedJwt.payload;\n    }\n    /**\n     * Verify (asynchronously) a JWT that is signed by Amazon Cognito.\n     * This call is asynchronous, and the JWKS will be fetched from the JWKS uri,\n     * in case it is not yet available in the cache.\n     *\n     * @param jwt The JWT, as string\n     * @param props Verification properties\n     * @returns Promise that resolves to the payload of the JWT\u2013\u2013if the JWT is valid, otherwise the promise rejects\n     */\n    async verify(...[jwt, properties]) {\n        const { decomposedJwt, jwksUri, verifyProperties } = this.getVerifyParameters(jwt, properties);\n        await this.verifyDecomposedJwt(decomposedJwt, jwksUri, verifyProperties);\n        try {\n            validateCognitoJwtFields(decomposedJwt.payload, verifyProperties);\n        }\n        catch (err) {\n            if (verifyProperties.includeRawJwtInErrors &&\n                err instanceof JwtInvalidClaimError) {\n                throw err.withRawJwt(decomposedJwt);\n            }\n            throw err;\n        }\n        return decomposedJwt.payload;\n    }\n    /**\n     * This method loads a JWKS that you provide, into the JWKS cache, so that it is\n     * available for JWT verification. Use this method to speed up the first JWT verification\n     * (when the JWKS would otherwise have to be downloaded from the JWKS uri), or to provide the JWKS\n     * in case the JwtVerifier does not have internet access to download the JWKS\n     *\n     * @param jwks The JWKS\n     * @param userPoolId The userPoolId for which you want to cache the JWKS\n     *  Supply this field, if you instantiated the CognitoJwtVerifier with multiple userPoolIds\n     * @returns void\n     */\n    cacheJwks(...[jwks, userPoolId]) {\n        let issuer;\n        if (userPoolId !== undefined) {\n            issuer = CognitoJwtVerifier.parseUserPoolId(userPoolId).issuer;\n        }\n        else if (this.expectedIssuers.length > 1) {\n            throw new ParameterValidationError(\"userPoolId must be provided\");\n        }\n        const issuerConfig = this.getIssuerConfig(issuer);\n        super.cacheJwks(jwks, issuerConfig.issuer);\n    }\n}\n", "/**\n * The response for a successful request.\n * Should include a body for GET, PUT, or POST.\n * Need not include a body for DELETE\n */\nexport const ok = (body?: Object) => new HttpResponse(StatusCodes.Ok, body);\n\n/**\n * The response for a successful POST or PUT request,\n * which resulted in the creation of a new resource.\n */\nexport const created = (body: Object) =>\n  new HttpResponse(StatusCodes.Created, body);\n\n/**\n * The response for a successful request which returns no content.\n */\nexport const noContent = (body?: Object) =>\n  new HttpResponse(StatusCodes.NoContent, body);\n\n/**\n * The response for a failed request, due to client-side issues.\n * Typically indicates a missing parameter or malformed body.\n */\nexport const badRequest = (body?: Object) =>\n  new HttpResponse(StatusCodes.BadRequest, body);\n\n/**\n * The response for a client without any authorization.\n * Typically indicates an issue with the request's headers or token.\n *\n * Note: The usual name for HTTP 401 is \"Unauthorized\", but that's misleading.\n * Authentication is for identity; authorization is for permissions.\n */\nexport const unauthenticated = (body?: Object) =>\n  new HttpResponse(StatusCodes.Unauthenticated, body);\n\n/**\n * The response for a client without sufficient permissions.\n * This is specific to the requested operation.\n * For example, a regular user requesting an admin-only endpoint.\n */\nexport const forbidden = (body?: Object) =>\n  new HttpResponse(StatusCodes.Forbidden, body);\n\n/**\n * The response for a request that assumes the existence of a missing resource.\n * For example, attempting to submit a report that isn't in the database.\n */\nexport const notFound = (body?: Object) =>\n  new HttpResponse(StatusCodes.NotFound, body);\n\n/**\n * The response for a request that assumes the server is in a different state.\n * For example, attempting to submit a report that's already submitted.\n */\nexport const conflict = (body?: Object) =>\n  new HttpResponse(StatusCodes.Conflict, body);\n\n/**\n * The response for a request that errored out on the server side.\n * Typically indicates there is nothing the client can do to resolve the issue.\n */\nexport const internalServerError = (body?: Object) =>\n  new HttpResponse(StatusCodes.InternalServerError, body);\n\n/**\n * Note: Production code shouldn't need to reference this directly.\n * Use a helper method instead.\n *\n * This enum is listed mainly for the purpose of unit testing.\n */\nexport enum StatusCodes {\n  Ok = 200,\n  Created = 201,\n  NoContent = 204,\n  BadRequest = 400,\n  Unauthenticated = 401,\n  Forbidden = 403,\n  NotFound = 404,\n  Conflict = 409,\n  InternalServerError = 500,\n}\n\n/**\n * Note: Production code shouldn't need to reference this directly.\n * Use a helper method instead.\n */\nexport class HttpResponse {\n  readonly statusCode: number;\n  readonly body: string | undefined;\n  readonly headers = {\n    \"Access-Control-Allow-Origin\": \"*\",\n    \"Access-Control-Allow-Credentials\": true,\n  };\n  constructor(statusCode: number, body?: Object | undefined) {\n    this.statusCode = statusCode;\n    if (body !== undefined) {\n      this.body = JSON.stringify(body);\n    }\n  }\n}\n", "import { ReportType } from \"../types/reports\";\n\nexport const error = {\n  // generic errors\n  UNAUTHORIZED: \"User is not authorized to access this resource.\",\n  NO_KEY: \"Must provide key for table.\",\n  MISSING_DATA: \"Missing required data.\",\n  INVALID_DATA: \"Provided data is not valid.\",\n  NO_MATCHING_RECORD: \"No matching record found.\",\n  SERVER_ERROR: \"An unspecified server error occured.\",\n  // bucket errors\n  S3_OBJECT_CREATION_ERROR: \"Report could not be created due to an S3 error.\",\n  S3_OBJECT_UPDATE_ERROR: \"Report could not be updated due to an S3 error.\",\n  S3_OBJECT_GET_ERROR: \"Error while fetching report.\",\n  // dynamo errors\n  DYNAMO_CREATION_ERROR: \"Report could not be created due to a database error.\",\n  DYNAMO_UPDATE_ERROR: \"Report could not be updated due to a database error.\",\n  // template errors\n  NO_TEMPLATE_NAME: \"Must request template for download.\",\n  INVALID_TEMPLATE_NAME: \"Requested template does not exist or does not match.\",\n  NOT_IN_DATABASE: \"Record not found in database.\",\n  UNABLE_TO_COPY:\n    \"Unable to copy over report if todays date is in the same period and year as a previous report.\",\n  MISSING_FORM_TEMPLATE: \"Form Template not found in S3.\",\n  MISSING_FIELD_DATA: \"Field Data not found in S3.\",\n  NO_WORKPLANS_FOUND: \"No record of Work Plans found in database\",\n  // admin action errors\n  ALREADY_ARCHIVED: \"Cannot update archived report.\",\n  ALREADY_LOCKED: \"Cannot update locked report.\",\n  REPORT_INCOMPLETE: \"Cannot submit incomplete form.\",\n} as const;\n\nexport const buckets = {\n  FORM_TEMPLATE: \"formTemplates\",\n  FIELD_DATA: \"fieldData\",\n};\n\n// STATES\nexport enum States {\n  AL = \"Alabama\",\n  AK = \"Alaska\",\n  AS = \"American Samoa\",\n  AZ = \"Arizona\",\n  AR = \"Arkansas\",\n  CA = \"California\",\n  CO = \"Colorado\",\n  CT = \"Connecticut\",\n  DE = \"Delaware\",\n  DC = \"District of Columbia\",\n  FM = \"Federated States of Micronesia\",\n  FL = \"Florida\",\n  GA = \"Georgia\",\n  GU = \"Guam\",\n  HI = \"Hawaii\",\n  ID = \"Idaho\",\n  IL = \"Illinois\",\n  IN = \"Indiana\",\n  IA = \"Iowa\",\n  KS = \"Kansas\",\n  KY = \"Kentucky\",\n  LA = \"Louisiana\",\n  ME = \"Maine\",\n  MH = \"Marshall Islands\",\n  MD = \"Maryland\",\n  MA = \"Massachusetts\",\n  MI = \"Michigan\",\n  MN = \"Minnesota\",\n  MS = \"Mississippi\",\n  MO = \"Missouri\",\n  MT = \"Montana\",\n  NE = \"Nebraska\",\n  NV = \"Nevada\",\n  NH = \"New Hampshire\",\n  NJ = \"New Jersey\",\n  NM = \"New Mexico\",\n  NY = \"New York\",\n  NC = \"North Carolina\",\n  ND = \"North Dakota\",\n  MP = \"Northern Mariana Islands\",\n  OH = \"Ohio\",\n  OK = \"Oklahoma\",\n  OR = \"Oregon\",\n  PW = \"Palau\",\n  PA = \"Pennsylvania\",\n  PR = \"Puerto Rico\",\n  RI = \"Rhode Island\",\n  SC = \"South Carolina\",\n  SD = \"South Dakota\",\n  TN = \"Tennessee\",\n  TX = \"Texas\",\n  UT = \"Utah\",\n  VT = \"Vermont\",\n  VI = \"Virgin Islands\",\n  VA = \"Virginia\",\n  WA = \"Washington\",\n  WV = \"West Virginia\",\n  WI = \"Wisconsin\",\n  WY = \"Wyoming\",\n}\n\n// REPORTS\n\nexport const reportTables: { [key in ReportType]: string } = {\n  SAR: process.env.SarReportsTable!,\n  WP: process.env.WpReportsTable!,\n};\n\nexport const reportBuckets: { [key in ReportType]: string } = {\n  SAR: process.env.SAR_FORM_BUCKET!,\n  WP: process.env.WP_FORM_BUCKET!,\n};\n\nexport const reportNames: { [key in ReportType]: string } = {\n  SAR: \"SAR\",\n  WP: \"Work Plan\",\n};\n\nexport const tableTopics: { [key in ReportType]: string } = {\n  SAR: \"sar-reports\",\n  WP: \"wp-reports\",\n};\n\nexport const bucketTopics: { [key in ReportType]: string } = {\n  SAR: \"sar-form\",\n  WP: \"wp-form\",\n};\n\nexport const DEFAULT_TARGET_POPULATION_NAMES = [\n  \"Older adults\",\n  \"Individuals with physical disabilities (PD)\",\n  \"Individuals with intellectual and developmental disabilities (I/DD)\",\n  \"Individuals with mental health and substance use disorders (MH/SUD)\",\n  \"HCBS infrastructure/system-level development\",\n];\n", "/*! @license DOMPurify 3.2.4 | (c) Cure53 and other contributors | Released under the Apache license 2.0 and Mozilla Public License 2.0 | github.com/cure53/DOMPurify/blob/3.2.4/LICENSE */\n\nconst {\n  entries,\n  setPrototypeOf,\n  isFrozen,\n  getPrototypeOf,\n  getOwnPropertyDescriptor\n} = Object;\nlet {\n  freeze,\n  seal,\n  create\n} = Object; // eslint-disable-line import/no-mutable-exports\nlet {\n  apply,\n  construct\n} = typeof Reflect !== 'undefined' && Reflect;\nif (!freeze) {\n  freeze = function freeze(x) {\n    return x;\n  };\n}\nif (!seal) {\n  seal = function seal(x) {\n    return x;\n  };\n}\nif (!apply) {\n  apply = function apply(fun, thisValue, args) {\n    return fun.apply(thisValue, args);\n  };\n}\nif (!construct) {\n  construct = function construct(Func, args) {\n    return new Func(...args);\n  };\n}\nconst arrayForEach = unapply(Array.prototype.forEach);\nconst arrayLastIndexOf = unapply(Array.prototype.lastIndexOf);\nconst arrayPop = unapply(Array.prototype.pop);\nconst arrayPush = unapply(Array.prototype.push);\nconst arraySplice = unapply(Array.prototype.splice);\nconst stringToLowerCase = unapply(String.prototype.toLowerCase);\nconst stringToString = unapply(String.prototype.toString);\nconst stringMatch = unapply(String.prototype.match);\nconst stringReplace = unapply(String.prototype.replace);\nconst stringIndexOf = unapply(String.prototype.indexOf);\nconst stringTrim = unapply(String.prototype.trim);\nconst objectHasOwnProperty = unapply(Object.prototype.hasOwnProperty);\nconst regExpTest = unapply(RegExp.prototype.test);\nconst typeErrorCreate = unconstruct(TypeError);\n/**\n * Creates a new function that calls the given function with a specified thisArg and arguments.\n *\n * @param func - The function to be wrapped and called.\n * @returns A new function that calls the given function with a specified thisArg and arguments.\n */\nfunction unapply(func) {\n  return function (thisArg) {\n    for (var _len = arguments.length, args = new Array(_len > 1 ? _len - 1 : 0), _key = 1; _key < _len; _key++) {\n      args[_key - 1] = arguments[_key];\n    }\n    return apply(func, thisArg, args);\n  };\n}\n/**\n * Creates a new function that constructs an instance of the given constructor function with the provided arguments.\n *\n * @param func - The constructor function to be wrapped and called.\n * @returns A new function that constructs an instance of the given constructor function with the provided arguments.\n */\nfunction unconstruct(func) {\n  return function () {\n    for (var _len2 = arguments.length, args = new Array(_len2), _key2 = 0; _key2 < _len2; _key2++) {\n      args[_key2] = arguments[_key2];\n    }\n    return construct(func, args);\n  };\n}\n/**\n * Add properties to a lookup table\n *\n * @param set - The set to which elements will be added.\n * @param array - The array containing elements to be added to the set.\n * @param transformCaseFunc - An optional function to transform the case of each element before adding to the set.\n * @returns The modified set with added elements.\n */\nfunction addToSet(set, array) {\n  let transformCaseFunc = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : stringToLowerCase;\n  if (setPrototypeOf) {\n    // Make 'in' and truthy checks like Boolean(set.constructor)\n    // independent of any properties defined on Object.prototype.\n    // Prevent prototype setters from intercepting set as a this value.\n    setPrototypeOf(set, null);\n  }\n  let l = array.length;\n  while (l--) {\n    let element = array[l];\n    if (typeof element === 'string') {\n      const lcElement = transformCaseFunc(element);\n      if (lcElement !== element) {\n        // Config presets (e.g. tags.js, attrs.js) are immutable.\n        if (!isFrozen(array)) {\n          array[l] = lcElement;\n        }\n        element = lcElement;\n      }\n    }\n    set[element] = true;\n  }\n  return set;\n}\n/**\n * Clean up an array to harden against CSPP\n *\n * @param array - The array to be cleaned.\n * @returns The cleaned version of the array\n */\nfunction cleanArray(array) {\n  for (let index = 0; index < array.length; index++) {\n    const isPropertyExist = objectHasOwnProperty(array, index);\n    if (!isPropertyExist) {\n      array[index] = null;\n    }\n  }\n  return array;\n}\n/**\n * Shallow clone an object\n *\n * @param object - The object to be cloned.\n * @returns A new object that copies the original.\n */\nfunction clone(object) {\n  const newObject = create(null);\n  for (const [property, value] of entries(object)) {\n    const isPropertyExist = objectHasOwnProperty(object, property);\n    if (isPropertyExist) {\n      if (Array.isArray(value)) {\n        newObject[property] = cleanArray(value);\n      } else if (value && typeof value === 'object' && value.constructor === Object) {\n        newObject[property] = clone(value);\n      } else {\n        newObject[property] = value;\n      }\n    }\n  }\n  return newObject;\n}\n/**\n * This method automatically checks if the prop is function or getter and behaves accordingly.\n *\n * @param object - The object to look up the getter function in its prototype chain.\n * @param prop - The property name for which to find the getter function.\n * @returns The getter function found in the prototype chain or a fallback function.\n */\nfunction lookupGetter(object, prop) {\n  while (object !== null) {\n    const desc = getOwnPropertyDescriptor(object, prop);\n    if (desc) {\n      if (desc.get) {\n        return unapply(desc.get);\n      }\n      if (typeof desc.value === 'function') {\n        return unapply(desc.value);\n      }\n    }\n    object = getPrototypeOf(object);\n  }\n  function fallbackValue() {\n    return null;\n  }\n  return fallbackValue;\n}\n\nconst html$1 = freeze(['a', 'abbr', 'acronym', 'address', 'area', 'article', 'aside', 'audio', 'b', 'bdi', 'bdo', 'big', 'blink', 'blockquote', 'body', 'br', 'button', 'canvas', 'caption', 'center', 'cite', 'code', 'col', 'colgroup', 'content', 'data', 'datalist', 'dd', 'decorator', 'del', 'details', 'dfn', 'dialog', 'dir', 'div', 'dl', 'dt', 'element', 'em', 'fieldset', 'figcaption', 'figure', 'font', 'footer', 'form', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'head', 'header', 'hgroup', 'hr', 'html', 'i', 'img', 'input', 'ins', 'kbd', 'label', 'legend', 'li', 'main', 'map', 'mark', 'marquee', 'menu', 'menuitem', 'meter', 'nav', 'nobr', 'ol', 'optgroup', 'option', 'output', 'p', 'picture', 'pre', 'progress', 'q', 'rp', 'rt', 'ruby', 's', 'samp', 'section', 'select', 'shadow', 'small', 'source', 'spacer', 'span', 'strike', 'strong', 'style', 'sub', 'summary', 'sup', 'table', 'tbody', 'td', 'template', 'textarea', 'tfoot', 'th', 'thead', 'time', 'tr', 'track', 'tt', 'u', 'ul', 'var', 'video', 'wbr']);\nconst svg$1 = freeze(['svg', 'a', 'altglyph', 'altglyphdef', 'altglyphitem', 'animatecolor', 'animatemotion', 'animatetransform', 'circle', 'clippath', 'defs', 'desc', 'ellipse', 'filter', 'font', 'g', 'glyph', 'glyphref', 'hkern', 'image', 'line', 'lineargradient', 'marker', 'mask', 'metadata', 'mpath', 'path', 'pattern', 'polygon', 'polyline', 'radialgradient', 'rect', 'stop', 'style', 'switch', 'symbol', 'text', 'textpath', 'title', 'tref', 'tspan', 'view', 'vkern']);\nconst svgFilters = freeze(['feBlend', 'feColorMatrix', 'feComponentTransfer', 'feComposite', 'feConvolveMatrix', 'feDiffuseLighting', 'feDisplacementMap', 'feDistantLight', 'feDropShadow', 'feFlood', 'feFuncA', 'feFuncB', 'feFuncG', 'feFuncR', 'feGaussianBlur', 'feImage', 'feMerge', 'feMergeNode', 'feMorphology', 'feOffset', 'fePointLight', 'feSpecularLighting', 'feSpotLight', 'feTile', 'feTurbulence']);\n// List of SVG elements that are disallowed by default.\n// We still need to know them so that we can do namespace\n// checks properly in case one wants to add them to\n// allow-list.\nconst svgDisallowed = freeze(['animate', 'color-profile', 'cursor', 'discard', 'font-face', 'font-face-format', 'font-face-name', 'font-face-src', 'font-face-uri', 'foreignobject', 'hatch', 'hatchpath', 'mesh', 'meshgradient', 'meshpatch', 'meshrow', 'missing-glyph', 'script', 'set', 'solidcolor', 'unknown', 'use']);\nconst mathMl$1 = freeze(['math', 'menclose', 'merror', 'mfenced', 'mfrac', 'mglyph', 'mi', 'mlabeledtr', 'mmultiscripts', 'mn', 'mo', 'mover', 'mpadded', 'mphantom', 'mroot', 'mrow', 'ms', 'mspace', 'msqrt', 'mstyle', 'msub', 'msup', 'msubsup', 'mtable', 'mtd', 'mtext', 'mtr', 'munder', 'munderover', 'mprescripts']);\n// Similarly to SVG, we want to know all MathML elements,\n// even those that we disallow by default.\nconst mathMlDisallowed = freeze(['maction', 'maligngroup', 'malignmark', 'mlongdiv', 'mscarries', 'mscarry', 'msgroup', 'mstack', 'msline', 'msrow', 'semantics', 'annotation', 'annotation-xml', 'mprescripts', 'none']);\nconst text = freeze(['#text']);\n\nconst html = freeze(['accept', 'action', 'align', 'alt', 'autocapitalize', 'autocomplete', 'autopictureinpicture', 'autoplay', 'background', 'bgcolor', 'border', 'capture', 'cellpadding', 'cellspacing', 'checked', 'cite', 'class', 'clear', 'color', 'cols', 'colspan', 'controls', 'controlslist', 'coords', 'crossorigin', 'datetime', 'decoding', 'default', 'dir', 'disabled', 'disablepictureinpicture', 'disableremoteplayback', 'download', 'draggable', 'enctype', 'enterkeyhint', 'face', 'for', 'headers', 'height', 'hidden', 'high', 'href', 'hreflang', 'id', 'inputmode', 'integrity', 'ismap', 'kind', 'label', 'lang', 'list', 'loading', 'loop', 'low', 'max', 'maxlength', 'media', 'method', 'min', 'minlength', 'multiple', 'muted', 'name', 'nonce', 'noshade', 'novalidate', 'nowrap', 'open', 'optimum', 'pattern', 'placeholder', 'playsinline', 'popover', 'popovertarget', 'popovertargetaction', 'poster', 'preload', 'pubdate', 'radiogroup', 'readonly', 'rel', 'required', 'rev', 'reversed', 'role', 'rows', 'rowspan', 'spellcheck', 'scope', 'selected', 'shape', 'size', 'sizes', 'span', 'srclang', 'start', 'src', 'srcset', 'step', 'style', 'summary', 'tabindex', 'title', 'translate', 'type', 'usemap', 'valign', 'value', 'width', 'wrap', 'xmlns', 'slot']);\nconst svg = freeze(['accent-height', 'accumulate', 'additive', 'alignment-baseline', 'amplitude', 'ascent', 'attributename', 'attributetype', 'azimuth', 'basefrequency', 'baseline-shift', 'begin', 'bias', 'by', 'class', 'clip', 'clippathunits', 'clip-path', 'clip-rule', 'color', 'color-interpolation', 'color-interpolation-filters', 'color-profile', 'color-rendering', 'cx', 'cy', 'd', 'dx', 'dy', 'diffuseconstant', 'direction', 'display', 'divisor', 'dur', 'edgemode', 'elevation', 'end', 'exponent', 'fill', 'fill-opacity', 'fill-rule', 'filter', 'filterunits', 'flood-color', 'flood-opacity', 'font-family', 'font-size', 'font-size-adjust', 'font-stretch', 'font-style', 'font-variant', 'font-weight', 'fx', 'fy', 'g1', 'g2', 'glyph-name', 'glyphref', 'gradientunits', 'gradienttransform', 'height', 'href', 'id', 'image-rendering', 'in', 'in2', 'intercept', 'k', 'k1', 'k2', 'k3', 'k4', 'kerning', 'keypoints', 'keysplines', 'keytimes', 'lang', 'lengthadjust', 'letter-spacing', 'kernelmatrix', 'kernelunitlength', 'lighting-color', 'local', 'marker-end', 'marker-mid', 'marker-start', 'markerheight', 'markerunits', 'markerwidth', 'maskcontentunits', 'maskunits', 'max', 'mask', 'media', 'method', 'mode', 'min', 'name', 'numoctaves', 'offset', 'operator', 'opacity', 'order', 'orient', 'orientation', 'origin', 'overflow', 'paint-order', 'path', 'pathlength', 'patterncontentunits', 'patterntransform', 'patternunits', 'points', 'preservealpha', 'preserveaspectratio', 'primitiveunits', 'r', 'rx', 'ry', 'radius', 'refx', 'refy', 'repeatcount', 'repeatdur', 'restart', 'result', 'rotate', 'scale', 'seed', 'shape-rendering', 'slope', 'specularconstant', 'specularexponent', 'spreadmethod', 'startoffset', 'stddeviation', 'stitchtiles', 'stop-color', 'stop-opacity', 'stroke-dasharray', 'stroke-dashoffset', 'stroke-linecap', 'stroke-linejoin', 'stroke-miterlimit', 'stroke-opacity', 'stroke', 'stroke-width', 'style', 'surfacescale', 'systemlanguage', 'tabindex', 'tablevalues', 'targetx', 'targety', 'transform', 'transform-origin', 'text-anchor', 'text-decoration', 'text-rendering', 'textlength', 'type', 'u1', 'u2', 'unicode', 'values', 'viewbox', 'visibility', 'version', 'vert-adv-y', 'vert-origin-x', 'vert-origin-y', 'width', 'word-spacing', 'wrap', 'writing-mode', 'xchannelselector', 'ychannelselector', 'x', 'x1', 'x2', 'xmlns', 'y', 'y1', 'y2', 'z', 'zoomandpan']);\nconst mathMl = freeze(['accent', 'accentunder', 'align', 'bevelled', 'close', 'columnsalign', 'columnlines', 'columnspan', 'denomalign', 'depth', 'dir', 'display', 'displaystyle', 'encoding', 'fence', 'frame', 'height', 'href', 'id', 'largeop', 'length', 'linethickness', 'lspace', 'lquote', 'mathbackground', 'mathcolor', 'mathsize', 'mathvariant', 'maxsize', 'minsize', 'movablelimits', 'notation', 'numalign', 'open', 'rowalign', 'rowlines', 'rowspacing', 'rowspan', 'rspace', 'rquote', 'scriptlevel', 'scriptminsize', 'scriptsizemultiplier', 'selection', 'separator', 'separators', 'stretchy', 'subscriptshift', 'supscriptshift', 'symmetric', 'voffset', 'width', 'xmlns']);\nconst xml = freeze(['xlink:href', 'xml:id', 'xlink:title', 'xml:space', 'xmlns:xlink']);\n\n// eslint-disable-next-line unicorn/better-regex\nconst MUSTACHE_EXPR = seal(/\\{\\{[\\w\\W]*|[\\w\\W]*\\}\\}/gm); // Specify template detection regex for SAFE_FOR_TEMPLATES mode\nconst ERB_EXPR = seal(/<%[\\w\\W]*|[\\w\\W]*%>/gm);\nconst TMPLIT_EXPR = seal(/\\$\\{[\\w\\W]*/gm); // eslint-disable-line unicorn/better-regex\nconst DATA_ATTR = seal(/^data-[\\-\\w.\\u00B7-\\uFFFF]+$/); // eslint-disable-line no-useless-escape\nconst ARIA_ATTR = seal(/^aria-[\\-\\w]+$/); // eslint-disable-line no-useless-escape\nconst IS_ALLOWED_URI = seal(/^(?:(?:(?:f|ht)tps?|mailto|tel|callto|sms|cid|xmpp):|[^a-z]|[a-z+.\\-]+(?:[^a-z+.\\-:]|$))/i // eslint-disable-line no-useless-escape\n);\nconst IS_SCRIPT_OR_DATA = seal(/^(?:\\w+script|data):/i);\nconst ATTR_WHITESPACE = seal(/[\\u0000-\\u0020\\u00A0\\u1680\\u180E\\u2000-\\u2029\\u205F\\u3000]/g // eslint-disable-line no-control-regex\n);\nconst DOCTYPE_NAME = seal(/^html$/i);\nconst CUSTOM_ELEMENT = seal(/^[a-z][.\\w]*(-[.\\w]+)+$/i);\n\nvar EXPRESSIONS = /*#__PURE__*/Object.freeze({\n  __proto__: null,\n  ARIA_ATTR: ARIA_ATTR,\n  ATTR_WHITESPACE: ATTR_WHITESPACE,\n  CUSTOM_ELEMENT: CUSTOM_ELEMENT,\n  DATA_ATTR: DATA_ATTR,\n  DOCTYPE_NAME: DOCTYPE_NAME,\n  ERB_EXPR: ERB_EXPR,\n  IS_ALLOWED_URI: IS_ALLOWED_URI,\n  IS_SCRIPT_OR_DATA: IS_SCRIPT_OR_DATA,\n  MUSTACHE_EXPR: MUSTACHE_EXPR,\n  TMPLIT_EXPR: TMPLIT_EXPR\n});\n\n/* eslint-disable @typescript-eslint/indent */\n// https://developer.mozilla.org/en-US/docs/Web/API/Node/nodeType\nconst NODE_TYPE = {\n  element: 1,\n  attribute: 2,\n  text: 3,\n  cdataSection: 4,\n  entityReference: 5,\n  // Deprecated\n  entityNode: 6,\n  // Deprecated\n  progressingInstruction: 7,\n  comment: 8,\n  document: 9,\n  documentType: 10,\n  documentFragment: 11,\n  notation: 12 // Deprecated\n};\nconst getGlobal = function getGlobal() {\n  return typeof window === 'undefined' ? null : window;\n};\n/**\n * Creates a no-op policy for internal use only.\n * Don't export this function outside this module!\n * @param trustedTypes The policy factory.\n * @param purifyHostElement The Script element used to load DOMPurify (to determine policy name suffix).\n * @return The policy created (or null, if Trusted Types\n * are not supported or creating the policy failed).\n */\nconst _createTrustedTypesPolicy = function _createTrustedTypesPolicy(trustedTypes, purifyHostElement) {\n  if (typeof trustedTypes !== 'object' || typeof trustedTypes.createPolicy !== 'function') {\n    return null;\n  }\n  // Allow the callers to control the unique policy name\n  // by adding a data-tt-policy-suffix to the script element with the DOMPurify.\n  // Policy creation with duplicate names throws in Trusted Types.\n  let suffix = null;\n  const ATTR_NAME = 'data-tt-policy-suffix';\n  if (purifyHostElement && purifyHostElement.hasAttribute(ATTR_NAME)) {\n    suffix = purifyHostElement.getAttribute(ATTR_NAME);\n  }\n  const policyName = 'dompurify' + (suffix ? '#' + suffix : '');\n  try {\n    return trustedTypes.createPolicy(policyName, {\n      createHTML(html) {\n        return html;\n      },\n      createScriptURL(scriptUrl) {\n        return scriptUrl;\n      }\n    });\n  } catch (_) {\n    // Policy creation failed (most likely another DOMPurify script has\n    // already run). Skip creating the policy, as this will only cause errors\n    // if TT are enforced.\n    console.warn('TrustedTypes policy ' + policyName + ' could not be created.');\n    return null;\n  }\n};\nconst _createHooksMap = function _createHooksMap() {\n  return {\n    afterSanitizeAttributes: [],\n    afterSanitizeElements: [],\n    afterSanitizeShadowDOM: [],\n    beforeSanitizeAttributes: [],\n    beforeSanitizeElements: [],\n    beforeSanitizeShadowDOM: [],\n    uponSanitizeAttribute: [],\n    uponSanitizeElement: [],\n    uponSanitizeShadowNode: []\n  };\n};\nfunction createDOMPurify() {\n  let window = arguments.length > 0 && arguments[0] !== undefined ? arguments[0] : getGlobal();\n  const DOMPurify = root => createDOMPurify(root);\n  DOMPurify.version = '3.2.4';\n  DOMPurify.removed = [];\n  if (!window || !window.document || window.document.nodeType !== NODE_TYPE.document || !window.Element) {\n    // Not running in a browser, provide a factory function\n    // so that you can pass your own Window\n    DOMPurify.isSupported = false;\n    return DOMPurify;\n  }\n  let {\n    document\n  } = window;\n  const originalDocument = document;\n  const currentScript = originalDocument.currentScript;\n  const {\n    DocumentFragment,\n    HTMLTemplateElement,\n    Node,\n    Element,\n    NodeFilter,\n    NamedNodeMap = window.NamedNodeMap || window.MozNamedAttrMap,\n    HTMLFormElement,\n    DOMParser,\n    trustedTypes\n  } = window;\n  const ElementPrototype = Element.prototype;\n  const cloneNode = lookupGetter(ElementPrototype, 'cloneNode');\n  const remove = lookupGetter(ElementPrototype, 'remove');\n  const getNextSibling = lookupGetter(ElementPrototype, 'nextSibling');\n  const getChildNodes = lookupGetter(ElementPrototype, 'childNodes');\n  const getParentNode = lookupGetter(ElementPrototype, 'parentNode');\n  // As per issue #47, the web-components registry is inherited by a\n  // new document created via createHTMLDocument. As per the spec\n  // (http://w3c.github.io/webcomponents/spec/custom/#creating-and-passing-registries)\n  // a new empty registry is used when creating a template contents owner\n  // document, so we use that as our parent document to ensure nothing\n  // is inherited.\n  if (typeof HTMLTemplateElement === 'function') {\n    const template = document.createElement('template');\n    if (template.content && template.content.ownerDocument) {\n      document = template.content.ownerDocument;\n    }\n  }\n  let trustedTypesPolicy;\n  let emptyHTML = '';\n  const {\n    implementation,\n    createNodeIterator,\n    createDocumentFragment,\n    getElementsByTagName\n  } = document;\n  const {\n    importNode\n  } = originalDocument;\n  let hooks = _createHooksMap();\n  /**\n   * Expose whether this browser supports running the full DOMPurify.\n   */\n  DOMPurify.isSupported = typeof entries === 'function' && typeof getParentNode === 'function' && implementation && implementation.createHTMLDocument !== undefined;\n  const {\n    MUSTACHE_EXPR,\n    ERB_EXPR,\n    TMPLIT_EXPR,\n    DATA_ATTR,\n    ARIA_ATTR,\n    IS_SCRIPT_OR_DATA,\n    ATTR_WHITESPACE,\n    CUSTOM_ELEMENT\n  } = EXPRESSIONS;\n  let {\n    IS_ALLOWED_URI: IS_ALLOWED_URI$1\n  } = EXPRESSIONS;\n  /**\n   * We consider the elements and attributes below to be safe. Ideally\n   * don't add any new ones but feel free to remove unwanted ones.\n   */\n  /* allowed element names */\n  let ALLOWED_TAGS = null;\n  const DEFAULT_ALLOWED_TAGS = addToSet({}, [...html$1, ...svg$1, ...svgFilters, ...mathMl$1, ...text]);\n  /* Allowed attribute names */\n  let ALLOWED_ATTR = null;\n  const DEFAULT_ALLOWED_ATTR = addToSet({}, [...html, ...svg, ...mathMl, ...xml]);\n  /*\n   * Configure how DOMPurify should handle custom elements and their attributes as well as customized built-in elements.\n   * @property {RegExp|Function|null} tagNameCheck one of [null, regexPattern, predicate]. Default: `null` (disallow any custom elements)\n   * @property {RegExp|Function|null} attributeNameCheck one of [null, regexPattern, predicate]. Default: `null` (disallow any attributes not on the allow list)\n   * @property {boolean} allowCustomizedBuiltInElements allow custom elements derived from built-ins if they pass CUSTOM_ELEMENT_HANDLING.tagNameCheck. Default: `false`.\n   */\n  let CUSTOM_ELEMENT_HANDLING = Object.seal(create(null, {\n    tagNameCheck: {\n      writable: true,\n      configurable: false,\n      enumerable: true,\n      value: null\n    },\n    attributeNameCheck: {\n      writable: true,\n      configurable: false,\n      enumerable: true,\n      value: null\n    },\n    allowCustomizedBuiltInElements: {\n      writable: true,\n      configurable: false,\n      enumerable: true,\n      value: false\n    }\n  }));\n  /* Explicitly forbidden tags (overrides ALLOWED_TAGS/ADD_TAGS) */\n  let FORBID_TAGS = null;\n  /* Explicitly forbidden attributes (overrides ALLOWED_ATTR/ADD_ATTR) */\n  let FORBID_ATTR = null;\n  /* Decide if ARIA attributes are okay */\n  let ALLOW_ARIA_ATTR = true;\n  /* Decide if custom data attributes are okay */\n  let ALLOW_DATA_ATTR = true;\n  /* Decide if unknown protocols are okay */\n  let ALLOW_UNKNOWN_PROTOCOLS = false;\n  /* Decide if self-closing tags in attributes are allowed.\n   * Usually removed due to a mXSS issue in jQuery 3.0 */\n  let ALLOW_SELF_CLOSE_IN_ATTR = true;\n  /* Output should be safe for common template engines.\n   * This means, DOMPurify removes data attributes, mustaches and ERB\n   */\n  let SAFE_FOR_TEMPLATES = false;\n  /* Output should be safe even for XML used within HTML and alike.\n   * This means, DOMPurify removes comments when containing risky content.\n   */\n  let SAFE_FOR_XML = true;\n  /* Decide if document with <html>... should be returned */\n  let WHOLE_DOCUMENT = false;\n  /* Track whether config is already set on this instance of DOMPurify. */\n  let SET_CONFIG = false;\n  /* Decide if all elements (e.g. style, script) must be children of\n   * document.body. By default, browsers might move them to document.head */\n  let FORCE_BODY = false;\n  /* Decide if a DOM `HTMLBodyElement` should be returned, instead of a html\n   * string (or a TrustedHTML object if Trusted Types are supported).\n   * If `WHOLE_DOCUMENT` is enabled a `HTMLHtmlElement` will be returned instead\n   */\n  let RETURN_DOM = false;\n  /* Decide if a DOM `DocumentFragment` should be returned, instead of a html\n   * string  (or a TrustedHTML object if Trusted Types are supported) */\n  let RETURN_DOM_FRAGMENT = false;\n  /* Try to return a Trusted Type object instead of a string, return a string in\n   * case Trusted Types are not supported  */\n  let RETURN_TRUSTED_TYPE = false;\n  /* Output should be free from DOM clobbering attacks?\n   * This sanitizes markups named with colliding, clobberable built-in DOM APIs.\n   */\n  let SANITIZE_DOM = true;\n  /* Achieve full DOM Clobbering protection by isolating the namespace of named\n   * properties and JS variables, mitigating attacks that abuse the HTML/DOM spec rules.\n   *\n   * HTML/DOM spec rules that enable DOM Clobbering:\n   *   - Named Access on Window (\u00A77.3.3)\n   *   - DOM Tree Accessors (\u00A73.1.5)\n   *   - Form Element Parent-Child Relations (\u00A74.10.3)\n   *   - Iframe srcdoc / Nested WindowProxies (\u00A74.8.5)\n   *   - HTMLCollection (\u00A74.2.10.2)\n   *\n   * Namespace isolation is implemented by prefixing `id` and `name` attributes\n   * with a constant string, i.e., `user-content-`\n   */\n  let SANITIZE_NAMED_PROPS = false;\n  const SANITIZE_NAMED_PROPS_PREFIX = 'user-content-';\n  /* Keep element content when removing element? */\n  let KEEP_CONTENT = true;\n  /* If a `Node` is passed to sanitize(), then performs sanitization in-place instead\n   * of importing it into a new Document and returning a sanitized copy */\n  let IN_PLACE = false;\n  /* Allow usage of profiles like html, svg and mathMl */\n  let USE_PROFILES = {};\n  /* Tags to ignore content of when KEEP_CONTENT is true */\n  let FORBID_CONTENTS = null;\n  const DEFAULT_FORBID_CONTENTS = addToSet({}, ['annotation-xml', 'audio', 'colgroup', 'desc', 'foreignobject', 'head', 'iframe', 'math', 'mi', 'mn', 'mo', 'ms', 'mtext', 'noembed', 'noframes', 'noscript', 'plaintext', 'script', 'style', 'svg', 'template', 'thead', 'title', 'video', 'xmp']);\n  /* Tags that are safe for data: URIs */\n  let DATA_URI_TAGS = null;\n  const DEFAULT_DATA_URI_TAGS = addToSet({}, ['audio', 'video', 'img', 'source', 'image', 'track']);\n  /* Attributes safe for values like \"javascript:\" */\n  let URI_SAFE_ATTRIBUTES = null;\n  const DEFAULT_URI_SAFE_ATTRIBUTES = addToSet({}, ['alt', 'class', 'for', 'id', 'label', 'name', 'pattern', 'placeholder', 'role', 'summary', 'title', 'value', 'style', 'xmlns']);\n  const MATHML_NAMESPACE = 'http://www.w3.org/1998/Math/MathML';\n  const SVG_NAMESPACE = 'http://www.w3.org/2000/svg';\n  const HTML_NAMESPACE = 'http://www.w3.org/1999/xhtml';\n  /* Document namespace */\n  let NAMESPACE = HTML_NAMESPACE;\n  let IS_EMPTY_INPUT = false;\n  /* Allowed XHTML+XML namespaces */\n  let ALLOWED_NAMESPACES = null;\n  const DEFAULT_ALLOWED_NAMESPACES = addToSet({}, [MATHML_NAMESPACE, SVG_NAMESPACE, HTML_NAMESPACE], stringToString);\n  let MATHML_TEXT_INTEGRATION_POINTS = addToSet({}, ['mi', 'mo', 'mn', 'ms', 'mtext']);\n  let HTML_INTEGRATION_POINTS = addToSet({}, ['annotation-xml']);\n  // Certain elements are allowed in both SVG and HTML\n  // namespace. We need to specify them explicitly\n  // so that they don't get erroneously deleted from\n  // HTML namespace.\n  const COMMON_SVG_AND_HTML_ELEMENTS = addToSet({}, ['title', 'style', 'font', 'a', 'script']);\n  /* Parsing of strict XHTML documents */\n  let PARSER_MEDIA_TYPE = null;\n  const SUPPORTED_PARSER_MEDIA_TYPES = ['application/xhtml+xml', 'text/html'];\n  const DEFAULT_PARSER_MEDIA_TYPE = 'text/html';\n  let transformCaseFunc = null;\n  /* Keep a reference to config to pass to hooks */\n  let CONFIG = null;\n  /* Ideally, do not touch anything below this line */\n  /* ______________________________________________ */\n  const formElement = document.createElement('form');\n  const isRegexOrFunction = function isRegexOrFunction(testValue) {\n    return testValue instanceof RegExp || testValue instanceof Function;\n  };\n  /**\n   * _parseConfig\n   *\n   * @param cfg optional config literal\n   */\n  // eslint-disable-next-line complexity\n  const _parseConfig = function _parseConfig() {\n    let cfg = arguments.length > 0 && arguments[0] !== undefined ? arguments[0] : {};\n    if (CONFIG && CONFIG === cfg) {\n      return;\n    }\n    /* Shield configuration object from tampering */\n    if (!cfg || typeof cfg !== 'object') {\n      cfg = {};\n    }\n    /* Shield configuration object from prototype pollution */\n    cfg = clone(cfg);\n    PARSER_MEDIA_TYPE =\n    // eslint-disable-next-line unicorn/prefer-includes\n    SUPPORTED_PARSER_MEDIA_TYPES.indexOf(cfg.PARSER_MEDIA_TYPE) === -1 ? DEFAULT_PARSER_MEDIA_TYPE : cfg.PARSER_MEDIA_TYPE;\n    // HTML tags and attributes are not case-sensitive, converting to lowercase. Keeping XHTML as is.\n    transformCaseFunc = PARSER_MEDIA_TYPE === 'application/xhtml+xml' ? stringToString : stringToLowerCase;\n    /* Set configuration parameters */\n    ALLOWED_TAGS = objectHasOwnProperty(cfg, 'ALLOWED_TAGS') ? addToSet({}, cfg.ALLOWED_TAGS, transformCaseFunc) : DEFAULT_ALLOWED_TAGS;\n    ALLOWED_ATTR = objectHasOwnProperty(cfg, 'ALLOWED_ATTR') ? addToSet({}, cfg.ALLOWED_ATTR, transformCaseFunc) : DEFAULT_ALLOWED_ATTR;\n    ALLOWED_NAMESPACES = objectHasOwnProperty(cfg, 'ALLOWED_NAMESPACES') ? addToSet({}, cfg.ALLOWED_NAMESPACES, stringToString) : DEFAULT_ALLOWED_NAMESPACES;\n    URI_SAFE_ATTRIBUTES = objectHasOwnProperty(cfg, 'ADD_URI_SAFE_ATTR') ? addToSet(clone(DEFAULT_URI_SAFE_ATTRIBUTES), cfg.ADD_URI_SAFE_ATTR, transformCaseFunc) : DEFAULT_URI_SAFE_ATTRIBUTES;\n    DATA_URI_TAGS = objectHasOwnProperty(cfg, 'ADD_DATA_URI_TAGS') ? addToSet(clone(DEFAULT_DATA_URI_TAGS), cfg.ADD_DATA_URI_TAGS, transformCaseFunc) : DEFAULT_DATA_URI_TAGS;\n    FORBID_CONTENTS = objectHasOwnProperty(cfg, 'FORBID_CONTENTS') ? addToSet({}, cfg.FORBID_CONTENTS, transformCaseFunc) : DEFAULT_FORBID_CONTENTS;\n    FORBID_TAGS = objectHasOwnProperty(cfg, 'FORBID_TAGS') ? addToSet({}, cfg.FORBID_TAGS, transformCaseFunc) : {};\n    FORBID_ATTR = objectHasOwnProperty(cfg, 'FORBID_ATTR') ? addToSet({}, cfg.FORBID_ATTR, transformCaseFunc) : {};\n    USE_PROFILES = objectHasOwnProperty(cfg, 'USE_PROFILES') ? cfg.USE_PROFILES : false;\n    ALLOW_ARIA_ATTR = cfg.ALLOW_ARIA_ATTR !== false; // Default true\n    ALLOW_DATA_ATTR = cfg.ALLOW_DATA_ATTR !== false; // Default true\n    ALLOW_UNKNOWN_PROTOCOLS = cfg.ALLOW_UNKNOWN_PROTOCOLS || false; // Default false\n    ALLOW_SELF_CLOSE_IN_ATTR = cfg.ALLOW_SELF_CLOSE_IN_ATTR !== false; // Default true\n    SAFE_FOR_TEMPLATES = cfg.SAFE_FOR_TEMPLATES || false; // Default false\n    SAFE_FOR_XML = cfg.SAFE_FOR_XML !== false; // Default true\n    WHOLE_DOCUMENT = cfg.WHOLE_DOCUMENT || false; // Default false\n    RETURN_DOM = cfg.RETURN_DOM || false; // Default false\n    RETURN_DOM_FRAGMENT = cfg.RETURN_DOM_FRAGMENT || false; // Default false\n    RETURN_TRUSTED_TYPE = cfg.RETURN_TRUSTED_TYPE || false; // Default false\n    FORCE_BODY = cfg.FORCE_BODY || false; // Default false\n    SANITIZE_DOM = cfg.SANITIZE_DOM !== false; // Default true\n    SANITIZE_NAMED_PROPS = cfg.SANITIZE_NAMED_PROPS || false; // Default false\n    KEEP_CONTENT = cfg.KEEP_CONTENT !== false; // Default true\n    IN_PLACE = cfg.IN_PLACE || false; // Default false\n    IS_ALLOWED_URI$1 = cfg.ALLOWED_URI_REGEXP || IS_ALLOWED_URI;\n    NAMESPACE = cfg.NAMESPACE || HTML_NAMESPACE;\n    MATHML_TEXT_INTEGRATION_POINTS = cfg.MATHML_TEXT_INTEGRATION_POINTS || MATHML_TEXT_INTEGRATION_POINTS;\n    HTML_INTEGRATION_POINTS = cfg.HTML_INTEGRATION_POINTS || HTML_INTEGRATION_POINTS;\n    CUSTOM_ELEMENT_HANDLING = cfg.CUSTOM_ELEMENT_HANDLING || {};\n    if (cfg.CUSTOM_ELEMENT_HANDLING && isRegexOrFunction(cfg.CUSTOM_ELEMENT_HANDLING.tagNameCheck)) {\n      CUSTOM_ELEMENT_HANDLING.tagNameCheck = cfg.CUSTOM_ELEMENT_HANDLING.tagNameCheck;\n    }\n    if (cfg.CUSTOM_ELEMENT_HANDLING && isRegexOrFunction(cfg.CUSTOM_ELEMENT_HANDLING.attributeNameCheck)) {\n      CUSTOM_ELEMENT_HANDLING.attributeNameCheck = cfg.CUSTOM_ELEMENT_HANDLING.attributeNameCheck;\n    }\n    if (cfg.CUSTOM_ELEMENT_HANDLING && typeof cfg.CUSTOM_ELEMENT_HANDLING.allowCustomizedBuiltInElements === 'boolean') {\n      CUSTOM_ELEMENT_HANDLING.allowCustomizedBuiltInElements = cfg.CUSTOM_ELEMENT_HANDLING.allowCustomizedBuiltInElements;\n    }\n    if (SAFE_FOR_TEMPLATES) {\n      ALLOW_DATA_ATTR = false;\n    }\n    if (RETURN_DOM_FRAGMENT) {\n      RETURN_DOM = true;\n    }\n    /* Parse profile info */\n    if (USE_PROFILES) {\n      ALLOWED_TAGS = addToSet({}, text);\n      ALLOWED_ATTR = [];\n      if (USE_PROFILES.html === true) {\n        addToSet(ALLOWED_TAGS, html$1);\n        addToSet(ALLOWED_ATTR, html);\n      }\n      if (USE_PROFILES.svg === true) {\n        addToSet(ALLOWED_TAGS, svg$1);\n        addToSet(ALLOWED_ATTR, svg);\n        addToSet(ALLOWED_ATTR, xml);\n      }\n      if (USE_PROFILES.svgFilters === true) {\n        addToSet(ALLOWED_TAGS, svgFilters);\n        addToSet(ALLOWED_ATTR, svg);\n        addToSet(ALLOWED_ATTR, xml);\n      }\n      if (USE_PROFILES.mathMl === true) {\n        addToSet(ALLOWED_TAGS, mathMl$1);\n        addToSet(ALLOWED_ATTR, mathMl);\n        addToSet(ALLOWED_ATTR, xml);\n      }\n    }\n    /* Merge configuration parameters */\n    if (cfg.ADD_TAGS) {\n      if (ALLOWED_TAGS === DEFAULT_ALLOWED_TAGS) {\n        ALLOWED_TAGS = clone(ALLOWED_TAGS);\n      }\n      addToSet(ALLOWED_TAGS, cfg.ADD_TAGS, transformCaseFunc);\n    }\n    if (cfg.ADD_ATTR) {\n      if (ALLOWED_ATTR === DEFAULT_ALLOWED_ATTR) {\n        ALLOWED_ATTR = clone(ALLOWED_ATTR);\n      }\n      addToSet(ALLOWED_ATTR, cfg.ADD_ATTR, transformCaseFunc);\n    }\n    if (cfg.ADD_URI_SAFE_ATTR) {\n      addToSet(URI_SAFE_ATTRIBUTES, cfg.ADD_URI_SAFE_ATTR, transformCaseFunc);\n    }\n    if (cfg.FORBID_CONTENTS) {\n      if (FORBID_CONTENTS === DEFAULT_FORBID_CONTENTS) {\n        FORBID_CONTENTS = clone(FORBID_CONTENTS);\n      }\n      addToSet(FORBID_CONTENTS, cfg.FORBID_CONTENTS, transformCaseFunc);\n    }\n    /* Add #text in case KEEP_CONTENT is set to true */\n    if (KEEP_CONTENT) {\n      ALLOWED_TAGS['#text'] = true;\n    }\n    /* Add html, head and body to ALLOWED_TAGS in case WHOLE_DOCUMENT is true */\n    if (WHOLE_DOCUMENT) {\n      addToSet(ALLOWED_TAGS, ['html', 'head', 'body']);\n    }\n    /* Add tbody to ALLOWED_TAGS in case tables are permitted, see #286, #365 */\n    if (ALLOWED_TAGS.table) {\n      addToSet(ALLOWED_TAGS, ['tbody']);\n      delete FORBID_TAGS.tbody;\n    }\n    if (cfg.TRUSTED_TYPES_POLICY) {\n      if (typeof cfg.TRUSTED_TYPES_POLICY.createHTML !== 'function') {\n        throw typeErrorCreate('TRUSTED_TYPES_POLICY configuration option must provide a \"createHTML\" hook.');\n      }\n      if (typeof cfg.TRUSTED_TYPES_POLICY.createScriptURL !== 'function') {\n        throw typeErrorCreate('TRUSTED_TYPES_POLICY configuration option must provide a \"createScriptURL\" hook.');\n      }\n      // Overwrite existing TrustedTypes policy.\n      trustedTypesPolicy = cfg.TRUSTED_TYPES_POLICY;\n      // Sign local variables required by `sanitize`.\n      emptyHTML = trustedTypesPolicy.createHTML('');\n    } else {\n      // Uninitialized policy, attempt to initialize the internal dompurify policy.\n      if (trustedTypesPolicy === undefined) {\n        trustedTypesPolicy = _createTrustedTypesPolicy(trustedTypes, currentScript);\n      }\n      // If creating the internal policy succeeded sign internal variables.\n      if (trustedTypesPolicy !== null && typeof emptyHTML === 'string') {\n        emptyHTML = trustedTypesPolicy.createHTML('');\n      }\n    }\n    // Prevent further manipulation of configuration.\n    // Not available in IE8, Safari 5, etc.\n    if (freeze) {\n      freeze(cfg);\n    }\n    CONFIG = cfg;\n  };\n  /* Keep track of all possible SVG and MathML tags\n   * so that we can perform the namespace checks\n   * correctly. */\n  const ALL_SVG_TAGS = addToSet({}, [...svg$1, ...svgFilters, ...svgDisallowed]);\n  const ALL_MATHML_TAGS = addToSet({}, [...mathMl$1, ...mathMlDisallowed]);\n  /**\n   * @param element a DOM element whose namespace is being checked\n   * @returns Return false if the element has a\n   *  namespace that a spec-compliant parser would never\n   *  return. Return true otherwise.\n   */\n  const _checkValidNamespace = function _checkValidNamespace(element) {\n    let parent = getParentNode(element);\n    // In JSDOM, if we're inside shadow DOM, then parentNode\n    // can be null. We just simulate parent in this case.\n    if (!parent || !parent.tagName) {\n      parent = {\n        namespaceURI: NAMESPACE,\n        tagName: 'template'\n      };\n    }\n    const tagName = stringToLowerCase(element.tagName);\n    const parentTagName = stringToLowerCase(parent.tagName);\n    if (!ALLOWED_NAMESPACES[element.namespaceURI]) {\n      return false;\n    }\n    if (element.namespaceURI === SVG_NAMESPACE) {\n      // The only way to switch from HTML namespace to SVG\n      // is via <svg>. If it happens via any other tag, then\n      // it should be killed.\n      if (parent.namespaceURI === HTML_NAMESPACE) {\n        return tagName === 'svg';\n      }\n      // The only way to switch from MathML to SVG is via`\n      // svg if parent is either <annotation-xml> or MathML\n      // text integration points.\n      if (parent.namespaceURI === MATHML_NAMESPACE) {\n        return tagName === 'svg' && (parentTagName === 'annotation-xml' || MATHML_TEXT_INTEGRATION_POINTS[parentTagName]);\n      }\n      // We only allow elements that are defined in SVG\n      // spec. All others are disallowed in SVG namespace.\n      return Boolean(ALL_SVG_TAGS[tagName]);\n    }\n    if (element.namespaceURI === MATHML_NAMESPACE) {\n      // The only way to switch from HTML namespace to MathML\n      // is via <math>. If it happens via any other tag, then\n      // it should be killed.\n      if (parent.namespaceURI === HTML_NAMESPACE) {\n        return tagName === 'math';\n      }\n      // The only way to switch from SVG to MathML is via\n      // <math> and HTML integration points\n      if (parent.namespaceURI === SVG_NAMESPACE) {\n        return tagName === 'math' && HTML_INTEGRATION_POINTS[parentTagName];\n      }\n      // We only allow elements that are defined in MathML\n      // spec. All others are disallowed in MathML namespace.\n      return Boolean(ALL_MATHML_TAGS[tagName]);\n    }\n    if (element.namespaceURI === HTML_NAMESPACE) {\n      // The only way to switch from SVG to HTML is via\n      // HTML integration points, and from MathML to HTML\n      // is via MathML text integration points\n      if (parent.namespaceURI === SVG_NAMESPACE && !HTML_INTEGRATION_POINTS[parentTagName]) {\n        return false;\n      }\n      if (parent.namespaceURI === MATHML_NAMESPACE && !MATHML_TEXT_INTEGRATION_POINTS[parentTagName]) {\n        return false;\n      }\n      // We disallow tags that are specific for MathML\n      // or SVG and should never appear in HTML namespace\n      return !ALL_MATHML_TAGS[tagName] && (COMMON_SVG_AND_HTML_ELEMENTS[tagName] || !ALL_SVG_TAGS[tagName]);\n    }\n    // For XHTML and XML documents that support custom namespaces\n    if (PARSER_MEDIA_TYPE === 'application/xhtml+xml' && ALLOWED_NAMESPACES[element.namespaceURI]) {\n      return true;\n    }\n    // The code should never reach this place (this means\n    // that the element somehow got namespace that is not\n    // HTML, SVG, MathML or allowed via ALLOWED_NAMESPACES).\n    // Return false just in case.\n    return false;\n  };\n  /**\n   * _forceRemove\n   *\n   * @param node a DOM node\n   */\n  const _forceRemove = function _forceRemove(node) {\n    arrayPush(DOMPurify.removed, {\n      element: node\n    });\n    try {\n      // eslint-disable-next-line unicorn/prefer-dom-node-remove\n      getParentNode(node).removeChild(node);\n    } catch (_) {\n      remove(node);\n    }\n  };\n  /**\n   * _removeAttribute\n   *\n   * @param name an Attribute name\n   * @param element a DOM node\n   */\n  const _removeAttribute = function _removeAttribute(name, element) {\n    try {\n      arrayPush(DOMPurify.removed, {\n        attribute: element.getAttributeNode(name),\n        from: element\n      });\n    } catch (_) {\n      arrayPush(DOMPurify.removed, {\n        attribute: null,\n        from: element\n      });\n    }\n    element.removeAttribute(name);\n    // We void attribute values for unremovable \"is\" attributes\n    if (name === 'is') {\n      if (RETURN_DOM || RETURN_DOM_FRAGMENT) {\n        try {\n          _forceRemove(element);\n        } catch (_) {}\n      } else {\n        try {\n          element.setAttribute(name, '');\n        } catch (_) {}\n      }\n    }\n  };\n  /**\n   * _initDocument\n   *\n   * @param dirty - a string of dirty markup\n   * @return a DOM, filled with the dirty markup\n   */\n  const _initDocument = function _initDocument(dirty) {\n    /* Create a HTML document */\n    let doc = null;\n    let leadingWhitespace = null;\n    if (FORCE_BODY) {\n      dirty = '<remove></remove>' + dirty;\n    } else {\n      /* If FORCE_BODY isn't used, leading whitespace needs to be preserved manually */\n      const matches = stringMatch(dirty, /^[\\r\\n\\t ]+/);\n      leadingWhitespace = matches && matches[0];\n    }\n    if (PARSER_MEDIA_TYPE === 'application/xhtml+xml' && NAMESPACE === HTML_NAMESPACE) {\n      // Root of XHTML doc must contain xmlns declaration (see https://www.w3.org/TR/xhtml1/normative.html#strict)\n      dirty = '<html xmlns=\"http://www.w3.org/1999/xhtml\"><head></head><body>' + dirty + '</body></html>';\n    }\n    const dirtyPayload = trustedTypesPolicy ? trustedTypesPolicy.createHTML(dirty) : dirty;\n    /*\n     * Use the DOMParser API by default, fallback later if needs be\n     * DOMParser not work for svg when has multiple root element.\n     */\n    if (NAMESPACE === HTML_NAMESPACE) {\n      try {\n        doc = new DOMParser().parseFromString(dirtyPayload, PARSER_MEDIA_TYPE);\n      } catch (_) {}\n    }\n    /* Use createHTMLDocument in case DOMParser is not available */\n    if (!doc || !doc.documentElement) {\n      doc = implementation.createDocument(NAMESPACE, 'template', null);\n      try {\n        doc.documentElement.innerHTML = IS_EMPTY_INPUT ? emptyHTML : dirtyPayload;\n      } catch (_) {\n        // Syntax error if dirtyPayload is invalid xml\n      }\n    }\n    const body = doc.body || doc.documentElement;\n    if (dirty && leadingWhitespace) {\n      body.insertBefore(document.createTextNode(leadingWhitespace), body.childNodes[0] || null);\n    }\n    /* Work on whole document or just its body */\n    if (NAMESPACE === HTML_NAMESPACE) {\n      return getElementsByTagName.call(doc, WHOLE_DOCUMENT ? 'html' : 'body')[0];\n    }\n    return WHOLE_DOCUMENT ? doc.documentElement : body;\n  };\n  /**\n   * Creates a NodeIterator object that you can use to traverse filtered lists of nodes or elements in a document.\n   *\n   * @param root The root element or node to start traversing on.\n   * @return The created NodeIterator\n   */\n  const _createNodeIterator = function _createNodeIterator(root) {\n    return createNodeIterator.call(root.ownerDocument || root, root,\n    // eslint-disable-next-line no-bitwise\n    NodeFilter.SHOW_ELEMENT | NodeFilter.SHOW_COMMENT | NodeFilter.SHOW_TEXT | NodeFilter.SHOW_PROCESSING_INSTRUCTION | NodeFilter.SHOW_CDATA_SECTION, null);\n  };\n  /**\n   * _isClobbered\n   *\n   * @param element element to check for clobbering attacks\n   * @return true if clobbered, false if safe\n   */\n  const _isClobbered = function _isClobbered(element) {\n    return element instanceof HTMLFormElement && (typeof element.nodeName !== 'string' || typeof element.textContent !== 'string' || typeof element.removeChild !== 'function' || !(element.attributes instanceof NamedNodeMap) || typeof element.removeAttribute !== 'function' || typeof element.setAttribute !== 'function' || typeof element.namespaceURI !== 'string' || typeof element.insertBefore !== 'function' || typeof element.hasChildNodes !== 'function');\n  };\n  /**\n   * Checks whether the given object is a DOM node.\n   *\n   * @param value object to check whether it's a DOM node\n   * @return true is object is a DOM node\n   */\n  const _isNode = function _isNode(value) {\n    return typeof Node === 'function' && value instanceof Node;\n  };\n  function _executeHooks(hooks, currentNode, data) {\n    arrayForEach(hooks, hook => {\n      hook.call(DOMPurify, currentNode, data, CONFIG);\n    });\n  }\n  /**\n   * _sanitizeElements\n   *\n   * @protect nodeName\n   * @protect textContent\n   * @protect removeChild\n   * @param currentNode to check for permission to exist\n   * @return true if node was killed, false if left alive\n   */\n  const _sanitizeElements = function _sanitizeElements(currentNode) {\n    let content = null;\n    /* Execute a hook if present */\n    _executeHooks(hooks.beforeSanitizeElements, currentNode, null);\n    /* Check if element is clobbered or can clobber */\n    if (_isClobbered(currentNode)) {\n      _forceRemove(currentNode);\n      return true;\n    }\n    /* Now let's check the element's type and name */\n    const tagName = transformCaseFunc(currentNode.nodeName);\n    /* Execute a hook if present */\n    _executeHooks(hooks.uponSanitizeElement, currentNode, {\n      tagName,\n      allowedTags: ALLOWED_TAGS\n    });\n    /* Detect mXSS attempts abusing namespace confusion */\n    if (currentNode.hasChildNodes() && !_isNode(currentNode.firstElementChild) && regExpTest(/<[/\\w]/g, currentNode.innerHTML) && regExpTest(/<[/\\w]/g, currentNode.textContent)) {\n      _forceRemove(currentNode);\n      return true;\n    }\n    /* Remove any occurrence of processing instructions */\n    if (currentNode.nodeType === NODE_TYPE.progressingInstruction) {\n      _forceRemove(currentNode);\n      return true;\n    }\n    /* Remove any kind of possibly harmful comments */\n    if (SAFE_FOR_XML && currentNode.nodeType === NODE_TYPE.comment && regExpTest(/<[/\\w]/g, currentNode.data)) {\n      _forceRemove(currentNode);\n      return true;\n    }\n    /* Remove element if anything forbids its presence */\n    if (!ALLOWED_TAGS[tagName] || FORBID_TAGS[tagName]) {\n      /* Check if we have a custom element to handle */\n      if (!FORBID_TAGS[tagName] && _isBasicCustomElement(tagName)) {\n        if (CUSTOM_ELEMENT_HANDLING.tagNameCheck instanceof RegExp && regExpTest(CUSTOM_ELEMENT_HANDLING.tagNameCheck, tagName)) {\n          return false;\n        }\n        if (CUSTOM_ELEMENT_HANDLING.tagNameCheck instanceof Function && CUSTOM_ELEMENT_HANDLING.tagNameCheck(tagName)) {\n          return false;\n        }\n      }\n      /* Keep content except for bad-listed elements */\n      if (KEEP_CONTENT && !FORBID_CONTENTS[tagName]) {\n        const parentNode = getParentNode(currentNode) || currentNode.parentNode;\n        const childNodes = getChildNodes(currentNode) || currentNode.childNodes;\n        if (childNodes && parentNode) {\n          const childCount = childNodes.length;\n          for (let i = childCount - 1; i >= 0; --i) {\n            const childClone = cloneNode(childNodes[i], true);\n            childClone.__removalCount = (currentNode.__removalCount || 0) + 1;\n            parentNode.insertBefore(childClone, getNextSibling(currentNode));\n          }\n        }\n      }\n      _forceRemove(currentNode);\n      return true;\n    }\n    /* Check whether element has a valid namespace */\n    if (currentNode instanceof Element && !_checkValidNamespace(currentNode)) {\n      _forceRemove(currentNode);\n      return true;\n    }\n    /* Make sure that older browsers don't get fallback-tag mXSS */\n    if ((tagName === 'noscript' || tagName === 'noembed' || tagName === 'noframes') && regExpTest(/<\\/no(script|embed|frames)/i, currentNode.innerHTML)) {\n      _forceRemove(currentNode);\n      return true;\n    }\n    /* Sanitize element content to be template-safe */\n    if (SAFE_FOR_TEMPLATES && currentNode.nodeType === NODE_TYPE.text) {\n      /* Get the element's text content */\n      content = currentNode.textContent;\n      arrayForEach([MUSTACHE_EXPR, ERB_EXPR, TMPLIT_EXPR], expr => {\n        content = stringReplace(content, expr, ' ');\n      });\n      if (currentNode.textContent !== content) {\n        arrayPush(DOMPurify.removed, {\n          element: currentNode.cloneNode()\n        });\n        currentNode.textContent = content;\n      }\n    }\n    /* Execute a hook if present */\n    _executeHooks(hooks.afterSanitizeElements, currentNode, null);\n    return false;\n  };\n  /**\n   * _isValidAttribute\n   *\n   * @param lcTag Lowercase tag name of containing element.\n   * @param lcName Lowercase attribute name.\n   * @param value Attribute value.\n   * @return Returns true if `value` is valid, otherwise false.\n   */\n  // eslint-disable-next-line complexity\n  const _isValidAttribute = function _isValidAttribute(lcTag, lcName, value) {\n    /* Make sure attribute cannot clobber */\n    if (SANITIZE_DOM && (lcName === 'id' || lcName === 'name') && (value in document || value in formElement)) {\n      return false;\n    }\n    /* Allow valid data-* attributes: At least one character after \"-\"\n        (https://html.spec.whatwg.org/multipage/dom.html#embedding-custom-non-visible-data-with-the-data-*-attributes)\n        XML-compatible (https://html.spec.whatwg.org/multipage/infrastructure.html#xml-compatible and http://www.w3.org/TR/xml/#d0e804)\n        We don't need to check the value; it's always URI safe. */\n    if (ALLOW_DATA_ATTR && !FORBID_ATTR[lcName] && regExpTest(DATA_ATTR, lcName)) ; else if (ALLOW_ARIA_ATTR && regExpTest(ARIA_ATTR, lcName)) ; else if (!ALLOWED_ATTR[lcName] || FORBID_ATTR[lcName]) {\n      if (\n      // First condition does a very basic check if a) it's basically a valid custom element tagname AND\n      // b) if the tagName passes whatever the user has configured for CUSTOM_ELEMENT_HANDLING.tagNameCheck\n      // and c) if the attribute name passes whatever the user has configured for CUSTOM_ELEMENT_HANDLING.attributeNameCheck\n      _isBasicCustomElement(lcTag) && (CUSTOM_ELEMENT_HANDLING.tagNameCheck instanceof RegExp && regExpTest(CUSTOM_ELEMENT_HANDLING.tagNameCheck, lcTag) || CUSTOM_ELEMENT_HANDLING.tagNameCheck instanceof Function && CUSTOM_ELEMENT_HANDLING.tagNameCheck(lcTag)) && (CUSTOM_ELEMENT_HANDLING.attributeNameCheck instanceof RegExp && regExpTest(CUSTOM_ELEMENT_HANDLING.attributeNameCheck, lcName) || CUSTOM_ELEMENT_HANDLING.attributeNameCheck instanceof Function && CUSTOM_ELEMENT_HANDLING.attributeNameCheck(lcName)) ||\n      // Alternative, second condition checks if it's an `is`-attribute, AND\n      // the value passes whatever the user has configured for CUSTOM_ELEMENT_HANDLING.tagNameCheck\n      lcName === 'is' && CUSTOM_ELEMENT_HANDLING.allowCustomizedBuiltInElements && (CUSTOM_ELEMENT_HANDLING.tagNameCheck instanceof RegExp && regExpTest(CUSTOM_ELEMENT_HANDLING.tagNameCheck, value) || CUSTOM_ELEMENT_HANDLING.tagNameCheck instanceof Function && CUSTOM_ELEMENT_HANDLING.tagNameCheck(value))) ; else {\n        return false;\n      }\n      /* Check value is safe. First, is attr inert? If so, is safe */\n    } else if (URI_SAFE_ATTRIBUTES[lcName]) ; else if (regExpTest(IS_ALLOWED_URI$1, stringReplace(value, ATTR_WHITESPACE, ''))) ; else if ((lcName === 'src' || lcName === 'xlink:href' || lcName === 'href') && lcTag !== 'script' && stringIndexOf(value, 'data:') === 0 && DATA_URI_TAGS[lcTag]) ; else if (ALLOW_UNKNOWN_PROTOCOLS && !regExpTest(IS_SCRIPT_OR_DATA, stringReplace(value, ATTR_WHITESPACE, ''))) ; else if (value) {\n      return false;\n    } else ;\n    return true;\n  };\n  /**\n   * _isBasicCustomElement\n   * checks if at least one dash is included in tagName, and it's not the first char\n   * for more sophisticated checking see https://github.com/sindresorhus/validate-element-name\n   *\n   * @param tagName name of the tag of the node to sanitize\n   * @returns Returns true if the tag name meets the basic criteria for a custom element, otherwise false.\n   */\n  const _isBasicCustomElement = function _isBasicCustomElement(tagName) {\n    return tagName !== 'annotation-xml' && stringMatch(tagName, CUSTOM_ELEMENT);\n  };\n  /**\n   * _sanitizeAttributes\n   *\n   * @protect attributes\n   * @protect nodeName\n   * @protect removeAttribute\n   * @protect setAttribute\n   *\n   * @param currentNode to sanitize\n   */\n  const _sanitizeAttributes = function _sanitizeAttributes(currentNode) {\n    /* Execute a hook if present */\n    _executeHooks(hooks.beforeSanitizeAttributes, currentNode, null);\n    const {\n      attributes\n    } = currentNode;\n    /* Check if we have attributes; if not we might have a text node */\n    if (!attributes || _isClobbered(currentNode)) {\n      return;\n    }\n    const hookEvent = {\n      attrName: '',\n      attrValue: '',\n      keepAttr: true,\n      allowedAttributes: ALLOWED_ATTR,\n      forceKeepAttr: undefined\n    };\n    let l = attributes.length;\n    /* Go backwards over all attributes; safely remove bad ones */\n    while (l--) {\n      const attr = attributes[l];\n      const {\n        name,\n        namespaceURI,\n        value: attrValue\n      } = attr;\n      const lcName = transformCaseFunc(name);\n      let value = name === 'value' ? attrValue : stringTrim(attrValue);\n      /* Execute a hook if present */\n      hookEvent.attrName = lcName;\n      hookEvent.attrValue = value;\n      hookEvent.keepAttr = true;\n      hookEvent.forceKeepAttr = undefined; // Allows developers to see this is a property they can set\n      _executeHooks(hooks.uponSanitizeAttribute, currentNode, hookEvent);\n      value = hookEvent.attrValue;\n      /* Full DOM Clobbering protection via namespace isolation,\n       * Prefix id and name attributes with `user-content-`\n       */\n      if (SANITIZE_NAMED_PROPS && (lcName === 'id' || lcName === 'name')) {\n        // Remove the attribute with this value\n        _removeAttribute(name, currentNode);\n        // Prefix the value and later re-create the attribute with the sanitized value\n        value = SANITIZE_NAMED_PROPS_PREFIX + value;\n      }\n      /* Work around a security issue with comments inside attributes */\n      if (SAFE_FOR_XML && regExpTest(/((--!?|])>)|<\\/(style|title)/i, value)) {\n        _removeAttribute(name, currentNode);\n        continue;\n      }\n      /* Did the hooks approve of the attribute? */\n      if (hookEvent.forceKeepAttr) {\n        continue;\n      }\n      /* Remove attribute */\n      _removeAttribute(name, currentNode);\n      /* Did the hooks approve of the attribute? */\n      if (!hookEvent.keepAttr) {\n        continue;\n      }\n      /* Work around a security issue in jQuery 3.0 */\n      if (!ALLOW_SELF_CLOSE_IN_ATTR && regExpTest(/\\/>/i, value)) {\n        _removeAttribute(name, currentNode);\n        continue;\n      }\n      /* Sanitize attribute content to be template-safe */\n      if (SAFE_FOR_TEMPLATES) {\n        arrayForEach([MUSTACHE_EXPR, ERB_EXPR, TMPLIT_EXPR], expr => {\n          value = stringReplace(value, expr, ' ');\n        });\n      }\n      /* Is `value` valid for this attribute? */\n      const lcTag = transformCaseFunc(currentNode.nodeName);\n      if (!_isValidAttribute(lcTag, lcName, value)) {\n        continue;\n      }\n      /* Handle attributes that require Trusted Types */\n      if (trustedTypesPolicy && typeof trustedTypes === 'object' && typeof trustedTypes.getAttributeType === 'function') {\n        if (namespaceURI) ; else {\n          switch (trustedTypes.getAttributeType(lcTag, lcName)) {\n            case 'TrustedHTML':\n              {\n                value = trustedTypesPolicy.createHTML(value);\n                break;\n              }\n            case 'TrustedScriptURL':\n              {\n                value = trustedTypesPolicy.createScriptURL(value);\n                break;\n              }\n          }\n        }\n      }\n      /* Handle invalid data-* attribute set by try-catching it */\n      try {\n        if (namespaceURI) {\n          currentNode.setAttributeNS(namespaceURI, name, value);\n        } else {\n          /* Fallback to setAttribute() for browser-unrecognized namespaces e.g. \"x-schema\". */\n          currentNode.setAttribute(name, value);\n        }\n        if (_isClobbered(currentNode)) {\n          _forceRemove(currentNode);\n        } else {\n          arrayPop(DOMPurify.removed);\n        }\n      } catch (_) {}\n    }\n    /* Execute a hook if present */\n    _executeHooks(hooks.afterSanitizeAttributes, currentNode, null);\n  };\n  /**\n   * _sanitizeShadowDOM\n   *\n   * @param fragment to iterate over recursively\n   */\n  const _sanitizeShadowDOM = function _sanitizeShadowDOM(fragment) {\n    let shadowNode = null;\n    const shadowIterator = _createNodeIterator(fragment);\n    /* Execute a hook if present */\n    _executeHooks(hooks.beforeSanitizeShadowDOM, fragment, null);\n    while (shadowNode = shadowIterator.nextNode()) {\n      /* Execute a hook if present */\n      _executeHooks(hooks.uponSanitizeShadowNode, shadowNode, null);\n      /* Sanitize tags and elements */\n      _sanitizeElements(shadowNode);\n      /* Check attributes next */\n      _sanitizeAttributes(shadowNode);\n      /* Deep shadow DOM detected */\n      if (shadowNode.content instanceof DocumentFragment) {\n        _sanitizeShadowDOM(shadowNode.content);\n      }\n    }\n    /* Execute a hook if present */\n    _executeHooks(hooks.afterSanitizeShadowDOM, fragment, null);\n  };\n  // eslint-disable-next-line complexity\n  DOMPurify.sanitize = function (dirty) {\n    let cfg = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : {};\n    let body = null;\n    let importedNode = null;\n    let currentNode = null;\n    let returnNode = null;\n    /* Make sure we have a string to sanitize.\n      DO NOT return early, as this will return the wrong type if\n      the user has requested a DOM object rather than a string */\n    IS_EMPTY_INPUT = !dirty;\n    if (IS_EMPTY_INPUT) {\n      dirty = '<!-->';\n    }\n    /* Stringify, in case dirty is an object */\n    if (typeof dirty !== 'string' && !_isNode(dirty)) {\n      if (typeof dirty.toString === 'function') {\n        dirty = dirty.toString();\n        if (typeof dirty !== 'string') {\n          throw typeErrorCreate('dirty is not a string, aborting');\n        }\n      } else {\n        throw typeErrorCreate('toString is not a function');\n      }\n    }\n    /* Return dirty HTML if DOMPurify cannot run */\n    if (!DOMPurify.isSupported) {\n      return dirty;\n    }\n    /* Assign config vars */\n    if (!SET_CONFIG) {\n      _parseConfig(cfg);\n    }\n    /* Clean up removed elements */\n    DOMPurify.removed = [];\n    /* Check if dirty is correctly typed for IN_PLACE */\n    if (typeof dirty === 'string') {\n      IN_PLACE = false;\n    }\n    if (IN_PLACE) {\n      /* Do some early pre-sanitization to avoid unsafe root nodes */\n      if (dirty.nodeName) {\n        const tagName = transformCaseFunc(dirty.nodeName);\n        if (!ALLOWED_TAGS[tagName] || FORBID_TAGS[tagName]) {\n          throw typeErrorCreate('root node is forbidden and cannot be sanitized in-place');\n        }\n      }\n    } else if (dirty instanceof Node) {\n      /* If dirty is a DOM element, append to an empty document to avoid\n         elements being stripped by the parser */\n      body = _initDocument('<!---->');\n      importedNode = body.ownerDocument.importNode(dirty, true);\n      if (importedNode.nodeType === NODE_TYPE.element && importedNode.nodeName === 'BODY') {\n        /* Node is already a body, use as is */\n        body = importedNode;\n      } else if (importedNode.nodeName === 'HTML') {\n        body = importedNode;\n      } else {\n        // eslint-disable-next-line unicorn/prefer-dom-node-append\n        body.appendChild(importedNode);\n      }\n    } else {\n      /* Exit directly if we have nothing to do */\n      if (!RETURN_DOM && !SAFE_FOR_TEMPLATES && !WHOLE_DOCUMENT &&\n      // eslint-disable-next-line unicorn/prefer-includes\n      dirty.indexOf('<') === -1) {\n        return trustedTypesPolicy && RETURN_TRUSTED_TYPE ? trustedTypesPolicy.createHTML(dirty) : dirty;\n      }\n      /* Initialize the document to work on */\n      body = _initDocument(dirty);\n      /* Check we have a DOM node from the data */\n      if (!body) {\n        return RETURN_DOM ? null : RETURN_TRUSTED_TYPE ? emptyHTML : '';\n      }\n    }\n    /* Remove first element node (ours) if FORCE_BODY is set */\n    if (body && FORCE_BODY) {\n      _forceRemove(body.firstChild);\n    }\n    /* Get node iterator */\n    const nodeIterator = _createNodeIterator(IN_PLACE ? dirty : body);\n    /* Now start iterating over the created document */\n    while (currentNode = nodeIterator.nextNode()) {\n      /* Sanitize tags and elements */\n      _sanitizeElements(currentNode);\n      /* Check attributes next */\n      _sanitizeAttributes(currentNode);\n      /* Shadow DOM detected, sanitize it */\n      if (currentNode.content instanceof DocumentFragment) {\n        _sanitizeShadowDOM(currentNode.content);\n      }\n    }\n    /* If we sanitized `dirty` in-place, return it. */\n    if (IN_PLACE) {\n      return dirty;\n    }\n    /* Return sanitized string or DOM */\n    if (RETURN_DOM) {\n      if (RETURN_DOM_FRAGMENT) {\n        returnNode = createDocumentFragment.call(body.ownerDocument);\n        while (body.firstChild) {\n          // eslint-disable-next-line unicorn/prefer-dom-node-append\n          returnNode.appendChild(body.firstChild);\n        }\n      } else {\n        returnNode = body;\n      }\n      if (ALLOWED_ATTR.shadowroot || ALLOWED_ATTR.shadowrootmode) {\n        /*\n          AdoptNode() is not used because internal state is not reset\n          (e.g. the past names map of a HTMLFormElement), this is safe\n          in theory but we would rather not risk another attack vector.\n          The state that is cloned by importNode() is explicitly defined\n          by the specs.\n        */\n        returnNode = importNode.call(originalDocument, returnNode, true);\n      }\n      return returnNode;\n    }\n    let serializedHTML = WHOLE_DOCUMENT ? body.outerHTML : body.innerHTML;\n    /* Serialize doctype if allowed */\n    if (WHOLE_DOCUMENT && ALLOWED_TAGS['!doctype'] && body.ownerDocument && body.ownerDocument.doctype && body.ownerDocument.doctype.name && regExpTest(DOCTYPE_NAME, body.ownerDocument.doctype.name)) {\n      serializedHTML = '<!DOCTYPE ' + body.ownerDocument.doctype.name + '>\\n' + serializedHTML;\n    }\n    /* Sanitize final string template-safe */\n    if (SAFE_FOR_TEMPLATES) {\n      arrayForEach([MUSTACHE_EXPR, ERB_EXPR, TMPLIT_EXPR], expr => {\n        serializedHTML = stringReplace(serializedHTML, expr, ' ');\n      });\n    }\n    return trustedTypesPolicy && RETURN_TRUSTED_TYPE ? trustedTypesPolicy.createHTML(serializedHTML) : serializedHTML;\n  };\n  DOMPurify.setConfig = function () {\n    let cfg = arguments.length > 0 && arguments[0] !== undefined ? arguments[0] : {};\n    _parseConfig(cfg);\n    SET_CONFIG = true;\n  };\n  DOMPurify.clearConfig = function () {\n    CONFIG = null;\n    SET_CONFIG = false;\n  };\n  DOMPurify.isValidAttribute = function (tag, attr, value) {\n    /* Initialize shared config vars if necessary. */\n    if (!CONFIG) {\n      _parseConfig({});\n    }\n    const lcTag = transformCaseFunc(tag);\n    const lcName = transformCaseFunc(attr);\n    return _isValidAttribute(lcTag, lcName, value);\n  };\n  DOMPurify.addHook = function (entryPoint, hookFunction) {\n    if (typeof hookFunction !== 'function') {\n      return;\n    }\n    arrayPush(hooks[entryPoint], hookFunction);\n  };\n  DOMPurify.removeHook = function (entryPoint, hookFunction) {\n    if (hookFunction !== undefined) {\n      const index = arrayLastIndexOf(hooks[entryPoint], hookFunction);\n      return index === -1 ? undefined : arraySplice(hooks[entryPoint], index, 1)[0];\n    }\n    return arrayPop(hooks[entryPoint]);\n  };\n  DOMPurify.removeHooks = function (entryPoint) {\n    hooks[entryPoint] = [];\n  };\n  DOMPurify.removeAllHooks = function () {\n    hooks = _createHooksMap();\n  };\n  return DOMPurify;\n}\nvar purify = createDOMPurify();\n\nexport { purify as default };\n//# sourceMappingURL=purify.es.mjs.map\n", "import createDOMPurify from \"dompurify\";\nimport { JSDOM } from \"jsdom\";\n\nconst windowEmulator: any = new JSDOM(\"\").window;\nconst DOMPurify = createDOMPurify(windowEmulator);\n\n/*\n * DOMPurify prevents all XSS attacks by default. With these settings, it also\n * prevents \"deception\" attacks. If an attacker could put <div style=\"...\">\n * into the site's admin banner, they could make give the banner any appearance,\n * overlaid anywhere on the page. For example, a fake \"session expired\" modal\n * with a malicious link. Thus, this very strict DOMPurify config.\n */\nDOMPurify.setConfig({\n  // Only these tags will be allowed through\n  ALLOWED_TAGS: [\"ul\", \"ol\", \"li\", \"a\", \"#text\"],\n  // On those tags, only these attributes are allowed\n  ALLOWED_ATTR: [\"href\", \"alt\"],\n  // If a tag is removed, so will all its child elements & text\n  KEEP_CONTENT: false,\n});\n\n// sanitize string\nexport const sanitizeString = (string: string) => {\n  if (DOMPurify.isSupported) {\n    return DOMPurify.sanitize(string);\n  }\n};\n\n// iterates over array items, sanitizing items recursively\nexport const sanitizeArray = (array: unknown[]): unknown[] =>\n  array.map((entry: unknown) => sanitizeEntry(entry));\n\n// iterates over object key-value pairs, sanitizing values recursively\nexport const sanitizeObject = (object: { [key: string]: unknown }) => {\n  if (object) {\n    const entries = Object.entries(object);\n    const sanitizedEntries = entries.map((entry: [string, unknown]) => {\n      const [key, value] = entry;\n      return [key, sanitizeEntry(value)];\n    });\n    return Object.fromEntries(sanitizedEntries);\n  }\n};\n\nconst sanitizerMap: any = {\n  string: sanitizeString,\n  array: sanitizeArray,\n  object: sanitizeObject,\n};\n\n// return sanitized entry, or if safe type, return entry\nconst sanitizeEntry = (entry: unknown) => {\n  const entryType = Array.isArray(entry) ? \"array\" : typeof entry;\n  const sanitizer = sanitizerMap[entryType];\n  return sanitizer?.(entry) || entry;\n};\n", "// utils\nimport * as logger from \"../utils/debugging/debug-lib\";\nimport { isAuthenticated } from \"../utils/auth/authorization\";\nimport {\n  HttpResponse,\n  internalServerError,\n  unauthenticated,\n} from \"../utils/responses/response-lib\";\nimport { error } from \"../utils/constants/constants\";\nimport { sanitizeObject } from \"../utils/sanitize/sanitize\";\n// types\nimport { APIGatewayProxyEvent } from \"../utils/types\";\n\ntype LambdaFunction = (\n  event: APIGatewayProxyEvent, // eslint-disable-line no-unused-vars\n  context: any // eslint-disable-line no-unused-vars\n) => Promise<HttpResponse>;\n\nexport default function handler(lambda: LambdaFunction) {\n  return async function (event: APIGatewayProxyEvent, context: any) {\n    // Start debugger\n    logger.init();\n    logger.debug(\"API event: %O\", {\n      body: event.body,\n      pathParameters: event.pathParameters,\n      queryStringParameters: event.queryStringParameters,\n    });\n    if (await isAuthenticated(event)) {\n      try {\n        if (event.body) {\n          const newEventBody = sanitizeObject(JSON.parse(event.body));\n          event.body = JSON.stringify(newEventBody);\n        }\n        return await lambda(event, context);\n      } catch (error: any) {\n        logger.error(\"Error: %O\", error);\n\n        const body = { error: error.message };\n        return internalServerError(body);\n      } finally {\n        logger.flush();\n      }\n    } else {\n      return unauthenticated(error.UNAUTHORIZED);\n    }\n  };\n}\n", "import { DeleteCommand, paginateScan, PutCommand } from \"@aws-sdk/lib-dynamodb\";\nimport { createClient } from \"./dynamodb-lib\";\nimport { AdminBannerData } from \"../utils/types/banner\";\nimport { AnyObject } from \"../utils/types\";\n\nconst bannerTableName = process.env.BannerTable!;\nconst client = createClient();\n\nexport const putBanner = async (banner: AdminBannerData) => {\n  await client.send(\n    new PutCommand({\n      TableName: bannerTableName,\n      Item: banner,\n    })\n  );\n};\n\nexport const getBanners = async () => {\n  let items: AnyObject[] = [];\n  const params = {\n    TableName: bannerTableName,\n  };\n\n  for await (const page of paginateScan({ client }, params)) {\n    items = items.concat(page.Items ?? []);\n  }\n\n  return items as AdminBannerData[] | undefined;\n};\n\nexport const deleteBanner = async (bannerId: string) => {\n  await client.send(\n    new DeleteCommand({\n      TableName: bannerTableName,\n      Key: {\n        key: bannerId,\n      },\n    })\n  );\n};\n", "import {\n  DynamoDBClient,\n  QueryCommandOutput,\n  ScanCommandOutput,\n} from \"@aws-sdk/client-dynamodb\";\nimport { DynamoDBDocumentClient, Paginator } from \"@aws-sdk/lib-dynamodb\";\n// utils\nimport { logger } from \"../utils/debugging/debug-lib\";\n\nconst localConfig = {\n  endpoint: process.env.DYNAMODB_URL,\n  region: \"localhost\",\n  credentials: {\n    accessKeyId: \"LOCALFAKEKEY\", // pragma: allowlist secret\n    secretAccessKey: \"LOCALFAKESECRET\", // pragma: allowlist secret\n  },\n  logger,\n};\n\nconst awsConfig = {\n  region: \"us-east-1\",\n  logger,\n};\n\nconst getConfig = () => {\n  return process.env.DYNAMODB_URL ? localConfig : awsConfig;\n};\n\nexport const createClient = () => {\n  return DynamoDBDocumentClient.from(new DynamoDBClient(getConfig()));\n};\n\nexport const collectPageItems = async <\n  T extends QueryCommandOutput | ScanCommandOutput\n>(\n  paginator: Paginator<T>\n) => {\n  let items: Record<string, any>[] = [];\n  for await (let page of paginator) {\n    items = items.concat(page.Items ?? []);\n  }\n  return items;\n};\n", "import { object } from \"yup\";\nimport { error } from \"../constants/constants\";\n// types\nimport { AnyObject } from \"../types\";\n// utils\nimport { nested, endDate, schemaMap } from \"./schemaMap\";\n\n// compare payload data against validation schema\nexport const validateData = async (\n  validationSchema: AnyObject,\n  data: AnyObject,\n  options?: AnyObject\n) => {\n  try {\n    // returns valid data to be passed through API\n    return await validationSchema.validate(data, {\n      stripUnknown: true,\n      ...options,\n    });\n  } catch {\n    throw new Error(error.INVALID_DATA);\n  }\n};\n\n// filter field validation to just what's needed for the passed fields\nexport const filterValidationSchema = (\n  validationObject: AnyObject,\n  data: AnyObject\n): AnyObject => {\n  const validationEntries = Object.entries(validationObject);\n  const dataKeys = Object.keys(data);\n  const filteredEntries = validationEntries.filter(\n    (entry: [string, string | AnyObject]) => {\n      const [entryKey] = entry;\n      return dataKeys.includes(entryKey);\n    }\n  );\n  return Object.fromEntries(filteredEntries);\n};\n\n// map field validation types to validation schema\nexport const mapValidationTypesToSchema = (fieldValidationTypes: AnyObject) => {\n  let validationSchema: AnyObject = {};\n  // for each field to be validated,\n  Object.entries(fieldValidationTypes).forEach(\n    (fieldValidationType: [string, string | AnyObject]) => {\n      const [key, fieldValidation] = fieldValidationType;\n      // if standard validation type, set corresponding schema from map\n      if (typeof fieldValidation === \"string\") {\n        const correspondingSchema = schemaMap[fieldValidation];\n        if (correspondingSchema) {\n          validationSchema[key] = correspondingSchema;\n        }\n      }\n      // else if nested validation type, make and set nested schema\n      else if (fieldValidation.nested) {\n        validationSchema[key] = makeNestedFieldSchema(fieldValidation);\n        // else if not nested, make and set other dependent field types\n      } else if (fieldValidation.type === \"endDate\") {\n        validationSchema[key] = makeEndDateFieldSchema(fieldValidation);\n      }\n    }\n  );\n  return validationSchema;\n};\n\n// return created endDate schema\nexport const makeEndDateFieldSchema = (fieldValidationObject: AnyObject) => {\n  const { dependentFieldName } = fieldValidationObject;\n  return endDate(dependentFieldName);\n};\n\n// return created nested field schema\nexport const makeNestedFieldSchema = (fieldValidationObject: AnyObject) => {\n  const { type, parentFieldName, parentOptionId } = fieldValidationObject;\n  if (fieldValidationObject.type === \"endDate\") {\n    return nested(\n      () => makeEndDateFieldSchema(fieldValidationObject),\n      parentFieldName,\n      parentOptionId\n    );\n  } else {\n    const fieldValidationSchema = schemaMap[type];\n    return nested(() => fieldValidationSchema, parentFieldName, parentOptionId);\n  }\n};\n\nexport const validateFieldData = async (\n  validationJson: AnyObject,\n  unvalidatedFieldData: AnyObject\n) => {\n  let validatedFieldData: AnyObject | undefined = undefined;\n  // filter field validation to just what's needed for the passed fields\n  const filteredFieldDataValidationJson = filterValidationSchema(\n    validationJson,\n    unvalidatedFieldData\n  );\n  // transform field validation instructions to yup validation schema\n  const fieldDataValidationSchema = object().shape(\n    mapValidationTypesToSchema(filteredFieldDataValidationJson)\n  );\n  if (fieldDataValidationSchema) {\n    validatedFieldData = await validateData(\n      fieldDataValidationSchema,\n      unvalidatedFieldData\n    );\n  }\n  return validatedFieldData;\n};\n", "import { array, boolean, mixed, object, string } from \"yup\";\nimport { Choice } from \"../types/index\";\nimport {\n  checkRatioInputAgainstRegexes,\n  checkStandardIntegerInputAgainstRegexes,\n  checkStandardNumberInputAgainstRegexes,\n} from \"./checkInputValidity\";\n\nconst error = {\n  REQUIRED_GENERIC: \"A response is required\",\n  REQUIRED_CHECKBOX: \"Select at least one response\",\n  INVALID_GENERIC: \"Response must be valid\",\n  INVALID_EMAIL: \"Response must be a valid email address\",\n  INVALID_URL: \"Response must be a valid hyperlink/URL\",\n  INVALID_DATE: \"Response must be a valid date\",\n  INVALID_END_DATE: \"End date can't be before start date\",\n  NUMBER_LESS_THAN_ZERO: \"Response must be greater than or equal to zero\",\n  INVALID_NUMBER: \"Response must be a valid number\",\n  INVALID_NUMBER_OR_NA: 'Response must be a valid number or \"N/A\"',\n  INVALID_RATIO: \"Response must be a valid ratio\",\n};\n\n// TEXT - Helpers\nconst isWhitespaceString = (value?: string) => value?.trim().length === 0;\n\n// TEXT\nexport const text = () =>\n  string()\n    .typeError(error.INVALID_GENERIC)\n    .required(error.REQUIRED_GENERIC)\n    .test({\n      test: (value) => !isWhitespaceString(value),\n      message: error.REQUIRED_GENERIC,\n    });\nexport const textOptional = () => string().typeError(error.INVALID_GENERIC);\n\n// NUMBER - Helpers\nconst validNAValues = [\"N/A\", \"Data not available\"];\n// const validNumberRegex = /^\\.$|[0-9]/;\nconst validIntegerRegex = /^[0-9\\s,$%]+$/;\n\n// NUMBER - Number or Valid Strings\nexport const numberSchema = () =>\n  string()\n    .test({\n      message: error.INVALID_NUMBER_OR_NA,\n      test: (value) => {\n        if (value) {\n          const isValidStringValue = validNAValues.includes(value);\n          const isValidNumberValue =\n            checkStandardNumberInputAgainstRegexes(value);\n          return isValidStringValue || isValidNumberValue;\n        } else return true;\n      },\n    })\n    .test({\n      test: (value) => {\n        if (checkStandardNumberInputAgainstRegexes(value!)) {\n          return parseFloat(value!) >= 0;\n        } else return true;\n      },\n      message: error.NUMBER_LESS_THAN_ZERO,\n    });\n\nexport const number = () =>\n  numberSchema()\n    .required(error.REQUIRED_GENERIC)\n    .test({\n      test: (value) => !isWhitespaceString(value),\n      message: error.REQUIRED_GENERIC,\n    });\n\nexport const numberOptional = () => numberSchema().notRequired().nullable();\n\n// Integer or Valid Strings\nexport const validIntegerSchema = () =>\n  string()\n    .test({\n      message: error.INVALID_NUMBER_OR_NA,\n      test: (value) => {\n        if (value) {\n          const isValidStringValue = validNAValues.includes(value);\n          const isValidIntegerValue = validIntegerRegex.test(value);\n          return isValidStringValue || isValidIntegerValue;\n        } else return true;\n      },\n    })\n    .test({\n      test: (value) => {\n        if (checkStandardIntegerInputAgainstRegexes(value!)) {\n          return parseFloat(value!) >= 0;\n        } else return true;\n      },\n      message: error.NUMBER_LESS_THAN_ZERO,\n    });\n\nexport const validInteger = () =>\n  validIntegerSchema().required(error.REQUIRED_GENERIC);\n\nexport const validIntegerOptional = () =>\n  validIntegerSchema().notRequired().nullable();\n\n// Number - Ratio\nexport const ratio = () =>\n  mixed()\n    .test({\n      message: error.REQUIRED_GENERIC,\n      test: (val) => val != \"\",\n    })\n    .required(error.REQUIRED_GENERIC)\n    .test({\n      message: error.INVALID_RATIO,\n      test: (val) => {\n        return checkRatioInputAgainstRegexes(val).isValid;\n      },\n    });\n\n// EMAIL\nexport const email = () => text().email(error.INVALID_EMAIL);\nexport const emailOptional = () => email().notRequired();\n\n// URL\nexport const url = () => text().url(error.INVALID_URL);\nexport const urlOptional = () => url().notRequired();\n\n// DATE\nexport const date = () =>\n  string()\n    .required(error.REQUIRED_GENERIC)\n    .matches(dateFormatRegex, error.INVALID_DATE)\n    .test({\n      message: error.REQUIRED_GENERIC,\n      test: (value) => !isWhitespaceString(value),\n    });\n\nexport const dateOptional = () =>\n  string()\n    .typeError(error.INVALID_GENERIC)\n    .test({\n      message: error.INVALID_DATE,\n      test: (value) => dateFormatRegex.test(value!),\n    });\n\nexport const endDate = (startDateField: string) =>\n  date()\n    .typeError(error.INVALID_DATE)\n    .test({\n      message: error.INVALID_END_DATE,\n      test: (endDateString, context) => {\n        return isEndDateAfterStartDate(\n          context.parent[startDateField],\n          endDateString as string\n        );\n      },\n    });\n\nexport const isEndDateAfterStartDate = (\n  startDateString: string,\n  endDateString: string\n) => {\n  const startDate = new Date(startDateString);\n  const endDate = new Date(endDateString!);\n  return endDate >= startDate;\n};\n\n// DROPDOWN\nexport const dropdown = () =>\n  object({ label: text(), value: text() }).required(error.REQUIRED_GENERIC);\n\n// CHECKBOX\nexport const checkbox = () =>\n  array()\n    .min(1, error.REQUIRED_CHECKBOX)\n    .of(object({ key: text(), value: text() }))\n    .required(error.REQUIRED_CHECKBOX);\nexport const checkboxOptional = () =>\n  array().notRequired().typeError(error.INVALID_GENERIC);\nexport const checkboxSingle = () => boolean();\n\n// RADIO\nexport const radio = () =>\n  array()\n    .min(1, error.REQUIRED_GENERIC)\n    .of(object({ key: text(), value: text() }))\n    .required(error.REQUIRED_GENERIC);\nexport const radioOptional = () => radio().notRequired();\n\n// DYNAMIC\nexport const dynamic = () =>\n  array()\n    .min(1)\n    .of(\n      object().shape({\n        id: text(),\n        name: text(),\n      })\n    )\n    .required(error.REQUIRED_GENERIC);\nexport const dynamicOptional = () => dynamic().notRequired();\n\n// NESTED\nexport const nested = (\n  fieldSchema: Function,\n  parentFieldName: string,\n  parentOptionId: string\n) => {\n  const fieldTypeMap = {\n    array: array(),\n    string: string(),\n    date: date(),\n    object: object(),\n  };\n  const fieldType: keyof typeof fieldTypeMap = fieldSchema().type;\n  const baseSchema: any = fieldTypeMap[fieldType];\n  return baseSchema.when(parentFieldName, {\n    is: (value: Choice[]) =>\n      // look for parentOptionId in checked choices\n      value?.find((option: Choice) => option.key.endsWith(parentOptionId)),\n    then: () => fieldSchema(), // returns standard field schema (required)\n    otherwise: () => baseSchema, // returns not-required Yup base schema\n  });\n};\n\n// OBJECT ARRAY\nexport const objectArray = () => array().of(mixed());\n\n// REGEX\nexport const dateFormatRegex =\n  /^((0[1-9]|1[0-2])\\/(0[1-9]|1\\d|2\\d|3[01])\\/(19|20)\\d{2})|((0[1-9]|1[0-2])(0[1-9]|1\\d|2\\d|3[01])(19|20)\\d{2})$/;\n\n// SCHEMA MAP\nexport const schemaMap: any = {\n  checkbox: checkbox(),\n  checkboxOptional: checkboxOptional(),\n  checkboxSingle: checkboxSingle(),\n  date: date(),\n  dateOptional: dateOptional(),\n  dropdown: dropdown(),\n  dynamic: dynamic(),\n  dynamicOptional: dynamicOptional(),\n  email: email(),\n  emailOptional: emailOptional(),\n  number: number(),\n  numberOptional: numberOptional(),\n  objectArray: objectArray(),\n  radio: radio(),\n  radioOptional: radioOptional(),\n  ratio: ratio(),\n  text: text(),\n  textOptional: textOptional(),\n  url: url(),\n  urlOptional: urlOptional(),\n  validInteger: validInteger(),\n  validIntegerOptional: validIntegerOptional(),\n};\n", "// REGEX\n\n// basic check for all possible characters -- standard number\nconst validCharactersStandardNumberRegex = /^[0-9\\s.,$%-]+$/;\n// basic check for all possible characters -- standard number\nconst validCharactersStandardIntegerRegex = /^[0-9\\s,$%]+$/;\n// basic check for all possible characters -- ratio\nconst validCharactersRatioNumberRegex = /^[0-9.,-]+$/;\n// at most 1 decimal point\nconst atMost1DecimalPointRegex = /^[^.]*\\.?[^.]*$/;\n// commas only exist before decimal point\nconst validCommaLocationRegex = /^[0-9,$-]*\\.?[0-9%]*$/;\n// at most 1 $%\nconst atMost1SpecialCharacterRegex = /^([^$%]*\\$[^$%]*|[^$%]*%[^$%]*|[^$%]*)$/;\n// at most 1 $ at the beginning of the input\nconst validDollarSignPlacementRegex = /^[$]?[^$%]+$/;\n// at most 1 % at the end of the input\nconst validPercentSignPlacementRegex = /^[^%$]+[%]?$/;\n// at most 1 - at the beginning of the input (but after any potential $s)\nconst validNegativeSignPlacementRegex = /^[$]?[-]?[^$-]+[%]?$/;\n// exactly one ratio character in between other characters\nconst exactlyOneRatioCharacterRegex = /^[^:]+:[^:]+$/;\n\nexport const checkStandardNumberInputAgainstRegexes = (\n  value: string\n): boolean => {\n  if (\n    !validCharactersStandardNumberRegex.test(value) ||\n    !atMost1DecimalPointRegex.test(value) ||\n    !validCommaLocationRegex.test(value) ||\n    !atMost1SpecialCharacterRegex.test(value) ||\n    !(\n      validDollarSignPlacementRegex.test(value) ||\n      validPercentSignPlacementRegex.test(value)\n    ) ||\n    !validNegativeSignPlacementRegex.test(value)\n  )\n    return false;\n  return true;\n};\n\nexport const checkStandardIntegerInputAgainstRegexes = (\n  value: string\n): boolean => {\n  if (\n    !validCharactersStandardIntegerRegex.test(value) ||\n    !validCommaLocationRegex.test(value) ||\n    !atMost1SpecialCharacterRegex.test(value) ||\n    !(\n      validDollarSignPlacementRegex.test(value) ||\n      validPercentSignPlacementRegex.test(value)\n    )\n  )\n    return false;\n  return true;\n};\n\nexport const checkRatioInputAgainstRegexes = (\n  value: string\n): { isValid: boolean; leftSide: string; rightSide: string } => {\n  if (!exactlyOneRatioCharacterRegex.test(value))\n    return { isValid: false, leftSide: \"\", rightSide: \"\" };\n\n  // Grab the left and right side of the ratio sign\n  let values = value.split(\":\");\n\n  // Check left and right side for valid inputs\n  if (\n    !checkASideOfRatioAgainstRegexes(values[0]) ||\n    !checkASideOfRatioAgainstRegexes(values[1])\n  )\n    return { isValid: false, leftSide: values[0], rightSide: values[1] };\n\n  return { isValid: true, leftSide: values[0], rightSide: values[1] };\n};\n\nexport const checkASideOfRatioAgainstRegexes = (value: string): boolean => {\n  if (\n    !validCharactersRatioNumberRegex.test(value) ||\n    !atMost1DecimalPointRegex.test(value) ||\n    !validCommaLocationRegex.test(value) ||\n    !validNegativeSignPlacementRegex.test(value)\n  )\n    return false;\n  return true;\n};\n"],
./deployment/constructs/lambda.ts:132: TODO: the options calls previously included "X-Amzn-Trace-Id" in Access-Control-Allow-Headers as well
./deployment/constructs/lambda.ts:133: TODO: the options calls previously returned a 200 instead of a 204
./deployment/deployment-config.ts:27: TODO: remove jon-cdk after main is deployed
./deployment/stacks/api.ts:21: TODO: does this need to point to the tsconfig.json file in services/app-api?
./deployment/stacks/ui-auth.ts:281: TODO: do we need the size exclusion for this WAF?
./services/ui-src/src/components/layout/Timeout.tsx:33: TODO: When autosave is implemented, set up a callback function to listen to calls to update in authLifecycle
./services/ui-src/src/components/pages/Admin/AdminPage.test.tsx:112: TODO: actually toggle active status
./services/ui-src/src/components/reports/DynamicModalOverlayReportPage.tsx:50: TODO, do we need to set isCopied here, or is setting it in the API during the copy sufficient?
./services/ui-src/src/components/reports/ModalOverlayReportPage.test.tsx:76: TODO: test delete modal + functionality
./services/ui-src/src/components/reports/ReportProvider.test.tsx:200: TODO: This test passes when run by itself, but fails when the whole suite runs.
./services/ui-src/src/components/reports/ReportProvider.tsx:157: TODO: Remove casting
./services/ui-src/src/components/reports/ReportProvider.tsx:174: TODO: Remove casting
./services/ui-src/src/components/tables/Table.tsx:151: TODO: add additional styling for two-column dynamic field tables if needed
./services/ui-src/src/components/tables/getEntityStatus.test.ts:174: TODO this feels like a bug, right? Or can it never come up?
./services/ui-src/src/components/tables/getEntityStatus.test.ts:510: TODO test modalForm
./services/ui-src/src/utils/api/requestMethods/report.ts:22: TODO: Swap report from AnyObject to a ReportMetaData + FieldData type
./services/ui-src/src/utils/forms/forms.test.ts:253: TODO: Evidently it should not */
./services/ui-src/src/utils/reports/entities.test.ts:174: TODO:
./src/run.ts:301: TODO: FYI, I got this error when my internet connection was down, so we could improve the logic here.
./src/run.ts:303: TODO: FYI, I got this error when my AWS credentials were expired, so we could improve the logic here.
./tests/cypress/e2e/login.cy.js:1: TODO: make an assertion in these tests
